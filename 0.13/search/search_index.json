{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "concepts/", "text": "", "title": "Concepts"}, {"location": "getting_started/", "text": "Installation \u00b6", "title": "Getting Started"}, {"location": "getting_started/#installation", "text": "", "title": "Installation"}, {"location": "tutorials/", "text": "", "title": "Tutorials"}, {"location": "about/changelog/", "text": "Release notes \u00b6 0.13.8 -- Sep 21, 2022 \u00b6 Add - New documentation structure based on markdown PR #1052 Bugfix - Fix queries with backslashes ( #999 ) PR #1052 0.13.7 -- Jul 13, 2022 \u00b6 Bugfix - Fix networkx incompatable change by version pinning to 2.6.3 (#1035) PR #1036 Add - Support for serializing numpy datetime64 types (#1022) PR #1036 Update - Add traceback to default logging PR #1036 0.13.6 -- Jun 13, 2022 \u00b6 Add - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025 Add - Unified package level logger for package (#667) PR #1031 Update - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031 Bugfix - Fix query caching deleting non-datajoint files PR #1027 Update - Minimum Python version for Datajoint-Python is now 3.7 PR #1027 0.13.5 -- May 19, 2022 \u00b6 Update - Import ABC from collections.abc for Python 3.10 compatibility Bugfix - Fix multiprocessing value error (#1013) PR #1026 0.13.4 -- Mar, 28 2022 \u00b6 Add - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995 Bugfix - Add missing jobs argument for multiprocessing PR #997 Add - Test for multiprocessing PR #1008 Bugfix - Fix external store key name doesn't allow '-' (#1005) PR #1006 Add - Adopted black formatting into code base PR #998 0.13.3 -- Feb 9, 2022 \u00b6 Bugfix - Fix error in listing ancestors, descendants with part tables. Bugfix - Fix Python 3.10 compatibility (#983) PR #972 Bugfix - Allow renaming non-conforming attributes in proj (#982) PR #972 Add - Expose proxy feature for S3 external stores (#961) PR #962 Add - implement multiprocessing in populate (#695) PR #704, #969 Bugfix - Dependencies not properly loaded on populate. (#902) PR #919 Bugfix - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939 Bugfix - Deletes and drops must include the master of each part. (#151, #374) PR #957 Bugfix - ExternalTable.delete should not remove row on error (#953) PR #956 Bugfix - Fix error handling of remove_object function in s3.py (#952) PR #955 Bugfix - Fix regression issue with DISTINCT clause and GROUP_BY (#914) PR #963 Bugfix - Fix sql code generation to comply with sql mode ONLY_FULL_GROUP_BY (#916) PR #965 Bugfix - Fix count for left-joined QueryExpressions (#951) PR #966 Bugfix - Fix assertion error when performing a union into a join (#930) PR #967 Update ~jobs.error_stack from blob to mediumblob to allow error stacks >64kB in jobs (#984) PR #986 Bugfix - Fix error when performing a union on multiple tables (#926) PR #964 Add - Allow optional keyword arguments for make() in populate() PR #971 0.13.2 -- May 7, 2021 \u00b6 Update setuptools_certificate dependency to new name otumat Bugfix - Explicit calls to dj.Connection throw error due to missing host_input (#895) PR #907 Bugfix - Correct count of deleted items. (#897) PR #912 0.13.1 -- Apr 16, 2021 \u00b6 Add None as an alias for IS NULL comparison in dict restrictions (#824) PR #893 Drop support for MySQL 5.6 since it has reached EOL PR #893 Bugfix - schema.list_tables() is not topologically sorted (#838) PR #893 Bugfix - Diagram part tables do not show proper class name (#882) PR #893 Bugfix - Error in complex restrictions (#892) PR #893 Bugfix - WHERE and GROUP BY clases are dropped on joins with aggregation (#898, #899) PR #893 0.13.0 -- Mar 24, 2021 \u00b6 Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754 Re-implement cascading deletes for better performance. PR #839 Add support for deferred schema activation to allow for greater modularity. (#834) PR #839 Add query caching mechanism for offline development (#550) PR #839 Add table method .update1 to update a row in the table with new values (#867) PR #763, #889 Python datatypes are now enabled by default in blobs (#761). PR #859 Added permissive join and restriction operators @ and ^ (#785) PR #754 Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735 Add dj.key_hash alias to dj.hash.key_hash (#804) PR #862 Default enable_python_native_blobs to True Bugfix - Regression error on joins with same attribute name (#857) PR #878 Bugfix - Error when fetch1('KEY') when dj.config['fetch_format']='frame' set (#876) PR #880, #878 Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839 Add deprecation warning for _update . PR #889 Add purge_query_cache utility. PR #889 Add tests for query caching and permissive join and restriction. PR #889 Drop support for Python 3.5 (#829) PR #861 0.12.9 -- Mar 12, 2021 \u00b6 Fix bug with fetch1 with dj.config['fetch_format']=\"frame\" . (#876) PR #880 0.12.8 -- Jan 12, 2021 \u00b6 table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833 Load dependencies before querying dependencies. (#179) PR #833 Fix display of part tables in schema.save . (#821) PR #833 Add schema.list_tables . (#838) PR #844 Fix minio new version regression. PR #847 Add more S3 logging for debugging. (#831) PR #832 Convert testing framework from TravisCI to GitHub Actions (#841) PR #840 0.12.7 -- Oct 27, 2020 \u00b6 Fix case sensitivity issues to adapt to MySQL 8+. PR #819 Fix pymysql regression bug (#814) PR #816 Adapted attribute types now have dtype=object in all recarray results. PR #811 0.12.6 -- May 15, 2020 \u00b6 Add order_by to dj.kill (#668, #779) PR #775, #783 Add explicit S3 bucket and file storage location existence checks (#748) PR #781 Modify _update to allow nullable updates for strings/date (#664) PR #760 Avoid logging events on auxiliary tables (#737) PR #753 Add kill_quick and expand display to include host (#740) PR #741 Bugfix - pandas insert fails due to additional index field (#666) PR #776 Bugfix - delete_external_files=True does not remove from S3 (#686) PR #781 Bugfix - pandas fetch throws error when fetch_format='frame' PR #774 0.12.5 -- Feb 24, 2020 \u00b6 Rename module dj.schema into dj.schemas . dj.schema remains an alias for class dj.Schema . (#731) PR #732 dj.create_virtual_module is now called dj.VirtualModule (#731) PR #732 Bugfix - SSL KeyError on failed connection (#716) PR #725 Bugfix - Unable to run unit tests using nosetests (#723) PR #724 Bugfix - suppress_errors does not suppress loss of connection error (#720) PR #721 0.12.4 -- Jan 14, 2020 \u00b6 Support for simple scalar datatypes in blobs (#690) PR #709 Add support for the serial data type in declarations: alias for bigint unsigned auto_increment PR #713 Improve the log table to avoid primary key collisions PR #713 Improve documentation in README PR #713 0.12.3 -- Nov 22, 2019 \u00b6 Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705 Bugfix - include table definition in doc string and help (#698, #699) PR #706 Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702 0.12.2 -- Nov 11, 2019 \u00b6 Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696 Bugfix - Insert into external does not trim leading slash if defined in dj.config['stores']['<store>']['location'] (#692) PR #693 0.12.1 -- Nov 2, 2019 \u00b6 Bugfix - AttributeAdapter converts into a string (#684) PR #688 0.12.0 -- Oct 31, 2019 \u00b6 Dropped support for Python 3.4 Support secure connections with TLS (aka SSL) PR #620 Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608 Remove expression requirement to have additional attributes (#604) PR #604 Support for filepath datatype (#481) PR #603, #659 Support file attachment datatype (#480, #592, #637) PR #659 Fetch return a dict array when specifying as_dict=True for specified attributes. (#595) PR #593 Support of ellipsis in proj : query_expression.proj(.., '-movie') (#499) PR #578 Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577 Support for alter (#110) PR #573 Support for conda install datajoint via conda-forge channel (#293) dj.conn() accepts a port keyword argument (#563) PR #571 Support for UUID datatype (#562) PR #567 query_expr.fetch(\"KEY\", as_dict=False) returns results as np.recarray (#414) PR #574 dj.ERD is now called dj.Diagram (#255, #546) PR #565 dj.Diagram underlines \"distinguished\" classes (#378) PR #557 Accept alias for supported MySQL datatypes (#544) PR #545 Support for pandas in fetch (#459, #537) PR #534 Support for ordering by \"KEY\" in fetch (#541) PR #534 Add config to enable python native blobs PR #672, #676 Add secure option for external storage (#663) PR #674, #676 Add blob migration utility from DJ011 to DJ012 PR #673 Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532 Increase default display rows (#523) PR #526 Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656) Minor improvements (#538) 0.11.3 -- Jul 26, 2019 \u00b6 Fix incompatibility with pyparsing 2.4.1 (#629) PR #631 0.11.2 -- Jul 25, 2019 \u00b6 Fix #628 - incompatibility with pyparsing 2.4.1 0.11.1 -- Nov 15, 2018 \u00b6 Fix ordering of attributes in proj (#483, #516) Prohibit direct insert into auto-populated tables (#511) 0.11.0 -- Oct 25, 2018 \u00b6 Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507) Rename internal class and module names to comply with terminology in documentation (#494, #500) Full support of secondary indexes (#498, 500) ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500) Full support of unique and nullable dependencies (#254, #301, #493, #495, #500) Improve memory management in populate (#461, #486) Fix query errors and redundancies (#456, #463, #482) 0.10.1 -- Aug 28, 2018 \u00b6 Fix ERD Tooltip message (#431) Networkx 2.0 support (#443) Fix insert from query with skip_duplicates=True (#451) Sped up queries (#458) Bugfix in restriction of the form (A & B) * B (#463) Improved error messages (#466) 0.10.0 -- Jan 10, 2018 \u00b6 Deletes are more efficient (#424) ERD shows table definition on tooltip hover in Jupyter (#422) S3 external storage Garbage collection for external sorage Most operators and methods of tables can be invoked as class methods rather than instance methods (#407) The schema decorator object no longer requires locals() to specify the context Compatibility with pymysql 0.8.0+ More efficient loading of dependencies (#403) 0.9.0 -- Nov 17, 2017 \u00b6 Made graphviz installation optional Implement file-based external storage Implement union operator + Implement file-based external storage 0.8.0 -- Jul 26, 2017 \u00b6 Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io * improved the ERD graphics and features using the graphviz libraries (#207, #333) * improved password handling logic (#322, #321) * the use of the contents property to populate tables now only works in dj.Lookup classes (#310). * allow suppressing the display of size of query results through the show_tuple_count configuration option (#309) * implemented renamed foreign keys to spec (#333) * added the limit keyword argument to populate (#329) * reduced the number of displayed messages (#308) * added size_on_disk property for dj.Schema() objects (#323) * job keys are entered in the jobs table (#316, #243) * simplified the fetch and fetch1 syntax, deprecating the fetch[...] syntax (#319) * the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317) 0.5.0 (#298) -- Mar 8, 2017 \u00b6 All fetched integers are now 64-bit long and all fetched floats are double precision. Added dj.create_virtual_module 0.4.10 (#286) -- Feb 6, 2017 \u00b6 Removed Vagrant and Readthedocs support Explicit saving of configuration (issue #284) 0.4.9 (#285) -- Feb 2, 2017 \u00b6 Fixed setup.py for pip install 0.4.7 (#281) -- Jan 24, 2017 \u00b6 Fixed issues related to order of attributes in projection. 0.4.6 (#277) -- Dec 22, 2016 \u00b6 Proper handling of interruptions during populate 0.4.5 (#274) -- Dec 20, 2016 \u00b6 Populate reports how many keys remain to be populated at the start. 0.4.3 (#271) -- Dec 6, 2016 \u00b6 Fixed aggregation issues (#270) datajoint no longer attempts to connect to server at import time dropped support of view (reversed #257) more elegant handling of insufficient privileges (#268) 0.4.2 (#267) -- Dec 6, 2016 \u00b6 improved table appearance in Jupyter 0.4.1 (#266) -- Oct 28, 2016 \u00b6 bugfix for very long error messages 0.3.9 -- Sep 27, 2016 \u00b6 Added support for datatype YEAR Fixed issues with dj.U and the aggr operator (#246, #247) 0.3.8 -- Aug 2, 2016 \u00b6 added the _update method in base_relation . It allows updating values in existing tuples. bugfix in reading values of type double. Previously it was cast as float32. 0.3.7 -- Jul 31, 2016 \u00b6 added parameter ignore_extra_fields in insert insert(..., skip_duplicates=True) now relies on SELECT IGNORE . Previously it explicitly checked if tuple already exists. table previews now include blob attributes displaying the string 0.3.6 -- Jul 30, 2016 \u00b6 bugfix in schema.spawn_missing_classes . Previously, spawned part classes would not show in ERDs. dj.key now causes fetch to return as a list of dicts. Previously it was a recarray. 0.3.5 \u00b6 dj.set_password() now asks for user confirmation before changing the password. fixed issue #228 0.3.4 \u00b6 Added method the ERD.add_parts method, which adds the part tables of all tables currently in the ERD. ERD() + arg and ERD() - arg can now accept relation classes as arg. 0.3.3 \u00b6 Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example. Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources ERD() no longer text the context argument. ERD.draw() now takes an optional context argument. By default uses the caller's locals. 0.3.2. \u00b6 Fixed issue #223: insert can insert relations without fetching. ERD() now takes the context argument, which specifies in which context to look for classes. The default is taken from the argument (schema or relation). ERD.draw() no longer has the prefix argument: class names are shown as found in the context.", "title": "Changelog"}, {"location": "about/changelog/#release-notes", "text": "", "title": "Release notes"}, {"location": "about/changelog/#0138-sep-21-2022", "text": "Add - New documentation structure based on markdown PR #1052 Bugfix - Fix queries with backslashes ( #999 ) PR #1052", "title": "0.13.8 -- Sep 21, 2022"}, {"location": "about/changelog/#0137-jul-13-2022", "text": "Bugfix - Fix networkx incompatable change by version pinning to 2.6.3 (#1035) PR #1036 Add - Support for serializing numpy datetime64 types (#1022) PR #1036 Update - Add traceback to default logging PR #1036", "title": "0.13.7 -- Jul 13, 2022"}, {"location": "about/changelog/#0136-jun-13-2022", "text": "Add - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025 Add - Unified package level logger for package (#667) PR #1031 Update - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031 Bugfix - Fix query caching deleting non-datajoint files PR #1027 Update - Minimum Python version for Datajoint-Python is now 3.7 PR #1027", "title": "0.13.6 -- Jun 13, 2022"}, {"location": "about/changelog/#0135-may-19-2022", "text": "Update - Import ABC from collections.abc for Python 3.10 compatibility Bugfix - Fix multiprocessing value error (#1013) PR #1026", "title": "0.13.5 -- May 19, 2022"}, {"location": "about/changelog/#0134-mar-28-2022", "text": "Add - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995 Bugfix - Add missing jobs argument for multiprocessing PR #997 Add - Test for multiprocessing PR #1008 Bugfix - Fix external store key name doesn't allow '-' (#1005) PR #1006 Add - Adopted black formatting into code base PR #998", "title": "0.13.4 -- Mar, 28 2022"}, {"location": "about/changelog/#0133-feb-9-2022", "text": "Bugfix - Fix error in listing ancestors, descendants with part tables. Bugfix - Fix Python 3.10 compatibility (#983) PR #972 Bugfix - Allow renaming non-conforming attributes in proj (#982) PR #972 Add - Expose proxy feature for S3 external stores (#961) PR #962 Add - implement multiprocessing in populate (#695) PR #704, #969 Bugfix - Dependencies not properly loaded on populate. (#902) PR #919 Bugfix - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939 Bugfix - Deletes and drops must include the master of each part. (#151, #374) PR #957 Bugfix - ExternalTable.delete should not remove row on error (#953) PR #956 Bugfix - Fix error handling of remove_object function in s3.py (#952) PR #955 Bugfix - Fix regression issue with DISTINCT clause and GROUP_BY (#914) PR #963 Bugfix - Fix sql code generation to comply with sql mode ONLY_FULL_GROUP_BY (#916) PR #965 Bugfix - Fix count for left-joined QueryExpressions (#951) PR #966 Bugfix - Fix assertion error when performing a union into a join (#930) PR #967 Update ~jobs.error_stack from blob to mediumblob to allow error stacks >64kB in jobs (#984) PR #986 Bugfix - Fix error when performing a union on multiple tables (#926) PR #964 Add - Allow optional keyword arguments for make() in populate() PR #971", "title": "0.13.3 -- Feb 9, 2022"}, {"location": "about/changelog/#0132-may-7-2021", "text": "Update setuptools_certificate dependency to new name otumat Bugfix - Explicit calls to dj.Connection throw error due to missing host_input (#895) PR #907 Bugfix - Correct count of deleted items. (#897) PR #912", "title": "0.13.2 -- May 7, 2021"}, {"location": "about/changelog/#0131-apr-16-2021", "text": "Add None as an alias for IS NULL comparison in dict restrictions (#824) PR #893 Drop support for MySQL 5.6 since it has reached EOL PR #893 Bugfix - schema.list_tables() is not topologically sorted (#838) PR #893 Bugfix - Diagram part tables do not show proper class name (#882) PR #893 Bugfix - Error in complex restrictions (#892) PR #893 Bugfix - WHERE and GROUP BY clases are dropped on joins with aggregation (#898, #899) PR #893", "title": "0.13.1 -- Apr 16, 2021"}, {"location": "about/changelog/#0130-mar-24-2021", "text": "Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754 Re-implement cascading deletes for better performance. PR #839 Add support for deferred schema activation to allow for greater modularity. (#834) PR #839 Add query caching mechanism for offline development (#550) PR #839 Add table method .update1 to update a row in the table with new values (#867) PR #763, #889 Python datatypes are now enabled by default in blobs (#761). PR #859 Added permissive join and restriction operators @ and ^ (#785) PR #754 Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735 Add dj.key_hash alias to dj.hash.key_hash (#804) PR #862 Default enable_python_native_blobs to True Bugfix - Regression error on joins with same attribute name (#857) PR #878 Bugfix - Error when fetch1('KEY') when dj.config['fetch_format']='frame' set (#876) PR #880, #878 Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839 Add deprecation warning for _update . PR #889 Add purge_query_cache utility. PR #889 Add tests for query caching and permissive join and restriction. PR #889 Drop support for Python 3.5 (#829) PR #861", "title": "0.13.0 -- Mar 24, 2021"}, {"location": "about/changelog/#0129-mar-12-2021", "text": "Fix bug with fetch1 with dj.config['fetch_format']=\"frame\" . (#876) PR #880", "title": "0.12.9 -- Mar 12, 2021"}, {"location": "about/changelog/#0128-jan-12-2021", "text": "table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833 Load dependencies before querying dependencies. (#179) PR #833 Fix display of part tables in schema.save . (#821) PR #833 Add schema.list_tables . (#838) PR #844 Fix minio new version regression. PR #847 Add more S3 logging for debugging. (#831) PR #832 Convert testing framework from TravisCI to GitHub Actions (#841) PR #840", "title": "0.12.8 -- Jan 12, 2021"}, {"location": "about/changelog/#0127-oct-27-2020", "text": "Fix case sensitivity issues to adapt to MySQL 8+. PR #819 Fix pymysql regression bug (#814) PR #816 Adapted attribute types now have dtype=object in all recarray results. PR #811", "title": "0.12.7 -- Oct 27, 2020"}, {"location": "about/changelog/#0126-may-15-2020", "text": "Add order_by to dj.kill (#668, #779) PR #775, #783 Add explicit S3 bucket and file storage location existence checks (#748) PR #781 Modify _update to allow nullable updates for strings/date (#664) PR #760 Avoid logging events on auxiliary tables (#737) PR #753 Add kill_quick and expand display to include host (#740) PR #741 Bugfix - pandas insert fails due to additional index field (#666) PR #776 Bugfix - delete_external_files=True does not remove from S3 (#686) PR #781 Bugfix - pandas fetch throws error when fetch_format='frame' PR #774", "title": "0.12.6 -- May 15, 2020"}, {"location": "about/changelog/#0125-feb-24-2020", "text": "Rename module dj.schema into dj.schemas . dj.schema remains an alias for class dj.Schema . (#731) PR #732 dj.create_virtual_module is now called dj.VirtualModule (#731) PR #732 Bugfix - SSL KeyError on failed connection (#716) PR #725 Bugfix - Unable to run unit tests using nosetests (#723) PR #724 Bugfix - suppress_errors does not suppress loss of connection error (#720) PR #721", "title": "0.12.5 -- Feb 24, 2020"}, {"location": "about/changelog/#0124-jan-14-2020", "text": "Support for simple scalar datatypes in blobs (#690) PR #709 Add support for the serial data type in declarations: alias for bigint unsigned auto_increment PR #713 Improve the log table to avoid primary key collisions PR #713 Improve documentation in README PR #713", "title": "0.12.4 -- Jan 14, 2020"}, {"location": "about/changelog/#0123-nov-22-2019", "text": "Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705 Bugfix - include table definition in doc string and help (#698, #699) PR #706 Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702", "title": "0.12.3 -- Nov 22, 2019"}, {"location": "about/changelog/#0122-nov-11-2019", "text": "Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696 Bugfix - Insert into external does not trim leading slash if defined in dj.config['stores']['<store>']['location'] (#692) PR #693", "title": "0.12.2 -- Nov 11, 2019"}, {"location": "about/changelog/#0121-nov-2-2019", "text": "Bugfix - AttributeAdapter converts into a string (#684) PR #688", "title": "0.12.1 -- Nov 2, 2019"}, {"location": "about/changelog/#0120-oct-31-2019", "text": "Dropped support for Python 3.4 Support secure connections with TLS (aka SSL) PR #620 Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608 Remove expression requirement to have additional attributes (#604) PR #604 Support for filepath datatype (#481) PR #603, #659 Support file attachment datatype (#480, #592, #637) PR #659 Fetch return a dict array when specifying as_dict=True for specified attributes. (#595) PR #593 Support of ellipsis in proj : query_expression.proj(.., '-movie') (#499) PR #578 Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577 Support for alter (#110) PR #573 Support for conda install datajoint via conda-forge channel (#293) dj.conn() accepts a port keyword argument (#563) PR #571 Support for UUID datatype (#562) PR #567 query_expr.fetch(\"KEY\", as_dict=False) returns results as np.recarray (#414) PR #574 dj.ERD is now called dj.Diagram (#255, #546) PR #565 dj.Diagram underlines \"distinguished\" classes (#378) PR #557 Accept alias for supported MySQL datatypes (#544) PR #545 Support for pandas in fetch (#459, #537) PR #534 Support for ordering by \"KEY\" in fetch (#541) PR #534 Add config to enable python native blobs PR #672, #676 Add secure option for external storage (#663) PR #674, #676 Add blob migration utility from DJ011 to DJ012 PR #673 Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532 Increase default display rows (#523) PR #526 Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656) Minor improvements (#538)", "title": "0.12.0 -- Oct 31, 2019"}, {"location": "about/changelog/#0113-jul-26-2019", "text": "Fix incompatibility with pyparsing 2.4.1 (#629) PR #631", "title": "0.11.3 -- Jul 26, 2019"}, {"location": "about/changelog/#0112-jul-25-2019", "text": "Fix #628 - incompatibility with pyparsing 2.4.1", "title": "0.11.2 -- Jul 25, 2019"}, {"location": "about/changelog/#0111-nov-15-2018", "text": "Fix ordering of attributes in proj (#483, #516) Prohibit direct insert into auto-populated tables (#511)", "title": "0.11.1 -- Nov 15, 2018"}, {"location": "about/changelog/#0110-oct-25-2018", "text": "Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507) Rename internal class and module names to comply with terminology in documentation (#494, #500) Full support of secondary indexes (#498, 500) ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500) Full support of unique and nullable dependencies (#254, #301, #493, #495, #500) Improve memory management in populate (#461, #486) Fix query errors and redundancies (#456, #463, #482)", "title": "0.11.0 -- Oct 25, 2018"}, {"location": "about/changelog/#0101-aug-28-2018", "text": "Fix ERD Tooltip message (#431) Networkx 2.0 support (#443) Fix insert from query with skip_duplicates=True (#451) Sped up queries (#458) Bugfix in restriction of the form (A & B) * B (#463) Improved error messages (#466)", "title": "0.10.1  -- Aug 28, 2018"}, {"location": "about/changelog/#0100-jan-10-2018", "text": "Deletes are more efficient (#424) ERD shows table definition on tooltip hover in Jupyter (#422) S3 external storage Garbage collection for external sorage Most operators and methods of tables can be invoked as class methods rather than instance methods (#407) The schema decorator object no longer requires locals() to specify the context Compatibility with pymysql 0.8.0+ More efficient loading of dependencies (#403)", "title": "0.10.0 -- Jan 10, 2018"}, {"location": "about/changelog/#090-nov-17-2017", "text": "Made graphviz installation optional Implement file-based external storage Implement union operator + Implement file-based external storage", "title": "0.9.0 -- Nov 17, 2017"}, {"location": "about/changelog/#080-jul-26-2017", "text": "Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io * improved the ERD graphics and features using the graphviz libraries (#207, #333) * improved password handling logic (#322, #321) * the use of the contents property to populate tables now only works in dj.Lookup classes (#310). * allow suppressing the display of size of query results through the show_tuple_count configuration option (#309) * implemented renamed foreign keys to spec (#333) * added the limit keyword argument to populate (#329) * reduced the number of displayed messages (#308) * added size_on_disk property for dj.Schema() objects (#323) * job keys are entered in the jobs table (#316, #243) * simplified the fetch and fetch1 syntax, deprecating the fetch[...] syntax (#319) * the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317)", "title": "0.8.0 -- Jul 26, 2017"}, {"location": "about/changelog/#050-298-mar-8-2017", "text": "All fetched integers are now 64-bit long and all fetched floats are double precision. Added dj.create_virtual_module", "title": "0.5.0 (#298) -- Mar 8, 2017"}, {"location": "about/changelog/#0410-286-feb-6-2017", "text": "Removed Vagrant and Readthedocs support Explicit saving of configuration (issue #284)", "title": "0.4.10 (#286) -- Feb 6, 2017"}, {"location": "about/changelog/#049-285-feb-2-2017", "text": "Fixed setup.py for pip install", "title": "0.4.9 (#285) -- Feb 2, 2017"}, {"location": "about/changelog/#047-281-jan-24-2017", "text": "Fixed issues related to order of attributes in projection.", "title": "0.4.7 (#281) -- Jan 24, 2017"}, {"location": "about/changelog/#046-277-dec-22-2016", "text": "Proper handling of interruptions during populate", "title": "0.4.6 (#277) -- Dec 22, 2016"}, {"location": "about/changelog/#045-274-dec-20-2016", "text": "Populate reports how many keys remain to be populated at the start.", "title": "0.4.5 (#274) -- Dec 20, 2016"}, {"location": "about/changelog/#043-271-dec-6-2016", "text": "Fixed aggregation issues (#270) datajoint no longer attempts to connect to server at import time dropped support of view (reversed #257) more elegant handling of insufficient privileges (#268)", "title": "0.4.3  (#271) -- Dec 6, 2016"}, {"location": "about/changelog/#042-267-dec-6-2016", "text": "improved table appearance in Jupyter", "title": "0.4.2 (#267)  -- Dec 6, 2016"}, {"location": "about/changelog/#041-266-oct-28-2016", "text": "bugfix for very long error messages", "title": "0.4.1 (#266) -- Oct 28, 2016"}, {"location": "about/changelog/#039-sep-27-2016", "text": "Added support for datatype YEAR Fixed issues with dj.U and the aggr operator (#246, #247)", "title": "0.3.9 -- Sep 27, 2016"}, {"location": "about/changelog/#038-aug-2-2016", "text": "added the _update method in base_relation . It allows updating values in existing tuples. bugfix in reading values of type double. Previously it was cast as float32.", "title": "0.3.8  -- Aug 2, 2016"}, {"location": "about/changelog/#037-jul-31-2016", "text": "added parameter ignore_extra_fields in insert insert(..., skip_duplicates=True) now relies on SELECT IGNORE . Previously it explicitly checked if tuple already exists. table previews now include blob attributes displaying the string", "title": "0.3.7  -- Jul 31, 2016"}, {"location": "about/changelog/#036-jul-30-2016", "text": "bugfix in schema.spawn_missing_classes . Previously, spawned part classes would not show in ERDs. dj.key now causes fetch to return as a list of dicts. Previously it was a recarray.", "title": "0.3.6  -- Jul 30, 2016"}, {"location": "about/changelog/#035", "text": "dj.set_password() now asks for user confirmation before changing the password. fixed issue #228", "title": "0.3.5"}, {"location": "about/changelog/#034", "text": "Added method the ERD.add_parts method, which adds the part tables of all tables currently in the ERD. ERD() + arg and ERD() - arg can now accept relation classes as arg.", "title": "0.3.4"}, {"location": "about/changelog/#033", "text": "Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example. Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources ERD() no longer text the context argument. ERD.draw() now takes an optional context argument. By default uses the caller's locals.", "title": "0.3.3"}, {"location": "about/changelog/#032", "text": "Fixed issue #223: insert can insert relations without fetching. ERD() now takes the context argument, which specifies in which context to look for classes. The default is taken from the argument (schema or relation). ERD.draw() no longer has the prefix argument: class names are shown as found in the context.", "title": "0.3.2."}, {"location": "api/datajoint/__init__/", "text": "DataJoint for Python is a framework for building data piplines using MySQL databases to represent pipeline structure and bulk storage systems for large objects. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, and querying data. The DataJoint data model is described in https://arxiv.org/abs/1807.11104 DataJoint is free software under the LGPL License. In addition, we request that any use of DataJoint leading to a publication be acknowledged in the publication. Please cite http://biorxiv.org/content/early/2015/11/14/031658 http://dx.doi.org/10.1101/031658 AndList \u00b6 Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction ) AttributeAdapter \u00b6 Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) attribute_type () property \u00b6 :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" Source code in datajoint/attribute_adapter.py 11 12 13 14 15 16 @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) get ( value ) \u00b6 convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) put ( obj ) \u00b6 convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) Computed \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\" Connection \u00b6 A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () cancel_transaction () \u00b6 Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) commit_transaction () \u00b6 Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) connect () \u00b6 Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) get_user () \u00b6 :return: the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] in_transaction () property \u00b6 :return: True if there is an open transaction. Source code in datajoint/connection.py 369 370 371 372 373 374 375 @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction is_connected () property \u00b6 Return true if the object is connected to the database server. Source code in datajoint/connection.py 278 279 280 281 282 283 284 285 @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True ping () \u00b6 Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) purge_query_cache () \u00b6 Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () query ( query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ) \u00b6 Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor set_query_cache ( query_cache = None ) \u00b6 When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache start_transaction () \u00b6 Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) transaction () property \u00b6 Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here Source code in datajoint/connection.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () DataJointError \u00b6 Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) suggest ( * args ) \u00b6 regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) Diagram \u00b6 Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = ( cls () . describe ( context = self . context , printout = False ) . split ( \" \\n \" ) ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs ) __add__ ( arg ) \u00b6 :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. Source code in datajoint/diagram.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self __mul__ ( arg ) \u00b6 Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. Source code in datajoint/diagram.py 250 251 252 253 254 255 256 257 258 def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self __sub__ ( arg ) \u00b6 :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. Source code in datajoint/diagram.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self add_parts () \u00b6 Adds to the diagram the part tables of tables already included in the diagram :return: Source code in datajoint/diagram.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self from_sequence ( sequence ) classmethod \u00b6 The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 146 147 148 149 150 151 152 153 154 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) topological_sort () \u00b6 :return: list of nodes in topological order Source code in datajoint/diagram.py 183 184 185 186 187 188 189 190 191 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) FreeTable \u00b6 Bases: Table A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format database . table_name Source code in datajoint/table.py 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 class FreeTable ( Table ): \"\"\" A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () ) Imported \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\" Lookup \u00b6 Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" ) Manual \u00b6 Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\" MatCell \u00b6 Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass MatStruct \u00b6 Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass Not \u00b6 invert restriction Source code in datajoint/condition.py 43 44 45 46 47 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction Part \u00b6 Bases: UserTable Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) delete ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) drop ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) Schema \u00b6 A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared : if not self . create_tables or assert_declared : raise DataJointError ( \"Table ` %s ` not declared\" % instance . table_name ) instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] __call__ ( cls , * , context = None ) \u00b6 Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes Source code in datajoint/schemas.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls __init__ ( schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) activate ( schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) drop ( force = False ) \u00b6 Drop the associated schema if it exists Source code in datajoint/schemas.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) exists () property \u00b6 :return: true if the associated schema exists on the server Source code in datajoint/schemas.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) jobs () property \u00b6 schema.jobs provides a view of the job reservation table for the schema :return: jobs table Source code in datajoint/schemas.py 394 395 396 397 398 399 400 401 402 403 404 @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs list_tables () \u00b6 Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. Source code in datajoint/schemas.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] save ( python_filename = None ) \u00b6 Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) size_on_disk () property \u00b6 :return: size of the entire schema in bytes Source code in datajoint/schemas.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) spawn_missing_classes ( context = None ) \u00b6 Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() Source code in datajoint/schemas.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) Table \u00b6 Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition def _update ( self , attrname , value = None ): \"\"\" This is a deprecated function to be removed in datajoint 0.14. Use ``.update1`` instead. Updates a field in one existing tuple. self must be restricted to exactly one entry. In DataJoint the principal way of updating data is to delete and re-insert the entire record and updates are reserved for corrective actions. This is because referential integrity is observed on the level of entire records rather than individual attributes. Safety constraints: 1. self must be restricted to exactly one tuple 2. the update attribute must not be in primary key Example: >>> (v2p.Mice() & key)._update('mouse_dob', '2011-01-01') >>> (v2p.Mice() & key)._update( 'lens') # set the value to NULL \"\"\" logger . warning ( \"`_update` is a deprecated function to be removed in datajoint 0.14. \" \"Use `.update1` instead.\" ) if len ( self ) != 1 : raise DataJointError ( \"Update is only allowed on one tuple at a time\" ) if attrname not in self . heading : raise DataJointError ( \"Invalid attribute name\" ) if attrname in self . heading . primary_key : raise DataJointError ( \"Cannot update a key value.\" ) attr = self . heading [ attrname ] if attr . is_blob : value = blob . pack ( value ) placeholder = \" %s \" elif attr . numeric : if value is None or np . isnan ( float ( value )): # nans are turned into NULLs placeholder = \"NULL\" value = None else : placeholder = \" %s \" value = str ( int ( value ) if isinstance ( value , bool ) else value ) else : placeholder = \" %s \" if value is not None else \"NULL\" command = \"UPDATE {full_table_name} SET ` {attrname} `= {placeholder} {where_clause} \" . format ( full_table_name = self . from_clause (), attrname = attrname , placeholder = placeholder , where_clause = self . where_clause (), ) self . connection . query ( command , args = ( value ,) if value is not None else ()) # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert __make_placeholder ( name , value , ignore_extra_fields = False ) \u00b6 For a given attribute name with value , return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted Source code in datajoint/table.py 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value __make_row_to_insert ( row , field_list , ignore_extra_fields ) \u00b6 Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' Source code in datajoint/table.py 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert alter ( prompt = True , context = None ) \u00b6 Alter the table definition from self.definition Source code in datajoint/table.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) ancestors ( as_objects = False ) \u00b6 :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. Source code in datajoint/table.py 216 217 218 219 220 221 222 223 224 225 226 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] children ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes declare ( context = None ) \u00b6 Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. Source code in datajoint/table.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) delete ( transaction = True , safemode = None , force_parts = False ) \u00b6 Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) Source code in datajoint/table.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count delete_quick ( get_count = False ) \u00b6 Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count descendants ( as_objects = False ) \u00b6 :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. Source code in datajoint/table.py 204 205 206 207 208 209 210 211 212 213 214 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] describe ( context = None , printout = True ) \u00b6 :return: the definition string for the relation using DataJoint DDL. Source code in datajoint/table.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition drop () \u00b6 Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) drop_quick () \u00b6 Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. Source code in datajoint/table.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) from_clause () \u00b6 :return: the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 146 147 148 149 150 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name full_table_name () property \u00b6 :return: full table name in the schema Source code in datajoint/table.py 255 256 257 258 259 260 @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) get_select_fields ( select_fields = None ) \u00b6 :return: the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 152 153 154 155 156 157 158 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) insert ( rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None ) \u00b6 Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example relation.insert([ dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) Source code in datajoint/table.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) insert1 ( row , ** kwargs ) \u00b6 Insert one data record into the table. For kwargs , see insert() . :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. Source code in datajoint/table.py 327 328 329 330 331 332 333 334 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) is_declared () property \u00b6 :return: True is the table is declared in the schema. Source code in datajoint/table.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) parents ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes parts ( as_objects = False ) \u00b6 return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. Source code in datajoint/table.py 228 229 230 231 232 233 234 235 236 237 238 239 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes size_on_disk () property \u00b6 :return: size of data and indices in bytes on the storage device Source code in datajoint/table.py 649 650 651 652 653 654 655 656 657 658 659 660 @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] update1 ( row ) \u00b6 update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: table.update1({'id': 1, 'value': 3}) # update value in record with id=1 table.update1({'id': 1, 'value': None}) # reset value to default Source code in datajoint/table.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) U \u00b6 dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr __mul__ ( other ) \u00b6 shorthand for join Source code in datajoint/expression.py 908 909 910 def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) aggr ( group , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) join ( other , left = False ) \u00b6 Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result VirtualModule \u00b6 Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) __init__ ( module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ) \u00b6 Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes Source code in datajoint/schemas.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) key \u00b6 object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 18 19 20 21 22 23 24 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ) \u00b6 Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection key_hash ( mapping ) \u00b6 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest () kill ( restriction = None , connection = None , order_by = None ) \u00b6 view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes Source code in datajoint/admin.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : print ( \"Process not found\" ) list_schemas ( connection = None ) \u00b6 :param connection: a dj.Connection object :return: list of all accessible schemas on the server Source code in datajoint/schemas.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ] migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ) \u00b6 Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated Source code in datajoint/migrate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ): \"\"\" Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated \"\"\" if not isinstance ( migration_schema , str ): raise ValueError ( \"Expected type {} for migration_schema, not {} .\" . format ( str , type ( migration_schema ) ) ) do_migration = False do_migration = ( user_choice ( \"\"\" Warning: Ensure the following are completed before proceeding. - Appropriate backups have been taken, - Any existing DJ 0.11.X connections are suspended, and - External config has been updated to new dj.config['stores'] structure. Proceed? \"\"\" , default = \"no\" , ) == \"yes\" ) if do_migration : _migrate_dj011_blob ( dj . Schema ( migration_schema ), store ) print ( \"Migration completed for schema: {} , store: {} .\" . format ( migration_schema , store ) ) return print ( \"No migration performed.\" )", "title": "__init__.py"}, {"location": "api/datajoint/__init__/#datajoint.AndList", "text": "Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction )", "title": "AndList"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter", "text": "Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "AttributeAdapter"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "text": ":return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" Source code in datajoint/attribute_adapter.py 11 12 13 14 15 16 @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "attribute_type()"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.get", "text": "convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "get()"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.put", "text": "convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "put()"}, {"location": "api/datajoint/__init__/#datajoint.Computed", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\"", "title": "Computed"}, {"location": "api/datajoint/__init__/#datajoint.Connection", "text": "A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "Connection"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.cancel_transaction", "text": "Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" )", "title": "cancel_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.commit_transaction", "text": "Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" )", "title": "commit_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.connect", "text": "Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True )", "title": "connect()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.get_user", "text": ":return: the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ]", "title": "get_user()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.in_transaction", "text": ":return: True if there is an open transaction. Source code in datajoint/connection.py 369 370 371 372 373 374 375 @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction", "title": "in_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.is_connected", "text": "Return true if the object is connected to the database server. Source code in datajoint/connection.py 278 279 280 281 282 283 284 285 @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True", "title": "is_connected()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.ping", "text": "Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False )", "title": "ping()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.purge_query_cache", "text": "Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink ()", "title": "purge_query_cache()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.query", "text": "Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor", "title": "query()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.set_query_cache", "text": "When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache", "title": "set_query_cache()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.start_transaction", "text": "Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" )", "title": "start_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.transaction", "text": "Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here Source code in datajoint/connection.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "transaction()"}, {"location": "api/datajoint/__init__/#datajoint.DataJointError", "text": "Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "DataJointError"}, {"location": "api/datajoint/__init__/#datajoint.errors.DataJointError.suggest", "text": "regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "suggest()"}, {"location": "api/datajoint/__init__/#datajoint.Diagram", "text": "Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = ( cls () . describe ( context = self . context , printout = False ) . split ( \" \\n \" ) ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs )", "title": "Diagram"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.__add__", "text": ":param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. Source code in datajoint/diagram.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self", "title": "__add__()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.__mul__", "text": "Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. Source code in datajoint/diagram.py 250 251 252 253 254 255 256 257 258 def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self", "title": "__mul__()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.__sub__", "text": ":param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. Source code in datajoint/diagram.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self", "title": "__sub__()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.add_parts", "text": "Adds to the diagram the part tables of tables already included in the diagram :return: Source code in datajoint/diagram.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self", "title": "add_parts()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.from_sequence", "text": "The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 146 147 148 149 150 151 152 153 154 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence ))", "title": "from_sequence()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.topological_sort", "text": ":return: list of nodes in topological order Source code in datajoint/diagram.py 183 184 185 186 187 188 189 190 191 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "topological_sort()"}, {"location": "api/datajoint/__init__/#datajoint.FreeTable", "text": "Bases: Table A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format database . table_name Source code in datajoint/table.py 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 class FreeTable ( Table ): \"\"\" A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () )", "title": "FreeTable"}, {"location": "api/datajoint/__init__/#datajoint.Imported", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\"", "title": "Imported"}, {"location": "api/datajoint/__init__/#datajoint.Lookup", "text": "Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" )", "title": "Lookup"}, {"location": "api/datajoint/__init__/#datajoint.Manual", "text": "Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\"", "title": "Manual"}, {"location": "api/datajoint/__init__/#datajoint.MatCell", "text": "Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass", "title": "MatCell"}, {"location": "api/datajoint/__init__/#datajoint.MatStruct", "text": "Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass", "title": "MatStruct"}, {"location": "api/datajoint/__init__/#datajoint.Not", "text": "invert restriction Source code in datajoint/condition.py 43 44 45 46 47 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction", "title": "Not"}, {"location": "api/datajoint/__init__/#datajoint.Part", "text": "Bases: UserTable Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "Part"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.delete", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" )", "title": "delete()"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.drop", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.Schema", "text": "A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared : if not self . create_tables or assert_declared : raise DataJointError ( \"Table ` %s ` not declared\" % instance . table_name ) instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "Schema"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.__call__", "text": "Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes Source code in datajoint/schemas.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls", "title": "__call__()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.__init__", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name )", "title": "__init__()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.activate", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context )", "title": "activate()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.drop", "text": "Drop the associated schema if it exists Source code in datajoint/schemas.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.exists", "text": ":return: true if the associated schema exists on the server Source code in datajoint/schemas.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount )", "title": "exists()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.jobs", "text": "schema.jobs provides a view of the job reservation table for the schema :return: jobs table Source code in datajoint/schemas.py 394 395 396 397 398 399 400 401 402 403 404 @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs", "title": "jobs()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.list_tables", "text": "Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. Source code in datajoint/schemas.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "list_tables()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.save", "text": "Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code )", "title": "save()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.size_on_disk", "text": ":return: size of the entire schema in bytes Source code in datajoint/schemas.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] )", "title": "size_on_disk()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.spawn_missing_classes", "text": "Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() Source code in datajoint/schemas.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class )", "title": "spawn_missing_classes()"}, {"location": "api/datajoint/__init__/#datajoint.Table", "text": "Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition def _update ( self , attrname , value = None ): \"\"\" This is a deprecated function to be removed in datajoint 0.14. Use ``.update1`` instead. Updates a field in one existing tuple. self must be restricted to exactly one entry. In DataJoint the principal way of updating data is to delete and re-insert the entire record and updates are reserved for corrective actions. This is because referential integrity is observed on the level of entire records rather than individual attributes. Safety constraints: 1. self must be restricted to exactly one tuple 2. the update attribute must not be in primary key Example: >>> (v2p.Mice() & key)._update('mouse_dob', '2011-01-01') >>> (v2p.Mice() & key)._update( 'lens') # set the value to NULL \"\"\" logger . warning ( \"`_update` is a deprecated function to be removed in datajoint 0.14. \" \"Use `.update1` instead.\" ) if len ( self ) != 1 : raise DataJointError ( \"Update is only allowed on one tuple at a time\" ) if attrname not in self . heading : raise DataJointError ( \"Invalid attribute name\" ) if attrname in self . heading . primary_key : raise DataJointError ( \"Cannot update a key value.\" ) attr = self . heading [ attrname ] if attr . is_blob : value = blob . pack ( value ) placeholder = \" %s \" elif attr . numeric : if value is None or np . isnan ( float ( value )): # nans are turned into NULLs placeholder = \"NULL\" value = None else : placeholder = \" %s \" value = str ( int ( value ) if isinstance ( value , bool ) else value ) else : placeholder = \" %s \" if value is not None else \"NULL\" command = \"UPDATE {full_table_name} SET ` {attrname} `= {placeholder} {where_clause} \" . format ( full_table_name = self . from_clause (), attrname = attrname , placeholder = placeholder , where_clause = self . where_clause (), ) self . connection . query ( command , args = ( value ,) if value is not None else ()) # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "Table"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.__make_placeholder", "text": "For a given attribute name with value , return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted Source code in datajoint/table.py 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value", "title": "__make_placeholder()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.__make_row_to_insert", "text": "Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' Source code in datajoint/table.py 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "__make_row_to_insert()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.alter", "text": "Alter the table definition from self.definition Source code in datajoint/table.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name )", "title": "alter()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.ancestors", "text": ":param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. Source code in datajoint/table.py 216 217 218 219 220 221 222 223 224 225 226 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ]", "title": "ancestors()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.children", "text": ":param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "children()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.declare", "text": "Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. Source code in datajoint/table.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name )", "title": "declare()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete", "text": "Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) Source code in datajoint/table.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count", "title": "delete()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete_quick", "text": "Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count", "title": "delete_quick()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.descendants", "text": ":param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. Source code in datajoint/table.py 204 205 206 207 208 209 210 211 212 213 214 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ]", "title": "descendants()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.describe", "text": ":return: the definition string for the relation using DataJoint DDL. Source code in datajoint/table.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition", "title": "describe()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop", "text": "Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop_quick", "text": "Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. Source code in datajoint/table.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name )", "title": "drop_quick()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.from_clause", "text": ":return: the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 146 147 148 149 150 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name", "title": "from_clause()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.full_table_name", "text": ":return: full table name in the schema Source code in datajoint/table.py 255 256 257 258 259 260 @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name )", "title": "full_table_name()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.get_select_fields", "text": ":return: the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 152 153 154 155 156 157 158 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql )", "title": "get_select_fields()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert", "text": "Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example relation.insert([ dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) Source code in datajoint/table.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" )", "title": "insert()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert1", "text": "Insert one data record into the table. For kwargs , see insert() . :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. Source code in datajoint/table.py 327 328 329 330 331 332 333 334 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs )", "title": "insert1()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.is_declared", "text": ":return: True is the table is declared in the schema. Source code in datajoint/table.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 )", "title": "is_declared()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parents", "text": ":param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "parents()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parts", "text": "return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. Source code in datajoint/table.py 228 229 230 231 232 233 234 235 236 237 238 239 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes", "title": "parts()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.size_on_disk", "text": ":return: size of data and indices in bytes on the storage device Source code in datajoint/table.py 649 650 651 652 653 654 655 656 657 658 659 660 @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ]", "title": "size_on_disk()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.update1", "text": "update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: table.update1({'id': 1, 'value': 3}) # update value in record with id=1 table.update1({'id': 1, 'value': None}) # reset value to default Source code in datajoint/table.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None ))", "title": "update1()"}, {"location": "api/datajoint/__init__/#datajoint.U", "text": "dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr", "title": "U"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.__mul__", "text": "shorthand for join Source code in datajoint/expression.py 908 909 910 def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other )", "title": "__mul__()"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.join", "text": "Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result", "title": "join()"}, {"location": "api/datajoint/__init__/#datajoint.VirtualModule", "text": "Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "VirtualModule"}, {"location": "api/datajoint/__init__/#datajoint.schemas.VirtualModule.__init__", "text": "Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes Source code in datajoint/schemas.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "__init__()"}, {"location": "api/datajoint/__init__/#datajoint.key", "text": "object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 18 19 20 21 22 23 24 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass", "title": "key"}, {"location": "api/datajoint/__init__/#datajoint.conn", "text": "Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection", "title": "conn()"}, {"location": "api/datajoint/__init__/#datajoint.key_hash", "text": "32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest ()", "title": "key_hash()"}, {"location": "api/datajoint/__init__/#datajoint.kill", "text": "view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes Source code in datajoint/admin.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : print ( \"Process not found\" )", "title": "kill()"}, {"location": "api/datajoint/__init__/#datajoint.list_schemas", "text": ":param connection: a dj.Connection object :return: list of all accessible schemas on the server Source code in datajoint/schemas.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ]", "title": "list_schemas()"}, {"location": "api/datajoint/__init__/#datajoint.migrate_dj011_external_blob_storage_to_dj012", "text": "Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated Source code in datajoint/migrate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ): \"\"\" Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated \"\"\" if not isinstance ( migration_schema , str ): raise ValueError ( \"Expected type {} for migration_schema, not {} .\" . format ( str , type ( migration_schema ) ) ) do_migration = False do_migration = ( user_choice ( \"\"\" Warning: Ensure the following are completed before proceeding. - Appropriate backups have been taken, - Any existing DJ 0.11.X connections are suspended, and - External config has been updated to new dj.config['stores'] structure. Proceed? \"\"\" , default = \"no\" , ) == \"yes\" ) if do_migration : _migrate_dj011_blob ( dj . Schema ( migration_schema ), store ) print ( \"Migration completed for schema: {} , store: {} .\" . format ( migration_schema , store ) ) return print ( \"No migration performed.\" )", "title": "migrate_dj011_external_blob_storage_to_dj012()"}, {"location": "api/datajoint/admin/", "text": "kill ( restriction = None , connection = None , order_by = None ) \u00b6 view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes Source code in datajoint/admin.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : print ( \"Process not found\" ) kill_quick ( restriction = None , connection = None ) \u00b6 Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". Source code in datajoint/admin.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def kill_quick ( restriction = None , connection = None ): \"\"\" Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". \"\"\" if connection is None : connection = conn () query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) nkill = 0 for process in cur : connection . query ( \"kill %d \" % process [ \"id\" ]) nkill += 1 return nkill", "title": "admin.py"}, {"location": "api/datajoint/admin/#datajoint.admin.kill", "text": "view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes Source code in datajoint/admin.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : print ( \"Process not found\" )", "title": "kill()"}, {"location": "api/datajoint/admin/#datajoint.admin.kill_quick", "text": "Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". Source code in datajoint/admin.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def kill_quick ( restriction = None , connection = None ): \"\"\" Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". \"\"\" if connection is None : connection = conn () query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) nkill = 0 for process in cur : connection . query ( \"kill %d \" % process [ \"id\" ]) nkill += 1 return nkill", "title": "kill_quick()"}, {"location": "api/datajoint/attribute_adapter/", "text": "AttributeAdapter \u00b6 Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) attribute_type () property \u00b6 :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" Source code in datajoint/attribute_adapter.py 11 12 13 14 15 16 @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) get ( value ) \u00b6 convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) put ( obj ) \u00b6 convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) get_adapter ( context , adapter_name ) \u00b6 Extract the AttributeAdapter object by its name from the context and validate. Source code in datajoint/attribute_adapter.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_adapter ( context , adapter_name ): \"\"\" Extract the AttributeAdapter object by its name from the context and validate. \"\"\" if not _support_adapted_types (): raise DataJointError ( \"Support for Adapted Attribute types is disabled.\" ) adapter_name = adapter_name . lstrip ( \"<\" ) . rstrip ( \">\" ) try : adapter = ( context [ adapter_name ] if adapter_name in context else type_plugins [ adapter_name ][ \"object\" ] . load () ) except KeyError : raise DataJointError ( \"Attribute adapter ' {adapter_name} ' is not defined.\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter , AttributeAdapter ): raise DataJointError ( \"Attribute adapter ' {adapter_name} ' must be an instance of datajoint.AttributeAdapter\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter . attribute_type , str ) or not re . match ( r \"^\\w\" , adapter . attribute_type ): raise DataJointError ( \"Invalid attribute type {type} in attribute adapter ' {adapter_name} '\" . format ( type = adapter . attribute_type , adapter_name = adapter_name ) ) return adapter", "title": "attribute_adapter.py"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter", "text": "Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "AttributeAdapter"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "text": ":return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" Source code in datajoint/attribute_adapter.py 11 12 13 14 15 16 @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "attribute_type()"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.get", "text": "convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "get()"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.put", "text": "convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "put()"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.get_adapter", "text": "Extract the AttributeAdapter object by its name from the context and validate. Source code in datajoint/attribute_adapter.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_adapter ( context , adapter_name ): \"\"\" Extract the AttributeAdapter object by its name from the context and validate. \"\"\" if not _support_adapted_types (): raise DataJointError ( \"Support for Adapted Attribute types is disabled.\" ) adapter_name = adapter_name . lstrip ( \"<\" ) . rstrip ( \">\" ) try : adapter = ( context [ adapter_name ] if adapter_name in context else type_plugins [ adapter_name ][ \"object\" ] . load () ) except KeyError : raise DataJointError ( \"Attribute adapter ' {adapter_name} ' is not defined.\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter , AttributeAdapter ): raise DataJointError ( \"Attribute adapter ' {adapter_name} ' must be an instance of datajoint.AttributeAdapter\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter . attribute_type , str ) or not re . match ( r \"^\\w\" , adapter . attribute_type ): raise DataJointError ( \"Invalid attribute type {type} in attribute adapter ' {adapter_name} '\" . format ( type = adapter . attribute_type , adapter_name = adapter_name ) ) return adapter", "title": "get_adapter()"}, {"location": "api/datajoint/autopopulate/", "text": "This module defines class dj.AutoPopulate AutoPopulate \u00b6 AutoPopulate is a mixin class that adds the method populate() to a Relation class. Auto-populated relations must inherit from both Relation and AutoPopulate, must define the property key_source , and must define the callback method make . Source code in datajoint/autopopulate.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class AutoPopulate : \"\"\" AutoPopulate is a mixin class that adds the method populate() to a Relation class. Auto-populated relations must inherit from both Relation and AutoPopulate, must define the property `key_source`, and must define the callback method `make`. \"\"\" _key_source = None _allow_insert = False @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self def _job_key ( self , key ): \"\"\" :param key: they key returned for the job from the key source :return: the dict to use to generate the job reservation hash This method allows subclasses to control the job reservation granularity. \"\"\" return key def _jobs_to_do ( self , restrictions ): \"\"\" :return: the relation containing the keys to be computed (derived from self.key_source) \"\"\" if self . restriction : raise DataJointError ( \"Cannot call populate on a restricted table. \" \"Instead, pass conditions to populate() as arguments.\" ) todo = self . key_source # key_source is a QueryExpression subclass -- trigger instantiation if inspect . isclass ( todo ) and issubclass ( todo , QueryExpression ): todo = todo () if not isinstance ( todo , QueryExpression ): raise DataJointError ( \"Invalid key_source value\" ) try : # check if target lacks any attributes from the primary key of key_source raise DataJointError ( \"The populate target lacks attribute %s \" \"from the primary key of key_source\" % next ( name for name in todo . heading . primary_key if name not in self . target . heading ) ) except StopIteration : pass return ( todo & AndList ( restrictions )) . proj () def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list def _populate1 ( self , key , jobs , suppress_errors , return_exception_objects , make_kwargs = None ): \"\"\" populates table for one source key, calling self.make inside a transaction. :param jobs: the jobs table or None if not reserve_jobs :param key: dict specifying job to populate :param suppress_errors: bool if errors should be suppressed and returned :param return_exception_objects: if True, errors must be returned as objects :return: (key, error) when suppress_errors=True, otherwise None \"\"\" make = self . _make_tuples if hasattr ( self , \"_make_tuples\" ) else self . make if jobs is None or jobs . reserve ( self . target . table_name , self . _job_key ( key )): self . connection . start_transaction () if key in self . target : # already populated self . connection . cancel_transaction () if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) else : logger . debug ( f \"Making { key } -> { self . target . full_table_name } \" ) self . __class__ . _allow_insert = True try : make ( dict ( key ), ** ( make_kwargs or {})) except ( KeyboardInterrupt , SystemExit , Exception ) as error : try : self . connection . cancel_transaction () except LostConnectionError : pass error_message = \" {exception}{msg} \" . format ( exception = error . __class__ . __name__ , msg = \": \" + str ( error ) if str ( error ) else \"\" , ) logger . debug ( f \"Error making { key } -> { self . target . full_table_name } - { error_message } \" ) if jobs is not None : # show error name and error message (if any) jobs . error ( self . target . table_name , self . _job_key ( key ), error_message = error_message , error_stack = traceback . format_exc (), ) if not suppress_errors or isinstance ( error , SystemExit ): raise else : logger . error ( error ) return key , error if return_exception_objects else error_message else : self . connection . commit_transaction () logger . debug ( f \"Success making { key } -> { self . target . full_table_name } \" ) if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) finally : self . __class__ . _allow_insert = False def progress ( self , * restrictions , display = True ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : print ( \" %-20s \" % self . __class__ . __name__ , \"Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), flush = True , ) return remaining , total key_source () property \u00b6 :return: the query expression that yields primary key values to be passed, sequentially, to the make method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. Source code in datajoint/autopopulate.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source make ( key ) \u00b6 Derived classes must implement method make that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. Source code in datajoint/autopopulate.py 91 92 93 94 95 96 97 98 99 def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) populate ( * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = 'original' , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None ) \u00b6 table.populate() calls table.make(key) for every primary key in self.key_source for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table. :type make_kwargs: dict, optional Source code in datajoint/autopopulate.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list progress ( * restrictions , display = True ) \u00b6 Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated Source code in datajoint/autopopulate.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def progress ( self , * restrictions , display = True ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : print ( \" %-20s \" % self . __class__ . __name__ , \"Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), flush = True , ) return remaining , total target () property \u00b6 :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. Source code in datajoint/autopopulate.py 101 102 103 104 105 106 107 108 @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self", "title": "autopopulate.py"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate", "text": "AutoPopulate is a mixin class that adds the method populate() to a Relation class. Auto-populated relations must inherit from both Relation and AutoPopulate, must define the property key_source , and must define the callback method make . Source code in datajoint/autopopulate.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class AutoPopulate : \"\"\" AutoPopulate is a mixin class that adds the method populate() to a Relation class. Auto-populated relations must inherit from both Relation and AutoPopulate, must define the property `key_source`, and must define the callback method `make`. \"\"\" _key_source = None _allow_insert = False @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self def _job_key ( self , key ): \"\"\" :param key: they key returned for the job from the key source :return: the dict to use to generate the job reservation hash This method allows subclasses to control the job reservation granularity. \"\"\" return key def _jobs_to_do ( self , restrictions ): \"\"\" :return: the relation containing the keys to be computed (derived from self.key_source) \"\"\" if self . restriction : raise DataJointError ( \"Cannot call populate on a restricted table. \" \"Instead, pass conditions to populate() as arguments.\" ) todo = self . key_source # key_source is a QueryExpression subclass -- trigger instantiation if inspect . isclass ( todo ) and issubclass ( todo , QueryExpression ): todo = todo () if not isinstance ( todo , QueryExpression ): raise DataJointError ( \"Invalid key_source value\" ) try : # check if target lacks any attributes from the primary key of key_source raise DataJointError ( \"The populate target lacks attribute %s \" \"from the primary key of key_source\" % next ( name for name in todo . heading . primary_key if name not in self . target . heading ) ) except StopIteration : pass return ( todo & AndList ( restrictions )) . proj () def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list def _populate1 ( self , key , jobs , suppress_errors , return_exception_objects , make_kwargs = None ): \"\"\" populates table for one source key, calling self.make inside a transaction. :param jobs: the jobs table or None if not reserve_jobs :param key: dict specifying job to populate :param suppress_errors: bool if errors should be suppressed and returned :param return_exception_objects: if True, errors must be returned as objects :return: (key, error) when suppress_errors=True, otherwise None \"\"\" make = self . _make_tuples if hasattr ( self , \"_make_tuples\" ) else self . make if jobs is None or jobs . reserve ( self . target . table_name , self . _job_key ( key )): self . connection . start_transaction () if key in self . target : # already populated self . connection . cancel_transaction () if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) else : logger . debug ( f \"Making { key } -> { self . target . full_table_name } \" ) self . __class__ . _allow_insert = True try : make ( dict ( key ), ** ( make_kwargs or {})) except ( KeyboardInterrupt , SystemExit , Exception ) as error : try : self . connection . cancel_transaction () except LostConnectionError : pass error_message = \" {exception}{msg} \" . format ( exception = error . __class__ . __name__ , msg = \": \" + str ( error ) if str ( error ) else \"\" , ) logger . debug ( f \"Error making { key } -> { self . target . full_table_name } - { error_message } \" ) if jobs is not None : # show error name and error message (if any) jobs . error ( self . target . table_name , self . _job_key ( key ), error_message = error_message , error_stack = traceback . format_exc (), ) if not suppress_errors or isinstance ( error , SystemExit ): raise else : logger . error ( error ) return key , error if return_exception_objects else error_message else : self . connection . commit_transaction () logger . debug ( f \"Success making { key } -> { self . target . full_table_name } \" ) if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) finally : self . __class__ . _allow_insert = False def progress ( self , * restrictions , display = True ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : print ( \" %-20s \" % self . __class__ . __name__ , \"Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), flush = True , ) return remaining , total", "title": "AutoPopulate"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.key_source", "text": ":return: the query expression that yields primary key values to be passed, sequentially, to the make method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. Source code in datajoint/autopopulate.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source", "title": "key_source()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.make", "text": "Derived classes must implement method make that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. Source code in datajoint/autopopulate.py 91 92 93 94 95 96 97 98 99 def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" )", "title": "make()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.populate", "text": "table.populate() calls table.make(key) for every primary key in self.key_source for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table. :type make_kwargs: dict, optional Source code in datajoint/autopopulate.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list", "title": "populate()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.progress", "text": "Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated Source code in datajoint/autopopulate.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def progress ( self , * restrictions , display = True ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : print ( \" %-20s \" % self . __class__ . __name__ , \"Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), flush = True , ) return remaining , total", "title": "progress()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.target", "text": ":return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. Source code in datajoint/autopopulate.py 101 102 103 104 105 106 107 108 @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self", "title": "target()"}, {"location": "api/datajoint/blob/", "text": "(De)serialization methods for basic datatypes and numpy.ndarrays with provisions for mutual compatibility with Matlab-based serialization implemented by mYm. Blob \u00b6 Source code in datajoint/blob.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 class Blob : def __init__ ( self , squeeze = False ): self . _squeeze = squeeze self . _blob = None self . _pos = 0 self . protocol = None def set_dj0 ( self ): if not config . get ( \"enable_python_native_blobs\" ): raise DataJointError ( \"\"\"v0.12+ python native blobs disabled. See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\" ) self . protocol = b \"dj0 \\0 \" # when using new blob features def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array def unpack ( self , blob ): self . _blob = blob try : # decompress prefix = next ( p for p in compression if self . _blob [ self . _pos :] . startswith ( p ) ) except StopIteration : pass # assume uncompressed but could be unrecognized compression else : self . _pos += len ( prefix ) blob_size = self . read_value () blob = compression [ prefix ]( self . _blob [ self . _pos :]) assert len ( blob ) == blob_size self . _blob = blob self . _pos = 0 blob_format = self . read_zero_terminated_string () if blob_format in ( \"mYm\" , \"dj0\" ): return self . read_blob ( n_bytes = len ( self . _blob ) - self . _pos ) def read_blob ( self , n_bytes = None ): start = self . _pos data_structure_code = chr ( self . read_value ( \"uint8\" )) try : call = { # MATLAB-compatible, inherited from original mYm \"A\" : self . read_array , # matlab-compatible numeric arrays and scalars with ndim==0 \"P\" : self . read_sparse_array , # matlab sparse array -- not supported yet \"S\" : self . read_struct , # matlab struct array \"C\" : self . read_cell_array , # matlab cell array # basic data types \" \\xFF \" : self . read_none , # None \" \\x01 \" : self . read_tuple , # a Sequence (e.g. tuple) \" \\x02 \" : self . read_list , # a MutableSequence (e.g. list) \" \\x03 \" : self . read_set , # a Set \" \\x04 \" : self . read_dict , # a Mapping (e.g. dict) \" \\x05 \" : self . read_string , # a UTF8-encoded string \" \\x06 \" : self . read_bytes , # a ByteString \" \\x0a \" : self . read_int , # unbounded scalar int \" \\x0b \" : self . read_bool , # scalar boolean \" \\x0c \" : self . read_complex , # scalar 128-bit complex number \" \\x0d \" : self . read_float , # scalar 64-bit float \"F\" : self . read_recarray , # numpy array with fields, including recarrays \"d\" : self . read_decimal , # a decimal \"t\" : self . read_datetime , # date, time, or datetime \"u\" : self . read_uuid , # UUID }[ data_structure_code ] except KeyError : raise DataJointError ( 'Unknown data structure code \" %s \". Upgrade datajoint.' % data_structure_code ) v = call () if n_bytes is not None and self . _pos - start != n_bytes : raise DataJointError ( \"Blob length check failed! Invalid blob\" ) return v def pack_blob ( self , obj ): # original mYm-based serialization from datajoint-matlab if isinstance ( obj , MatCell ): return self . pack_cell_array ( obj ) if isinstance ( obj , MatStruct ): return self . pack_struct ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields is None : return self . pack_array ( obj ) # blob types in the expanded dj0 blob format self . set_dj0 () if not isinstance ( obj , ( np . ndarray , np . number )): # python built-in data types if isinstance ( obj , bool ): return self . pack_bool ( obj ) if isinstance ( obj , int ): return self . pack_int ( obj ) if isinstance ( obj , complex ): return self . pack_complex ( obj ) if isinstance ( obj , float ): return self . pack_float ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields : return self . pack_recarray ( np . array ( obj )) if isinstance ( obj , ( np . number , np . datetime64 )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( bool , np . bool_ )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( float , int , complex )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time )): return self . pack_datetime ( obj ) if isinstance ( obj , Decimal ): return self . pack_decimal ( obj ) if isinstance ( obj , uuid . UUID ): return self . pack_uuid ( obj ) if isinstance ( obj , collections . abc . Mapping ): return self . pack_dict ( obj ) if isinstance ( obj , str ): return self . pack_string ( obj ) if isinstance ( obj , collections . abc . ByteString ): return self . pack_bytes ( obj ) if isinstance ( obj , collections . abc . MutableSequence ): return self . pack_list ( obj ) if isinstance ( obj , collections . abc . Sequence ): return self . pack_tuple ( obj ) if isinstance ( obj , collections . abc . Set ): return self . pack_set ( obj ) if obj is None : return self . pack_none () raise DataJointError ( \"Packing object of type %s currently not supported!\" % type ( obj ) ) def read_array ( self ): n_dims = int ( self . read_value ()) shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) dtype_id , is_complex = self . read_value ( \"uint32\" , 2 ) # Get dtype from type id dtype = deserialize_lookup [ dtype_id ][ \"dtype\" ] # Check if name is void if deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"VOID\" : data = np . array ( list ( self . read_blob ( self . read_value ()) for _ in range ( n_elem )), dtype = np . dtype ( \"O\" ), ) # Check if name is char elif deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"CHAR\" : # compensate for MATLAB packing of char arrays data = self . read_value ( dtype , count = 2 * n_elem ) data = data [:: 2 ] . astype ( \"U1\" ) if n_dims == 2 and shape [ 0 ] == 1 or n_dims == 1 : compact = data . squeeze () data = ( compact if compact . shape == () else np . array ( \"\" . join ( data . squeeze ())) ) shape = ( 1 ,) else : data = self . read_value ( dtype , count = n_elem ) if is_complex : data = data + 1 j * self . read_value ( dtype , count = n_elem ) return self . squeeze ( data . reshape ( shape , order = \"F\" )) def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) def read_sparse_array ( self ): raise DataJointError ( \"datajoint-python does not yet support sparse arrays. Issue (#590)\" ) def read_int ( self ): return int . from_bytes ( self . read_binary ( self . read_value ( \"uint16\" )), byteorder = \"little\" , signed = True ) @staticmethod def pack_int ( v ): n_bytes = v . bit_length () // 8 + 1 assert 0 < n_bytes <= 0xFFFF , \"Integers are limited to 65535 bytes\" return ( b \" \\x0a \" + np . uint16 ( n_bytes ) . tobytes () + v . to_bytes ( n_bytes , byteorder = \"little\" , signed = True ) ) def read_bool ( self ): return bool ( self . read_value ( \"bool\" )) @staticmethod def pack_bool ( v ): return b \" \\x0b \" + np . array ( v , dtype = \"bool\" ) . tobytes () def read_complex ( self ): return complex ( self . read_value ( \"complex128\" )) @staticmethod def pack_complex ( v ): return b \" \\x0c \" + np . array ( v , dtype = \"complex128\" ) . tobytes () def read_float ( self ): return float ( self . read_value ( \"float64\" )) @staticmethod def pack_float ( v ): return b \" \\x0d \" + np . array ( v , dtype = \"float64\" ) . tobytes () def read_decimal ( self ): return Decimal ( self . read_string ()) @staticmethod def pack_decimal ( d ): s = str ( d ) return b \"d\" + len_u64 ( s ) + s . encode () def read_string ( self ): return self . read_binary ( self . read_value ()) . decode () @staticmethod def pack_string ( s ): blob = s . encode () return b \" \\5 \" + len_u64 ( blob ) + blob def read_bytes ( self ): return self . read_binary ( self . read_value ()) @staticmethod def pack_bytes ( s ): return b \" \\6 \" + len_u64 ( s ) + s def read_none ( self ): pass @staticmethod def pack_none (): return b \" \\xFF \" def read_tuple ( self ): return tuple ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ()) ) def pack_tuple ( self , t ): return ( b \" \\1 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_list ( self ): return list ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_list ( self , t ): return ( b \" \\2 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_set ( self ): return set ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_set ( self , t ): return ( b \" \\3 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_dict ( self ): return dict ( ( self . read_blob ( self . read_value ()), self . read_blob ( self . read_value ())) for _ in range ( self . read_value ()) ) def pack_dict ( self , d ): return ( b \" \\4 \" + len_u64 ( d ) + b \"\" . join ( b \"\" . join (( len_u64 ( it ) + it ) for it in packed ) for packed in ( map ( self . pack_blob , pair ) for pair in d . items ()) ) ) def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) def pack_cell_array ( self , array ): return ( b \"C\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) ) def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date @staticmethod def pack_datetime ( d ): if isinstance ( d , datetime . datetime ): date , time = d . date (), d . time () elif isinstance ( d , datetime . date ): date , time = d , None else : date , time = None , d return b \"t\" + ( np . int32 ( - 1 if date is None else ( date . year * 100 + date . month ) * 100 + date . day ) . tobytes () + np . int64 ( - 1 if time is None else (( time . hour * 100 + time . minute ) * 100 + time . second ) * 1000000 + time . microsecond ) . tobytes () ) def read_uuid ( self ): q = self . read_binary ( 16 ) return uuid . UUID ( bytes = q ) @staticmethod def pack_uuid ( obj ): return b \"u\" + obj . bytes def read_zero_terminated_string ( self ): target = self . _blob . find ( b \" \\0 \" , self . _pos ) data = self . _blob [ self . _pos : target ] . decode () self . _pos = target + 1 return data def read_value ( self , dtype = None , count = 1 ): if dtype is None : dtype = \"uint32\" if use_32bit_dims else \"uint64\" data = np . frombuffer ( self . _blob , dtype = dtype , count = count , offset = self . _pos ) self . _pos += data . dtype . itemsize * data . size return data [ 0 ] if count == 1 else data def read_binary ( self , size ): self . _pos += int ( size ) return self . _blob [ self . _pos - int ( size ) : self . _pos ] def pack ( self , obj , compress ): self . protocol = b \"mYm \\0 \" # will be replaced with dj0 if new features are used blob = self . pack_blob ( obj ) # this may reset the protocol and must precede protocol evaluation blob = self . protocol + blob if compress and len ( blob ) > 1000 : compressed = b \"ZL123 \\0 \" + len_u64 ( blob ) + zlib . compress ( blob ) if len ( compressed ) < len ( blob ): blob = compressed return blob pack_array ( array ) \u00b6 Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. Source code in datajoint/blob.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob pack_recarray ( array ) \u00b6 Serialize a Matlab struct array Source code in datajoint/blob.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) pack_struct ( array ) \u00b6 Serialize a Matlab struct array Source code in datajoint/blob.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values read_cell_array () \u00b6 deserialize MATLAB cell array Source code in datajoint/blob.py 487 488 489 490 491 492 493 494 495 496 497 def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) read_datetime () \u00b6 deserialize datetime.date, .time, or .datetime Source code in datajoint/blob.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date read_recarray () \u00b6 Serialize an np.ndarray with fields, including recarrays Source code in datajoint/blob.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) read_struct () \u00b6 deserialize matlab stuct Source code in datajoint/blob.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) squeeze ( array , convert_to_scalar = True ) \u00b6 Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars Source code in datajoint/blob.py 101 102 103 104 105 106 107 108 109 def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array MatCell \u00b6 Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass MatStruct \u00b6 Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass", "title": "blob.py"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob", "text": "Source code in datajoint/blob.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 class Blob : def __init__ ( self , squeeze = False ): self . _squeeze = squeeze self . _blob = None self . _pos = 0 self . protocol = None def set_dj0 ( self ): if not config . get ( \"enable_python_native_blobs\" ): raise DataJointError ( \"\"\"v0.12+ python native blobs disabled. See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\" ) self . protocol = b \"dj0 \\0 \" # when using new blob features def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array def unpack ( self , blob ): self . _blob = blob try : # decompress prefix = next ( p for p in compression if self . _blob [ self . _pos :] . startswith ( p ) ) except StopIteration : pass # assume uncompressed but could be unrecognized compression else : self . _pos += len ( prefix ) blob_size = self . read_value () blob = compression [ prefix ]( self . _blob [ self . _pos :]) assert len ( blob ) == blob_size self . _blob = blob self . _pos = 0 blob_format = self . read_zero_terminated_string () if blob_format in ( \"mYm\" , \"dj0\" ): return self . read_blob ( n_bytes = len ( self . _blob ) - self . _pos ) def read_blob ( self , n_bytes = None ): start = self . _pos data_structure_code = chr ( self . read_value ( \"uint8\" )) try : call = { # MATLAB-compatible, inherited from original mYm \"A\" : self . read_array , # matlab-compatible numeric arrays and scalars with ndim==0 \"P\" : self . read_sparse_array , # matlab sparse array -- not supported yet \"S\" : self . read_struct , # matlab struct array \"C\" : self . read_cell_array , # matlab cell array # basic data types \" \\xFF \" : self . read_none , # None \" \\x01 \" : self . read_tuple , # a Sequence (e.g. tuple) \" \\x02 \" : self . read_list , # a MutableSequence (e.g. list) \" \\x03 \" : self . read_set , # a Set \" \\x04 \" : self . read_dict , # a Mapping (e.g. dict) \" \\x05 \" : self . read_string , # a UTF8-encoded string \" \\x06 \" : self . read_bytes , # a ByteString \" \\x0a \" : self . read_int , # unbounded scalar int \" \\x0b \" : self . read_bool , # scalar boolean \" \\x0c \" : self . read_complex , # scalar 128-bit complex number \" \\x0d \" : self . read_float , # scalar 64-bit float \"F\" : self . read_recarray , # numpy array with fields, including recarrays \"d\" : self . read_decimal , # a decimal \"t\" : self . read_datetime , # date, time, or datetime \"u\" : self . read_uuid , # UUID }[ data_structure_code ] except KeyError : raise DataJointError ( 'Unknown data structure code \" %s \". Upgrade datajoint.' % data_structure_code ) v = call () if n_bytes is not None and self . _pos - start != n_bytes : raise DataJointError ( \"Blob length check failed! Invalid blob\" ) return v def pack_blob ( self , obj ): # original mYm-based serialization from datajoint-matlab if isinstance ( obj , MatCell ): return self . pack_cell_array ( obj ) if isinstance ( obj , MatStruct ): return self . pack_struct ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields is None : return self . pack_array ( obj ) # blob types in the expanded dj0 blob format self . set_dj0 () if not isinstance ( obj , ( np . ndarray , np . number )): # python built-in data types if isinstance ( obj , bool ): return self . pack_bool ( obj ) if isinstance ( obj , int ): return self . pack_int ( obj ) if isinstance ( obj , complex ): return self . pack_complex ( obj ) if isinstance ( obj , float ): return self . pack_float ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields : return self . pack_recarray ( np . array ( obj )) if isinstance ( obj , ( np . number , np . datetime64 )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( bool , np . bool_ )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( float , int , complex )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time )): return self . pack_datetime ( obj ) if isinstance ( obj , Decimal ): return self . pack_decimal ( obj ) if isinstance ( obj , uuid . UUID ): return self . pack_uuid ( obj ) if isinstance ( obj , collections . abc . Mapping ): return self . pack_dict ( obj ) if isinstance ( obj , str ): return self . pack_string ( obj ) if isinstance ( obj , collections . abc . ByteString ): return self . pack_bytes ( obj ) if isinstance ( obj , collections . abc . MutableSequence ): return self . pack_list ( obj ) if isinstance ( obj , collections . abc . Sequence ): return self . pack_tuple ( obj ) if isinstance ( obj , collections . abc . Set ): return self . pack_set ( obj ) if obj is None : return self . pack_none () raise DataJointError ( \"Packing object of type %s currently not supported!\" % type ( obj ) ) def read_array ( self ): n_dims = int ( self . read_value ()) shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) dtype_id , is_complex = self . read_value ( \"uint32\" , 2 ) # Get dtype from type id dtype = deserialize_lookup [ dtype_id ][ \"dtype\" ] # Check if name is void if deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"VOID\" : data = np . array ( list ( self . read_blob ( self . read_value ()) for _ in range ( n_elem )), dtype = np . dtype ( \"O\" ), ) # Check if name is char elif deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"CHAR\" : # compensate for MATLAB packing of char arrays data = self . read_value ( dtype , count = 2 * n_elem ) data = data [:: 2 ] . astype ( \"U1\" ) if n_dims == 2 and shape [ 0 ] == 1 or n_dims == 1 : compact = data . squeeze () data = ( compact if compact . shape == () else np . array ( \"\" . join ( data . squeeze ())) ) shape = ( 1 ,) else : data = self . read_value ( dtype , count = n_elem ) if is_complex : data = data + 1 j * self . read_value ( dtype , count = n_elem ) return self . squeeze ( data . reshape ( shape , order = \"F\" )) def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) def read_sparse_array ( self ): raise DataJointError ( \"datajoint-python does not yet support sparse arrays. Issue (#590)\" ) def read_int ( self ): return int . from_bytes ( self . read_binary ( self . read_value ( \"uint16\" )), byteorder = \"little\" , signed = True ) @staticmethod def pack_int ( v ): n_bytes = v . bit_length () // 8 + 1 assert 0 < n_bytes <= 0xFFFF , \"Integers are limited to 65535 bytes\" return ( b \" \\x0a \" + np . uint16 ( n_bytes ) . tobytes () + v . to_bytes ( n_bytes , byteorder = \"little\" , signed = True ) ) def read_bool ( self ): return bool ( self . read_value ( \"bool\" )) @staticmethod def pack_bool ( v ): return b \" \\x0b \" + np . array ( v , dtype = \"bool\" ) . tobytes () def read_complex ( self ): return complex ( self . read_value ( \"complex128\" )) @staticmethod def pack_complex ( v ): return b \" \\x0c \" + np . array ( v , dtype = \"complex128\" ) . tobytes () def read_float ( self ): return float ( self . read_value ( \"float64\" )) @staticmethod def pack_float ( v ): return b \" \\x0d \" + np . array ( v , dtype = \"float64\" ) . tobytes () def read_decimal ( self ): return Decimal ( self . read_string ()) @staticmethod def pack_decimal ( d ): s = str ( d ) return b \"d\" + len_u64 ( s ) + s . encode () def read_string ( self ): return self . read_binary ( self . read_value ()) . decode () @staticmethod def pack_string ( s ): blob = s . encode () return b \" \\5 \" + len_u64 ( blob ) + blob def read_bytes ( self ): return self . read_binary ( self . read_value ()) @staticmethod def pack_bytes ( s ): return b \" \\6 \" + len_u64 ( s ) + s def read_none ( self ): pass @staticmethod def pack_none (): return b \" \\xFF \" def read_tuple ( self ): return tuple ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ()) ) def pack_tuple ( self , t ): return ( b \" \\1 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_list ( self ): return list ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_list ( self , t ): return ( b \" \\2 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_set ( self ): return set ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_set ( self , t ): return ( b \" \\3 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_dict ( self ): return dict ( ( self . read_blob ( self . read_value ()), self . read_blob ( self . read_value ())) for _ in range ( self . read_value ()) ) def pack_dict ( self , d ): return ( b \" \\4 \" + len_u64 ( d ) + b \"\" . join ( b \"\" . join (( len_u64 ( it ) + it ) for it in packed ) for packed in ( map ( self . pack_blob , pair ) for pair in d . items ()) ) ) def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) def pack_cell_array ( self , array ): return ( b \"C\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) ) def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date @staticmethod def pack_datetime ( d ): if isinstance ( d , datetime . datetime ): date , time = d . date (), d . time () elif isinstance ( d , datetime . date ): date , time = d , None else : date , time = None , d return b \"t\" + ( np . int32 ( - 1 if date is None else ( date . year * 100 + date . month ) * 100 + date . day ) . tobytes () + np . int64 ( - 1 if time is None else (( time . hour * 100 + time . minute ) * 100 + time . second ) * 1000000 + time . microsecond ) . tobytes () ) def read_uuid ( self ): q = self . read_binary ( 16 ) return uuid . UUID ( bytes = q ) @staticmethod def pack_uuid ( obj ): return b \"u\" + obj . bytes def read_zero_terminated_string ( self ): target = self . _blob . find ( b \" \\0 \" , self . _pos ) data = self . _blob [ self . _pos : target ] . decode () self . _pos = target + 1 return data def read_value ( self , dtype = None , count = 1 ): if dtype is None : dtype = \"uint32\" if use_32bit_dims else \"uint64\" data = np . frombuffer ( self . _blob , dtype = dtype , count = count , offset = self . _pos ) self . _pos += data . dtype . itemsize * data . size return data [ 0 ] if count == 1 else data def read_binary ( self , size ): self . _pos += int ( size ) return self . _blob [ self . _pos - int ( size ) : self . _pos ] def pack ( self , obj , compress ): self . protocol = b \"mYm \\0 \" # will be replaced with dj0 if new features are used blob = self . pack_blob ( obj ) # this may reset the protocol and must precede protocol evaluation blob = self . protocol + blob if compress and len ( blob ) > 1000 : compressed = b \"ZL123 \\0 \" + len_u64 ( blob ) + zlib . compress ( blob ) if len ( compressed ) < len ( blob ): blob = compressed return blob", "title": "Blob"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_array", "text": "Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. Source code in datajoint/blob.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob", "title": "pack_array()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_recarray", "text": "Serialize a Matlab struct array Source code in datajoint/blob.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) )", "title": "pack_recarray()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_struct", "text": "Serialize a Matlab struct array Source code in datajoint/blob.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values", "title": "pack_struct()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_cell_array", "text": "deserialize MATLAB cell array Source code in datajoint/blob.py 487 488 489 490 491 492 493 494 495 496 497 def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell )", "title": "read_cell_array()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_datetime", "text": "deserialize datetime.date, .time, or .datetime Source code in datajoint/blob.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date", "title": "read_datetime()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_recarray", "text": "Serialize an np.ndarray with fields, including recarrays Source code in datajoint/blob.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray )", "title": "read_recarray()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_struct", "text": "deserialize matlab stuct Source code in datajoint/blob.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct )", "title": "read_struct()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.squeeze", "text": "Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars Source code in datajoint/blob.py 101 102 103 104 105 106 107 108 109 def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array", "title": "squeeze()"}, {"location": "api/datajoint/blob/#datajoint.blob.MatCell", "text": "Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass", "title": "MatCell"}, {"location": "api/datajoint/blob/#datajoint.blob.MatStruct", "text": "Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass", "title": "MatStruct"}, {"location": "api/datajoint/condition/", "text": "methods for generating SQL WHERE clauses from datajoint restriction conditions AndList \u00b6 Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction ) Not \u00b6 invert restriction Source code in datajoint/condition.py 43 44 45 46 47 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction PromiscuousOperand \u00b6 A container for an operand to ignore join compatibility Source code in datajoint/condition.py 14 15 16 17 18 19 20 class PromiscuousOperand : \"\"\" A container for an operand to ignore join compatibility \"\"\" def __init__ ( self , operand ): self . operand = operand assert_join_compatibility ( expr1 , expr2 ) \u00b6 Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object Source code in datajoint/condition.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def assert_join_compatibility ( expr1 , expr2 ): \"\"\" Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object \"\"\" from .expression import QueryExpression , U for rel in ( expr1 , expr2 ): if not isinstance ( rel , ( U , QueryExpression )): raise DataJointError ( \"Object %r is not a QueryExpression and cannot be joined.\" % rel ) if not isinstance ( expr1 , U ) and not isinstance ( expr2 , U ): # dj.U is always compatible try : raise DataJointError ( \"Cannot join query expressions on dependent attribute ` %s `\" % next ( r for r in set ( expr1 . heading . secondary_attributes ) . intersection ( expr2 . heading . secondary_attributes ) ) ) except StopIteration : pass # all ok extract_column_names ( sql_expression ) \u00b6 extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. Source code in datajoint/condition.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def extract_column_names ( sql_expression ): \"\"\" extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. \"\"\" assert isinstance ( sql_expression , str ) result = set () s = sql_expression # for terseness # remove escaped quotes s = re . sub ( r \"( \\\\\\\" )|( \\\\ \\')\" , \"\" , s ) # remove quoted text s = re . sub ( r \"'[^']*'\" , \"\" , s ) s = re . sub ( r '\"[^\"]*\"' , \"\" , s ) # find all tokens in back quotes and remove them result . update ( re . findall ( r \"`([a-z][a-z_0-9]*)`\" , s )) s = re . sub ( r \"`[a-z][a-z_0-9]*`\" , \"\" , s ) # remove space before parentheses s = re . sub ( r \"\\s*\\(\" , \"(\" , s ) # remove tokens followed by ( since they must be functions s = re . sub ( r \"(\\b[a-z][a-z_0-9]*)\\(\" , \"(\" , s ) remaining_tokens = set ( re . findall ( r \"\\b[a-z][a-z_0-9]*\\b\" , s )) # update result removing reserved words result . update ( remaining_tokens - { \"is\" , \"in\" , \"between\" , \"like\" , \"and\" , \"or\" , \"null\" , \"not\" , \"interval\" , \"second\" , \"minute\" , \"hour\" , \"day\" , \"month\" , \"week\" , \"year\" , } ) return result make_condition ( query_expression , condition , columns ) \u00b6 Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. Source code in datajoint/condition.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def make_condition ( query_expression , condition , columns ): \"\"\" Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. \"\"\" from .expression import QueryExpression , Aggregation , U def prep_value ( k , v ): \"\"\"prepare value v for inclusion as a string in an SQL condition\"\"\" if query_expression . heading [ k ] . uuid : if not isinstance ( v , uuid . UUID ): try : v = uuid . UUID ( v ) except ( AttributeError , ValueError ): raise DataJointError ( \"Badly formed UUID {v} in restriction by ` {k} `\" . format ( k = k , v = v ) ) return \"X' %s '\" % v . bytes . hex () if isinstance ( v , ( datetime . date , datetime . datetime , datetime . time , decimal . Decimal ) ): return '\" %s \"' % v if isinstance ( v , str ): return '\" %s \"' % v . replace ( \"%\" , \" %% \" ) . replace ( \" \\\\ \" , \" \\\\\\\\ \" ) return \" %r \" % v negate = False while isinstance ( condition , Not ): negate = not negate condition = condition . restriction template = \"NOT ( %s )\" if negate else \" %s \" # restrict by string if isinstance ( condition , str ): columns . update ( extract_column_names ( condition )) return template % condition . strip () . replace ( \"%\" , \" %% \" ) # escape %, see issue #376 # restrict by AndList if isinstance ( condition , AndList ): # omit all conditions that evaluate to True items = [ item for item in ( make_condition ( query_expression , cond , columns ) for cond in condition ) if item is not True ] if any ( item is False for item in items ): return negate # if any item is False, the whole thing is False if not items : return not negate # and empty AndList is True return template % ( \"(\" + \") AND (\" . join ( items ) + \")\" ) # restriction by dj.U evaluates to True if isinstance ( condition , U ): return not negate # restrict by boolean if isinstance ( condition , bool ): return negate != condition # restrict by a mapping/dict -- convert to an AndList of string equality conditions if isinstance ( condition , collections . abc . Mapping ): common_attributes = set ( condition ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluates to True columns . update ( common_attributes ) return template % ( \"(\" + \") AND (\" . join ( \"` %s ` %s \" % ( k , \" IS NULL\" if condition [ k ] is None else f \"= { prep_value ( k , condition [ k ]) } \" , ) for k in common_attributes ) + \")\" ) # restrict by a numpy record -- convert to an AndList of string equality conditions if isinstance ( condition , numpy . void ): common_attributes = set ( condition . dtype . fields ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluate to True columns . update ( common_attributes ) return template % ( \"(\" + \") AND (\" . join ( \"` %s `= %s \" % ( k , prep_value ( k , condition [ k ])) for k in common_attributes ) + \")\" ) # restrict by a QueryExpression subclass -- trigger instantiation and move on if inspect . isclass ( condition ) and issubclass ( condition , QueryExpression ): condition = condition () # restrict by another expression (aka semijoin and antijoin) check_compatibility = True if isinstance ( condition , PromiscuousOperand ): condition = condition . operand check_compatibility = False if isinstance ( condition , QueryExpression ): if check_compatibility : assert_join_compatibility ( query_expression , condition ) common_attributes = [ q for q in condition . heading . names if q in query_expression . heading . names ] columns . update ( common_attributes ) if isinstance ( condition , Aggregation ): condition = condition . make_subquery () return ( # without common attributes, any non-empty set matches everything ( not negate if condition else negate ) if not common_attributes else \"( {fields} ) {not_} in ( {subquery} )\" . format ( fields = \"`\" + \"`,`\" . join ( common_attributes ) + \"`\" , not_ = \"not \" if negate else \"\" , subquery = condition . make_sql ( common_attributes ), ) ) # restrict by pandas.DataFrames if isinstance ( condition , pandas . DataFrame ): condition = condition . to_records () # convert to numpy.recarray and move on # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList try : or_list = [ make_condition ( query_expression , q , columns ) for q in condition ] except TypeError : raise DataJointError ( \"Invalid restriction type %r \" % condition ) else : or_list = [ item for item in or_list if item is not False ] # ignore False conditions if any ( item is True for item in or_list ): # if any item is True, entirely True return not negate return template % ( \"( %s )\" % \" OR \" . join ( or_list )) if or_list else negate", "title": "condition.py"}, {"location": "api/datajoint/condition/#datajoint.condition.AndList", "text": "Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction )", "title": "AndList"}, {"location": "api/datajoint/condition/#datajoint.condition.Not", "text": "invert restriction Source code in datajoint/condition.py 43 44 45 46 47 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction", "title": "Not"}, {"location": "api/datajoint/condition/#datajoint.condition.PromiscuousOperand", "text": "A container for an operand to ignore join compatibility Source code in datajoint/condition.py 14 15 16 17 18 19 20 class PromiscuousOperand : \"\"\" A container for an operand to ignore join compatibility \"\"\" def __init__ ( self , operand ): self . operand = operand", "title": "PromiscuousOperand"}, {"location": "api/datajoint/condition/#datajoint.condition.assert_join_compatibility", "text": "Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object Source code in datajoint/condition.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def assert_join_compatibility ( expr1 , expr2 ): \"\"\" Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object \"\"\" from .expression import QueryExpression , U for rel in ( expr1 , expr2 ): if not isinstance ( rel , ( U , QueryExpression )): raise DataJointError ( \"Object %r is not a QueryExpression and cannot be joined.\" % rel ) if not isinstance ( expr1 , U ) and not isinstance ( expr2 , U ): # dj.U is always compatible try : raise DataJointError ( \"Cannot join query expressions on dependent attribute ` %s `\" % next ( r for r in set ( expr1 . heading . secondary_attributes ) . intersection ( expr2 . heading . secondary_attributes ) ) ) except StopIteration : pass # all ok", "title": "assert_join_compatibility()"}, {"location": "api/datajoint/condition/#datajoint.condition.extract_column_names", "text": "extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. Source code in datajoint/condition.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def extract_column_names ( sql_expression ): \"\"\" extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. \"\"\" assert isinstance ( sql_expression , str ) result = set () s = sql_expression # for terseness # remove escaped quotes s = re . sub ( r \"( \\\\\\\" )|( \\\\ \\')\" , \"\" , s ) # remove quoted text s = re . sub ( r \"'[^']*'\" , \"\" , s ) s = re . sub ( r '\"[^\"]*\"' , \"\" , s ) # find all tokens in back quotes and remove them result . update ( re . findall ( r \"`([a-z][a-z_0-9]*)`\" , s )) s = re . sub ( r \"`[a-z][a-z_0-9]*`\" , \"\" , s ) # remove space before parentheses s = re . sub ( r \"\\s*\\(\" , \"(\" , s ) # remove tokens followed by ( since they must be functions s = re . sub ( r \"(\\b[a-z][a-z_0-9]*)\\(\" , \"(\" , s ) remaining_tokens = set ( re . findall ( r \"\\b[a-z][a-z_0-9]*\\b\" , s )) # update result removing reserved words result . update ( remaining_tokens - { \"is\" , \"in\" , \"between\" , \"like\" , \"and\" , \"or\" , \"null\" , \"not\" , \"interval\" , \"second\" , \"minute\" , \"hour\" , \"day\" , \"month\" , \"week\" , \"year\" , } ) return result", "title": "extract_column_names()"}, {"location": "api/datajoint/condition/#datajoint.condition.make_condition", "text": "Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. Source code in datajoint/condition.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def make_condition ( query_expression , condition , columns ): \"\"\" Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. \"\"\" from .expression import QueryExpression , Aggregation , U def prep_value ( k , v ): \"\"\"prepare value v for inclusion as a string in an SQL condition\"\"\" if query_expression . heading [ k ] . uuid : if not isinstance ( v , uuid . UUID ): try : v = uuid . UUID ( v ) except ( AttributeError , ValueError ): raise DataJointError ( \"Badly formed UUID {v} in restriction by ` {k} `\" . format ( k = k , v = v ) ) return \"X' %s '\" % v . bytes . hex () if isinstance ( v , ( datetime . date , datetime . datetime , datetime . time , decimal . Decimal ) ): return '\" %s \"' % v if isinstance ( v , str ): return '\" %s \"' % v . replace ( \"%\" , \" %% \" ) . replace ( \" \\\\ \" , \" \\\\\\\\ \" ) return \" %r \" % v negate = False while isinstance ( condition , Not ): negate = not negate condition = condition . restriction template = \"NOT ( %s )\" if negate else \" %s \" # restrict by string if isinstance ( condition , str ): columns . update ( extract_column_names ( condition )) return template % condition . strip () . replace ( \"%\" , \" %% \" ) # escape %, see issue #376 # restrict by AndList if isinstance ( condition , AndList ): # omit all conditions that evaluate to True items = [ item for item in ( make_condition ( query_expression , cond , columns ) for cond in condition ) if item is not True ] if any ( item is False for item in items ): return negate # if any item is False, the whole thing is False if not items : return not negate # and empty AndList is True return template % ( \"(\" + \") AND (\" . join ( items ) + \")\" ) # restriction by dj.U evaluates to True if isinstance ( condition , U ): return not negate # restrict by boolean if isinstance ( condition , bool ): return negate != condition # restrict by a mapping/dict -- convert to an AndList of string equality conditions if isinstance ( condition , collections . abc . Mapping ): common_attributes = set ( condition ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluates to True columns . update ( common_attributes ) return template % ( \"(\" + \") AND (\" . join ( \"` %s ` %s \" % ( k , \" IS NULL\" if condition [ k ] is None else f \"= { prep_value ( k , condition [ k ]) } \" , ) for k in common_attributes ) + \")\" ) # restrict by a numpy record -- convert to an AndList of string equality conditions if isinstance ( condition , numpy . void ): common_attributes = set ( condition . dtype . fields ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluate to True columns . update ( common_attributes ) return template % ( \"(\" + \") AND (\" . join ( \"` %s `= %s \" % ( k , prep_value ( k , condition [ k ])) for k in common_attributes ) + \")\" ) # restrict by a QueryExpression subclass -- trigger instantiation and move on if inspect . isclass ( condition ) and issubclass ( condition , QueryExpression ): condition = condition () # restrict by another expression (aka semijoin and antijoin) check_compatibility = True if isinstance ( condition , PromiscuousOperand ): condition = condition . operand check_compatibility = False if isinstance ( condition , QueryExpression ): if check_compatibility : assert_join_compatibility ( query_expression , condition ) common_attributes = [ q for q in condition . heading . names if q in query_expression . heading . names ] columns . update ( common_attributes ) if isinstance ( condition , Aggregation ): condition = condition . make_subquery () return ( # without common attributes, any non-empty set matches everything ( not negate if condition else negate ) if not common_attributes else \"( {fields} ) {not_} in ( {subquery} )\" . format ( fields = \"`\" + \"`,`\" . join ( common_attributes ) + \"`\" , not_ = \"not \" if negate else \"\" , subquery = condition . make_sql ( common_attributes ), ) ) # restrict by pandas.DataFrames if isinstance ( condition , pandas . DataFrame ): condition = condition . to_records () # convert to numpy.recarray and move on # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList try : or_list = [ make_condition ( query_expression , q , columns ) for q in condition ] except TypeError : raise DataJointError ( \"Invalid restriction type %r \" % condition ) else : or_list = [ item for item in or_list if item is not False ] # ignore False conditions if any ( item is True for item in or_list ): # if any item is True, entirely True return not negate return template % ( \"( %s )\" % \" OR \" . join ( or_list )) if or_list else negate", "title": "make_condition()"}, {"location": "api/datajoint/connection/", "text": "This module contains the Connection class that manages the connection to the database, and the conn function that provides access to a persistent connection in datajoint. Connection \u00b6 A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () cancel_transaction () \u00b6 Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) commit_transaction () \u00b6 Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) connect () \u00b6 Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) get_user () \u00b6 :return: the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] in_transaction () property \u00b6 :return: True if there is an open transaction. Source code in datajoint/connection.py 369 370 371 372 373 374 375 @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction is_connected () property \u00b6 Return true if the object is connected to the database server. Source code in datajoint/connection.py 278 279 280 281 282 283 284 285 @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True ping () \u00b6 Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) purge_query_cache () \u00b6 Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () query ( query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ) \u00b6 Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor set_query_cache ( query_cache = None ) \u00b6 When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache start_transaction () \u00b6 Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) transaction () property \u00b6 Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here Source code in datajoint/connection.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () EmulatedCursor \u00b6 acts like a cursor Source code in datajoint/connection.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class EmulatedCursor : \"\"\"acts like a cursor\"\"\" def __init__ ( self , data ): self . _data = data self . _iter = iter ( self . _data ) def __iter__ ( self ): return self def __next__ ( self ): return next ( self . _iter ) def fetchall ( self ): return self . _data def fetchone ( self ): return next ( self . _iter ) @property def rowcount ( self ): return len ( self . _data ) conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ) \u00b6 Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection translate_query_error ( client_error , query ) \u00b6 Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError Source code in datajoint/connection.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def translate_query_error ( client_error , query ): \"\"\" Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError \"\"\" logger . debug ( \"type: {} , args: {} \" . format ( type ( client_error ), client_error . args )) err , * args = client_error . args # Loss of connection errors if err in ( 0 , \"(0, '')\" ): return errors . LostConnectionError ( \"Server connection lost due to an interface error.\" , * args ) if err == 2006 : return errors . LostConnectionError ( \"Connection timed out\" , * args ) if err == 2013 : return errors . LostConnectionError ( \"Server connection lost\" , * args ) # Access errors if err in ( 1044 , 1142 ): return errors . AccessError ( \"Insufficient privileges.\" , args [ 0 ], query ) # Integrity errors if err == 1062 : return errors . DuplicateError ( * args ) if err == 1451 : return errors . IntegrityError ( * args ) if err == 1452 : return errors . IntegrityError ( * args ) # Syntax errors if err == 1064 : return errors . QuerySyntaxError ( args [ 0 ], query ) # Existence errors if err == 1146 : return errors . MissingTableError ( args [ 0 ], query ) if err == 1364 : return errors . MissingAttributeError ( * args ) if err == 1054 : return errors . UnknownAttributeError ( * args ) # all the other errors are re-raised in original form return client_error", "title": "connection.py"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection", "text": "A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "Connection"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.cancel_transaction", "text": "Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" )", "title": "cancel_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.commit_transaction", "text": "Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" )", "title": "commit_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.connect", "text": "Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True )", "title": "connect()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.get_user", "text": ":return: the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ]", "title": "get_user()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.in_transaction", "text": ":return: True if there is an open transaction. Source code in datajoint/connection.py 369 370 371 372 373 374 375 @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction", "title": "in_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.is_connected", "text": "Return true if the object is connected to the database server. Source code in datajoint/connection.py 278 279 280 281 282 283 284 285 @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True", "title": "is_connected()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.ping", "text": "Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False )", "title": "ping()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.purge_query_cache", "text": "Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink ()", "title": "purge_query_cache()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.query", "text": "Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor", "title": "query()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.set_query_cache", "text": "When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache", "title": "set_query_cache()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.start_transaction", "text": "Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" )", "title": "start_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.transaction", "text": "Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here Source code in datajoint/connection.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.EmulatedCursor", "text": "acts like a cursor Source code in datajoint/connection.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class EmulatedCursor : \"\"\"acts like a cursor\"\"\" def __init__ ( self , data ): self . _data = data self . _iter = iter ( self . _data ) def __iter__ ( self ): return self def __next__ ( self ): return next ( self . _iter ) def fetchall ( self ): return self . _data def fetchone ( self ): return next ( self . _iter ) @property def rowcount ( self ): return len ( self . _data )", "title": "EmulatedCursor"}, {"location": "api/datajoint/connection/#datajoint.connection.conn", "text": "Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection", "title": "conn()"}, {"location": "api/datajoint/connection/#datajoint.connection.translate_query_error", "text": "Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError Source code in datajoint/connection.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def translate_query_error ( client_error , query ): \"\"\" Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError \"\"\" logger . debug ( \"type: {} , args: {} \" . format ( type ( client_error ), client_error . args )) err , * args = client_error . args # Loss of connection errors if err in ( 0 , \"(0, '')\" ): return errors . LostConnectionError ( \"Server connection lost due to an interface error.\" , * args ) if err == 2006 : return errors . LostConnectionError ( \"Connection timed out\" , * args ) if err == 2013 : return errors . LostConnectionError ( \"Server connection lost\" , * args ) # Access errors if err in ( 1044 , 1142 ): return errors . AccessError ( \"Insufficient privileges.\" , args [ 0 ], query ) # Integrity errors if err == 1062 : return errors . DuplicateError ( * args ) if err == 1451 : return errors . IntegrityError ( * args ) if err == 1452 : return errors . IntegrityError ( * args ) # Syntax errors if err == 1064 : return errors . QuerySyntaxError ( args [ 0 ], query ) # Existence errors if err == 1146 : return errors . MissingTableError ( args [ 0 ], query ) if err == 1364 : return errors . MissingAttributeError ( * args ) if err == 1054 : return errors . UnknownAttributeError ( * args ) # all the other errors are re-raised in original form return client_error", "title": "translate_query_error()"}, {"location": "api/datajoint/declare/", "text": "This module hosts functions to convert DataJoint table definitions into mysql table definitions, and to declare the corresponding mysql tables. alter ( definition , old_definition , context ) \u00b6 :param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage Source code in datajoint/declare.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def alter ( definition , old_definition , context ): \"\"\" :param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage \"\"\" ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) ( table_comment_ , primary_key_ , attribute_sql_ , foreign_key_sql_ , index_sql_ , external_stores_ , ) = prepare_declare ( old_definition , context ) # analyze differences between declarations sql = list () if primary_key != primary_key_ : raise NotImplementedError ( \"table.alter cannot alter the primary key (yet).\" ) if foreign_key_sql != foreign_key_sql_ : raise NotImplementedError ( \"table.alter cannot alter foreign keys (yet).\" ) if index_sql != index_sql_ : raise NotImplementedError ( \"table.alter cannot alter indexes (yet)\" ) if attribute_sql != attribute_sql_ : sql . extend ( _make_attribute_alter ( attribute_sql , attribute_sql_ , primary_key )) if table_comment != table_comment_ : sql . append ( 'COMMENT=\" %s \"' % table_comment ) return sql , [ e for e in external_stores if e not in external_stores_ ] compile_attribute ( line , in_key , foreign_key_sql , context ) \u00b6 Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration Source code in datajoint/declare.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def compile_attribute ( line , in_key , foreign_key_sql , context ): \"\"\" Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration \"\"\" try : match = attribute_parser . parseString ( line + \"#\" , parseAll = True ) except pp . ParseException as err : raise DataJointError ( \"Declaration error in position {pos} in line: \\n {line} \\n {msg} \" . format ( line = err . args [ 0 ], pos = err . args [ 1 ], msg = err . args [ 2 ] ) ) match [ \"comment\" ] = match [ \"comment\" ] . rstrip ( \"#\" ) if \"default\" not in match : match [ \"default\" ] = \"\" match = { k : v . strip () for k , v in match . items ()} match [ \"nullable\" ] = match [ \"default\" ] . lower () == \"null\" if match [ \"nullable\" ]: if in_key : raise DataJointError ( 'Primary key attributes cannot be nullable in line \" %s \"' % line ) match [ \"default\" ] = \"DEFAULT NULL\" # nullable attributes default to null else : if match [ \"default\" ]: quote = ( match [ \"default\" ] . split ( \"(\" )[ 0 ] . upper () not in CONSTANT_LITERALS and match [ \"default\" ][ 0 ] not in \" \\\" '\" ) match [ \"default\" ] = ( \"NOT NULL DEFAULT \" + ( '\" %s \"' if quote else \" %s \" ) % match [ \"default\" ] ) else : match [ \"default\" ] = \"NOT NULL\" match [ \"comment\" ] = match [ \"comment\" ] . replace ( '\"' , ' \\\\ \"' ) # escape double quotes in comment if match [ \"comment\" ] . startswith ( \":\" ): raise DataJointError ( 'An attribute comment must not start with a colon in comment \" {comment} \"' . format ( ** match ) ) category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : match [ \"comment\" ] = \": {type} : {comment} \" . format ( ** match ) # insert custom type into comment substitute_special_type ( match , category , foreign_key_sql , context ) if category in SERIALIZED_TYPES and match [ \"default\" ] not in { \"DEFAULT NULL\" , \"NOT NULL\" , }: raise DataJointError ( \"The default value for a blob or attachment attributes can only be NULL in: \\n {line} \" . format ( line = line ) ) sql = ( \"` {name} ` {type} {default} \" + ( ' COMMENT \" {comment} \"' if match [ \"comment\" ] else \"\" ) ) . format ( ** match ) return match [ \"name\" ], sql , match . get ( \"store\" ) compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ) \u00b6 :param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. Source code in datajoint/declare.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ): \"\"\" :param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. \"\"\" # Parse and validate from .table import Table from .expression import QueryExpression obsolete = False # See issue #436. Old style to be deprecated in a future release try : result = foreign_key_parser . parseString ( line ) except pp . ParseException : try : result = foreign_key_parser_old . parseString ( line ) except pp . ParseBaseException as err : raise DataJointError ( 'Parsing error in line \" %s \". %s .' % ( line , err )) else : obsolete = True try : ref = eval ( result . ref_table , context ) except NameError if obsolete else Exception : raise DataJointError ( \"Foreign key reference %s could not be resolved\" % result . ref_table ) options = [ opt . upper () for opt in result . options ] for opt in options : # check for invalid options if opt not in { \"NULLABLE\" , \"UNIQUE\" }: raise DataJointError ( 'Invalid foreign key option \" {opt} \"' . format ( opt = opt )) is_nullable = \"NULLABLE\" in options is_unique = \"UNIQUE\" in options if is_nullable and primary_key is not None : raise DataJointError ( 'Primary dependencies cannot be nullable in line \" {line} \"' . format ( line = line ) ) if obsolete : logger . warning ( 'Line \" {line} \" uses obsolete syntax that will no longer be supported in datajoint 0.14. ' \"For details, see issue #780 https://github.com/datajoint/datajoint-python/issues/780\" . format ( line = line ) ) if not isinstance ( ref , type ) or not issubclass ( ref , Table ): raise DataJointError ( \"Foreign key reference %r must be a valid query\" % result . ref_table ) if isinstance ( ref , type ) and issubclass ( ref , Table ): ref = ref () # check that dependency is of a supported type if ( not isinstance ( ref , QueryExpression ) or len ( ref . restriction ) or len ( ref . support ) != 1 or not isinstance ( ref . support [ 0 ], str ) ): raise DataJointError ( 'Dependency \" %s \" is not supported (yet). Use a base table or its projection.' % result . ref_table ) if obsolete : # for backward compatibility with old-style dependency declarations. See issue #436 if not isinstance ( ref , Table ): DataJointError ( 'Dependency \" %s \" is not supported. Check documentation.' % result . ref_table ) if not all ( r in ref . primary_key for r in result . ref_attrs ): raise DataJointError ( 'Invalid foreign key attributes in \" %s \"' % line ) try : raise DataJointError ( 'Duplicate attributes \" {attr} \" in \" {line} \"' . format ( attr = next ( attr for attr in result . new_attrs if attr in attributes ), line = line , ) ) except StopIteration : pass # the normal outcome # Match the primary attributes of the referenced table to local attributes new_attrs = list ( result . new_attrs ) ref_attrs = list ( result . ref_attrs ) # special case, the renamed attribute is implicit if new_attrs and not ref_attrs : if len ( new_attrs ) != 1 : raise DataJointError ( 'Renamed foreign key must be mapped to the primary key in \" %s \"' % line ) if len ( ref . primary_key ) == 1 : # if the primary key has one attribute, allow implicit renaming ref_attrs = ref . primary_key else : # if only one primary key attribute remains, then allow implicit renaming ref_attrs = [ attr for attr in ref . primary_key if attr not in attributes ] if len ( ref_attrs ) != 1 : raise DataJointError ( 'Could not resolve which primary key attribute should be referenced in \" %s \"' % line ) if len ( new_attrs ) != len ( ref_attrs ): raise DataJointError ( 'Mismatched attributes in foreign key \" %s \"' % line ) if ref_attrs : # convert to projected dependency ref = ref . proj ( ** dict ( zip ( new_attrs , ref_attrs ))) # declare new foreign key attributes for attr in ref . primary_key : if attr not in attributes : attributes . append ( attr ) if primary_key is not None : primary_key . append ( attr ) attr_sql . append ( ref . heading [ attr ] . sql . replace ( \"NOT NULL \" , \"\" , int ( is_nullable )) ) # declare the foreign key foreign_key_sql . append ( \"FOREIGN KEY (` {fk} `) REFERENCES {ref} (` {pk} `) ON UPDATE CASCADE ON DELETE RESTRICT\" . format ( fk = \"`,`\" . join ( ref . primary_key ), pk = \"`,`\" . join ( ref . heading [ name ] . original_name for name in ref . primary_key ), ref = ref . support [ 0 ], ) ) # declare unique index if is_unique : index_sql . append ( \"UNIQUE INDEX ( {attrs} )\" . format ( attrs = \",\" . join ( \"` %s `\" % attr for attr in ref . primary_key ) ) ) declare ( full_table_name , definition , context ) \u00b6 Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used Source code in datajoint/declare.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 def declare ( full_table_name , definition , context ): \"\"\" Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used \"\"\" table_name = full_table_name . strip ( \"`\" ) . split ( \".\" )[ 1 ] if len ( table_name ) > MAX_TABLE_NAME_LENGTH : raise DataJointError ( \"Table name ` {name} ` exceeds the max length of {max_length} \" . format ( name = table_name , max_length = MAX_TABLE_NAME_LENGTH ) ) ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) if not primary_key : raise DataJointError ( \"Table must have a primary key\" ) return ( \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % full_table_name + \", \\n \" . join ( attribute_sql + [ \"PRIMARY KEY (`\" + \"`,`\" . join ( primary_key ) + \"`)\" ] + foreign_key_sql + index_sql ) + ' \\n ) ENGINE=InnoDB, COMMENT \" %s \"' % table_comment ), external_stores is_foreign_key ( line ) \u00b6 :param line: a line from the table definition :return: true if the line appears to be a foreign key definition Source code in datajoint/declare.py 153 154 155 156 157 158 159 160 def is_foreign_key ( line ): \"\"\" :param line: a line from the table definition :return: true if the line appears to be a foreign key definition \"\"\" arrow_position = line . find ( \"->\" ) return arrow_position >= 0 and not any ( c in line [: arrow_position ] for c in \" \\\" #'\" ) substitute_special_type ( match , category , foreign_key_sql , context ) \u00b6 :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters Source code in datajoint/declare.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 def substitute_special_type ( match , category , foreign_key_sql , context ): \"\"\" :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters \"\"\" if category == \"UUID\" : match [ \"type\" ] = UUID_DATA_TYPE elif category == \"INTERNAL_ATTACH\" : match [ \"type\" ] = \"LONGBLOB\" elif category in EXTERNAL_TYPES : if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) match [ \"store\" ] = match [ \"type\" ] . split ( \"@\" , 1 )[ 1 ] match [ \"type\" ] = UUID_DATA_TYPE foreign_key_sql . append ( \"FOREIGN KEY (` {name} `) REFERENCES `{{database}}`.` {external_table_root} _ {store} ` (`hash`) \" \"ON UPDATE RESTRICT ON DELETE RESTRICT\" . format ( external_table_root = EXTERNAL_TABLE_ROOT , ** match ) ) elif category == \"ADAPTED\" : adapter = get_adapter ( context , match [ \"type\" ]) match [ \"type\" ] = adapter . attribute_type category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : # recursive redefinition from user-defined datatypes. substitute_special_type ( match , category , foreign_key_sql , context ) else : assert False , \"Unknown special type\"", "title": "declare.py"}, {"location": "api/datajoint/declare/#datajoint.declare.alter", "text": ":param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage Source code in datajoint/declare.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def alter ( definition , old_definition , context ): \"\"\" :param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage \"\"\" ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) ( table_comment_ , primary_key_ , attribute_sql_ , foreign_key_sql_ , index_sql_ , external_stores_ , ) = prepare_declare ( old_definition , context ) # analyze differences between declarations sql = list () if primary_key != primary_key_ : raise NotImplementedError ( \"table.alter cannot alter the primary key (yet).\" ) if foreign_key_sql != foreign_key_sql_ : raise NotImplementedError ( \"table.alter cannot alter foreign keys (yet).\" ) if index_sql != index_sql_ : raise NotImplementedError ( \"table.alter cannot alter indexes (yet)\" ) if attribute_sql != attribute_sql_ : sql . extend ( _make_attribute_alter ( attribute_sql , attribute_sql_ , primary_key )) if table_comment != table_comment_ : sql . append ( 'COMMENT=\" %s \"' % table_comment ) return sql , [ e for e in external_stores if e not in external_stores_ ]", "title": "alter()"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_attribute", "text": "Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration Source code in datajoint/declare.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def compile_attribute ( line , in_key , foreign_key_sql , context ): \"\"\" Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration \"\"\" try : match = attribute_parser . parseString ( line + \"#\" , parseAll = True ) except pp . ParseException as err : raise DataJointError ( \"Declaration error in position {pos} in line: \\n {line} \\n {msg} \" . format ( line = err . args [ 0 ], pos = err . args [ 1 ], msg = err . args [ 2 ] ) ) match [ \"comment\" ] = match [ \"comment\" ] . rstrip ( \"#\" ) if \"default\" not in match : match [ \"default\" ] = \"\" match = { k : v . strip () for k , v in match . items ()} match [ \"nullable\" ] = match [ \"default\" ] . lower () == \"null\" if match [ \"nullable\" ]: if in_key : raise DataJointError ( 'Primary key attributes cannot be nullable in line \" %s \"' % line ) match [ \"default\" ] = \"DEFAULT NULL\" # nullable attributes default to null else : if match [ \"default\" ]: quote = ( match [ \"default\" ] . split ( \"(\" )[ 0 ] . upper () not in CONSTANT_LITERALS and match [ \"default\" ][ 0 ] not in \" \\\" '\" ) match [ \"default\" ] = ( \"NOT NULL DEFAULT \" + ( '\" %s \"' if quote else \" %s \" ) % match [ \"default\" ] ) else : match [ \"default\" ] = \"NOT NULL\" match [ \"comment\" ] = match [ \"comment\" ] . replace ( '\"' , ' \\\\ \"' ) # escape double quotes in comment if match [ \"comment\" ] . startswith ( \":\" ): raise DataJointError ( 'An attribute comment must not start with a colon in comment \" {comment} \"' . format ( ** match ) ) category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : match [ \"comment\" ] = \": {type} : {comment} \" . format ( ** match ) # insert custom type into comment substitute_special_type ( match , category , foreign_key_sql , context ) if category in SERIALIZED_TYPES and match [ \"default\" ] not in { \"DEFAULT NULL\" , \"NOT NULL\" , }: raise DataJointError ( \"The default value for a blob or attachment attributes can only be NULL in: \\n {line} \" . format ( line = line ) ) sql = ( \"` {name} ` {type} {default} \" + ( ' COMMENT \" {comment} \"' if match [ \"comment\" ] else \"\" ) ) . format ( ** match ) return match [ \"name\" ], sql , match . get ( \"store\" )", "title": "compile_attribute()"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_foreign_key", "text": ":param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. Source code in datajoint/declare.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ): \"\"\" :param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. \"\"\" # Parse and validate from .table import Table from .expression import QueryExpression obsolete = False # See issue #436. Old style to be deprecated in a future release try : result = foreign_key_parser . parseString ( line ) except pp . ParseException : try : result = foreign_key_parser_old . parseString ( line ) except pp . ParseBaseException as err : raise DataJointError ( 'Parsing error in line \" %s \". %s .' % ( line , err )) else : obsolete = True try : ref = eval ( result . ref_table , context ) except NameError if obsolete else Exception : raise DataJointError ( \"Foreign key reference %s could not be resolved\" % result . ref_table ) options = [ opt . upper () for opt in result . options ] for opt in options : # check for invalid options if opt not in { \"NULLABLE\" , \"UNIQUE\" }: raise DataJointError ( 'Invalid foreign key option \" {opt} \"' . format ( opt = opt )) is_nullable = \"NULLABLE\" in options is_unique = \"UNIQUE\" in options if is_nullable and primary_key is not None : raise DataJointError ( 'Primary dependencies cannot be nullable in line \" {line} \"' . format ( line = line ) ) if obsolete : logger . warning ( 'Line \" {line} \" uses obsolete syntax that will no longer be supported in datajoint 0.14. ' \"For details, see issue #780 https://github.com/datajoint/datajoint-python/issues/780\" . format ( line = line ) ) if not isinstance ( ref , type ) or not issubclass ( ref , Table ): raise DataJointError ( \"Foreign key reference %r must be a valid query\" % result . ref_table ) if isinstance ( ref , type ) and issubclass ( ref , Table ): ref = ref () # check that dependency is of a supported type if ( not isinstance ( ref , QueryExpression ) or len ( ref . restriction ) or len ( ref . support ) != 1 or not isinstance ( ref . support [ 0 ], str ) ): raise DataJointError ( 'Dependency \" %s \" is not supported (yet). Use a base table or its projection.' % result . ref_table ) if obsolete : # for backward compatibility with old-style dependency declarations. See issue #436 if not isinstance ( ref , Table ): DataJointError ( 'Dependency \" %s \" is not supported. Check documentation.' % result . ref_table ) if not all ( r in ref . primary_key for r in result . ref_attrs ): raise DataJointError ( 'Invalid foreign key attributes in \" %s \"' % line ) try : raise DataJointError ( 'Duplicate attributes \" {attr} \" in \" {line} \"' . format ( attr = next ( attr for attr in result . new_attrs if attr in attributes ), line = line , ) ) except StopIteration : pass # the normal outcome # Match the primary attributes of the referenced table to local attributes new_attrs = list ( result . new_attrs ) ref_attrs = list ( result . ref_attrs ) # special case, the renamed attribute is implicit if new_attrs and not ref_attrs : if len ( new_attrs ) != 1 : raise DataJointError ( 'Renamed foreign key must be mapped to the primary key in \" %s \"' % line ) if len ( ref . primary_key ) == 1 : # if the primary key has one attribute, allow implicit renaming ref_attrs = ref . primary_key else : # if only one primary key attribute remains, then allow implicit renaming ref_attrs = [ attr for attr in ref . primary_key if attr not in attributes ] if len ( ref_attrs ) != 1 : raise DataJointError ( 'Could not resolve which primary key attribute should be referenced in \" %s \"' % line ) if len ( new_attrs ) != len ( ref_attrs ): raise DataJointError ( 'Mismatched attributes in foreign key \" %s \"' % line ) if ref_attrs : # convert to projected dependency ref = ref . proj ( ** dict ( zip ( new_attrs , ref_attrs ))) # declare new foreign key attributes for attr in ref . primary_key : if attr not in attributes : attributes . append ( attr ) if primary_key is not None : primary_key . append ( attr ) attr_sql . append ( ref . heading [ attr ] . sql . replace ( \"NOT NULL \" , \"\" , int ( is_nullable )) ) # declare the foreign key foreign_key_sql . append ( \"FOREIGN KEY (` {fk} `) REFERENCES {ref} (` {pk} `) ON UPDATE CASCADE ON DELETE RESTRICT\" . format ( fk = \"`,`\" . join ( ref . primary_key ), pk = \"`,`\" . join ( ref . heading [ name ] . original_name for name in ref . primary_key ), ref = ref . support [ 0 ], ) ) # declare unique index if is_unique : index_sql . append ( \"UNIQUE INDEX ( {attrs} )\" . format ( attrs = \",\" . join ( \"` %s `\" % attr for attr in ref . primary_key ) ) )", "title": "compile_foreign_key()"}, {"location": "api/datajoint/declare/#datajoint.declare.declare", "text": "Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used Source code in datajoint/declare.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 def declare ( full_table_name , definition , context ): \"\"\" Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used \"\"\" table_name = full_table_name . strip ( \"`\" ) . split ( \".\" )[ 1 ] if len ( table_name ) > MAX_TABLE_NAME_LENGTH : raise DataJointError ( \"Table name ` {name} ` exceeds the max length of {max_length} \" . format ( name = table_name , max_length = MAX_TABLE_NAME_LENGTH ) ) ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) if not primary_key : raise DataJointError ( \"Table must have a primary key\" ) return ( \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % full_table_name + \", \\n \" . join ( attribute_sql + [ \"PRIMARY KEY (`\" + \"`,`\" . join ( primary_key ) + \"`)\" ] + foreign_key_sql + index_sql ) + ' \\n ) ENGINE=InnoDB, COMMENT \" %s \"' % table_comment ), external_stores", "title": "declare()"}, {"location": "api/datajoint/declare/#datajoint.declare.is_foreign_key", "text": ":param line: a line from the table definition :return: true if the line appears to be a foreign key definition Source code in datajoint/declare.py 153 154 155 156 157 158 159 160 def is_foreign_key ( line ): \"\"\" :param line: a line from the table definition :return: true if the line appears to be a foreign key definition \"\"\" arrow_position = line . find ( \"->\" ) return arrow_position >= 0 and not any ( c in line [: arrow_position ] for c in \" \\\" #'\" )", "title": "is_foreign_key()"}, {"location": "api/datajoint/declare/#datajoint.declare.substitute_special_type", "text": ":param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters Source code in datajoint/declare.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 def substitute_special_type ( match , category , foreign_key_sql , context ): \"\"\" :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters \"\"\" if category == \"UUID\" : match [ \"type\" ] = UUID_DATA_TYPE elif category == \"INTERNAL_ATTACH\" : match [ \"type\" ] = \"LONGBLOB\" elif category in EXTERNAL_TYPES : if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) match [ \"store\" ] = match [ \"type\" ] . split ( \"@\" , 1 )[ 1 ] match [ \"type\" ] = UUID_DATA_TYPE foreign_key_sql . append ( \"FOREIGN KEY (` {name} `) REFERENCES `{{database}}`.` {external_table_root} _ {store} ` (`hash`) \" \"ON UPDATE RESTRICT ON DELETE RESTRICT\" . format ( external_table_root = EXTERNAL_TABLE_ROOT , ** match ) ) elif category == \"ADAPTED\" : adapter = get_adapter ( context , match [ \"type\" ]) match [ \"type\" ] = adapter . attribute_type category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : # recursive redefinition from user-defined datatypes. substitute_special_type ( match , category , foreign_key_sql , context ) else : assert False , \"Unknown special type\"", "title": "substitute_special_type()"}, {"location": "api/datajoint/dependencies/", "text": "Dependencies \u00b6 Bases: nx . DiGraph The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 Source code in datajoint/dependencies.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Dependencies ( nx . DiGraph ): \"\"\" The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 \"\"\" def __init__ ( self , connection = None ): self . _conn = connection self . _node_alias_count = itertools . count () self . _loaded = False super () . __init__ ( self ) def clear ( self ): self . _loaded = False super () . clear () def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) ) ancestors ( full_table_name ) \u00b6 :param full_table_name: In form schema . table_name :return: all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) ) children ( table_name , primary = None ) \u00b6 :param table_name: schema . table :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys Source code in datajoint/dependencies.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } descendants ( full_table_name ) \u00b6 :param full_table_name: In form schema . table_name :return: all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 164 165 166 167 168 169 170 171 172 173 def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) load ( force = True ) \u00b6 Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. Source code in datajoint/dependencies.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True parents ( table_name , primary = None ) \u00b6 :param table_name: schema . table :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table Source code in datajoint/dependencies.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } unite_master_parts ( lst ) \u00b6 re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( [' s . a ', ' s . a__q ', ' s . b ', ' s . c ', ' s . c__q ', ' s . b__q ', ' s . d ', ' s . a__r ']) -> [' s . a ', ' s . a__q ', ' s . a__r ', ' s . b ', ' s . b__q ', ' s . c ', ' s . c__q ', ' s . d '] Source code in datajoint/dependencies.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def unite_master_parts ( lst ): \"\"\" re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -> ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`'] \"\"\" for i in range ( 2 , len ( lst )): name = lst [ i ] match = re . match ( r \"(?P<master>`\\w+`.`#?\\w+)__\\w+`\" , name ) if match : # name is a part table master = match . group ( \"master\" ) for j in range ( i - 1 , - 1 , - 1 ): if lst [ j ] == master + \"`\" or lst [ j ] . startswith ( master + \"__\" ): # move from the ith position to the (j+1)th position lst [ j + 1 : i + 1 ] = [ name ] + lst [ j + 1 : i ] break return lst", "title": "dependencies.py"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies", "text": "Bases: nx . DiGraph The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 Source code in datajoint/dependencies.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Dependencies ( nx . DiGraph ): \"\"\" The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 \"\"\" def __init__ ( self , connection = None ): self . _conn = connection self . _node_alias_count = itertools . count () self . _loaded = False super () . __init__ ( self ) def clear ( self ): self . _loaded = False super () . clear () def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) )", "title": "Dependencies"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.ancestors", "text": ":param full_table_name: In form schema . table_name :return: all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) )", "title": "ancestors()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.children", "text": ":param table_name: schema . table :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys Source code in datajoint/dependencies.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary }", "title": "children()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.descendants", "text": ":param full_table_name: In form schema . table_name :return: all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 164 165 166 167 168 169 170 171 172 173 def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) )", "title": "descendants()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.load", "text": "Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. Source code in datajoint/dependencies.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True", "title": "load()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.parents", "text": ":param table_name: schema . table :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table Source code in datajoint/dependencies.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary }", "title": "parents()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.unite_master_parts", "text": "re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( [' s . a ', ' s . a__q ', ' s . b ', ' s . c ', ' s . c__q ', ' s . b__q ', ' s . d ', ' s . a__r ']) -> [' s . a ', ' s . a__q ', ' s . a__r ', ' s . b ', ' s . b__q ', ' s . c ', ' s . c__q ', ' s . d '] Source code in datajoint/dependencies.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def unite_master_parts ( lst ): \"\"\" re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -> ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`'] \"\"\" for i in range ( 2 , len ( lst )): name = lst [ i ] match = re . match ( r \"(?P<master>`\\w+`.`#?\\w+)__\\w+`\" , name ) if match : # name is a part table master = match . group ( \"master\" ) for j in range ( i - 1 , - 1 , - 1 ): if lst [ j ] == master + \"`\" or lst [ j ] . startswith ( master + \"__\" ): # move from the ith position to the (j+1)th position lst [ j + 1 : i + 1 ] = [ name ] + lst [ j + 1 : i ] break return lst", "title": "unite_master_parts()"}, {"location": "api/datajoint/diagram/", "text": "Diagram \u00b6 Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = ( cls () . describe ( context = self . context , printout = False ) . split ( \" \\n \" ) ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs ) __add__ ( arg ) \u00b6 :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. Source code in datajoint/diagram.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self __mul__ ( arg ) \u00b6 Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. Source code in datajoint/diagram.py 250 251 252 253 254 255 256 257 258 def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self __sub__ ( arg ) \u00b6 :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. Source code in datajoint/diagram.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self add_parts () \u00b6 Adds to the diagram the part tables of tables already included in the diagram :return: Source code in datajoint/diagram.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self from_sequence ( sequence ) classmethod \u00b6 The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 146 147 148 149 150 151 152 153 154 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) topological_sort () \u00b6 :return: list of nodes in topological order Source code in datajoint/diagram.py 183 184 185 186 187 188 189 190 191 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "diagram.py"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram", "text": "Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base relation object, a base relation class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = ( cls () . describe ( context = self . context , printout = False ) . split ( \" \\n \" ) ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs )", "title": "Diagram"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.__add__", "text": ":param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. Source code in datajoint/diagram.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self", "title": "__add__()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.__mul__", "text": "Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. Source code in datajoint/diagram.py 250 251 252 253 254 255 256 257 258 def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self", "title": "__mul__()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.__sub__", "text": ":param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. Source code in datajoint/diagram.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self", "title": "__sub__()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.add_parts", "text": "Adds to the diagram the part tables of tables already included in the diagram :return: Source code in datajoint/diagram.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self", "title": "add_parts()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.from_sequence", "text": "The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 146 147 148 149 150 151 152 153 154 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence ))", "title": "from_sequence()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.topological_sort", "text": ":return: list of nodes in topological order Source code in datajoint/diagram.py 183 184 185 186 187 188 189 190 191 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "topological_sort()"}, {"location": "api/datajoint/errors/", "text": "Exception classes for the DataJoint library AccessError \u00b6 Bases: QueryError User access error: insufficient privileges. Source code in datajoint/errors.py 64 65 66 67 class AccessError ( QueryError ): \"\"\" User access error: insufficient privileges. \"\"\" BucketInaccessible \u00b6 Bases: DataJointError Error raised when a S3 bucket is inaccessible Source code in datajoint/errors.py 106 107 108 109 class BucketInaccessible ( DataJointError ): \"\"\" Error raised when a S3 bucket is inaccessible \"\"\" DataJointError \u00b6 Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) suggest ( * args ) \u00b6 regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) DuplicateError \u00b6 Bases: QueryError An integrity error caused by a duplicate entry into a unique key Source code in datajoint/errors.py 76 77 78 79 class DuplicateError ( QueryError ): \"\"\" An integrity error caused by a duplicate entry into a unique key \"\"\" IntegrityError \u00b6 Bases: QueryError An integrity error triggered by foreign key constraints Source code in datajoint/errors.py 82 83 84 85 class IntegrityError ( QueryError ): \"\"\" An integrity error triggered by foreign key constraints \"\"\" LostConnectionError \u00b6 Bases: DataJointError Loss of server connection Source code in datajoint/errors.py 45 46 47 48 class LostConnectionError ( DataJointError ): \"\"\" Loss of server connection \"\"\" MissingAttributeError \u00b6 Bases: QueryError An error arising when a required attribute value is not provided in INSERT Source code in datajoint/errors.py 94 95 96 97 class MissingAttributeError ( QueryError ): \"\"\" An error arising when a required attribute value is not provided in INSERT \"\"\" MissingExternalFile \u00b6 Bases: DataJointError Error raised when an external file managed by DataJoint is no longer accessible Source code in datajoint/errors.py 100 101 102 103 class MissingExternalFile ( DataJointError ): \"\"\" Error raised when an external file managed by DataJoint is no longer accessible \"\"\" MissingTableError \u00b6 Bases: DataJointError Query on a table that has not been declared Source code in datajoint/errors.py 70 71 72 73 class MissingTableError ( DataJointError ): \"\"\" Query on a table that has not been declared \"\"\" QueryError \u00b6 Bases: DataJointError Errors arising from queries to the database Source code in datajoint/errors.py 51 52 53 54 class QueryError ( DataJointError ): \"\"\" Errors arising from queries to the database \"\"\" QuerySyntaxError \u00b6 Bases: QueryError Errors arising from incorrect query syntax Source code in datajoint/errors.py 58 59 60 61 class QuerySyntaxError ( QueryError ): \"\"\" Errors arising from incorrect query syntax \"\"\" UnknownAttributeError \u00b6 Bases: QueryError User requests an attribute name not found in query heading Source code in datajoint/errors.py 88 89 90 91 class UnknownAttributeError ( QueryError ): \"\"\" User requests an attribute name not found in query heading \"\"\"", "title": "errors.py"}, {"location": "api/datajoint/errors/#datajoint.errors.AccessError", "text": "Bases: QueryError User access error: insufficient privileges. Source code in datajoint/errors.py 64 65 66 67 class AccessError ( QueryError ): \"\"\" User access error: insufficient privileges. \"\"\"", "title": "AccessError"}, {"location": "api/datajoint/errors/#datajoint.errors.BucketInaccessible", "text": "Bases: DataJointError Error raised when a S3 bucket is inaccessible Source code in datajoint/errors.py 106 107 108 109 class BucketInaccessible ( DataJointError ): \"\"\" Error raised when a S3 bucket is inaccessible \"\"\"", "title": "BucketInaccessible"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError", "text": "Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "DataJointError"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError.suggest", "text": "regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "suggest()"}, {"location": "api/datajoint/errors/#datajoint.errors.DuplicateError", "text": "Bases: QueryError An integrity error caused by a duplicate entry into a unique key Source code in datajoint/errors.py 76 77 78 79 class DuplicateError ( QueryError ): \"\"\" An integrity error caused by a duplicate entry into a unique key \"\"\"", "title": "DuplicateError"}, {"location": "api/datajoint/errors/#datajoint.errors.IntegrityError", "text": "Bases: QueryError An integrity error triggered by foreign key constraints Source code in datajoint/errors.py 82 83 84 85 class IntegrityError ( QueryError ): \"\"\" An integrity error triggered by foreign key constraints \"\"\"", "title": "IntegrityError"}, {"location": "api/datajoint/errors/#datajoint.errors.LostConnectionError", "text": "Bases: DataJointError Loss of server connection Source code in datajoint/errors.py 45 46 47 48 class LostConnectionError ( DataJointError ): \"\"\" Loss of server connection \"\"\"", "title": "LostConnectionError"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingAttributeError", "text": "Bases: QueryError An error arising when a required attribute value is not provided in INSERT Source code in datajoint/errors.py 94 95 96 97 class MissingAttributeError ( QueryError ): \"\"\" An error arising when a required attribute value is not provided in INSERT \"\"\"", "title": "MissingAttributeError"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingExternalFile", "text": "Bases: DataJointError Error raised when an external file managed by DataJoint is no longer accessible Source code in datajoint/errors.py 100 101 102 103 class MissingExternalFile ( DataJointError ): \"\"\" Error raised when an external file managed by DataJoint is no longer accessible \"\"\"", "title": "MissingExternalFile"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingTableError", "text": "Bases: DataJointError Query on a table that has not been declared Source code in datajoint/errors.py 70 71 72 73 class MissingTableError ( DataJointError ): \"\"\" Query on a table that has not been declared \"\"\"", "title": "MissingTableError"}, {"location": "api/datajoint/errors/#datajoint.errors.QueryError", "text": "Bases: DataJointError Errors arising from queries to the database Source code in datajoint/errors.py 51 52 53 54 class QueryError ( DataJointError ): \"\"\" Errors arising from queries to the database \"\"\"", "title": "QueryError"}, {"location": "api/datajoint/errors/#datajoint.errors.QuerySyntaxError", "text": "Bases: QueryError Errors arising from incorrect query syntax Source code in datajoint/errors.py 58 59 60 61 class QuerySyntaxError ( QueryError ): \"\"\" Errors arising from incorrect query syntax \"\"\"", "title": "QuerySyntaxError"}, {"location": "api/datajoint/errors/#datajoint.errors.UnknownAttributeError", "text": "Bases: QueryError User requests an attribute name not found in query heading Source code in datajoint/errors.py 88 89 90 91 class UnknownAttributeError ( QueryError ): \"\"\" User requests an attribute name not found in query heading \"\"\"", "title": "UnknownAttributeError"}, {"location": "api/datajoint/expression/", "text": "Aggregation \u00b6 Bases: QueryExpression Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. Source code in datajoint/expression.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 class Aggregation ( QueryExpression ): \"\"\" Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. \"\"\" _left_restrict = None # the pre-GROUP BY conditions for the WHERE clause _subquery_alias_count = count () @classmethod def create ( cls , arg , group , keep_all_rows = False ): if inspect . isclass ( group ) and issubclass ( group , QueryExpression ): group = group () # instantiate if a class assert isinstance ( group , QueryExpression ) if keep_all_rows and len ( group . support ) > 1 or group . heading . new_attributes : group = group . make_subquery () # subquery if left joining a join join = arg . join ( group , left = keep_all_rows ) # reuse the join logic result = cls () result . _connection = join . connection result . _heading = join . heading . set_primary_key ( arg . primary_key ) # use left operand's primary key result . _support = join . support result . _left = join . _left result . _left_restrict = join . restriction # WHERE clause applied before GROUP BY result . _grouping_attributes = result . primary_key return result def where_clause ( self ): return ( \"\" if not self . _left_restrict else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . _left_restrict ) ) def make_sql ( self , fields = None ): fields = self . heading . as_sql ( fields or self . heading . names ) assert self . _grouping_attributes or not self . restriction distinct = set ( self . heading . names ) == set ( self . primary_key ) return \"SELECT {distinct}{fields} FROM {from_}{where}{group_by} \" . format ( distinct = \"DISTINCT \" if distinct else \"\" , fields = fields , from_ = self . from_clause (), where = self . where_clause (), group_by = \"\" if not self . primary_key else ( \" GROUP BY ` %s `\" % \"`,`\" . join ( self . _grouping_attributes ) + ( \"\" if not self . restriction else \" HAVING ( %s )\" % \")AND(\" . join ( self . restriction ) ) ), ) def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( self . _subquery_alias_count ) ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) ) QueryExpression \u00b6 QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property heading (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property support is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when A restriction is applied on any computed or renamed attributes A projection is applied remapping remapped attributes Subclasses: Join, Aggregation, and Union have additional specific rules. Source code in datajoint/expression.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 class QueryExpression : \"\"\" QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property `heading` (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property `support` is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. \"\"\" _restriction = None _restriction_attributes = None _left = [] # list of booleans True for left joins, False for inner joins _original_heading = None # heading before projections # subclasses or instantiators must provide values _connection = None _heading = None _support = None # If the query will be using distinct _distinct = False @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes @property def primary_key ( self ): return self . heading . primary_key _subquery_alias_count = count () # count for alias names used in the FROM clause def from_clause ( self ): support = ( \"(\" + src . make_sql () + \") as `$ %x `\" % next ( self . _subquery_alias_count ) if isinstance ( src , QueryExpression ) else src for src in self . support ) clause = next ( support ) for s , left in zip ( support , self . _left ): clause += \" NATURAL {left} JOIN {clause} \" . format ( left = \" LEFT\" if left else \"\" , clause = s ) return clause def where_clause ( self ): return ( \"\" if not self . restriction else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . restriction ) ) def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) # --------- query operators ----------- def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result def restrict_in_place ( self , restriction ): self . __dict__ . update ( self . restrict ( restriction ) . __dict__ ) def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction ) def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction )) def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction )) def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self ) def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other ) def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False ) def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other ) def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) aggregate = aggr # alias for aggr # ---------- Fetch operators -------------------- @property def fetch1 ( self ): return Fetch1 ( self ) @property def fetch ( self ): return Fetch ( self ) def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] ) def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self ) def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () ) def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) def _repr_html_ ( self ): \"\"\":return: HTML to display table in Jupyter notebook.\"\"\" return repr_html ( self ) __add__ ( other ) \u00b6 union e.g. q1 + q2 . Source code in datajoint/expression.py 320 321 322 def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other ) __and__ ( restriction ) \u00b6 Restriction operator e.g. q1 & q2 . :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 219 220 221 222 223 224 225 def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction ) __bool__ () \u00b6 :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. bool(q1) . Source code in datajoint/expression.py 552 553 554 555 556 557 558 559 560 561 562 563 def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] ) __contains__ ( item ) \u00b6 returns True if the restriction in item matches any entries in self e.g. restriction in q1 . :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. Source code in datajoint/expression.py 565 566 567 568 569 570 571 572 573 574 def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query __iter__ () \u00b6 returns an iterator-compatible QueryExpression object e.g. iter(q1) . :param self: iterator-compatible QueryExpression object Source code in datajoint/expression.py 576 577 578 579 580 581 582 583 584 def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self __len__ () \u00b6 :return: number of elements in the result set e.g. len(q1) . Source code in datajoint/expression.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ] __matmul__ ( other ) \u00b6 Permissive join of query expressions self and other ignoring compatibility check e.g. q1 @ q2 . Source code in datajoint/expression.py 261 262 263 264 265 266 267 268 def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False ) __mul__ ( other ) \u00b6 join of query expressions self and other e.g. q1 * q2 . Source code in datajoint/expression.py 255 256 257 258 259 def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other ) __neg__ () \u00b6 Convert between restriction and inverted restriction e.g. -q1 . :return: target restriction See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 245 246 247 248 249 250 251 252 253 def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self ) __next__ () \u00b6 returns the next record on an iterator-compatible QueryExpression object e.g. next(q1) . :param self: A query expression :type self: :class: QueryExpression :rtype: dict Source code in datajoint/expression.py 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self ) __repr__ () \u00b6 returns the string representation of a QueryExpression object e.g. str(q1) . :param self: A query expression :type self: :class: QueryExpression :rtype: str Source code in datajoint/expression.py 631 632 633 634 635 636 637 638 639 640 641 642 643 def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () ) __sub__ ( restriction ) \u00b6 Inverted restriction e.g. q1 - q2 . :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 237 238 239 240 241 242 243 def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction )) __xor__ ( restriction ) \u00b6 Permissive restriction operator ignoring compatibility check e.g. q1 ^ q2 . Source code in datajoint/expression.py 227 228 229 230 231 232 233 234 235 def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction )) aggr ( group , * attributes , keep_all_rows = False , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) connection () property \u00b6 a dj.Connection object Source code in datajoint/expression.py 58 59 60 61 62 @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection cursor ( offset = 0 , limit = None , order_by = None , as_dict = False ) \u00b6 See expression.fetch() for input description. :return: query cursor Source code in datajoint/expression.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) head ( limit = 25 , ** fetch_kwargs ) \u00b6 shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result Source code in datajoint/expression.py 512 513 514 515 516 517 518 519 520 521 def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) heading () property \u00b6 a dj.Heading object, reflects the effects of the projection operator .proj Source code in datajoint/expression.py 70 71 72 73 @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading join ( other , semantic_check = True , left = False ) \u00b6 create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. Source code in datajoint/expression.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result make_sql ( fields = None ) \u00b6 Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes Source code in datajoint/expression.py 121 122 123 124 125 126 127 128 129 130 131 132 def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) make_subquery () \u00b6 create a new SELECT statement where self is the FROM clause Source code in datajoint/expression.py 135 136 137 138 139 140 141 def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result original_heading () property \u00b6 a dj.Heading object reflecting the attributes before projection Source code in datajoint/expression.py 75 76 77 78 @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading preview ( limit = None , width = None ) \u00b6 :return: a string of preview of the contents of the query. Source code in datajoint/expression.py 645 646 647 def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) proj ( * attributes , ** named_attributes ) \u00b6 Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. Source code in datajoint/expression.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result restrict ( restriction ) \u00b6 Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. Source code in datajoint/expression.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result restriction () property \u00b6 a AndList object of restrictions applied to input to produce the result Source code in datajoint/expression.py 80 81 82 83 84 85 @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction restriction_attributes () property \u00b6 the set of attribute names invoked in the WHERE clause Source code in datajoint/expression.py 87 88 89 90 91 92 @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes support () property \u00b6 A list of table names or subqueries to from the FROM clause Source code in datajoint/expression.py 64 65 66 67 68 @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support tail ( limit = 25 , ** fetch_kwargs ) \u00b6 shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result Source code in datajoint/expression.py 523 524 525 526 527 528 529 530 531 532 def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] U \u00b6 dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr __mul__ ( other ) \u00b6 shorthand for join Source code in datajoint/expression.py 908 909 910 def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) aggr ( group , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) join ( other , left = False ) \u00b6 Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result Union \u00b6 Bases: QueryExpression Union is the private DataJoint class that implements the union operator. Source code in datajoint/expression.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 class Union ( QueryExpression ): \"\"\" Union is the private DataJoint class that implements the union operator. \"\"\" __count = count () @classmethod def create ( cls , arg1 , arg2 ): if inspect . isclass ( arg2 ) and issubclass ( arg2 , QueryExpression ): arg2 = arg2 () # instantiate if a class if not isinstance ( arg2 , QueryExpression ): raise DataJointError ( \"A QueryExpression can only be unioned with another QueryExpression\" ) if arg1 . connection != arg2 . connection : raise DataJointError ( \"Cannot operate on QueryExpressions originating from different connections.\" ) if set ( arg1 . primary_key ) != set ( arg2 . primary_key ): raise DataJointError ( \"The operands of a union must share the same primary key.\" ) if set ( arg1 . heading . secondary_attributes ) & set ( arg2 . heading . secondary_attributes ): raise DataJointError ( \"The operands of a union must not share any secondary attributes.\" ) result = cls () result . _connection = arg1 . connection result . _heading = arg1 . heading . join ( arg2 . heading ) result . _support = [ arg1 , arg2 ] return result def make_sql ( self ): arg1 , arg2 = self . _support if ( not arg1 . heading . secondary_attributes and not arg2 . heading . secondary_attributes ): # no secondary attributes: use UNION DISTINCT fields = arg1 . primary_key return \"SELECT * FROM (( {sql1} ) UNION ( {sql2} )) as `_u {alias} `\" . format ( sql1 = arg1 . make_sql () if isinstance ( arg1 , Union ) else arg1 . make_sql ( fields ), sql2 = arg2 . make_sql () if isinstance ( arg2 , Union ) else arg2 . make_sql ( fields ), alias = next ( self . __count ), ) # with secondary attributes, use union of left join with antijoin fields = self . heading . names sql1 = arg1 . join ( arg2 , left = True ) . make_sql ( fields ) sql2 = ( ( arg2 - arg1 ) . proj ( ... , ** { k : \"NULL\" for k in arg1 . heading . secondary_attributes }) . make_sql ( fields ) ) return \"( {sql1} ) UNION ( {sql2} )\" . format ( sql1 = sql1 , sql2 = sql2 ) def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( QueryExpression . _subquery_alias_count ), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) ) from_clause () \u00b6 The union does not use a FROM clause Source code in datajoint/expression.py 790 791 792 def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False where_clause () \u00b6 The union does not use a WHERE clause Source code in datajoint/expression.py 794 795 796 def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False", "title": "expression.py"}, {"location": "api/datajoint/expression/#datajoint.expression.Aggregation", "text": "Bases: QueryExpression Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. Source code in datajoint/expression.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 class Aggregation ( QueryExpression ): \"\"\" Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. \"\"\" _left_restrict = None # the pre-GROUP BY conditions for the WHERE clause _subquery_alias_count = count () @classmethod def create ( cls , arg , group , keep_all_rows = False ): if inspect . isclass ( group ) and issubclass ( group , QueryExpression ): group = group () # instantiate if a class assert isinstance ( group , QueryExpression ) if keep_all_rows and len ( group . support ) > 1 or group . heading . new_attributes : group = group . make_subquery () # subquery if left joining a join join = arg . join ( group , left = keep_all_rows ) # reuse the join logic result = cls () result . _connection = join . connection result . _heading = join . heading . set_primary_key ( arg . primary_key ) # use left operand's primary key result . _support = join . support result . _left = join . _left result . _left_restrict = join . restriction # WHERE clause applied before GROUP BY result . _grouping_attributes = result . primary_key return result def where_clause ( self ): return ( \"\" if not self . _left_restrict else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . _left_restrict ) ) def make_sql ( self , fields = None ): fields = self . heading . as_sql ( fields or self . heading . names ) assert self . _grouping_attributes or not self . restriction distinct = set ( self . heading . names ) == set ( self . primary_key ) return \"SELECT {distinct}{fields} FROM {from_}{where}{group_by} \" . format ( distinct = \"DISTINCT \" if distinct else \"\" , fields = fields , from_ = self . from_clause (), where = self . where_clause (), group_by = \"\" if not self . primary_key else ( \" GROUP BY ` %s `\" % \"`,`\" . join ( self . _grouping_attributes ) + ( \"\" if not self . restriction else \" HAVING ( %s )\" % \")AND(\" . join ( self . restriction ) ) ), ) def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( self . _subquery_alias_count ) ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) )", "title": "Aggregation"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression", "text": "QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property heading (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property support is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when A restriction is applied on any computed or renamed attributes A projection is applied remapping remapped attributes Subclasses: Join, Aggregation, and Union have additional specific rules. Source code in datajoint/expression.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 class QueryExpression : \"\"\" QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property `heading` (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property `support` is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. \"\"\" _restriction = None _restriction_attributes = None _left = [] # list of booleans True for left joins, False for inner joins _original_heading = None # heading before projections # subclasses or instantiators must provide values _connection = None _heading = None _support = None # If the query will be using distinct _distinct = False @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes @property def primary_key ( self ): return self . heading . primary_key _subquery_alias_count = count () # count for alias names used in the FROM clause def from_clause ( self ): support = ( \"(\" + src . make_sql () + \") as `$ %x `\" % next ( self . _subquery_alias_count ) if isinstance ( src , QueryExpression ) else src for src in self . support ) clause = next ( support ) for s , left in zip ( support , self . _left ): clause += \" NATURAL {left} JOIN {clause} \" . format ( left = \" LEFT\" if left else \"\" , clause = s ) return clause def where_clause ( self ): return ( \"\" if not self . restriction else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . restriction ) ) def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) # --------- query operators ----------- def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result def restrict_in_place ( self , restriction ): self . __dict__ . update ( self . restrict ( restriction ) . __dict__ ) def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction ) def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction )) def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction )) def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self ) def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other ) def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False ) def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other ) def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) aggregate = aggr # alias for aggr # ---------- Fetch operators -------------------- @property def fetch1 ( self ): return Fetch1 ( self ) @property def fetch ( self ): return Fetch ( self ) def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] ) def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self ) def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () ) def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) def _repr_html_ ( self ): \"\"\":return: HTML to display table in Jupyter notebook.\"\"\" return repr_html ( self )", "title": "QueryExpression"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__add__", "text": "union e.g. q1 + q2 . Source code in datajoint/expression.py 320 321 322 def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other )", "title": "__add__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__and__", "text": "Restriction operator e.g. q1 & q2 . :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 219 220 221 222 223 224 225 def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction )", "title": "__and__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__bool__", "text": ":return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. bool(q1) . Source code in datajoint/expression.py 552 553 554 555 556 557 558 559 560 561 562 563 def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] )", "title": "__bool__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__contains__", "text": "returns True if the restriction in item matches any entries in self e.g. restriction in q1 . :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. Source code in datajoint/expression.py 565 566 567 568 569 570 571 572 573 574 def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query", "title": "__contains__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__iter__", "text": "returns an iterator-compatible QueryExpression object e.g. iter(q1) . :param self: iterator-compatible QueryExpression object Source code in datajoint/expression.py 576 577 578 579 580 581 582 583 584 def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self", "title": "__iter__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__len__", "text": ":return: number of elements in the result set e.g. len(q1) . Source code in datajoint/expression.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ]", "title": "__len__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__matmul__", "text": "Permissive join of query expressions self and other ignoring compatibility check e.g. q1 @ q2 . Source code in datajoint/expression.py 261 262 263 264 265 266 267 268 def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False )", "title": "__matmul__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__mul__", "text": "join of query expressions self and other e.g. q1 * q2 . Source code in datajoint/expression.py 255 256 257 258 259 def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other )", "title": "__mul__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__neg__", "text": "Convert between restriction and inverted restriction e.g. -q1 . :return: target restriction See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 245 246 247 248 249 250 251 252 253 def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self )", "title": "__neg__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__next__", "text": "returns the next record on an iterator-compatible QueryExpression object e.g. next(q1) . :param self: A query expression :type self: :class: QueryExpression :rtype: dict Source code in datajoint/expression.py 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self )", "title": "__next__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__repr__", "text": "returns the string representation of a QueryExpression object e.g. str(q1) . :param self: A query expression :type self: :class: QueryExpression :rtype: str Source code in datajoint/expression.py 631 632 633 634 635 636 637 638 639 640 641 642 643 def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () )", "title": "__repr__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__sub__", "text": "Inverted restriction e.g. q1 - q2 . :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. Source code in datajoint/expression.py 237 238 239 240 241 242 243 def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction ))", "title": "__sub__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.__xor__", "text": "Permissive restriction operator ignoring compatibility check e.g. q1 ^ q2 . Source code in datajoint/expression.py 227 228 229 230 231 232 233 234 235 def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction ))", "title": "__xor__()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.connection", "text": "a dj.Connection object Source code in datajoint/expression.py 58 59 60 61 62 @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection", "title": "connection()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.cursor", "text": "See expression.fetch() for input description. :return: query cursor Source code in datajoint/expression.py 616 617 618 619 620 621 622 623 624 625 626 627 628 629 def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict )", "title": "cursor()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.head", "text": "shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result Source code in datajoint/expression.py 512 513 514 515 516 517 518 519 520 521 def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs )", "title": "head()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.heading", "text": "a dj.Heading object, reflects the effects of the projection operator .proj Source code in datajoint/expression.py 70 71 72 73 @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading", "title": "heading()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.join", "text": "create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. Source code in datajoint/expression.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result", "title": "join()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_sql", "text": "Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes Source code in datajoint/expression.py 121 122 123 124 125 126 127 128 129 130 131 132 def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), )", "title": "make_sql()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_subquery", "text": "create a new SELECT statement where self is the FROM clause Source code in datajoint/expression.py 135 136 137 138 139 140 141 def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result", "title": "make_subquery()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.original_heading", "text": "a dj.Heading object reflecting the attributes before projection Source code in datajoint/expression.py 75 76 77 78 @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading", "title": "original_heading()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.preview", "text": ":return: a string of preview of the contents of the query. Source code in datajoint/expression.py 645 646 647 def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width )", "title": "preview()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.proj", "text": "Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. Source code in datajoint/expression.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result", "title": "proj()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restrict", "text": "Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. Source code in datajoint/expression.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result", "title": "restrict()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction", "text": "a AndList object of restrictions applied to input to produce the result Source code in datajoint/expression.py 80 81 82 83 84 85 @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction", "title": "restriction()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction_attributes", "text": "the set of attribute names invoked in the WHERE clause Source code in datajoint/expression.py 87 88 89 90 91 92 @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes", "title": "restriction_attributes()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.support", "text": "A list of table names or subqueries to from the FROM clause Source code in datajoint/expression.py 64 65 66 67 68 @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support", "title": "support()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.tail", "text": "shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result Source code in datajoint/expression.py 523 524 525 526 527 528 529 530 531 532 def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ]", "title": "tail()"}, {"location": "api/datajoint/expression/#datajoint.expression.U", "text": "dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr", "title": "U"}, {"location": "api/datajoint/expression/#datajoint.expression.U.__mul__", "text": "shorthand for join Source code in datajoint/expression.py 908 909 910 def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other )", "title": "__mul__()"}, {"location": "api/datajoint/expression/#datajoint.expression.U.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression Source code in datajoint/expression.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/expression/#datajoint.expression.U.join", "text": "Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result", "title": "join()"}, {"location": "api/datajoint/expression/#datajoint.expression.Union", "text": "Bases: QueryExpression Union is the private DataJoint class that implements the union operator. Source code in datajoint/expression.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 class Union ( QueryExpression ): \"\"\" Union is the private DataJoint class that implements the union operator. \"\"\" __count = count () @classmethod def create ( cls , arg1 , arg2 ): if inspect . isclass ( arg2 ) and issubclass ( arg2 , QueryExpression ): arg2 = arg2 () # instantiate if a class if not isinstance ( arg2 , QueryExpression ): raise DataJointError ( \"A QueryExpression can only be unioned with another QueryExpression\" ) if arg1 . connection != arg2 . connection : raise DataJointError ( \"Cannot operate on QueryExpressions originating from different connections.\" ) if set ( arg1 . primary_key ) != set ( arg2 . primary_key ): raise DataJointError ( \"The operands of a union must share the same primary key.\" ) if set ( arg1 . heading . secondary_attributes ) & set ( arg2 . heading . secondary_attributes ): raise DataJointError ( \"The operands of a union must not share any secondary attributes.\" ) result = cls () result . _connection = arg1 . connection result . _heading = arg1 . heading . join ( arg2 . heading ) result . _support = [ arg1 , arg2 ] return result def make_sql ( self ): arg1 , arg2 = self . _support if ( not arg1 . heading . secondary_attributes and not arg2 . heading . secondary_attributes ): # no secondary attributes: use UNION DISTINCT fields = arg1 . primary_key return \"SELECT * FROM (( {sql1} ) UNION ( {sql2} )) as `_u {alias} `\" . format ( sql1 = arg1 . make_sql () if isinstance ( arg1 , Union ) else arg1 . make_sql ( fields ), sql2 = arg2 . make_sql () if isinstance ( arg2 , Union ) else arg2 . make_sql ( fields ), alias = next ( self . __count ), ) # with secondary attributes, use union of left join with antijoin fields = self . heading . names sql1 = arg1 . join ( arg2 , left = True ) . make_sql ( fields ) sql2 = ( ( arg2 - arg1 ) . proj ( ... , ** { k : \"NULL\" for k in arg1 . heading . secondary_attributes }) . make_sql ( fields ) ) return \"( {sql1} ) UNION ( {sql2} )\" . format ( sql1 = sql1 , sql2 = sql2 ) def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( QueryExpression . _subquery_alias_count ), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) )", "title": "Union"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.from_clause", "text": "The union does not use a FROM clause Source code in datajoint/expression.py 790 791 792 def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False", "title": "from_clause()"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.where_clause", "text": "The union does not use a WHERE clause Source code in datajoint/expression.py 794 795 796 def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False", "title": "where_clause()"}, {"location": "api/datajoint/external/", "text": "ExternalMapping \u00b6 Bases: Mapping The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] Source code in datajoint/external.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class ExternalMapping ( Mapping ): \"\"\" The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] \"\"\" def __init__ ( self , schema ): self . schema = schema self . _tables = {} def __repr__ ( self ): return \"External file tables for schema ` {schema} `: \\n \" . format ( schema = self . schema . database ) + \" \\n \" . join ( '\" {store} \" {protocol} : {location} ' . format ( store = k , ** v . spec ) for k , v in self . items () ) def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ] def __len__ ( self ): return len ( self . _tables ) def __iter__ ( self ): return iter ( self . _tables ) __getitem__ ( store ) \u00b6 Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store Source code in datajoint/external.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ] ExternalTable \u00b6 Bases: Table The table tracking externally stored objects. Declare as ExternalTable(connection, database) Source code in datajoint/external.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 class ExternalTable ( Table ): \"\"\" The table tracking externally stored objects. Declare as ExternalTable(connection, database) \"\"\" def __init__ ( self , connection , store , database ): self . store = store self . spec = config . get_store_spec ( store ) self . _s3 = None self . database = database self . _connection = connection self . _heading = Heading ( table_info = dict ( conn = connection , database = database , table_name = self . table_name , context = None , ) ) self . _support = [ self . full_table_name ] if not self . is_declared : self . declare () self . _s3 = None if self . spec [ \"protocol\" ] == \"file\" and not Path ( self . spec [ \"location\" ]) . is_dir (): raise FileNotFoundError ( \"Inaccessible local directory %s \" % self . spec [ \"location\" ] ) from None @property def definition ( self ): return \"\"\" # external storage tracking hash : uuid # hash of contents (blob), of filename + contents (attach), or relative filepath (filepath) --- size :bigint unsigned # size of object in bytes attachment_name=null : varchar(255) # the filename of an attachment filepath=null : varchar(1000) # relative filepath or attachment filename contents_hash=null : uuid # used for the filepath datatype timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" @property def table_name ( self ): return f \" { EXTERNAL_TABLE_ROOT } _ { self . store } \" @property def s3 ( self ): if self . _s3 is None : self . _s3 = s3 . Folder ( ** self . spec ) return self . _s3 # - low-level operations - private def _make_external_filepath ( self , relative_filepath ): \"\"\"resolve the complete external path based on the relative path\"\"\" # Strip root if self . spec [ \"protocol\" ] == \"s3\" : posix_path = PurePosixPath ( PureWindowsPath ( self . spec [ \"location\" ])) location_path = ( Path ( * posix_path . parts [ 1 :]) if len ( self . spec [ \"location\" ]) > 0 and any ( case in posix_path . parts [ 0 ] for case in ( \" \\\\ \" , \":\" )) else Path ( posix_path ) ) return PurePosixPath ( location_path , relative_filepath ) # Preserve root elif self . spec [ \"protocol\" ] == \"file\" : return PurePosixPath ( Path ( self . spec [ \"location\" ]), relative_filepath ) else : assert False def _make_uuid_path ( self , uuid , suffix = \"\" ): \"\"\"create external path based on the uuid hash\"\"\" return self . _make_external_filepath ( PurePosixPath ( self . database , \"/\" . join ( subfold ( uuid . hex , self . spec [ \"subfolding\" ])), uuid . hex , ) . with_suffix ( suffix ) ) def _upload_file ( self , local_path , external_path , metadata = None ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fput ( local_path , external_path , metadata ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( local_path , external_path , overwrite = True ) else : assert False def _download_file ( self , external_path , download_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fget ( external_path , download_path ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( external_path , download_path ) else : assert False def _upload_buffer ( self , buffer , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . put ( external_path , buffer ) elif self . spec [ \"protocol\" ] == \"file\" : safe_write ( external_path , buffer ) else : assert False def _download_buffer ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . get ( external_path ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_path ) . read_bytes () assert False def _remove_external_file ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . remove_object ( external_path ) elif self . spec [ \"protocol\" ] == \"file\" : try : Path ( external_path ) . unlink () except FileNotFoundError : pass def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False # --- BLOBS ---- def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob # --- ATTACHMENTS --- def upload_attachment ( self , local_path ): attachment_name = Path ( local_path ) . name uuid = uuid_from_file ( local_path , init_string = attachment_name + \" \\0 \" ) external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _upload_file ( local_path , external_path ) # insert tracking info self . connection . query ( \"\"\" INSERT INTO {tab} (hash, size, attachment_name) VALUES (%s, {size}, \"{attachment_name}\") ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\" . format ( tab = self . full_table_name , size = Path ( local_path ) . stat () . st_size , attachment_name = attachment_name , ), args = [ uuid . bytes ], ) return uuid def get_attachment_name ( self , uuid ): return ( self & { \"hash\" : uuid }) . fetch1 ( \"attachment_name\" ) def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) # --- FILEPATH --- def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash # --- UTILITIES --- @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) ) def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list delete ( * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True ) \u00b6 :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors Source code in datajoint/external.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list download_attachment ( uuid , attachment_name , download_path ) \u00b6 save attachment from memory buffer into the save_path Source code in datajoint/external.py 245 246 247 248 def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) download_filepath ( filepath_hash ) \u00b6 sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones Source code in datajoint/external.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash exists ( external_filepath ) \u00b6 :return: True if the external file is accessible Source code in datajoint/external.py 156 157 158 159 160 161 162 163 164 def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False fetch_external_paths ( ** fetch_kwargs ) \u00b6 generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch Source code in datajoint/external.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths get ( uuid ) \u00b6 get an object from external store. Source code in datajoint/external.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob put ( blob ) \u00b6 put a binary string (blob) in external store Source code in datajoint/external.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid references () property \u00b6 :return: generator of referencing table names and their referencing columns Source code in datajoint/external.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) ) unused () \u00b6 query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema Source code in datajoint/external.py 388 389 390 391 392 393 394 395 396 397 398 399 def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] upload_filepath ( local_filepath ) \u00b6 Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur Source code in datajoint/external.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid used () \u00b6 query expression for used hashes :return: self restricted to elements that in use by tables in the schema Source code in datajoint/external.py 401 402 403 404 405 406 407 408 409 410 411 412 def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] subfold ( name , folds ) \u00b6 subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] Source code in datajoint/external.py 23 24 25 26 27 28 29 30 31 def subfold ( name , folds ): \"\"\" subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] \"\"\" return ( ( name [: folds [ 0 ]] . lower (),) + subfold ( name [ folds [ 0 ] :], folds [ 1 :]) if folds else () )", "title": "external.py"}, {"location": "api/datajoint/external/#datajoint.external.ExternalMapping", "text": "Bases: Mapping The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] Source code in datajoint/external.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class ExternalMapping ( Mapping ): \"\"\" The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] \"\"\" def __init__ ( self , schema ): self . schema = schema self . _tables = {} def __repr__ ( self ): return \"External file tables for schema ` {schema} `: \\n \" . format ( schema = self . schema . database ) + \" \\n \" . join ( '\" {store} \" {protocol} : {location} ' . format ( store = k , ** v . spec ) for k , v in self . items () ) def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ] def __len__ ( self ): return len ( self . _tables ) def __iter__ ( self ): return iter ( self . _tables )", "title": "ExternalMapping"}, {"location": "api/datajoint/external/#datajoint.external.ExternalMapping.__getitem__", "text": "Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store Source code in datajoint/external.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ]", "title": "__getitem__()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable", "text": "Bases: Table The table tracking externally stored objects. Declare as ExternalTable(connection, database) Source code in datajoint/external.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 class ExternalTable ( Table ): \"\"\" The table tracking externally stored objects. Declare as ExternalTable(connection, database) \"\"\" def __init__ ( self , connection , store , database ): self . store = store self . spec = config . get_store_spec ( store ) self . _s3 = None self . database = database self . _connection = connection self . _heading = Heading ( table_info = dict ( conn = connection , database = database , table_name = self . table_name , context = None , ) ) self . _support = [ self . full_table_name ] if not self . is_declared : self . declare () self . _s3 = None if self . spec [ \"protocol\" ] == \"file\" and not Path ( self . spec [ \"location\" ]) . is_dir (): raise FileNotFoundError ( \"Inaccessible local directory %s \" % self . spec [ \"location\" ] ) from None @property def definition ( self ): return \"\"\" # external storage tracking hash : uuid # hash of contents (blob), of filename + contents (attach), or relative filepath (filepath) --- size :bigint unsigned # size of object in bytes attachment_name=null : varchar(255) # the filename of an attachment filepath=null : varchar(1000) # relative filepath or attachment filename contents_hash=null : uuid # used for the filepath datatype timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" @property def table_name ( self ): return f \" { EXTERNAL_TABLE_ROOT } _ { self . store } \" @property def s3 ( self ): if self . _s3 is None : self . _s3 = s3 . Folder ( ** self . spec ) return self . _s3 # - low-level operations - private def _make_external_filepath ( self , relative_filepath ): \"\"\"resolve the complete external path based on the relative path\"\"\" # Strip root if self . spec [ \"protocol\" ] == \"s3\" : posix_path = PurePosixPath ( PureWindowsPath ( self . spec [ \"location\" ])) location_path = ( Path ( * posix_path . parts [ 1 :]) if len ( self . spec [ \"location\" ]) > 0 and any ( case in posix_path . parts [ 0 ] for case in ( \" \\\\ \" , \":\" )) else Path ( posix_path ) ) return PurePosixPath ( location_path , relative_filepath ) # Preserve root elif self . spec [ \"protocol\" ] == \"file\" : return PurePosixPath ( Path ( self . spec [ \"location\" ]), relative_filepath ) else : assert False def _make_uuid_path ( self , uuid , suffix = \"\" ): \"\"\"create external path based on the uuid hash\"\"\" return self . _make_external_filepath ( PurePosixPath ( self . database , \"/\" . join ( subfold ( uuid . hex , self . spec [ \"subfolding\" ])), uuid . hex , ) . with_suffix ( suffix ) ) def _upload_file ( self , local_path , external_path , metadata = None ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fput ( local_path , external_path , metadata ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( local_path , external_path , overwrite = True ) else : assert False def _download_file ( self , external_path , download_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fget ( external_path , download_path ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( external_path , download_path ) else : assert False def _upload_buffer ( self , buffer , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . put ( external_path , buffer ) elif self . spec [ \"protocol\" ] == \"file\" : safe_write ( external_path , buffer ) else : assert False def _download_buffer ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . get ( external_path ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_path ) . read_bytes () assert False def _remove_external_file ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . remove_object ( external_path ) elif self . spec [ \"protocol\" ] == \"file\" : try : Path ( external_path ) . unlink () except FileNotFoundError : pass def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False # --- BLOBS ---- def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob # --- ATTACHMENTS --- def upload_attachment ( self , local_path ): attachment_name = Path ( local_path ) . name uuid = uuid_from_file ( local_path , init_string = attachment_name + \" \\0 \" ) external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _upload_file ( local_path , external_path ) # insert tracking info self . connection . query ( \"\"\" INSERT INTO {tab} (hash, size, attachment_name) VALUES (%s, {size}, \"{attachment_name}\") ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\" . format ( tab = self . full_table_name , size = Path ( local_path ) . stat () . st_size , attachment_name = attachment_name , ), args = [ uuid . bytes ], ) return uuid def get_attachment_name ( self , uuid ): return ( self & { \"hash\" : uuid }) . fetch1 ( \"attachment_name\" ) def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) # --- FILEPATH --- def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash # --- UTILITIES --- @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) ) def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list", "title": "ExternalTable"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.delete", "text": ":param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors Source code in datajoint/external.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list", "title": "delete()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_attachment", "text": "save attachment from memory buffer into the save_path Source code in datajoint/external.py 245 246 247 248 def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path )", "title": "download_attachment()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_filepath", "text": "sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones Source code in datajoint/external.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash", "title": "download_filepath()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.exists", "text": ":return: True if the external file is accessible Source code in datajoint/external.py 156 157 158 159 160 161 162 163 164 def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False", "title": "exists()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.fetch_external_paths", "text": "generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch Source code in datajoint/external.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths", "title": "fetch_external_paths()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.get", "text": "get an object from external store. Source code in datajoint/external.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob", "title": "get()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.put", "text": "put a binary string (blob) in external store Source code in datajoint/external.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid", "title": "put()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.references", "text": ":return: generator of referencing table names and their referencing columns Source code in datajoint/external.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) )", "title": "references()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.unused", "text": "query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema Source code in datajoint/external.py 388 389 390 391 392 393 394 395 396 397 398 399 def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ]", "title": "unused()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.upload_filepath", "text": "Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur Source code in datajoint/external.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid", "title": "upload_filepath()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.used", "text": "query expression for used hashes :return: self restricted to elements that in use by tables in the schema Source code in datajoint/external.py 401 402 403 404 405 406 407 408 409 410 411 412 def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ]", "title": "used()"}, {"location": "api/datajoint/external/#datajoint.external.subfold", "text": "subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] Source code in datajoint/external.py 23 24 25 26 27 28 29 30 31 def subfold ( name , folds ): \"\"\" subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] \"\"\" return ( ( name [: folds [ 0 ]] . lower (),) + subfold ( name [ folds [ 0 ] :], folds [ 1 :]) if folds else () )", "title": "subfold()"}, {"location": "api/datajoint/fetch/", "text": "Fetch \u00b6 A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. Source code in datajoint/fetch.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class Fetch : \"\"\" A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret __call__ ( * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = '.' ) \u00b6 Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list Source code in datajoint/fetch.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret Fetch1 \u00b6 Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. Source code in datajoint/fetch.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class Fetch1 : \"\"\" Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret __call__ ( * attrs , squeeze = False , download_path = '.' ) \u00b6 Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict Source code in datajoint/fetch.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret key \u00b6 object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 18 19 20 21 22 23 24 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass to_dicts ( recarray ) \u00b6 convert record array to a dictionaries Source code in datajoint/fetch.py 31 32 33 34 def to_dicts ( recarray ): \"\"\"convert record array to a dictionaries\"\"\" for rec in recarray : yield dict ( zip ( recarray . dtype . names , rec . tolist ()))", "title": "fetch.py"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch", "text": "A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. Source code in datajoint/fetch.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class Fetch : \"\"\" A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret", "title": "Fetch"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch.__call__", "text": "Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list Source code in datajoint/fetch.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this relation. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the relation in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret", "title": "__call__()"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch1", "text": "Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. Source code in datajoint/fetch.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class Fetch1 : \"\"\" Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret", "title": "Fetch1"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch1.__call__", "text": "Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict Source code in datajoint/fetch.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the relation in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret", "title": "__call__()"}, {"location": "api/datajoint/fetch/#datajoint.fetch.key", "text": "object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 18 19 20 21 22 23 24 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass", "title": "key"}, {"location": "api/datajoint/fetch/#datajoint.fetch.to_dicts", "text": "convert record array to a dictionaries Source code in datajoint/fetch.py 31 32 33 34 def to_dicts ( recarray ): \"\"\"convert record array to a dictionaries\"\"\" for rec in recarray : yield dict ( zip ( recarray . dtype . names , rec . tolist ()))", "title": "to_dicts()"}, {"location": "api/datajoint/hash/", "text": "key_hash ( mapping ) \u00b6 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest () uuid_from_stream ( stream , * , init_string = '' ) \u00b6 :return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum Source code in datajoint/hash.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def uuid_from_stream ( stream , * , init_string = \"\" ): \"\"\" :return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum \"\"\" hashed = hashlib . md5 ( init_string . encode ()) chunk = True chunk_size = 1 << 14 while chunk : chunk = stream . read ( chunk_size ) hashed . update ( chunk ) return uuid . UUID ( bytes = hashed . digest ())", "title": "hash.py"}, {"location": "api/datajoint/hash/#datajoint.hash.key_hash", "text": "32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest ()", "title": "key_hash()"}, {"location": "api/datajoint/hash/#datajoint.hash.uuid_from_stream", "text": ":return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum Source code in datajoint/hash.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def uuid_from_stream ( stream , * , init_string = \"\" ): \"\"\" :return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum \"\"\" hashed = hashlib . md5 ( init_string . encode ()) chunk = True chunk_size = 1 << 14 while chunk : chunk = stream . read ( chunk_size ) hashed . update ( chunk ) return uuid . UUID ( bytes = hashed . digest ())", "title": "uuid_from_stream()"}, {"location": "api/datajoint/heading/", "text": "Attribute \u00b6 Bases: namedtuple ( _Attribute , default_attribute_properties ) Properties of a table column (attribute) Source code in datajoint/heading.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class Attribute ( namedtuple ( \"_Attribute\" , default_attribute_properties )): \"\"\" Properties of a table column (attribute) \"\"\" def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment ) @property def original_name ( self ): if self . attribute_expression is None : return self . name assert self . attribute_expression . startswith ( \"`\" ) return self . attribute_expression . strip ( \"`\" ) sql () property \u00b6 Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration Source code in datajoint/heading.py 64 65 66 67 68 69 70 71 72 73 74 75 @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment ) sql_comment () property \u00b6 :return: full comment for the SQL declaration. Includes custom type specification Source code in datajoint/heading.py 59 60 61 62 @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment sql_type () property \u00b6 :return: datatype (as string) in database. In most cases, it is the same as self.type Source code in datajoint/heading.py 54 55 56 57 @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type todict () \u00b6 Convert namedtuple to dict. Source code in datajoint/heading.py 50 51 52 def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) Heading \u00b6 Local class for relations' headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. Source code in datajoint/heading.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class Heading : \"\"\" Local class for relations' headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. \"\"\" def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) ) def __len__ ( self ): return 0 if self . attributes is None else len ( self . attributes ) @property def table_status ( self ): if self . table_info is None : return None if self . _table_status is None : self . _init_from_database () return self . _table_status @property def attributes ( self ): if self . _attributes is None : self . _init_from_database () # lazy loading from database return self . _attributes @property def names ( self ): return [ k for k in self . attributes ] @property def primary_key ( self ): return [ k for k , v in self . attributes . items () if v . in_key ] @property def secondary_attributes ( self ): return [ k for k , v in self . attributes . items () if not v . in_key ] @property def blobs ( self ): return [ k for k , v in self . attributes . items () if v . is_blob ] @property def non_blobs ( self ): return [ k for k , v in self . attributes . items () if not v . is_blob and not v . is_attachment and not v . is_filepath ] @property def new_attributes ( self ): return [ k for k , v in self . attributes . items () if v . attribute_expression is not None ] def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ] def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret @property def has_autoincrement ( self ): return any ( e . autoincrement for e in self . attributes . values ()) @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) ) def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) def __iter__ ( self ): return iter ( self . attributes ) def _init_from_database ( self ): \"\"\"initialize heading from an existing database table.\"\"\" conn , database , table_name , context = ( self . table_info [ k ] for k in ( \"conn\" , \"database\" , \"table_name\" , \"context\" ) ) info = conn . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE name=\" {table_name} \"' . format ( table_name = table_name , database = database ), as_dict = True , ) . fetchone () if info is None : if table_name == \"~log\" : logger . warning ( \"Could not create the ~log table\" ) return raise DataJointError ( \"The table ` {database} `.` {table_name} ` is not defined.\" . format ( table_name = table_name , database = database ) ) self . _table_status = { k . lower (): v for k , v in info . items ()} cur = conn . query ( \"SHOW FULL COLUMNS FROM ` {table_name} ` IN ` {database} `\" . format ( table_name = table_name , database = database ), as_dict = True , ) attributes = cur . fetchall () rename_map = { \"Field\" : \"name\" , \"Type\" : \"type\" , \"Null\" : \"nullable\" , \"Default\" : \"default\" , \"Key\" : \"in_key\" , \"Comment\" : \"comment\" , } fields_to_drop = ( \"Privileges\" , \"Collation\" ) # rename and drop attributes attributes = [ { rename_map [ k ] if k in rename_map else k : v for k , v in x . items () if k not in fields_to_drop } for x in attributes ] numeric_types = { ( \"float\" , False ): np . float64 , ( \"float\" , True ): np . float64 , ( \"double\" , False ): np . float64 , ( \"double\" , True ): np . float64 , ( \"tinyint\" , False ): np . int64 , ( \"tinyint\" , True ): np . int64 , ( \"smallint\" , False ): np . int64 , ( \"smallint\" , True ): np . int64 , ( \"mediumint\" , False ): np . int64 , ( \"mediumint\" , True ): np . int64 , ( \"int\" , False ): np . int64 , ( \"int\" , True ): np . int64 , ( \"bigint\" , False ): np . int64 , ( \"bigint\" , True ): np . uint64 , } sql_literals = [ \"CURRENT_TIMESTAMP\" ] # additional attribute properties for attr in attributes : attr . update ( in_key = ( attr [ \"in_key\" ] == \"PRI\" ), database = database , nullable = attr [ \"nullable\" ] == \"YES\" , autoincrement = bool ( re . search ( r \"auto_increment\" , attr [ \"Extra\" ], flags = re . I ) ), numeric = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"DECIMAL\" , \"INTEGER\" , \"FLOAT\" ) ), string = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"ENUM\" , \"TEMPORAL\" , \"STRING\" ) ), is_blob = bool ( TYPE_PATTERN [ \"INTERNAL_BLOB\" ] . match ( attr [ \"type\" ])), uuid = False , is_attachment = False , is_filepath = False , adapter = None , store = None , is_external = False , attribute_expression = None , ) if any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"INTEGER\" , \"FLOAT\" )): attr [ \"type\" ] = re . sub ( r \"\\(\\d+\\)\" , \"\" , attr [ \"type\" ], count = 1 ) # strip size off integers and floats attr [ \"unsupported\" ] = not any ( ( attr [ \"is_blob\" ], attr [ \"numeric\" ], attr [ \"numeric\" ]) ) attr . pop ( \"Extra\" ) # process custom DataJoint types special = re . match ( r \":(?P<type>[^:]+):(?P<comment>.*)\" , attr [ \"comment\" ]) if special : special = special . groupdict () attr . update ( special ) # process adapted attribute types if special and TYPE_PATTERN [ \"ADAPTED\" ] . match ( attr [ \"type\" ]): assert context is not None , \"Declaration context is not set\" adapter_name = special [ \"type\" ] try : attr . update ( adapter = get_adapter ( context , adapter_name )) except DataJointError : # if no adapter, then delay the error until the first invocation attr . update ( adapter = AttributeAdapter ()) else : attr . update ( type = attr [ \"adapter\" ] . attribute_type ) if not any ( r . match ( attr [ \"type\" ]) for r in TYPE_PATTERN . values ()): raise DataJointError ( \"Invalid attribute type ' {type} ' in adapter object < {adapter_name} >.\" . format ( adapter_name = adapter_name , ** attr ) ) special = not any ( TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) for c in NATIVE_TYPES ) if special : try : category = next ( c for c in SPECIAL_TYPES if TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) ) except StopIteration : if attr [ \"type\" ] . startswith ( \"external\" ): url = ( \"https://docs.datajoint.io/python/admin/5-blob-config.html\" \"#migration-between-datajoint-v0-11-and-v0-12\" ) raise DataJointError ( \"Legacy datatype ` {type} `. Migrate your external stores to \" \"datajoint 0.12: {url} \" . format ( url = url , ** attr ) ) raise DataJointError ( \"Unknown attribute type ` {type} `\" . format ( ** attr ) ) if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) attr . update ( unsupported = False , is_attachment = category in ( \"INTERNAL_ATTACH\" , \"EXTERNAL_ATTACH\" ), is_filepath = category == \"FILEPATH\" , # INTERNAL_BLOB is not a custom type but is included for completeness is_blob = category in ( \"INTERNAL_BLOB\" , \"EXTERNAL_BLOB\" ), uuid = category == \"UUID\" , is_external = category in EXTERNAL_TYPES , store = attr [ \"type\" ] . split ( \"@\" )[ 1 ] if category in EXTERNAL_TYPES else None , ) if attr [ \"in_key\" ] and any ( ( attr [ \"is_blob\" ], attr [ \"is_attachment\" ], attr [ \"is_filepath\" ]) ): raise DataJointError ( \"Blob, attachment, or filepath attributes are not allowed in the primary key\" ) if ( attr [ \"string\" ] and attr [ \"default\" ] is not None and attr [ \"default\" ] not in sql_literals ): attr [ \"default\" ] = '\" %s \"' % attr [ \"default\" ] if attr [ \"nullable\" ]: # nullable fields always default to null attr [ \"default\" ] = \"null\" # fill out dtype. All floats and non-nullable integers are turned into specific dtypes attr [ \"dtype\" ] = object if attr [ \"numeric\" ] and not attr [ \"adapter\" ]: is_integer = TYPE_PATTERN [ \"INTEGER\" ] . match ( attr [ \"type\" ]) is_float = TYPE_PATTERN [ \"FLOAT\" ] . match ( attr [ \"type\" ]) if is_integer and not attr [ \"nullable\" ] or is_float : is_unsigned = bool ( re . match ( \"sunsigned\" , attr [ \"type\" ], flags = re . I )) t = re . sub ( r \"\\(.*\\)\" , \"\" , attr [ \"type\" ]) # remove parentheses t = re . sub ( r \" unsigned$\" , \"\" , t ) # remove unsigned assert ( t , is_unsigned ) in numeric_types , ( \"dtype not found for type %s \" % t ) attr [ \"dtype\" ] = numeric_types [( t , is_unsigned )] if attr [ \"adapter\" ]: # restore adapted type name attr [ \"type\" ] = adapter_name self . _attributes = dict ((( q [ \"name\" ], Attribute ( ** q )) for q in attributes )) # Read and tabulate secondary indexes keys = defaultdict ( dict ) for item in conn . query ( \"SHOW KEYS FROM ` {db} `.` {tab} `\" . format ( db = database , tab = table_name ), as_dict = True , ): if item [ \"Key_name\" ] != \"PRIMARY\" : keys [ item [ \"Key_name\" ]][ item [ \"Seq_in_index\" ]] = dict ( column = item [ \"Column_name\" ], unique = ( item [ \"Non_unique\" ] == 0 ), nullable = item [ \"Null\" ] . lower () == \"yes\" , ) self . indexes = { tuple ( item [ k ][ \"column\" ] for k in sorted ( item . keys ())): dict ( unique = item [ 1 ][ \"unique\" ], nullable = any ( v [ \"nullable\" ] for v in item . values ()), ) for item in keys . values () } def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) ) def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () ) __getitem__ ( name ) \u00b6 shortcut to the attribute Source code in datajoint/heading.py 154 155 156 def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ] __init__ ( attribute_specs = None , table_info = None ) \u00b6 :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database Source code in datajoint/heading.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) ) __repr__ () \u00b6 :return: heading representation in DataJoint declaration format but without foreign key expansion Source code in datajoint/heading.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret as_dtype () property \u00b6 represent the heading as a numpy dtype Source code in datajoint/heading.py 181 182 183 184 185 186 187 188 @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) ) as_sql ( fields , include_aliases = True ) \u00b6 represent heading as the SQL SELECT clause. Source code in datajoint/heading.py 190 191 192 193 194 195 196 197 198 199 200 def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) join ( other ) \u00b6 Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. Source code in datajoint/heading.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) make_subquery_heading () \u00b6 Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. Source code in datajoint/heading.py 511 512 513 514 515 516 517 518 519 def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () ) select ( select_list , rename_map = None , compute_map = None ) \u00b6 derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. Source code in datajoint/heading.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) set_primary_key ( primary_key ) \u00b6 Create a new heading with the specified primary key. This low-level method performs no error checking. Source code in datajoint/heading.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) )", "title": "heading.py"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute", "text": "Bases: namedtuple ( _Attribute , default_attribute_properties ) Properties of a table column (attribute) Source code in datajoint/heading.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class Attribute ( namedtuple ( \"_Attribute\" , default_attribute_properties )): \"\"\" Properties of a table column (attribute) \"\"\" def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment ) @property def original_name ( self ): if self . attribute_expression is None : return self . name assert self . attribute_expression . startswith ( \"`\" ) return self . attribute_expression . strip ( \"`\" )", "title": "Attribute"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql", "text": "Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration Source code in datajoint/heading.py 64 65 66 67 68 69 70 71 72 73 74 75 @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment )", "title": "sql()"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_comment", "text": ":return: full comment for the SQL declaration. Includes custom type specification Source code in datajoint/heading.py 59 60 61 62 @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment", "title": "sql_comment()"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_type", "text": ":return: datatype (as string) in database. In most cases, it is the same as self.type Source code in datajoint/heading.py 54 55 56 57 @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type", "title": "sql_type()"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.todict", "text": "Convert namedtuple to dict. Source code in datajoint/heading.py 50 51 52 def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields ))", "title": "todict()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading", "text": "Local class for relations' headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. Source code in datajoint/heading.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class Heading : \"\"\" Local class for relations' headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. \"\"\" def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) ) def __len__ ( self ): return 0 if self . attributes is None else len ( self . attributes ) @property def table_status ( self ): if self . table_info is None : return None if self . _table_status is None : self . _init_from_database () return self . _table_status @property def attributes ( self ): if self . _attributes is None : self . _init_from_database () # lazy loading from database return self . _attributes @property def names ( self ): return [ k for k in self . attributes ] @property def primary_key ( self ): return [ k for k , v in self . attributes . items () if v . in_key ] @property def secondary_attributes ( self ): return [ k for k , v in self . attributes . items () if not v . in_key ] @property def blobs ( self ): return [ k for k , v in self . attributes . items () if v . is_blob ] @property def non_blobs ( self ): return [ k for k , v in self . attributes . items () if not v . is_blob and not v . is_attachment and not v . is_filepath ] @property def new_attributes ( self ): return [ k for k , v in self . attributes . items () if v . attribute_expression is not None ] def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ] def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret @property def has_autoincrement ( self ): return any ( e . autoincrement for e in self . attributes . values ()) @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) ) def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) def __iter__ ( self ): return iter ( self . attributes ) def _init_from_database ( self ): \"\"\"initialize heading from an existing database table.\"\"\" conn , database , table_name , context = ( self . table_info [ k ] for k in ( \"conn\" , \"database\" , \"table_name\" , \"context\" ) ) info = conn . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE name=\" {table_name} \"' . format ( table_name = table_name , database = database ), as_dict = True , ) . fetchone () if info is None : if table_name == \"~log\" : logger . warning ( \"Could not create the ~log table\" ) return raise DataJointError ( \"The table ` {database} `.` {table_name} ` is not defined.\" . format ( table_name = table_name , database = database ) ) self . _table_status = { k . lower (): v for k , v in info . items ()} cur = conn . query ( \"SHOW FULL COLUMNS FROM ` {table_name} ` IN ` {database} `\" . format ( table_name = table_name , database = database ), as_dict = True , ) attributes = cur . fetchall () rename_map = { \"Field\" : \"name\" , \"Type\" : \"type\" , \"Null\" : \"nullable\" , \"Default\" : \"default\" , \"Key\" : \"in_key\" , \"Comment\" : \"comment\" , } fields_to_drop = ( \"Privileges\" , \"Collation\" ) # rename and drop attributes attributes = [ { rename_map [ k ] if k in rename_map else k : v for k , v in x . items () if k not in fields_to_drop } for x in attributes ] numeric_types = { ( \"float\" , False ): np . float64 , ( \"float\" , True ): np . float64 , ( \"double\" , False ): np . float64 , ( \"double\" , True ): np . float64 , ( \"tinyint\" , False ): np . int64 , ( \"tinyint\" , True ): np . int64 , ( \"smallint\" , False ): np . int64 , ( \"smallint\" , True ): np . int64 , ( \"mediumint\" , False ): np . int64 , ( \"mediumint\" , True ): np . int64 , ( \"int\" , False ): np . int64 , ( \"int\" , True ): np . int64 , ( \"bigint\" , False ): np . int64 , ( \"bigint\" , True ): np . uint64 , } sql_literals = [ \"CURRENT_TIMESTAMP\" ] # additional attribute properties for attr in attributes : attr . update ( in_key = ( attr [ \"in_key\" ] == \"PRI\" ), database = database , nullable = attr [ \"nullable\" ] == \"YES\" , autoincrement = bool ( re . search ( r \"auto_increment\" , attr [ \"Extra\" ], flags = re . I ) ), numeric = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"DECIMAL\" , \"INTEGER\" , \"FLOAT\" ) ), string = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"ENUM\" , \"TEMPORAL\" , \"STRING\" ) ), is_blob = bool ( TYPE_PATTERN [ \"INTERNAL_BLOB\" ] . match ( attr [ \"type\" ])), uuid = False , is_attachment = False , is_filepath = False , adapter = None , store = None , is_external = False , attribute_expression = None , ) if any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"INTEGER\" , \"FLOAT\" )): attr [ \"type\" ] = re . sub ( r \"\\(\\d+\\)\" , \"\" , attr [ \"type\" ], count = 1 ) # strip size off integers and floats attr [ \"unsupported\" ] = not any ( ( attr [ \"is_blob\" ], attr [ \"numeric\" ], attr [ \"numeric\" ]) ) attr . pop ( \"Extra\" ) # process custom DataJoint types special = re . match ( r \":(?P<type>[^:]+):(?P<comment>.*)\" , attr [ \"comment\" ]) if special : special = special . groupdict () attr . update ( special ) # process adapted attribute types if special and TYPE_PATTERN [ \"ADAPTED\" ] . match ( attr [ \"type\" ]): assert context is not None , \"Declaration context is not set\" adapter_name = special [ \"type\" ] try : attr . update ( adapter = get_adapter ( context , adapter_name )) except DataJointError : # if no adapter, then delay the error until the first invocation attr . update ( adapter = AttributeAdapter ()) else : attr . update ( type = attr [ \"adapter\" ] . attribute_type ) if not any ( r . match ( attr [ \"type\" ]) for r in TYPE_PATTERN . values ()): raise DataJointError ( \"Invalid attribute type ' {type} ' in adapter object < {adapter_name} >.\" . format ( adapter_name = adapter_name , ** attr ) ) special = not any ( TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) for c in NATIVE_TYPES ) if special : try : category = next ( c for c in SPECIAL_TYPES if TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) ) except StopIteration : if attr [ \"type\" ] . startswith ( \"external\" ): url = ( \"https://docs.datajoint.io/python/admin/5-blob-config.html\" \"#migration-between-datajoint-v0-11-and-v0-12\" ) raise DataJointError ( \"Legacy datatype ` {type} `. Migrate your external stores to \" \"datajoint 0.12: {url} \" . format ( url = url , ** attr ) ) raise DataJointError ( \"Unknown attribute type ` {type} `\" . format ( ** attr ) ) if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) attr . update ( unsupported = False , is_attachment = category in ( \"INTERNAL_ATTACH\" , \"EXTERNAL_ATTACH\" ), is_filepath = category == \"FILEPATH\" , # INTERNAL_BLOB is not a custom type but is included for completeness is_blob = category in ( \"INTERNAL_BLOB\" , \"EXTERNAL_BLOB\" ), uuid = category == \"UUID\" , is_external = category in EXTERNAL_TYPES , store = attr [ \"type\" ] . split ( \"@\" )[ 1 ] if category in EXTERNAL_TYPES else None , ) if attr [ \"in_key\" ] and any ( ( attr [ \"is_blob\" ], attr [ \"is_attachment\" ], attr [ \"is_filepath\" ]) ): raise DataJointError ( \"Blob, attachment, or filepath attributes are not allowed in the primary key\" ) if ( attr [ \"string\" ] and attr [ \"default\" ] is not None and attr [ \"default\" ] not in sql_literals ): attr [ \"default\" ] = '\" %s \"' % attr [ \"default\" ] if attr [ \"nullable\" ]: # nullable fields always default to null attr [ \"default\" ] = \"null\" # fill out dtype. All floats and non-nullable integers are turned into specific dtypes attr [ \"dtype\" ] = object if attr [ \"numeric\" ] and not attr [ \"adapter\" ]: is_integer = TYPE_PATTERN [ \"INTEGER\" ] . match ( attr [ \"type\" ]) is_float = TYPE_PATTERN [ \"FLOAT\" ] . match ( attr [ \"type\" ]) if is_integer and not attr [ \"nullable\" ] or is_float : is_unsigned = bool ( re . match ( \"sunsigned\" , attr [ \"type\" ], flags = re . I )) t = re . sub ( r \"\\(.*\\)\" , \"\" , attr [ \"type\" ]) # remove parentheses t = re . sub ( r \" unsigned$\" , \"\" , t ) # remove unsigned assert ( t , is_unsigned ) in numeric_types , ( \"dtype not found for type %s \" % t ) attr [ \"dtype\" ] = numeric_types [( t , is_unsigned )] if attr [ \"adapter\" ]: # restore adapted type name attr [ \"type\" ] = adapter_name self . _attributes = dict ((( q [ \"name\" ], Attribute ( ** q )) for q in attributes )) # Read and tabulate secondary indexes keys = defaultdict ( dict ) for item in conn . query ( \"SHOW KEYS FROM ` {db} `.` {tab} `\" . format ( db = database , tab = table_name ), as_dict = True , ): if item [ \"Key_name\" ] != \"PRIMARY\" : keys [ item [ \"Key_name\" ]][ item [ \"Seq_in_index\" ]] = dict ( column = item [ \"Column_name\" ], unique = ( item [ \"Non_unique\" ] == 0 ), nullable = item [ \"Null\" ] . lower () == \"yes\" , ) self . indexes = { tuple ( item [ k ][ \"column\" ] for k in sorted ( item . keys ())): dict ( unique = item [ 1 ][ \"unique\" ], nullable = any ( v [ \"nullable\" ] for v in item . values ()), ) for item in keys . values () } def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) ) def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () )", "title": "Heading"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.__getitem__", "text": "shortcut to the attribute Source code in datajoint/heading.py 154 155 156 def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ]", "title": "__getitem__()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.__init__", "text": ":param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database Source code in datajoint/heading.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) )", "title": "__init__()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.__repr__", "text": ":return: heading representation in DataJoint declaration format but without foreign key expansion Source code in datajoint/heading.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret", "title": "__repr__()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_dtype", "text": "represent the heading as a numpy dtype Source code in datajoint/heading.py 181 182 183 184 185 186 187 188 @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) )", "title": "as_dtype()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_sql", "text": "represent heading as the SQL SELECT clause. Source code in datajoint/heading.py 190 191 192 193 194 195 196 197 198 199 200 def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields )", "title": "as_sql()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.join", "text": "Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. Source code in datajoint/heading.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] )", "title": "join()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.make_subquery_heading", "text": "Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. Source code in datajoint/heading.py 511 512 513 514 515 516 517 518 519 def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () )", "title": "make_subquery_heading()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.select", "text": "derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. Source code in datajoint/heading.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs ))", "title": "select()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.set_primary_key", "text": "Create a new heading with the specified primary key. This low-level method performs no error checking. Source code in datajoint/heading.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) )", "title": "set_primary_key()"}, {"location": "api/datajoint/jobs/", "text": "JobTable \u00b6 Bases: Table A base relation with no definition. Allows reserving jobs Source code in datajoint/jobs.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class JobTable ( Table ): \"\"\" A base relation with no definition. Allows reserving jobs \"\"\" def __init__ ( self , conn , database ): self . database = database self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # job reservation table for ` {database} ` table_name :varchar(255) # className of the table key_hash :char(32) # key hash --- status :enum('reserved','error','ignore') # if tuple is missing, the job is available key=null :blob # structure containing the key error_message=\"\" :varchar( {error_message_length} ) # error message returned if failed error_stack=null :mediumblob # error stack if failed user=\"\" :varchar(255) # database user host=\"\" :varchar(255) # system hostname pid=0 :int unsigned # system process id connection_id = 0 : bigint unsigned # connection_id() timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" . format ( database = database , error_message_length = ERROR_MESSAGE_LENGTH ) if not self . is_declared : self . declare () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition @property def table_name ( self ): return \"~jobs\" def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , ) complete ( table_name , key ) \u00b6 Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: database . table_name :param key: the dict of the job's primary key Source code in datajoint/jobs.py 90 91 92 93 94 95 96 97 98 def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () delete () \u00b6 bypass interactive prompts and dependencies Source code in datajoint/jobs.py 56 57 58 def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () drop () \u00b6 bypass interactive prompts and dependencies Source code in datajoint/jobs.py 60 61 62 def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () error ( table_name , key , error_message , error_stack = None ) \u00b6 Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: database . table_name :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace Source code in datajoint/jobs.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , ) reserve ( table_name , key ) \u00b6 Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: database . table_name :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True", "title": "jobs.py"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable", "text": "Bases: Table A base relation with no definition. Allows reserving jobs Source code in datajoint/jobs.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class JobTable ( Table ): \"\"\" A base relation with no definition. Allows reserving jobs \"\"\" def __init__ ( self , conn , database ): self . database = database self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # job reservation table for ` {database} ` table_name :varchar(255) # className of the table key_hash :char(32) # key hash --- status :enum('reserved','error','ignore') # if tuple is missing, the job is available key=null :blob # structure containing the key error_message=\"\" :varchar( {error_message_length} ) # error message returned if failed error_stack=null :mediumblob # error stack if failed user=\"\" :varchar(255) # database user host=\"\" :varchar(255) # system hostname pid=0 :int unsigned # system process id connection_id = 0 : bigint unsigned # connection_id() timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" . format ( database = database , error_message_length = ERROR_MESSAGE_LENGTH ) if not self . is_declared : self . declare () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition @property def table_name ( self ): return \"~jobs\" def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , )", "title": "JobTable"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.complete", "text": "Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: database . table_name :param key: the dict of the job's primary key Source code in datajoint/jobs.py 90 91 92 93 94 95 96 97 98 def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick ()", "title": "complete()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.delete", "text": "bypass interactive prompts and dependencies Source code in datajoint/jobs.py 56 57 58 def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick ()", "title": "delete()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.drop", "text": "bypass interactive prompts and dependencies Source code in datajoint/jobs.py 60 61 62 def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick ()", "title": "drop()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.error", "text": "Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: database . table_name :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace Source code in datajoint/jobs.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , )", "title": "error()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.reserve", "text": "Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: database . table_name :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True", "title": "reserve()"}, {"location": "api/datajoint/logging/", "text": "", "title": "logging.py"}, {"location": "api/datajoint/migrate/", "text": "migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ) \u00b6 Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated Source code in datajoint/migrate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ): \"\"\" Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated \"\"\" if not isinstance ( migration_schema , str ): raise ValueError ( \"Expected type {} for migration_schema, not {} .\" . format ( str , type ( migration_schema ) ) ) do_migration = False do_migration = ( user_choice ( \"\"\" Warning: Ensure the following are completed before proceeding. - Appropriate backups have been taken, - Any existing DJ 0.11.X connections are suspended, and - External config has been updated to new dj.config['stores'] structure. Proceed? \"\"\" , default = \"no\" , ) == \"yes\" ) if do_migration : _migrate_dj011_blob ( dj . Schema ( migration_schema ), store ) print ( \"Migration completed for schema: {} , store: {} .\" . format ( migration_schema , store ) ) return print ( \"No migration performed.\" )", "title": "migrate.py"}, {"location": "api/datajoint/migrate/#datajoint.migrate.migrate_dj011_external_blob_storage_to_dj012", "text": "Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated Source code in datajoint/migrate.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def migrate_dj011_external_blob_storage_to_dj012 ( migration_schema , store ): \"\"\" Utility function to migrate external blob data from 0.11 to 0.12. :param migration_schema: string of target schema to be migrated :param store: string of target dj.config['store'] to be migrated \"\"\" if not isinstance ( migration_schema , str ): raise ValueError ( \"Expected type {} for migration_schema, not {} .\" . format ( str , type ( migration_schema ) ) ) do_migration = False do_migration = ( user_choice ( \"\"\" Warning: Ensure the following are completed before proceeding. - Appropriate backups have been taken, - Any existing DJ 0.11.X connections are suspended, and - External config has been updated to new dj.config['stores'] structure. Proceed? \"\"\" , default = \"no\" , ) == \"yes\" ) if do_migration : _migrate_dj011_blob ( dj . Schema ( migration_schema ), store ) print ( \"Migration completed for schema: {} , store: {} .\" . format ( migration_schema , store ) ) return print ( \"No migration performed.\" )", "title": "migrate_dj011_external_blob_storage_to_dj012()"}, {"location": "api/datajoint/plugin/", "text": "", "title": "plugin.py"}, {"location": "api/datajoint/preview/", "text": "methods for generating previews of query expression results in python command line and Jupyter", "title": "preview.py"}, {"location": "api/datajoint/s3/", "text": "AWS S3 operations Folder \u00b6 A Folder instance manipulates a flat folder of objects within an S3-compatible object store Source code in datajoint/s3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class Folder : \"\"\" A Folder instance manipulates a flat folder of objects within an S3-compatible object store \"\"\" def __init__ ( self , endpoint , bucket , access_key , secret_key , * , secure = False , proxy_server = None , ** _ ): # from https://docs.min.io/docs/python-client-api-reference self . client = minio . Minio ( endpoint , access_key = access_key , secret_key = secret_key , secure = secure , http_client = ( urllib3 . ProxyManager ( proxy_server , timeout = urllib3 . Timeout . DEFAULT_TIMEOUT , cert_reqs = \"CERT_REQUIRED\" , retries = urllib3 . Retry ( total = 5 , backoff_factor = 0.2 , status_forcelist = [ 500 , 502 , 503 , 504 ], ), ) if proxy_server else None ), ) self . bucket = bucket if not self . client . bucket_exists ( bucket ): raise errors . BucketInaccessible ( \"Inaccessible s3 bucket %s \" % bucket ) def put ( self , name , buffer ): logger . debug ( \"put: {} : {} \" . format ( self . bucket , name )) return self . client . put_object ( self . bucket , str ( name ), BytesIO ( buffer ), length = len ( buffer ) ) def fput ( self , local_file , name , metadata = None ): logger . debug ( \"fput: {} -> {} : {} \" . format ( self . bucket , local_file , name )) return self . client . fput_object ( self . bucket , str ( name ), str ( local_file ), metadata = metadata ) def get ( self , name ): logger . debug ( \"get: {} : {} \" . format ( self . bucket , name )) try : return self . client . get_object ( self . bucket , str ( name )) . data except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile ( \"Missing s3 key %s \" % name ) else : raise e def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ]) def exists ( self , name ): logger . debug ( \"exists: {} : {} \" . format ( self . bucket , name )) try : self . client . stat_object ( self . bucket , str ( name )) except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : return False else : raise e return True def get_size ( self , name ): logger . debug ( \"get_size: {} : {} \" . format ( self . bucket , name )) try : return self . client . stat_object ( self . bucket , str ( name )) . size except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile raise e def remove_object ( self , name ): logger . debug ( \"remove_object: {} : {} \" . format ( self . bucket , name )) try : self . client . remove_object ( self . bucket , str ( name )) except minio . error . MinioException : raise errors . DataJointError ( \"Failed to delete %s from s3 storage\" % name ) fget ( name , local_filepath ) \u00b6 get file from object name to local filepath Source code in datajoint/s3.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ])", "title": "s3.py"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder", "text": "A Folder instance manipulates a flat folder of objects within an S3-compatible object store Source code in datajoint/s3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class Folder : \"\"\" A Folder instance manipulates a flat folder of objects within an S3-compatible object store \"\"\" def __init__ ( self , endpoint , bucket , access_key , secret_key , * , secure = False , proxy_server = None , ** _ ): # from https://docs.min.io/docs/python-client-api-reference self . client = minio . Minio ( endpoint , access_key = access_key , secret_key = secret_key , secure = secure , http_client = ( urllib3 . ProxyManager ( proxy_server , timeout = urllib3 . Timeout . DEFAULT_TIMEOUT , cert_reqs = \"CERT_REQUIRED\" , retries = urllib3 . Retry ( total = 5 , backoff_factor = 0.2 , status_forcelist = [ 500 , 502 , 503 , 504 ], ), ) if proxy_server else None ), ) self . bucket = bucket if not self . client . bucket_exists ( bucket ): raise errors . BucketInaccessible ( \"Inaccessible s3 bucket %s \" % bucket ) def put ( self , name , buffer ): logger . debug ( \"put: {} : {} \" . format ( self . bucket , name )) return self . client . put_object ( self . bucket , str ( name ), BytesIO ( buffer ), length = len ( buffer ) ) def fput ( self , local_file , name , metadata = None ): logger . debug ( \"fput: {} -> {} : {} \" . format ( self . bucket , local_file , name )) return self . client . fput_object ( self . bucket , str ( name ), str ( local_file ), metadata = metadata ) def get ( self , name ): logger . debug ( \"get: {} : {} \" . format ( self . bucket , name )) try : return self . client . get_object ( self . bucket , str ( name )) . data except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile ( \"Missing s3 key %s \" % name ) else : raise e def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ]) def exists ( self , name ): logger . debug ( \"exists: {} : {} \" . format ( self . bucket , name )) try : self . client . stat_object ( self . bucket , str ( name )) except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : return False else : raise e return True def get_size ( self , name ): logger . debug ( \"get_size: {} : {} \" . format ( self . bucket , name )) try : return self . client . stat_object ( self . bucket , str ( name )) . size except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile raise e def remove_object ( self , name ): logger . debug ( \"remove_object: {} : {} \" . format ( self . bucket , name )) try : self . client . remove_object ( self . bucket , str ( name )) except minio . error . MinioException : raise errors . DataJointError ( \"Failed to delete %s from s3 storage\" % name )", "title": "Folder"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder.fget", "text": "get file from object name to local filepath Source code in datajoint/s3.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ])", "title": "fget()"}, {"location": "api/datajoint/schemas/", "text": "Schema \u00b6 A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared : if not self . create_tables or assert_declared : raise DataJointError ( \"Table ` %s ` not declared\" % instance . table_name ) instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] __call__ ( cls , * , context = None ) \u00b6 Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes Source code in datajoint/schemas.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls __init__ ( schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) activate ( schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) drop ( force = False ) \u00b6 Drop the associated schema if it exists Source code in datajoint/schemas.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) exists () property \u00b6 :return: true if the associated schema exists on the server Source code in datajoint/schemas.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) jobs () property \u00b6 schema.jobs provides a view of the job reservation table for the schema :return: jobs table Source code in datajoint/schemas.py 394 395 396 397 398 399 400 401 402 403 404 @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs list_tables () \u00b6 Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. Source code in datajoint/schemas.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] save ( python_filename = None ) \u00b6 Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) size_on_disk () property \u00b6 :return: size of the entire schema in bytes Source code in datajoint/schemas.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) spawn_missing_classes ( context = None ) \u00b6 Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() Source code in datajoint/schemas.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) VirtualModule \u00b6 Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) __init__ ( module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ) \u00b6 Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes Source code in datajoint/schemas.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) list_schemas ( connection = None ) \u00b6 :param connection: a dj.Connection object :return: list of all accessible schemas on the server Source code in datajoint/schemas.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ] ordered_dir ( class_ ) \u00b6 List (most) attributes of the class including inherited ones, similar to dir build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses Source code in datajoint/schemas.py 22 23 24 25 26 27 28 29 30 31 32 33 def ordered_dir ( class_ ): \"\"\" List (most) attributes of the class including inherited ones, similar to `dir` build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses \"\"\" attr_list = list () for c in reversed ( class_ . mro ()): attr_list . extend ( e for e in c . __dict__ if e not in attr_list ) return attr_list", "title": "schemas.py"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema", "text": "A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared : if not self . create_tables or assert_declared : raise DataJointError ( \"Table ` %s ` not declared\" % instance . table_name ) instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "Schema"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.__call__", "text": "Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes Source code in datajoint/schemas.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part relations\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls", "title": "__call__()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.__init__", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name )", "title": "__init__()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.activate", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context )", "title": "activate()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.drop", "text": "Drop the associated schema if it exists Source code in datajoint/schemas.py 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) )", "title": "drop()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.exists", "text": ":return: true if the associated schema exists on the server Source code in datajoint/schemas.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount )", "title": "exists()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.jobs", "text": "schema.jobs provides a view of the job reservation table for the schema :return: jobs table Source code in datajoint/schemas.py 394 395 396 397 398 399 400 401 402 403 404 @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs", "title": "jobs()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.list_tables", "text": "Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. Source code in datajoint/schemas.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "list_tables()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.save", "text": "Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe ( printout = False ), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code )", "title": "save()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.size_on_disk", "text": ":return: size of the entire schema in bytes Source code in datajoint/schemas.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] )", "title": "size_on_disk()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.spawn_missing_classes", "text": "Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() Source code in datajoint/schemas.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user relation classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master relation classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class )", "title": "spawn_missing_classes()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.VirtualModule", "text": "Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "VirtualModule"}, {"location": "api/datajoint/schemas/#datajoint.schemas.VirtualModule.__init__", "text": "Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes Source code in datajoint/schemas.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "__init__()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.list_schemas", "text": ":param connection: a dj.Connection object :return: list of all accessible schemas on the server Source code in datajoint/schemas.py 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ]", "title": "list_schemas()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.ordered_dir", "text": "List (most) attributes of the class including inherited ones, similar to dir build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses Source code in datajoint/schemas.py 22 23 24 25 26 27 28 29 30 31 32 33 def ordered_dir ( class_ ): \"\"\" List (most) attributes of the class including inherited ones, similar to `dir` build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses \"\"\" attr_list = list () for c in reversed ( class_ . mro ()): attr_list . extend ( e for e in c . __dict__ if e not in attr_list ) return attr_list", "title": "ordered_dir()"}, {"location": "api/datajoint/settings/", "text": "Settings for DataJoint. Config \u00b6 Bases: collections . abc . MutableMapping Source code in datajoint/settings.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class Config ( collections . abc . MutableMapping ): instance = None def __init__ ( self , * args , ** kwargs ): if not Config . instance : Config . instance = Config . __Config ( * args , ** kwargs ) else : Config . instance . _conf . update ( dict ( * args , ** kwargs )) def __getattr__ ( self , name ): return getattr ( self . instance , name ) def __getitem__ ( self , item ): return self . instance . __getitem__ ( item ) def __setitem__ ( self , item , value ): self . instance . __setitem__ ( item , value ) def __str__ ( self ): return pprint . pformat ( self . instance . _conf , indent = 4 ) def __repr__ ( self ): return self . __str__ () def __delitem__ ( self , key ): del self . instance . _conf [ key ] def __iter__ ( self ): return iter ( self . instance . _conf ) def __len__ ( self ): return len ( self . instance . _conf ) def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose ) def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key )) __Config \u00b6 Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. Source code in datajoint/settings.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key )) __call__ ( ** kwargs ) \u00b6 The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: import datajoint as dj with dj.config(safemode=False, database__host=\"localhost\") as cfg: # do dangerous stuff here Source code in datajoint/settings.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup get_store_spec ( store ) \u00b6 find configuration of external stores for blobs and attachments Source code in datajoint/settings.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec load ( filename ) \u00b6 Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. Source code in datajoint/settings.py 110 111 112 113 114 115 116 117 118 119 def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) save ( filename , verbose = False ) \u00b6 Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file Source code in datajoint/settings.py 98 99 100 101 102 103 104 105 106 107 108 def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) save_global ( verbose = False ) \u00b6 saves the settings in the global config file Source code in datajoint/settings.py 127 128 129 130 131 def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) save_local ( verbose = False ) \u00b6 saves the settings in the local config file Source code in datajoint/settings.py 121 122 123 124 125 def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose )", "title": "settings.py"}, {"location": "api/datajoint/settings/#datajoint.settings.Config", "text": "Bases: collections . abc . MutableMapping Source code in datajoint/settings.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class Config ( collections . abc . MutableMapping ): instance = None def __init__ ( self , * args , ** kwargs ): if not Config . instance : Config . instance = Config . __Config ( * args , ** kwargs ) else : Config . instance . _conf . update ( dict ( * args , ** kwargs )) def __getattr__ ( self , name ): return getattr ( self . instance , name ) def __getitem__ ( self , item ): return self . instance . __getitem__ ( item ) def __setitem__ ( self , item , value ): self . instance . __setitem__ ( item , value ) def __str__ ( self ): return pprint . pformat ( self . instance . _conf , indent = 4 ) def __repr__ ( self ): return self . __str__ () def __delitem__ ( self , key ): del self . instance . _conf [ key ] def __iter__ ( self ): return iter ( self . instance . _conf ) def __len__ ( self ): return len ( self . instance . _conf ) def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose ) def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key ))", "title": "Config"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.__Config", "text": "Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. Source code in datajoint/settings.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key ))", "title": "__Config"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.__call__", "text": "The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: import datajoint as dj with dj.config(safemode=False, database__host=\"localhost\") as cfg: # do dangerous stuff here Source code in datajoint/settings.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup", "title": "__call__()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.get_store_spec", "text": "find configuration of external stores for blobs and attachments Source code in datajoint/settings.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec", "title": "get_store_spec()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.load", "text": "Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. Source code in datajoint/settings.py 110 111 112 113 114 115 116 117 118 119 def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid ))", "title": "load()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save", "text": "Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file Source code in datajoint/settings.py 98 99 100 101 102 103 104 105 106 107 108 def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename )", "title": "save()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_global", "text": "saves the settings in the global config file Source code in datajoint/settings.py 127 128 129 130 131 def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose )", "title": "save_global()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_local", "text": "saves the settings in the local config file Source code in datajoint/settings.py 121 122 123 124 125 def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose )", "title": "save_local()"}, {"location": "api/datajoint/table/", "text": "FreeTable \u00b6 Bases: Table A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format database . table_name Source code in datajoint/table.py 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 class FreeTable ( Table ): \"\"\" A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () ) Log \u00b6 Bases: Table The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See call Source code in datajoint/table.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 class Log ( Table ): \"\"\" The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See __call__ \"\"\" _table_name = \"~log\" def __init__ ( self , conn , database , skip_logging = False ): self . database = database self . skip_logging = skip_logging self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # event logging table for ` {database} ` id :int unsigned auto_increment # event order id --- timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp version :varchar(12) # datajoint version user :varchar(255) # user@host host=\"\" :varchar(255) # system hostname event=\"\" :varchar(255) # event message \"\"\" . format ( database = database ) super () . __init__ () if not self . is_declared : self . declare () self . connection . dependencies . clear () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" ) def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick () __call__ ( event , skip_logging = None ) \u00b6 :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging Source code in datajoint/table.py 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" ) delete () \u00b6 bypass interactive prompts and cascading dependencies :return: number of deleted items Source code in datajoint/table.py 1087 1088 1089 1090 1091 1092 1093 def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) drop () \u00b6 bypass interactive prompts and cascading dependencies Source code in datajoint/table.py 1095 1096 1097 def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick () Table \u00b6 Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition def _update ( self , attrname , value = None ): \"\"\" This is a deprecated function to be removed in datajoint 0.14. Use ``.update1`` instead. Updates a field in one existing tuple. self must be restricted to exactly one entry. In DataJoint the principal way of updating data is to delete and re-insert the entire record and updates are reserved for corrective actions. This is because referential integrity is observed on the level of entire records rather than individual attributes. Safety constraints: 1. self must be restricted to exactly one tuple 2. the update attribute must not be in primary key Example: >>> (v2p.Mice() & key)._update('mouse_dob', '2011-01-01') >>> (v2p.Mice() & key)._update( 'lens') # set the value to NULL \"\"\" logger . warning ( \"`_update` is a deprecated function to be removed in datajoint 0.14. \" \"Use `.update1` instead.\" ) if len ( self ) != 1 : raise DataJointError ( \"Update is only allowed on one tuple at a time\" ) if attrname not in self . heading : raise DataJointError ( \"Invalid attribute name\" ) if attrname in self . heading . primary_key : raise DataJointError ( \"Cannot update a key value.\" ) attr = self . heading [ attrname ] if attr . is_blob : value = blob . pack ( value ) placeholder = \" %s \" elif attr . numeric : if value is None or np . isnan ( float ( value )): # nans are turned into NULLs placeholder = \"NULL\" value = None else : placeholder = \" %s \" value = str ( int ( value ) if isinstance ( value , bool ) else value ) else : placeholder = \" %s \" if value is not None else \"NULL\" command = \"UPDATE {full_table_name} SET ` {attrname} `= {placeholder} {where_clause} \" . format ( full_table_name = self . from_clause (), attrname = attrname , placeholder = placeholder , where_clause = self . where_clause (), ) self . connection . query ( command , args = ( value ,) if value is not None else ()) # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert __make_placeholder ( name , value , ignore_extra_fields = False ) \u00b6 For a given attribute name with value , return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted Source code in datajoint/table.py 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value __make_row_to_insert ( row , field_list , ignore_extra_fields ) \u00b6 Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' Source code in datajoint/table.py 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert alter ( prompt = True , context = None ) \u00b6 Alter the table definition from self.definition Source code in datajoint/table.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) ancestors ( as_objects = False ) \u00b6 :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. Source code in datajoint/table.py 216 217 218 219 220 221 222 223 224 225 226 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] children ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes declare ( context = None ) \u00b6 Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. Source code in datajoint/table.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) delete ( transaction = True , safemode = None , force_parts = False ) \u00b6 Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) Source code in datajoint/table.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count delete_quick ( get_count = False ) \u00b6 Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count descendants ( as_objects = False ) \u00b6 :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. Source code in datajoint/table.py 204 205 206 207 208 209 210 211 212 213 214 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] describe ( context = None , printout = True ) \u00b6 :return: the definition string for the relation using DataJoint DDL. Source code in datajoint/table.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition drop () \u00b6 Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) drop_quick () \u00b6 Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. Source code in datajoint/table.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) from_clause () \u00b6 :return: the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 146 147 148 149 150 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name full_table_name () property \u00b6 :return: full table name in the schema Source code in datajoint/table.py 255 256 257 258 259 260 @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) get_select_fields ( select_fields = None ) \u00b6 :return: the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 152 153 154 155 156 157 158 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) insert ( rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None ) \u00b6 Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example relation.insert([ dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) Source code in datajoint/table.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) insert1 ( row , ** kwargs ) \u00b6 Insert one data record into the table. For kwargs , see insert() . :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. Source code in datajoint/table.py 327 328 329 330 331 332 333 334 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) is_declared () property \u00b6 :return: True is the table is declared in the schema. Source code in datajoint/table.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) parents ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes parts ( as_objects = False ) \u00b6 return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. Source code in datajoint/table.py 228 229 230 231 232 233 234 235 236 237 238 239 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes size_on_disk () property \u00b6 :return: size of data and indices in bytes on the storage device Source code in datajoint/table.py 649 650 651 652 653 654 655 656 657 658 659 660 @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] update1 ( row ) \u00b6 update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: table.update1({'id': 1, 'value': 3}) # update value in record with id=1 table.update1({'id': 1, 'value': None}) # reset value to default Source code in datajoint/table.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) lookup_class_name ( name , context , depth = 3 ) \u00b6 given a table name in the form schema_name . table_name , find its class in the context. :param name: schema_name . table_name :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found Source code in datajoint/table.py 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def lookup_class_name ( name , context , depth = 3 ): \"\"\" given a table name in the form `schema_name`.`table_name`, find its class in the context. :param name: `schema_name`.`table_name` :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found \"\"\" # breadth-first search nodes = [ dict ( context = context , context_name = \"\" , depth = depth )] while nodes : node = nodes . pop ( 0 ) for member_name , member in node [ \"context\" ] . items (): if not member_name . startswith ( \"_\" ): # skip IPython's implicit variables if inspect . isclass ( member ) and issubclass ( member , Table ): if member . full_table_name == name : # found it! return \".\" . join ([ node [ \"context_name\" ], member_name ]) . lstrip ( \".\" ) try : # look for part tables parts = member . __dict__ except AttributeError : pass # not a UserTable -- cannot have part tables. else : for part in ( getattr ( member , p ) for p in parts if p [ 0 ] . isupper () and hasattr ( member , p ) ): if ( inspect . isclass ( part ) and issubclass ( part , Table ) and part . full_table_name == name ): return \".\" . join ( [ node [ \"context_name\" ], member_name , part . __name__ ] ) . lstrip ( \".\" ) elif ( node [ \"depth\" ] > 0 and inspect . ismodule ( member ) and member . __name__ != \"datajoint\" ): try : nodes . append ( dict ( context = dict ( inspect . getmembers ( member )), context_name = node [ \"context_name\" ] + \".\" + member_name , depth = node [ \"depth\" ] - 1 , ) ) except ImportError : pass # could not import, so do not attempt return None", "title": "table.py"}, {"location": "api/datajoint/table/#datajoint.table.FreeTable", "text": "Bases: Table A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format database . table_name Source code in datajoint/table.py 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 class FreeTable ( Table ): \"\"\" A base relation without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () )", "title": "FreeTable"}, {"location": "api/datajoint/table/#datajoint.table.Log", "text": "Bases: Table The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See call Source code in datajoint/table.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 class Log ( Table ): \"\"\" The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See __call__ \"\"\" _table_name = \"~log\" def __init__ ( self , conn , database , skip_logging = False ): self . database = database self . skip_logging = skip_logging self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # event logging table for ` {database} ` id :int unsigned auto_increment # event order id --- timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp version :varchar(12) # datajoint version user :varchar(255) # user@host host=\"\" :varchar(255) # system hostname event=\"\" :varchar(255) # event message \"\"\" . format ( database = database ) super () . __init__ () if not self . is_declared : self . declare () self . connection . dependencies . clear () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" ) def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick ()", "title": "Log"}, {"location": "api/datajoint/table/#datajoint.table.Log.__call__", "text": ":param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging Source code in datajoint/table.py 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" )", "title": "__call__()"}, {"location": "api/datajoint/table/#datajoint.table.Log.delete", "text": "bypass interactive prompts and cascading dependencies :return: number of deleted items Source code in datajoint/table.py 1087 1088 1089 1090 1091 1092 1093 def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True )", "title": "delete()"}, {"location": "api/datajoint/table/#datajoint.table.Log.drop", "text": "bypass interactive prompts and cascading dependencies Source code in datajoint/table.py 1095 1096 1097 def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick ()", "title": "drop()"}, {"location": "api/datajoint/table/#datajoint.table.Table", "text": "Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition def _update ( self , attrname , value = None ): \"\"\" This is a deprecated function to be removed in datajoint 0.14. Use ``.update1`` instead. Updates a field in one existing tuple. self must be restricted to exactly one entry. In DataJoint the principal way of updating data is to delete and re-insert the entire record and updates are reserved for corrective actions. This is because referential integrity is observed on the level of entire records rather than individual attributes. Safety constraints: 1. self must be restricted to exactly one tuple 2. the update attribute must not be in primary key Example: >>> (v2p.Mice() & key)._update('mouse_dob', '2011-01-01') >>> (v2p.Mice() & key)._update( 'lens') # set the value to NULL \"\"\" logger . warning ( \"`_update` is a deprecated function to be removed in datajoint 0.14. \" \"Use `.update1` instead.\" ) if len ( self ) != 1 : raise DataJointError ( \"Update is only allowed on one tuple at a time\" ) if attrname not in self . heading : raise DataJointError ( \"Invalid attribute name\" ) if attrname in self . heading . primary_key : raise DataJointError ( \"Cannot update a key value.\" ) attr = self . heading [ attrname ] if attr . is_blob : value = blob . pack ( value ) placeholder = \" %s \" elif attr . numeric : if value is None or np . isnan ( float ( value )): # nans are turned into NULLs placeholder = \"NULL\" value = None else : placeholder = \" %s \" value = str ( int ( value ) if isinstance ( value , bool ) else value ) else : placeholder = \" %s \" if value is not None else \"NULL\" command = \"UPDATE {full_table_name} SET ` {attrname} `= {placeholder} {where_clause} \" . format ( full_table_name = self . from_clause (), attrname = attrname , placeholder = placeholder , where_clause = self . where_clause (), ) self . connection . query ( command , args = ( value ,) if value is not None else ()) # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "Table"}, {"location": "api/datajoint/table/#datajoint.table.Table.__make_placeholder", "text": "For a given attribute name with value , return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted Source code in datajoint/table.py 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) return name , placeholder , value", "title": "__make_placeholder()"}, {"location": "api/datajoint/table/#datajoint.table.Table.__make_row_to_insert", "text": "Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' Source code in datajoint/table.py 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "__make_row_to_insert()"}, {"location": "api/datajoint/table/#datajoint.table.Table.alter", "text": "Alter the table definition from self.definition Source code in datajoint/table.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context , printout = False ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : print ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : print ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name )", "title": "alter()"}, {"location": "api/datajoint/table/#datajoint.table.Table.ancestors", "text": ":param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. Source code in datajoint/table.py 216 217 218 219 220 221 222 223 224 225 226 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ]", "title": "ancestors()"}, {"location": "api/datajoint/table/#datajoint.table.Table.children", "text": ":param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "children()"}, {"location": "api/datajoint/table/#datajoint.table.Table.declare", "text": "Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. Source code in datajoint/table.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name )", "title": "declare()"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete", "text": "Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) Source code in datajoint/table.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def delete ( self , transaction = True , safemode = None , force_parts = False ): \"\"\" Deletes the contents of the table and its dependent tables, recursively. :param transaction: if True, use the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. :param safemode: If True, prohibit nested transactions and prompt to confirm. Default is dj.config['safemode']. :param force_parts: Delete from parts even when not deleting from their masters. :return: number of deleted rows (excluding those from dependent tables) \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : print ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : print ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : print ( \"Deletes cancelled\" ) return delete_count", "title": "delete()"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete_quick", "text": "Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count", "title": "delete_quick()"}, {"location": "api/datajoint/table/#datajoint.table.Table.descendants", "text": ":param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. Source code in datajoint/table.py 204 205 206 207 208 209 210 211 212 213 214 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ]", "title": "descendants()"}, {"location": "api/datajoint/table/#datajoint.table.Table.describe", "text": ":return: the definition string for the relation using DataJoint DDL. Source code in datajoint/table.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def describe ( self , context = None , printout = True ): \"\"\" :return: the definition string for the relation using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : print ( definition ) return definition", "title": "describe()"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop", "text": "Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : print ( table , \"( %d tuples)\" % len ( FreeTable ( self . connection , table ))) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () print ( \"Tables dropped. Restart kernel.\" )", "title": "drop()"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop_quick", "text": "Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. Source code in datajoint/table.py 598 599 600 601 602 603 604 605 606 607 608 609 610 611 def drop_quick ( self ): \"\"\" Drops the table associated with this relation without cascading and without user prompt. If the table has any dependent table(s), this call will fail with an error. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name )", "title": "drop_quick()"}, {"location": "api/datajoint/table/#datajoint.table.Table.from_clause", "text": ":return: the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 146 147 148 149 150 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name", "title": "from_clause()"}, {"location": "api/datajoint/table/#datajoint.table.Table.full_table_name", "text": ":return: full table name in the schema Source code in datajoint/table.py 255 256 257 258 259 260 @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name )", "title": "full_table_name()"}, {"location": "api/datajoint/table/#datajoint.table.Table.get_select_fields", "text": ":return: the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 152 153 154 155 156 157 158 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql )", "title": "get_select_fields()"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert", "text": "Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example relation.insert([ dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) Source code in datajoint/table.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: An iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: applies only in auto-populated tables. If False (default), insert are allowed only from inside the make callback. Example: >>> relation.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" )", "title": "insert()"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert1", "text": "Insert one data record into the table. For kwargs , see insert() . :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. Source code in datajoint/table.py 327 328 329 330 331 332 333 334 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs )", "title": "insert1()"}, {"location": "api/datajoint/table/#datajoint.table.Table.is_declared", "text": ":return: True is the table is declared in the schema. Source code in datajoint/table.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 )", "title": "is_declared()"}, {"location": "api/datajoint/table/#datajoint.table.Table.parents", "text": ":param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "parents()"}, {"location": "api/datajoint/table/#datajoint.table.Table.parts", "text": "return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. Source code in datajoint/table.py 228 229 230 231 232 233 234 235 236 237 238 239 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes", "title": "parts()"}, {"location": "api/datajoint/table/#datajoint.table.Table.size_on_disk", "text": ":return: size of data and indices in bytes on the storage device Source code in datajoint/table.py 649 650 651 652 653 654 655 656 657 658 659 660 @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ]", "title": "size_on_disk()"}, {"location": "api/datajoint/table/#datajoint.table.Table.update1", "text": "update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: table.update1({'id': 1, 'value': 3}) # update value in record with id=1 table.update1({'id': 1, 'value': None}) # reset value to default Source code in datajoint/table.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None ))", "title": "update1()"}, {"location": "api/datajoint/table/#datajoint.table.lookup_class_name", "text": "given a table name in the form schema_name . table_name , find its class in the context. :param name: schema_name . table_name :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found Source code in datajoint/table.py 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def lookup_class_name ( name , context , depth = 3 ): \"\"\" given a table name in the form `schema_name`.`table_name`, find its class in the context. :param name: `schema_name`.`table_name` :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found \"\"\" # breadth-first search nodes = [ dict ( context = context , context_name = \"\" , depth = depth )] while nodes : node = nodes . pop ( 0 ) for member_name , member in node [ \"context\" ] . items (): if not member_name . startswith ( \"_\" ): # skip IPython's implicit variables if inspect . isclass ( member ) and issubclass ( member , Table ): if member . full_table_name == name : # found it! return \".\" . join ([ node [ \"context_name\" ], member_name ]) . lstrip ( \".\" ) try : # look for part tables parts = member . __dict__ except AttributeError : pass # not a UserTable -- cannot have part tables. else : for part in ( getattr ( member , p ) for p in parts if p [ 0 ] . isupper () and hasattr ( member , p ) ): if ( inspect . isclass ( part ) and issubclass ( part , Table ) and part . full_table_name == name ): return \".\" . join ( [ node [ \"context_name\" ], member_name , part . __name__ ] ) . lstrip ( \".\" ) elif ( node [ \"depth\" ] > 0 and inspect . ismodule ( member ) and member . __name__ != \"datajoint\" ): try : nodes . append ( dict ( context = dict ( inspect . getmembers ( member )), context_name = node [ \"context_name\" ] + \".\" + member_name , depth = node [ \"depth\" ] - 1 , ) ) except ImportError : pass # could not import, so do not attempt return None", "title": "lookup_class_name()"}, {"location": "api/datajoint/user_tables/", "text": "Hosts the table tiers, user relations should be derived from. Computed \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\" Imported \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\" Lookup \u00b6 Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" ) Manual \u00b6 Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\" Part \u00b6 Bases: UserTable Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) delete ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) drop ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) TableMeta \u00b6 Bases: type TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). Source code in datajoint/user_tables.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class TableMeta ( type ): \"\"\" TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). \"\"\" def __getattribute__ ( cls , name ): # trigger instantiation for supported class attrs return ( cls () . __getattribute__ ( name ) if name in supported_class_attrs else super () . __getattribute__ ( name ) ) def __and__ ( cls , arg ): return cls () & arg def __xor__ ( cls , arg ): return cls () ^ arg def __sub__ ( cls , arg ): return cls () - arg def __neg__ ( cls ): return - cls () def __mul__ ( cls , arg ): return cls () * arg def __matmul__ ( cls , arg ): return cls () @ arg def __add__ ( cls , arg ): return cls () + arg def __iter__ ( cls ): return iter ( cls ()) UserTable \u00b6 Bases: Table A subclass of UserTable is a dedicated class interfacing a base relation. UserTable is initialized by the decorator generated by schema(). Source code in datajoint/user_tables.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class UserTable ( Table , metaclass = TableMeta ): \"\"\" A subclass of UserTable is a dedicated class interfacing a base relation. UserTable is initialized by the decorator generated by schema(). \"\"\" # set by @schema _connection = None _heading = None _support = None # set by subclass tier_regexp = None _prefix = None @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ ) @ClassProperty def full_table_name ( cls ): if cls not in { Manual , Imported , Lookup , Computed , Part , UserTable }: # for derived classes only if cls . database is None : raise DataJointError ( \"Class %s is not properly declared (schema decorator not applied?)\" % cls . __name__ ) return r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) definition () property \u00b6 :return: a string containing the table definition using the DataJoint DDL. Source code in datajoint/user_tables.py 99 100 101 102 103 104 105 106 @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' ) table_name () \u00b6 :return: the table name of the table formatted for mysql. Source code in datajoint/user_tables.py 112 113 114 115 116 117 118 119 @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ )", "title": "user_tables.py"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Computed", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other relations in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\"", "title": "Computed"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Imported", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\"", "title": "Imported"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Lookup", "text": "Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" )", "title": "Lookup"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Manual", "text": "Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\"", "title": "Manual"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part", "text": "Bases: UserTable Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another relation and if this table is populated by this relation. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part relations are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "Part"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.delete", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" )", "title": "delete()"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.drop", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "drop()"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.TableMeta", "text": "Bases: type TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). Source code in datajoint/user_tables.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class TableMeta ( type ): \"\"\" TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). \"\"\" def __getattribute__ ( cls , name ): # trigger instantiation for supported class attrs return ( cls () . __getattribute__ ( name ) if name in supported_class_attrs else super () . __getattribute__ ( name ) ) def __and__ ( cls , arg ): return cls () & arg def __xor__ ( cls , arg ): return cls () ^ arg def __sub__ ( cls , arg ): return cls () - arg def __neg__ ( cls ): return - cls () def __mul__ ( cls , arg ): return cls () * arg def __matmul__ ( cls , arg ): return cls () @ arg def __add__ ( cls , arg ): return cls () + arg def __iter__ ( cls ): return iter ( cls ())", "title": "TableMeta"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable", "text": "Bases: Table A subclass of UserTable is a dedicated class interfacing a base relation. UserTable is initialized by the decorator generated by schema(). Source code in datajoint/user_tables.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class UserTable ( Table , metaclass = TableMeta ): \"\"\" A subclass of UserTable is a dedicated class interfacing a base relation. UserTable is initialized by the decorator generated by schema(). \"\"\" # set by @schema _connection = None _heading = None _support = None # set by subclass tier_regexp = None _prefix = None @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ ) @ClassProperty def full_table_name ( cls ): if cls not in { Manual , Imported , Lookup , Computed , Part , UserTable }: # for derived classes only if cls . database is None : raise DataJointError ( \"Class %s is not properly declared (schema decorator not applied?)\" % cls . __name__ ) return r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name )", "title": "UserTable"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.definition", "text": ":return: a string containing the table definition using the DataJoint DDL. Source code in datajoint/user_tables.py 99 100 101 102 103 104 105 106 @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' )", "title": "definition()"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.table_name", "text": ":return: the table name of the table formatted for mysql. Source code in datajoint/user_tables.py 112 113 114 115 116 117 118 119 @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ )", "title": "table_name()"}, {"location": "api/datajoint/utils/", "text": "General-purpose utilities from_camel_case ( s ) \u00b6 Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: from_camel_case(\"TableName\") # yields \"table_name\" Source code in datajoint/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def from_camel_case ( s ): \"\"\" Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" \"\"\" def convert ( match ): return ( \"_\" if match . groups ()[ 0 ] else \"\" ) + match . group ( 0 ) . lower () if not re . match ( r \"[A-Z][a-zA-Z0-9]*\" , s ): raise DataJointError ( \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\" ) return re . sub ( r \"(\\B[A-Z])|(\\b[A-Z])\" , convert , s ) get_master ( full_table_name ) \u00b6 If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + __ . Example ephys . session -- master ephys . session__recording -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str Source code in datajoint/utils.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_master ( full_table_name : str ) -> str : \"\"\" If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + ``__``. Example: `ephys`.`session` -- master `ephys`.`session__recording` -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str \"\"\" match = re . match ( r \"(?P<master>`\\w+`.`\\w+)__(?P<part>\\w+)`\" , full_table_name ) return match [ \"master\" ] + \"`\" if match else \"\" parse_sql ( filepath ) \u00b6 yield SQL statements from an SQL file Source code in datajoint/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def parse_sql ( filepath ): \"\"\" yield SQL statements from an SQL file \"\"\" delimiter = \";\" statement = [] with Path ( filepath ) . open ( \"rt\" ) as f : for line in f : line = line . strip () if not line . startswith ( \"--\" ) and len ( line ) > 1 : if line . startswith ( \"delimiter\" ): delimiter = line . split ()[ 1 ] else : statement . append ( line ) if line . endswith ( delimiter ): yield \" \" . join ( statement ) statement = [] safe_copy ( src , dest , overwrite = False ) \u00b6 Copy the contents of src file into dest file as a two-step process. Skip if dest exists already Source code in datajoint/utils.py 107 108 109 110 111 112 113 114 115 116 def safe_copy ( src , dest , overwrite = False ): \"\"\" Copy the contents of src file into dest file as a two-step process. Skip if dest exists already \"\"\" src , dest = Path ( src ), Path ( dest ) if not ( dest . exists () and src . samefile ( dest )) and ( overwrite or not dest . is_file ()): dest . parent . mkdir ( parents = True , exist_ok = True ) temp_file = dest . with_suffix ( dest . suffix + \".copying\" ) shutil . copyfile ( str ( src ), str ( temp_file )) temp_file . rename ( dest ) safe_write ( filepath , blob ) \u00b6 A two-step write. :param filename: full path :param blob: binary data Source code in datajoint/utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 def safe_write ( filepath , blob ): \"\"\" A two-step write. :param filename: full path :param blob: binary data \"\"\" filepath = Path ( filepath ) if not filepath . is_file (): filepath . parent . mkdir ( parents = True , exist_ok = True ) temp_file = filepath . with_suffix ( filepath . suffix + \".saving\" ) temp_file . write_bytes ( blob ) temp_file . rename ( filepath ) to_camel_case ( s ) \u00b6 Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: to_camel_case(\"table_name\") # returns \"TableName\" Source code in datajoint/utils.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def to_camel_case ( s ): \"\"\" Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" \"\"\" def to_upper ( match ): return match . group ( 0 )[ - 1 ] . upper () return re . sub ( r \"(^|[_\\W])+[a-zA-Z]\" , to_upper , s ) user_choice ( prompt , choices = ( 'yes' , 'no' ), default = None ) \u00b6 Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice Source code in datajoint/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def user_choice ( prompt , choices = ( \"yes\" , \"no\" ), default = None ): \"\"\" Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice \"\"\" assert default is None or default in choices choice_list = \", \" . join ( ( choice . title () if choice == default else choice for choice in choices ) ) response = None while response not in choices : response = input ( prompt + \" [\" + choice_list + \"]: \" ) response = response . lower () if response else default return response", "title": "utils.py"}, {"location": "api/datajoint/utils/#datajoint.utils.from_camel_case", "text": "Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: from_camel_case(\"TableName\") # yields \"table_name\" Source code in datajoint/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def from_camel_case ( s ): \"\"\" Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" \"\"\" def convert ( match ): return ( \"_\" if match . groups ()[ 0 ] else \"\" ) + match . group ( 0 ) . lower () if not re . match ( r \"[A-Z][a-zA-Z0-9]*\" , s ): raise DataJointError ( \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\" ) return re . sub ( r \"(\\B[A-Z])|(\\b[A-Z])\" , convert , s )", "title": "from_camel_case()"}, {"location": "api/datajoint/utils/#datajoint.utils.get_master", "text": "If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + __ . Example ephys . session -- master ephys . session__recording -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str Source code in datajoint/utils.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_master ( full_table_name : str ) -> str : \"\"\" If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + ``__``. Example: `ephys`.`session` -- master `ephys`.`session__recording` -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str \"\"\" match = re . match ( r \"(?P<master>`\\w+`.`\\w+)__(?P<part>\\w+)`\" , full_table_name ) return match [ \"master\" ] + \"`\" if match else \"\"", "title": "get_master()"}, {"location": "api/datajoint/utils/#datajoint.utils.parse_sql", "text": "yield SQL statements from an SQL file Source code in datajoint/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def parse_sql ( filepath ): \"\"\" yield SQL statements from an SQL file \"\"\" delimiter = \";\" statement = [] with Path ( filepath ) . open ( \"rt\" ) as f : for line in f : line = line . strip () if not line . startswith ( \"--\" ) and len ( line ) > 1 : if line . startswith ( \"delimiter\" ): delimiter = line . split ()[ 1 ] else : statement . append ( line ) if line . endswith ( delimiter ): yield \" \" . join ( statement ) statement = []", "title": "parse_sql()"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_copy", "text": "Copy the contents of src file into dest file as a two-step process. Skip if dest exists already Source code in datajoint/utils.py 107 108 109 110 111 112 113 114 115 116 def safe_copy ( src , dest , overwrite = False ): \"\"\" Copy the contents of src file into dest file as a two-step process. Skip if dest exists already \"\"\" src , dest = Path ( src ), Path ( dest ) if not ( dest . exists () and src . samefile ( dest )) and ( overwrite or not dest . is_file ()): dest . parent . mkdir ( parents = True , exist_ok = True ) temp_file = dest . with_suffix ( dest . suffix + \".copying\" ) shutil . copyfile ( str ( src ), str ( temp_file )) temp_file . rename ( dest )", "title": "safe_copy()"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_write", "text": "A two-step write. :param filename: full path :param blob: binary data Source code in datajoint/utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 def safe_write ( filepath , blob ): \"\"\" A two-step write. :param filename: full path :param blob: binary data \"\"\" filepath = Path ( filepath ) if not filepath . is_file (): filepath . parent . mkdir ( parents = True , exist_ok = True ) temp_file = filepath . with_suffix ( filepath . suffix + \".saving\" ) temp_file . write_bytes ( blob ) temp_file . rename ( filepath )", "title": "safe_write()"}, {"location": "api/datajoint/utils/#datajoint.utils.to_camel_case", "text": "Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: to_camel_case(\"table_name\") # returns \"TableName\" Source code in datajoint/utils.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def to_camel_case ( s ): \"\"\" Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" \"\"\" def to_upper ( match ): return match . group ( 0 )[ - 1 ] . upper () return re . sub ( r \"(^|[_\\W])+[a-zA-Z]\" , to_upper , s )", "title": "to_camel_case()"}, {"location": "api/datajoint/utils/#datajoint.utils.user_choice", "text": "Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice Source code in datajoint/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def user_choice ( prompt , choices = ( \"yes\" , \"no\" ), default = None ): \"\"\" Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice \"\"\" assert default is None or default in choices choice_list = \", \" . join ( ( choice . title () if choice == default else choice for choice in choices ) ) response = None while response not in choices : response = input ( prompt + \" [\" + choice_list + \"]: \" ) response = response . lower () if response else default return response", "title": "user_choice()"}, {"location": "api/datajoint/version/", "text": "", "title": "version.py"}]}