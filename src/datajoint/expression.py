from __future__ import annotations

import copy
import inspect
import logging
import re
from itertools import count
from typing import TYPE_CHECKING, Any

from .condition import (
    AndList,
    Not,
    Top,
    assert_join_compatibility,
    extract_column_names,
    make_condition,
    translate_attribute,
)
from .declare import CONSTANT_LITERALS
import numpy as np
import pandas

from .errors import DataJointError
from .fetch import Fetch1, _get
from .preview import preview, repr_html
from .settings import config

if TYPE_CHECKING:
    from .connection import Connection
    from .heading import Heading

logger = logging.getLogger(__name__.split(".")[0])


class QueryExpression:
    """
    QueryExpression implements query operators to derive new entity set from its input.
    A QueryExpression object generates a SELECT statement in SQL.
    QueryExpression operators are restrict, join, proj, aggr, and union.

    A QueryExpression object has a support, a restriction (an AndList), and heading.
    Property `heading` (type dj.Heading) contains information about the attributes.
    It is loaded from the database and updated by proj.

    Property `support` is the list of table names or other QueryExpressions to be joined.

    The restriction is applied first without having access to the attributes generated by the projection.
    Then projection is applied by selecting modifying the heading attribute.

    Application of operators does not always lead to the creation of a subquery.
    A subquery is generated when:
        1. A restriction is applied on any computed or renamed attributes
        2. A projection is applied remapping remapped attributes
        3. Subclasses: Join, Aggregation, and Union have additional specific rules.
    """

    _restriction = None
    _restriction_attributes = None
    _joins = []  # list of (is_left: bool, using_attrs: list[str]) for each join
    _original_heading = None  # heading before projections

    # subclasses or instantiators must provide values
    _connection = None
    _heading = None
    _support = None
    _top = None

    # If the query will be using distinct
    _distinct = False

    @property
    def connection(self) -> Connection:
        """
        The database connection for this query.

        Returns
        -------
        dj.Connection
            Active database connection.
        """
        assert self._connection is not None
        return self._connection

    @property
    def support(self) -> list[str | QueryExpression]:
        """
        Tables or subqueries forming the FROM clause.

        Returns
        -------
        list
            Table names (str) or QueryExpression subqueries.
        """
        assert self._support is not None
        return self._support

    @property
    def heading(self) -> Heading:
        """
        Column information after projection.

        Returns
        -------
        dj.Heading
            Heading reflecting any applied projection.
        """
        return self._heading

    @property
    def original_heading(self) -> Heading:
        """
        Column information before projection.

        Returns
        -------
        dj.Heading
            Original heading without projection effects.
        """
        return self._original_heading or self.heading

    @property
    def restriction(self) -> AndList:
        """
        Restrictions applied to produce the result.

        Returns
        -------
        AndList
            Conjunction of restriction conditions.
        """
        if self._restriction is None:
            self._restriction = AndList()
        return self._restriction

    @property
    def restriction_attributes(self) -> set[str]:
        """
        Attribute names used in the WHERE clause.

        Returns
        -------
        set
            Names of attributes referenced by restrictions.
        """
        if self._restriction_attributes is None:
            self._restriction_attributes = set()
        return self._restriction_attributes

    @property
    def primary_key(self) -> list[str]:
        return self.heading.primary_key

    _subquery_alias_count = count()  # count for alias names used in the FROM clause

    def from_clause(self):
        support = (
            (
                "(" + src.make_sql() + ") as `$%x`" % next(self._subquery_alias_count)
                if isinstance(src, QueryExpression)
                else src
            )
            for src in self.support
        )
        clause = next(support)
        for s, (is_left, using_attrs) in zip(support, self._joins):
            left_kw = "LEFT " if is_left else ""
            if using_attrs:
                using = "USING ({})".format(", ".join(f"`{a}`" for a in using_attrs))
                clause += f" {left_kw}JOIN {s} {using}"
            else:
                # Cross join (no common non-hidden attributes)
                if is_left:
                    clause += f" LEFT JOIN {s} ON TRUE"
                else:
                    clause += f" CROSS JOIN {s}"
        return clause

    def where_clause(self):
        return "" if not self.restriction else " WHERE (%s)" % ")AND(".join(str(s) for s in self.restriction)

    def sorting_clauses(self):
        if not self._top:
            return ""
        clause = ", ".join(_wrap_attributes(_flatten_attribute_list(self.primary_key, self._top.order_by)))
        if clause:
            clause = f" ORDER BY {clause}"
        if self._top.limit is not None:
            clause += f" LIMIT {self._top.limit}{f' OFFSET {self._top.offset}' if self._top.offset else ''}"

        return clause

    def make_sql(self, fields: list[str] | None = None) -> str:
        """
        Generate the SQL SELECT statement for this query.

        Parameters
        ----------
        fields : list, optional
            Attribute names to select. If None, uses heading attributes.

        Returns
        -------
        str
            Complete SQL SELECT statement.
        """
        return "SELECT {distinct}{fields} FROM {from_}{where}{sorting}".format(
            distinct="DISTINCT " if self._distinct else "",
            fields=self.heading.as_sql(fields or self.heading.names),
            from_=self.from_clause(),
            where=self.where_clause(),
            sorting=self.sorting_clauses(),
        )

    # --------- query operators -----------
    def make_subquery(self) -> QueryExpression:
        """
        Create a new query with this expression as a subquery.

        Returns
        -------
        QueryExpression
            New expression with self in the FROM clause.
        """
        result = QueryExpression()
        result._connection = self.connection
        result._support = [self]
        result._heading = self.heading.make_subquery_heading()
        return result

    def restrict(self, restriction: Any, semantic_check: bool = True) -> QueryExpression:
        """
        Apply a restriction (WHERE clause) to this expression.

        Parameters
        ----------
        restriction : various
            Condition to apply. Can be:

            - str: SQL condition (e.g., ``"x > 5"``)
            - dict: Attribute-value pairs (equality)
            - QueryExpression: Match on common attributes
            - AndList: Conjunction of conditions
            - list/tuple: Disjunction (OR) of conditions
            - bool: True = no effect, False = empty result

        semantic_check : bool, optional
            If True (default), use semantic matching - only match on
            homologous namesakes and error on non-homologous namesakes.
            If False, use natural matching on all namesakes.

        Returns
        -------
        QueryExpression
            New expression with restriction applied.

        Notes
        -----
        ``rel & restriction`` is equivalent to ``rel.restrict(restriction)``.
        ``rel - restriction`` is equivalent to ``rel.restrict(Not(restriction))``.

        Successive restrictions combine as logical AND.
        Collections (except AndList) are treated as OR lists.

        Examples
        --------
        >>> table & "session_id > 5"        # SQL condition
        >>> table & {"subject": "mouse1"}   # Equality
        >>> table & other_table             # Match on common attributes
        >>> table - {"status": "failed"}    # Anti-restriction
        """
        attributes = set()
        if isinstance(restriction, Top):
            result = (
                self.make_subquery() if self._top and not self._top.__eq__(restriction) else copy.copy(self)
            )  # make subquery to avoid overwriting existing Top
            result._top = restriction
            return result
        new_condition = make_condition(self, restriction, attributes, semantic_check=semantic_check)
        if new_condition is True:
            return self  # restriction has no effect, return the same object
        # check that all attributes in condition are present in the query
        try:
            raise DataJointError(
                "Attribute `%s` is not found in query." % next(attr for attr in attributes if attr not in self.heading.names)
            )
        except StopIteration:
            pass  # all ok
        # If the new condition uses any new attributes, a subquery is required.
        # However, Aggregation's HAVING statement works fine with aliased attributes.
        need_subquery = (
            isinstance(self, Union) or (not isinstance(self, Aggregation) and self.heading.new_attributes) or self._top
        )
        if need_subquery:
            result = self.make_subquery()
        else:
            result = copy.copy(self)
            result._restriction = AndList(self.restriction)  # copy to preserve the original
        result.restriction.append(new_condition)
        result.restriction_attributes.update(attributes)
        return result

    def restrict_in_place(self, restriction):
        self.__dict__.update(self.restrict(restriction).__dict__)

    def __and__(self, restriction: Any) -> QueryExpression:
        """
        Restriction operator (``&``).

        Returns
        -------
        QueryExpression
            Restricted copy. See ``restrict()`` for details.
        """
        return self.restrict(restriction)

    def __xor__(self, restriction):
        """The ^ operator has been removed in DataJoint 2.0."""
        raise DataJointError(
            "The ^ operator has been removed in DataJoint 2.0. "
            "Use .restrict(other, semantic_check=False) for restrictions without semantic checking."
        )

    def __sub__(self, restriction: Any) -> QueryExpression:
        """
        Anti-restriction operator (``-``).

        Returns
        -------
        QueryExpression
            Rows NOT matching the restriction. See ``restrict()`` for details.
        """
        return self.restrict(Not(restriction))

    def __neg__(self) -> Not | QueryExpression:
        """
        Negation operator (``-expr``).

        Returns
        -------
        Not
            Negated restriction for use in other expressions.
        """
        if isinstance(self, Not):
            return self.restriction
        return Not(self)

    def __mul__(self, other: QueryExpression) -> QueryExpression:
        """
        Join operator (``*``).

        Returns
        -------
        QueryExpression
            Joined result. See ``join()`` for details.
        """
        return self.join(other)

    def __matmul__(self, other):
        """The @ operator has been removed in DataJoint 2.0."""
        raise DataJointError(
            "The @ operator has been removed in DataJoint 2.0. "
            "Use .join(other, semantic_check=False) for joins without semantic checking."
        )

    def join(
        self, other: QueryExpression | type, semantic_check: bool = True, left: bool = False, allow_nullable_pk: bool = False
    ) -> QueryExpression:
        """
        Join this expression with another.

        Parameters
        ----------
        other : QueryExpression
            Expression to join with.
        semantic_check : bool, optional
            If True (default), use semantic matching - only match on
            homologous namesakes (same lineage) and error on non-homologous
            namesakes. If False, use natural join on all namesakes.
        left : bool, optional
            If True, perform a left join (retain all rows from self).
            Requires self to determine other. Default False.
        allow_nullable_pk : bool, optional
            If True, bypass left join constraint. Used internally by
            aggregation with keep_all_rows=True. Default False.

        Returns
        -------
        QueryExpression
            Joined result with combined attributes.

        Notes
        -----
        ``a * b`` is equivalent to ``a.join(b)``.
        """
        # Joining with U is no longer supported
        if isinstance(other, U):
            raise DataJointError(
                "table * dj.U(...) is no longer supported in DataJoint 2.0. "
                "This pattern is no longer necessary with the new semantic matching system."
            )
        if inspect.isclass(other) and issubclass(other, QueryExpression):
            other = other()  # instantiate
        if not isinstance(other, QueryExpression):
            raise DataJointError("The argument of join must be a QueryExpression")
        assert_join_compatibility(self, other, semantic_check=semantic_check)

        # Left join constraint: requires self → other (left operand determines right)
        # This ensures the result's PK (which is PK(self) for left joins) can't have NULLs
        nullable_pk = False
        if left and not self.heading.determines(other.heading):
            if allow_nullable_pk:
                nullable_pk = True
            else:
                undetermined = [attr for attr in other.heading.primary_key if attr not in self.heading.names]
                raise DataJointError(
                    "Left join requires the left operand to determine the right operand (A → B). "
                    f"The following attributes from the right operand's primary key are not "
                    f"in the left operand: {undetermined}. "
                    "Use an inner join or restructure the query."
                )

        # Always join on all non-hidden namesakes
        join_attributes = set(n for n in self.heading.names if n in other.heading.names)
        # needs subquery if self's FROM clause has common attributes with other's FROM clause
        need_subquery1 = need_subquery2 = bool(
            (set(self.original_heading.names) & set(other.original_heading.names)) - join_attributes
        )
        # need subquery if any of the join attributes are derived
        need_subquery1 = (
            need_subquery1
            or isinstance(self, Aggregation)
            or any(n in self.heading.new_attributes for n in join_attributes)
            or isinstance(self, Union)
        )
        need_subquery2 = (
            need_subquery2
            or isinstance(other, Aggregation)
            or any(n in other.heading.new_attributes for n in join_attributes)
            or isinstance(self, Union)
        )
        # With USING clause (instead of NATURAL JOIN), we need subqueries when
        # joining with multi-table expressions to ensure correct column matching
        if len(self.support) > 1 and join_attributes:
            need_subquery1 = True
        if len(other.support) > 1 and join_attributes:
            need_subquery2 = True
        if need_subquery1:
            self = self.make_subquery()
        if need_subquery2:
            other = other.make_subquery()
        result = QueryExpression()
        result._connection = self.connection
        result._support = self.support + other.support
        # Store join info: (is_left, using_attrs) - using_attrs excludes hidden attributes
        using_attrs = [n for n in self.heading.names if n in other.heading.names]
        result._joins = self._joins + [(left, using_attrs)] + other._joins
        result._heading = self.heading.join(other.heading, nullable_pk=nullable_pk)
        result._restriction = AndList(self.restriction)
        result._restriction.append(other.restriction)
        result._original_heading = self.original_heading.join(other.original_heading, nullable_pk=nullable_pk)
        assert len(result.support) == len(result._joins) + 1
        return result

    def extend(self, other: QueryExpression, semantic_check: bool = True) -> QueryExpression:
        """
        Extend this expression with attributes from another.

        Adds attributes from ``other`` while preserving this expression's
        entity identity. Semantically equivalent to ``self.join(other, left=True)``.

        Parameters
        ----------
        other : QueryExpression
            Expression whose attributes will extend self.
        semantic_check : bool, optional
            If True (default), require homologous namesakes. If False, match
            on all namesakes without lineage checking.

        Returns
        -------
        QueryExpression
            Extended result with self's primary key and combined attributes.

        Raises
        ------
        DataJointError
            If self does not determine other (A → B required).

        Notes
        -----
        Requires ``self → other``: every attribute in other's primary key
        must exist in self. This ensures:

        - All rows of self are preserved
        - Self's primary key remains the result's primary key
        - No NULL values in primary key

        Examples
        --------
        >>> # Trial determines Session (session_id in Trial's PK)
        >>> Trial.extend(Session)  # Adds session attrs to each trial
        """
        return self.join(other, semantic_check=semantic_check, left=True)

    def __add__(self, other: QueryExpression) -> Union:
        """
        Union operator (``+``).

        Returns
        -------
        Union
            Combined entity set with matching primary keys.
        """
        return Union.create(self, other)

    def proj(self, *attributes: str, **named_attributes: str) -> QueryExpression:
        """
        Select, rename, or compute attributes.

        Parameters
        ----------
        *attributes : str
            Attributes to include (primary key always included).
            Use ``...`` (Ellipsis) to include all secondary attributes.
            Prefix with ``-`` to exclude (e.g., ``"-attr"``).
        **named_attributes : str
            New or renamed attributes. Values can be:

            - ``"attr"``: Rename existing attribute
            - ``"(attr)"``: Duplicate attribute with new name
            - ``"expr"``: SQL expression computing new attribute

        Returns
        -------
        QueryExpression
            Projected expression with selected attributes.

        Examples
        --------
        >>> table.proj()                    # Primary key only
        >>> table.proj(...)                 # All attributes
        >>> table.proj('a', 'b')            # PK + 'a' + 'b'
        >>> table.proj(..., '-secret')      # All except 'secret'
        >>> table.proj(new_name='old')      # Rename 'old' to 'new_name'
        >>> table.proj(total='x + y')       # Computed attribute
        """
        named_attributes = {k: translate_attribute(v)[1] for k, v in named_attributes.items()}
        # new attributes in parentheses are included again with the new name without removing original
        duplication_pattern = re.compile(rf"^\s*\(\s*(?!{'|'.join(CONSTANT_LITERALS)})(?P<name>[a-zA-Z_]\w*)\s*\)\s*$")
        # attributes without parentheses renamed
        rename_pattern = re.compile(rf"^\s*(?!{'|'.join(CONSTANT_LITERALS)})(?P<name>[a-zA-Z_]\w*)\s*$")
        replicate_map = {
            k: m.group("name") for k, m in ((k, duplication_pattern.match(v)) for k, v in named_attributes.items()) if m
        }
        rename_map = {k: m.group("name") for k, m in ((k, rename_pattern.match(v)) for k, v in named_attributes.items()) if m}
        compute_map = {
            k: v for k, v in named_attributes.items() if not duplication_pattern.match(v) and not rename_pattern.match(v)
        }
        attributes = set(attributes)
        # include primary key
        attributes.update((k for k in self.primary_key if k not in rename_map.values()))
        # include all secondary attributes with Ellipsis
        if Ellipsis in attributes:
            attributes.discard(Ellipsis)
            attributes.update(
                (a for a in self.heading.secondary_attributes if a not in attributes and a not in rename_map.values())
            )
        try:
            raise DataJointError(
                "%s is not a valid data type for an attribute in .proj" % next(a for a in attributes if not isinstance(a, str))
            )
        except StopIteration:
            pass  # normal case
        # remove excluded attributes, specified as `-attr'
        excluded = set(a for a in attributes if a.strip().startswith("-"))
        attributes.difference_update(excluded)
        excluded = set(a.lstrip("-").strip() for a in excluded)
        attributes.difference_update(excluded)
        try:
            raise DataJointError(
                "Cannot exclude primary key attribute %s",
                next(a for a in excluded if a in self.primary_key),
            )
        except StopIteration:
            pass  # all ok
        # check that all attributes exist in heading
        try:
            raise DataJointError("Attribute `%s` not found." % next(a for a in attributes if a not in self.heading.names))
        except StopIteration:
            pass  # all ok

        # check that all mentioned names are present in heading
        mentions = attributes.union(replicate_map.values()).union(rename_map.values())
        try:
            raise DataJointError("Attribute '%s' not found." % next(a for a in mentions if not self.heading.names))
        except StopIteration:
            pass  # all ok

        # check that newly created attributes do not clash with any other selected attributes
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in rename_map if a in attributes.union(compute_map).union(replicate_map))
            )
        except StopIteration:
            pass  # all ok
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in compute_map if a in attributes.union(rename_map).union(replicate_map))
            )
        except StopIteration:
            pass  # all ok
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in replicate_map if a in attributes.union(rename_map).union(compute_map))
            )
        except StopIteration:
            pass  # all ok

        # need a subquery if the projection remaps any remapped attributes
        used = set(q for v in compute_map.values() for q in extract_column_names(v))
        used.update(rename_map.values())
        used.update(replicate_map.values())
        used.intersection_update(self.heading.names)
        need_subquery = isinstance(self, Union) or any(self.heading[name].attribute_expression is not None for name in used)
        if not need_subquery and self.restriction:
            # need a subquery if the restriction applies to attributes that have been renamed
            need_subquery = any(name in self.restriction_attributes for name in self.heading.new_attributes)

        result = self.make_subquery() if need_subquery else copy.copy(self)
        result._original_heading = result.original_heading
        result._heading = result.heading.select(
            attributes,
            rename_map=dict(**rename_map, **replicate_map),
            compute_map=compute_map,
        )
        return result

    def aggr(
        self, group: QueryExpression, *attributes: str, keep_all_rows: bool = False, **named_attributes: str
    ) -> QueryExpression:
        """
        Aggregate data grouped by this expression's primary key.

        Parameters
        ----------
        group : QueryExpression
            Expression to be aggregated.
        *attributes : str
            Attributes from self to include (primary key always included).
        keep_all_rows : bool, optional
            If True, keep all rows from self (left join). If False (default),
            keep only rows matching entries in group.
        **named_attributes : str
            Aggregation computations as SQL expressions, e.g.,
            ``count="count(*)"``, ``total="sum(value)"``.

        Returns
        -------
        QueryExpression
            Aggregated result.

        Examples
        --------
        >>> Session.aggr(Trial, n_trials="count(*)")
        >>> Subject.aggr(Session, ..., first="min(session_date)")
        """
        if Ellipsis in attributes:
            # expand ellipsis to include only attributes from the left table
            attributes = set(attributes)
            attributes.discard(Ellipsis)
            attributes.update(self.heading.secondary_attributes)
        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(*attributes, **named_attributes)

    aggregate = aggr  # alias for aggr

    # ---------- Fetch operators --------------------
    @property
    def fetch(self):
        """
        The fetch() method has been removed in DataJoint 2.0.

        Use the new explicit output methods instead:
        - table.to_dicts()           # list of dictionaries
        - table.to_pandas()          # pandas DataFrame
        - table.to_arrays()          # numpy structured array
        - table.to_arrays('a', 'b')  # tuple of numpy arrays
        - table.keys()               # primary keys as list[dict]
        - table.to_polars()          # polars DataFrame (requires pip install datajoint[polars])
        - table.to_arrow()           # PyArrow Table (requires pip install datajoint[arrow])

        For single-row fetch, use fetch1() which is unchanged.

        See migration guide: https://docs.datajoint.com/migration/fetch-api
        """
        raise AttributeError(
            "fetch() has been removed in DataJoint 2.0. "
            "Use to_dicts(), to_pandas(), to_arrays(), or keys() instead. "
            "See table.fetch.__doc__ for details."
        )

    @property
    def fetch1(self):
        return Fetch1(self)

    def _apply_top(self, order_by=None, limit=None, offset=None):
        """Apply order_by, limit, offset if specified, return modified expression."""
        if order_by is not None or limit is not None or offset is not None:
            return self.restrict(Top(limit, order_by, offset))
        return self

    def to_dicts(
        self,
        order_by: str | list[str] | None = None,
        limit: int | None = None,
        offset: int | None = None,
        squeeze: bool = False,
        download_path: str = ".",
    ) -> list[dict[str, Any]]:
        """
        Fetch all rows as a list of dictionaries.

        Parameters
        ----------
        order_by : str or list, optional
            Attribute(s) to order by. Use ``"KEY"`` or ``"KEY DESC"`` for
            primary key ordering.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.
        squeeze : bool, optional
            If True, remove extra dimensions from arrays. Default False.
        download_path : str, optional
            Path for downloading external data. Default ``"."``.

        Returns
        -------
        list[dict]
            One dictionary per row with attribute names as keys.
        """
        expr = self._apply_top(order_by, limit, offset)
        cursor = expr.cursor(as_dict=True)
        heading = expr.heading
        return [
            {name: _get(expr.connection, heading[name], row[name], squeeze, download_path) for name in heading.names}
            for row in cursor
        ]

    def to_pandas(
        self,
        order_by: str | list[str] | None = None,
        limit: int | None = None,
        offset: int | None = None,
        squeeze: bool = False,
        download_path: str = ".",
    ) -> pandas.DataFrame:
        """
        Fetch all rows as a pandas DataFrame with primary key as index.

        Parameters
        ----------
        order_by : str or list, optional
            Attribute(s) to order by.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.
        squeeze : bool, optional
            If True, remove extra dimensions from arrays. Default False.
        download_path : str, optional
            Path for downloading external data. Default ``"."``.

        Returns
        -------
        pandas.DataFrame
            DataFrame with primary key columns as index.

        See Also
        --------
        insert_dataframe : Insert DataFrame back to table.
        """
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze, download_path=download_path)
        df = pandas.DataFrame(dicts)
        if len(df) > 0 and self.primary_key:
            df = df.set_index(self.primary_key)
        return df

    def to_polars(
        self,
        order_by: str | list[str] | None = None,
        limit: int | None = None,
        offset: int | None = None,
        squeeze: bool = False,
        download_path: str = ".",
    ):
        """
        Fetch all rows as a polars DataFrame.

        Parameters
        ----------
        order_by : str or list, optional
            Attribute(s) to order by.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.
        squeeze : bool, optional
            If True, remove extra dimensions from arrays. Default False.
        download_path : str, optional
            Path for downloading external data. Default ``"."``.

        Returns
        -------
        polars.DataFrame
            Polars DataFrame with all rows.

        Raises
        ------
        ImportError
            If polars is not installed. Install with ``pip install datajoint[polars]``.
        """
        try:
            import polars
        except ImportError:
            raise ImportError("polars is required for to_polars(). " "Install with: pip install datajoint[polars]")
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze, download_path=download_path)
        return polars.DataFrame(dicts)

    def to_arrow(
        self,
        order_by: str | list[str] | None = None,
        limit: int | None = None,
        offset: int | None = None,
        squeeze: bool = False,
        download_path: str = ".",
    ):
        """
        Fetch all rows as a PyArrow Table.

        Parameters
        ----------
        order_by : str or list, optional
            Attribute(s) to order by.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.
        squeeze : bool, optional
            If True, remove extra dimensions from arrays. Default False.
        download_path : str, optional
            Path for downloading external data. Default ``"."``.

        Returns
        -------
        pyarrow.Table
            PyArrow Table with all rows.

        Raises
        ------
        ImportError
            If pyarrow is not installed. Install with ``pip install datajoint[arrow]``.
        """
        try:
            import pyarrow
        except ImportError:
            raise ImportError("pyarrow is required for to_arrow(). " "Install with: pip install datajoint[arrow]")
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze, download_path=download_path)
        if not dicts:
            return pyarrow.table({})
        return pyarrow.Table.from_pylist(dicts)

    def to_arrays(
        self,
        *attrs: str,
        include_key: bool = False,
        order_by: str | list[str] | None = None,
        limit: int | None = None,
        offset: int | None = None,
        squeeze: bool = False,
        download_path: str = ".",
    ) -> np.ndarray | tuple[np.ndarray, ...]:
        """
        Fetch data as numpy arrays.

        Parameters
        ----------
        *attrs : str
            Attribute names to fetch. If empty, fetch all as structured array.
        include_key : bool, optional
            If True and attrs specified, include primary key columns. Default False.
        order_by : str or list, optional
            Attribute(s) to order by.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.
        squeeze : bool, optional
            If True, remove extra dimensions from arrays. Default False.
        download_path : str, optional
            Path for downloading external data. Default ``"."``.

        Returns
        -------
        numpy.ndarray or tuple
            If no attrs: structured array (recarray) of all columns.
            If single attr: 1D array of values.
            If multiple attrs: tuple of arrays.

        Examples
        --------
        >>> table.to_arrays()              # Structured array of all columns
        >>> table.to_arrays('x', 'y')      # Tuple of two arrays
        >>> x = table.to_arrays('x')       # Single array
        """
        from functools import partial

        expr = self._apply_top(order_by, limit, offset)
        heading = expr.heading

        if attrs:
            # Fetch specific attributes as tuple of arrays
            if include_key:
                fetch_attrs = list(expr.primary_key) + [a for a in attrs if a not in expr.primary_key]
            else:
                fetch_attrs = list(attrs)

            # Project to only needed columns
            projected = expr.proj(*fetch_attrs)
            dicts = projected.to_dicts(squeeze=squeeze, download_path=download_path)

            # Extract arrays for requested attributes
            result_arrays = []
            for attr in attrs:
                values = [d[attr] for d in dicts]
                # Try to create a homogeneous array, fall back to object array for variable-size data
                try:
                    arr = np.array(values)
                except ValueError:
                    # Variable-size data (e.g., arrays of different shapes)
                    arr = np.array(values, dtype=object)
                result_arrays.append(arr)

            return result_arrays[0] if len(attrs) == 1 else tuple(result_arrays)
        else:
            # Fetch all columns as structured array
            get = partial(_get, expr.connection, squeeze=squeeze, download_path=download_path)
            cursor = expr.cursor(as_dict=False)
            rows = list(cursor.fetchall())

            if not rows:
                return np.array([], dtype=heading.as_dtype)

            # Build dtype, detecting blob types from first row
            import numbers

            record_type = np.dtype(
                [
                    (name, type(value))
                    if heading[name].is_blob and isinstance(value, numbers.Number)
                    else (name, heading.as_dtype[name])
                    for value, name in zip(rows[0], heading.as_dtype.names)
                ]
            )

            ret = np.array(rows, dtype=record_type)
            # Decode blobs and codecs
            for name in heading:
                ret[name] = list(map(partial(get, heading[name]), ret[name]))
            return ret

    def keys(
        self, order_by: str | list[str] | None = None, limit: int | None = None, offset: int | None = None
    ) -> list[dict[str, Any]]:
        """
        Fetch primary key values as a list of dictionaries.

        Parameters
        ----------
        order_by : str or list, optional
            Attribute(s) to order by.
        limit : int, optional
            Maximum number of rows to return.
        offset : int, optional
            Number of rows to skip.

        Returns
        -------
        list[dict]
            Primary key values only.
        """
        return self.proj().to_dicts(order_by=order_by, limit=limit, offset=offset)

    def head(self, limit: int = 25) -> list[dict[str, Any]]:
        """
        Preview the first entries from the query result.

        Parameters
        ----------
        limit : int, optional
            Number of entries to return. Default 25.

        Returns
        -------
        list[dict]
            First entries ordered by primary key.
        """
        return self.to_dicts(order_by="KEY", limit=limit)

    def tail(self, limit: int = 25) -> list[dict[str, Any]]:
        """
        Preview the last entries from the query result.

        Parameters
        ----------
        limit : int, optional
            Number of entries to return. Default 25.

        Returns
        -------
        list[dict]
            Last entries ordered by primary key.
        """
        return list(reversed(self.to_dicts(order_by="KEY DESC", limit=limit)))

    def __len__(self) -> int:
        """
        Return the number of rows in the result set.

        Returns
        -------
        int
            Row count.
        """
        result = self.make_subquery() if self._top else copy.copy(self)
        has_left_join = any(is_left for is_left, _ in result._joins)
        return result.connection.query(
            "SELECT {select_} FROM {from_}{where}".format(
                select_=(
                    "count(*)"
                    if has_left_join
                    else "count(DISTINCT {fields})".format(
                        fields=result.heading.as_sql(result.primary_key, include_aliases=False)
                    )
                ),
                from_=result.from_clause(),
                where=result.where_clause(),
            )
        ).fetchone()[0]

    def __bool__(self) -> bool:
        """
        Check if the result set is non-empty.

        Returns
        -------
        bool
            True if at least one row exists. More efficient than ``len(self) > 0``.
        """
        return bool(
            self.connection.query(
                "SELECT EXISTS(SELECT 1 FROM {from_}{where})".format(from_=self.from_clause(), where=self.where_clause())
            ).fetchone()[0]
        )

    def __contains__(self, item):
        """
        returns True if the restriction in item matches any entries in self
            e.g. ``restriction in q1``.

        :param item: any restriction
        (item in query_expression) is equivalent to bool(query_expression & item) but may be
        executed more efficiently.
        """
        return bool(self & item)  # May be optimized e.g. using an EXISTS query

    def __iter__(self):
        """
        Lazy streaming iterator over rows as dictionaries.

        Yields one row at a time from a single database cursor, efficiently
        streaming data without loading all rows into memory.

        :yields: dict for each row
        """
        cursor = self.cursor(as_dict=True)
        heading = self.heading
        for row in cursor:
            yield {
                name: _get(self.connection, heading[name], row[name], squeeze=False, download_path=".")
                for name in heading.names
            }

    def cursor(self, as_dict=False):
        """
        Execute the query and return a database cursor.

        :param as_dict: if True, rows are returned as dictionaries
        :return: database query cursor
        """
        sql = self.make_sql()
        logger.debug(sql)
        return self.connection.query(sql, as_dict=as_dict)

    def __repr__(self):
        """
        returns the string representation of a QueryExpression object e.g. ``str(q1)``.

        :param self: A query expression
        :type self: :class:`QueryExpression`
        :rtype: str
        """
        return super().__repr__() if config["loglevel"].lower() == "debug" else self.preview()

    def preview(self, limit=None, width=None):
        """:return: a string of preview of the contents of the query."""
        return preview(self, limit, width)

    def _repr_html_(self):
        """:return: HTML to display table in Jupyter notebook."""
        return repr_html(self)


class Aggregation(QueryExpression):
    """
    Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set
    with primary key from arg.
    The computed arguments comp1, ..., compn use aggregation calculations on the attributes of
    group or simple projections and calculations on the attributes of arg.
    Aggregation is used QueryExpression.aggr and U.aggr.
    Aggregation is a private class in DataJoint, not exposed to users.
    """

    _left_restrict = None  # the pre-GROUP BY conditions for the WHERE clause
    _subquery_alias_count = count()

    @classmethod
    def create(cls, groupby, group, keep_all_rows=False):
        """
        Create an aggregation expression.

        :param groupby: The expression to GROUP BY (determines the result's primary key)
        :param group: The expression to aggregate over
        :param keep_all_rows: If True, use left join to keep all rows from groupby
        """
        if inspect.isclass(group) and issubclass(group, QueryExpression):
            group = group()  # instantiate if a class
        assert isinstance(group, QueryExpression)

        # Aggregation requires group → groupby: every attribute in groupby's PK
        # must be in group, so we can GROUP BY groupby's PK.
        # Skip check for U (universal set) which doesn't have a heading until joined.
        if not isinstance(groupby, U) and not group.heading.determines(groupby.heading):
            missing = [attr for attr in groupby.heading.primary_key if attr not in group.heading.names]
            raise DataJointError(
                "Aggregation requires the group expression to contain all primary key "
                f"attributes of the grouping expression. Missing attributes: {missing}."
            )

        if keep_all_rows and len(group.support) > 1 or group.heading.new_attributes:
            group = group.make_subquery()  # subquery if left joining a join
        # When keep_all_rows=True, we use a left join which normally requires A → B.
        # Aggregation has the opposite requirement (B → A). We bypass the left join
        # constraint because GROUP BY resets the PK to groupby's PK (never NULL).
        join = groupby.join(group, left=keep_all_rows, allow_nullable_pk=keep_all_rows)
        result = cls()
        result._connection = join.connection
        result._heading = join.heading.set_primary_key(groupby.primary_key)
        result._support = join.support
        result._joins = join._joins
        result._left_restrict = join.restriction  # WHERE clause applied before GROUP BY
        result._grouping_attributes = result.primary_key

        return result

    def where_clause(self):
        return "" if not self._left_restrict else " WHERE (%s)" % ")AND(".join(str(s) for s in self._left_restrict)

    def make_sql(self, fields=None):
        fields = self.heading.as_sql(fields or self.heading.names)
        assert self._grouping_attributes or not self.restriction
        distinct = set(self.heading.names) == set(self.primary_key)
        return "SELECT {distinct}{fields} FROM {from_}{where}{group_by}{sorting}".format(
            distinct="DISTINCT " if distinct else "",
            fields=fields,
            from_=self.from_clause(),
            where=self.where_clause(),
            group_by=(
                ""
                if not self.primary_key
                else (
                    " GROUP BY `%s`" % "`,`".join(self._grouping_attributes)
                    + ("" if not self.restriction else " HAVING (%s)" % ")AND(".join(self.restriction))
                )
            ),
            sorting=self.sorting_clauses(),
        )

    def __len__(self):
        return self.connection.query(
            "SELECT count(1) FROM ({subquery}) `${alias:x}`".format(
                subquery=self.make_sql(), alias=next(self._subquery_alias_count)
            )
        ).fetchone()[0]

    def __bool__(self):
        return bool(self.connection.query("SELECT EXISTS({sql})".format(sql=self.make_sql())))


class Union(QueryExpression):
    """
    Union is the private DataJoint class that implements the union operator.
    """

    __count = count()

    @classmethod
    def create(cls, arg1, arg2):
        if inspect.isclass(arg2) and issubclass(arg2, QueryExpression):
            arg2 = arg2()  # instantiate if a class
        if not isinstance(arg2, QueryExpression):
            raise DataJointError("A QueryExpression can only be unioned with another QueryExpression")
        if arg1.connection != arg2.connection:
            raise DataJointError("Cannot operate on QueryExpressions originating from different connections.")
        if set(arg1.primary_key) != set(arg2.primary_key):
            raise DataJointError("The operands of a union must share the same primary key.")
        if set(arg1.heading.secondary_attributes) & set(arg2.heading.secondary_attributes):
            raise DataJointError("The operands of a union must not share any secondary attributes.")
        result = cls()
        result._connection = arg1.connection
        result._heading = arg1.heading.join(arg2.heading)
        result._support = [arg1, arg2]
        return result

    def make_sql(self):
        arg1, arg2 = self._support
        if not arg1.heading.secondary_attributes and not arg2.heading.secondary_attributes:
            # no secondary attributes: use UNION DISTINCT
            fields = arg1.primary_key
            return "SELECT * FROM (({sql1}) UNION ({sql2})) as `_u{alias}{sorting}`".format(
                sql1=(arg1.make_sql() if isinstance(arg1, Union) else arg1.make_sql(fields)),
                sql2=(arg2.make_sql() if isinstance(arg2, Union) else arg2.make_sql(fields)),
                alias=next(self.__count),
                sorting=self.sorting_clauses(),
            )
        # with secondary attributes, use union of left join with antijoin
        fields = self.heading.names
        sql1 = arg1.join(arg2, left=True).make_sql(fields)
        sql2 = (arg2 - arg1).proj(..., **{k: "NULL" for k in arg1.heading.secondary_attributes}).make_sql(fields)
        return "({sql1})  UNION ({sql2})".format(sql1=sql1, sql2=sql2)

    def from_clause(self):
        """The union does not use a FROM clause"""
        assert False

    def where_clause(self):
        """The union does not use a WHERE clause"""
        assert False

    def __len__(self):
        return self.connection.query(
            "SELECT count(1) FROM ({subquery}) `${alias:x}`".format(
                subquery=self.make_sql(),
                alias=next(QueryExpression._subquery_alias_count),
            )
        ).fetchone()[0]

    def __bool__(self):
        return bool(self.connection.query("SELECT EXISTS({sql})".format(sql=self.make_sql())))


class U:
    """
    dj.U objects are the universal sets representing all possible values of their attributes.
    dj.U objects cannot be queried on their own but are useful for forming some queries.
    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.
    The universal set is the set of all possible combinations of values of the attributes.
    Without any attributes, dj.U() represents the set with one element that has no attributes.

    Restriction:

    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.

    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:

    >>> dj.U('contrast', 'brightness') & stimulus

    Aggregation:

    In aggregation, dj.U is used for summary calculation over an entire set:

    The following expression yields one element with one attribute `s` containing the total number of elements in
    query expression `expr`:

    >>> dj.U().aggr(expr, n='count(*)')

    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in
    query expression `expr`.

    >>> dj.U().aggr(expr, n='count(distinct attr)')
    >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')

    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`
    over entire result set of expression `expr`:

    >>> dj.U().aggr(expr, s='sum(attr)')

    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of
    their occurrences in the result set of query expression `expr`.

    >>> dj.U(attr1,attr2).aggr(expr, n='count(*)')

    Joins:

    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result
    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on
    non-primary key attributes.
    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw
    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first
    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.
    """

    def __init__(self, *primary_key):
        self._primary_key = primary_key

    @property
    def primary_key(self):
        return self._primary_key

    def __and__(self, other):
        if inspect.isclass(other) and issubclass(other, QueryExpression):
            other = other()  # instantiate if a class
        if not isinstance(other, QueryExpression):
            raise DataJointError("Set U can only be restricted with a QueryExpression.")
        result = copy.copy(other)
        result._distinct = True
        result._heading = result.heading.set_primary_key(self.primary_key)
        result = result.proj()
        return result

    def __mul__(self, other):
        """The * operator with dj.U has been removed in DataJoint 2.0."""
        raise DataJointError(
            "dj.U(...) * table is no longer supported in DataJoint 2.0. "
            "This pattern is no longer necessary with the new semantic matching system."
        )

    def __sub__(self, other):
        """Anti-restriction with dj.U produces an infinite set."""
        raise DataJointError(
            "dj.U(...) - table produces an infinite set and is not supported. "
            "Consider using a different approach for your query."
        )

    def aggr(self, group, **named_attributes):
        """
        Aggregation of the type U('attr1','attr2').aggr(group, computation="QueryExpression")
        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.

        :param group:  The query expression to be aggregated.
        :param named_attributes: computations of the form new_attribute="sql expression on attributes of group"
        :return: The derived query expression
        """
        if named_attributes.get("keep_all_rows", False):
            raise DataJointError("Cannot set keep_all_rows=True when aggregating on a universal set.")

        if inspect.isclass(group) and issubclass(group, QueryExpression):
            group = group()
        if not isinstance(group, QueryExpression):
            raise DataJointError("dj.U.aggr requires a QueryExpression as the group argument.")

        # Verify U's primary key attributes exist in group
        missing = [attr for attr in self.primary_key if attr not in group.heading.names]
        if missing:
            raise DataJointError(f"Attributes {missing} not found in the group expression.")

        # Create Aggregation directly without join - just group by U's primary key
        result = Aggregation()
        result._connection = group.connection
        result._heading = group.heading.set_primary_key(list(self.primary_key))
        result._support = group.support
        result._joins = group._joins
        result._left_restrict = group.restriction
        result._grouping_attributes = list(self.primary_key)

        return result.proj(**named_attributes)

    aggregate = aggr  # alias for aggr


def _flatten_attribute_list(primary_key, attrs):
    """
    :param primary_key: list of attributes in primary key
    :param attrs: list of attribute names, which may include "KEY", "KEY DESC" or "KEY ASC"
    :return: generator of attributes where "KEY" is replaced with its component attributes
    """
    for a in attrs:
        if re.match(r"^\s*KEY(\s+[aA][Ss][Cc])?\s*$", a):
            if primary_key:
                yield from primary_key
        elif re.match(r"^\s*KEY\s+[Dd][Ee][Ss][Cc]\s*$", a):
            if primary_key:
                yield from (q + " DESC" for q in primary_key)
        else:
            yield a


def _wrap_attributes(attr):
    for entry in attr:  # wrap attribute names in backquotes
        yield re.sub(r"\b((?!asc|desc)\w+)\b", r"`\1`", entry, flags=re.IGNORECASE)
