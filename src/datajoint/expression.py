import copy
import inspect
import logging
import re
from itertools import count

from .condition import (
    AndList,
    Not,
    Top,
    assert_join_compatibility,
    extract_column_names,
    make_condition,
    translate_attribute,
)
from .declare import CONSTANT_LITERALS
import numpy as np
import pandas

from .errors import DataJointError
from .codecs import decode_attribute
from .preview import preview, repr_html
from .settings import config

logger = logging.getLogger(__name__.split(".")[0])


class QueryExpression:
    """
    QueryExpression implements query operators to derive new entity set from its input.
    A QueryExpression object generates a SELECT statement in SQL.
    QueryExpression operators are restrict, join, proj, aggr, and union.

    A QueryExpression object has a support, a restriction (an AndList), and heading.
    Property `heading` (type dj.Heading) contains information about the attributes.
    It is loaded from the database and updated by proj.

    Property `support` is the list of table names or other QueryExpressions to be joined.

    The restriction is applied first without having access to the attributes generated by the projection.
    Then projection is applied by selecting modifying the heading attribute.

    Application of operators does not always lead to the creation of a subquery.
    A subquery is generated when:
        1. A restriction is applied on any computed or renamed attributes
        2. A projection is applied remapping remapped attributes
        3. Subclasses: Join, Aggregation, and Union have additional specific rules.
    """

    _restriction = None
    _restriction_attributes = None
    _joins = []  # list of (is_left: bool, using_attrs: list[str]) for each join
    _original_heading = None  # heading before projections

    # subclasses or instantiators must provide values
    _connection = None
    _heading = None
    _support = None
    _top = None

    # If the query will be using distinct
    _distinct = False

    @property
    def connection(self):
        """a dj.Connection object"""
        assert self._connection is not None
        return self._connection

    @property
    def support(self):
        """A list of table names or subqueries to from the FROM clause"""
        assert self._support is not None
        return self._support

    @property
    def heading(self):
        """a dj.Heading object, reflects the effects of the projection operator .proj"""
        return self._heading

    @property
    def original_heading(self):
        """a dj.Heading object reflecting the attributes before projection"""
        return self._original_heading or self.heading

    @property
    def restriction(self):
        """a AndList object of restrictions applied to input to produce the result"""
        if self._restriction is None:
            self._restriction = AndList()
        return self._restriction

    @property
    def restriction_attributes(self):
        """the set of attribute names invoked in the WHERE clause"""
        if self._restriction_attributes is None:
            self._restriction_attributes = set()
        return self._restriction_attributes

    @property
    def primary_key(self):
        return self.heading.primary_key

    _subquery_alias_count = count()  # count for alias names used in the FROM clause

    def from_clause(self):
        support = (
            (
                "(" + src.make_sql() + ") as `$%x`" % next(self._subquery_alias_count)
                if isinstance(src, QueryExpression)
                else src
            )
            for src in self.support
        )
        clause = next(support)
        for s, (is_left, using_attrs) in zip(support, self._joins):
            left_kw = "LEFT " if is_left else ""
            if using_attrs:
                using = "USING ({})".format(", ".join(f"`{a}`" for a in using_attrs))
                clause += f" {left_kw}JOIN {s} {using}"
            else:
                # Cross join (no common non-hidden attributes)
                if is_left:
                    clause += f" LEFT JOIN {s} ON TRUE"
                else:
                    clause += f" CROSS JOIN {s}"
        return clause

    def where_clause(self):
        return "" if not self.restriction else " WHERE (%s)" % ")AND(".join(str(s) for s in self.restriction)

    def sorting_clauses(self):
        if not self._top:
            return ""
        # Default to KEY ordering if order_by is None (inherit with no existing order)
        order_by = self._top.order_by if self._top.order_by is not None else ["KEY"]
        clause = ", ".join(_wrap_attributes(_flatten_attribute_list(self.primary_key, order_by)))
        if clause:
            clause = f" ORDER BY {clause}"
        if self._top.limit is not None:
            clause += f" LIMIT {self._top.limit}{f' OFFSET {self._top.offset}' if self._top.offset else ''}"

        return clause

    def make_sql(self, fields=None):
        """
        Make the SQL SELECT statement.

        :param fields: used to explicitly set the select attributes
        """
        return "SELECT {distinct}{fields} FROM {from_}{where}{sorting}".format(
            distinct="DISTINCT " if self._distinct else "",
            fields=self.heading.as_sql(fields or self.heading.names),
            from_=self.from_clause(),
            where=self.where_clause(),
            sorting=self.sorting_clauses(),
        )

    # --------- query operators -----------
    def make_subquery(self):
        """create a new SELECT statement where self is the FROM clause"""
        result = QueryExpression()
        result._connection = self.connection
        result._support = [self]
        result._heading = self.heading.make_subquery_heading()
        return result

    def restrict(self, restriction, semantic_check=True):
        """
        Produces a new expression with the new restriction applied.

        :param restriction: a sequence or an array (treated as OR list), another QueryExpression,
            an SQL condition string, or an AndList.
        :param semantic_check: If True (default), use semantic matching - only match on
            homologous namesakes and error on non-homologous namesakes.
            If False, use natural matching on all namesakes (no lineage checking).
        :return: A new QueryExpression with the restriction applied.

        rel.restrict(restriction) is equivalent to rel & restriction.
        rel.restrict(Not(restriction)) is equivalent to rel - restriction

        The primary key of the result is unaffected.
        Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b))
        Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists
        (logical disjunction of conditions)
        Inverse restriction is accomplished by either using the subtraction operator or the Not class.

        The expressions in each row equivalent:

        rel & True                          rel
        rel & False                         the empty entity set
        rel & 'TRUE'                        rel
        rel & 'FALSE'                       the empty entity set
        rel - cond                          rel & Not(cond)
        rel - 'TRUE'                        rel & False
        rel - 'FALSE'                       rel
        rel & AndList((cond1,cond2))        rel & cond1 & cond2
        rel & AndList()                     rel
        rel & [cond1, cond2]                rel & OrList((cond1, cond2))
        rel & []                            rel & False
        rel & None                          rel & False
        rel & any_empty_entity_set          rel & False
        rel - AndList((cond1,cond2))        rel & [Not(cond1), Not(cond2)]
        rel - [cond1, cond2]                rel & Not(cond1) & Not(cond2)
        rel - AndList()                     rel & False
        rel - []                            rel
        rel - None                          rel
        rel - any_empty_entity_set          rel

        When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least
        one element in arg (hence arg is treated as an OrList).
        Conversely, rel - arg restricts rel to elements that do not match any elements in arg.
        Two elements match when their common attributes have equal values or when they have no common attributes.
        All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.

        QueryExpression.restrict is the only access point that modifies restrictions. All other operators must
        ultimately call restrict()
        """
        attributes = set()
        if isinstance(restriction, Top):
            if self._top is None:
                # No existing Top — apply new one directly
                result = copy.copy(self)
                result._top = restriction
            elif restriction.order_by is None or restriction.order_by == self._top.order_by:
                # Merge: new Top inherits or matches existing ordering
                result = copy.copy(self)
                result._top = self._top.merge(restriction)
            else:
                # Different ordering — need subquery
                result = self.make_subquery()
                result._top = restriction
            return result
        new_condition = make_condition(self, restriction, attributes, semantic_check=semantic_check)
        if new_condition is True:
            return self  # restriction has no effect, return the same object
        # check that all attributes in condition are present in the query
        try:
            raise DataJointError(
                "Attribute `%s` is not found in query." % next(attr for attr in attributes if attr not in self.heading.names)
            )
        except StopIteration:
            pass  # all ok
        # If the new condition uses any new attributes, a subquery is required.
        # However, Aggregation's HAVING statement works fine with aliased attributes.
        need_subquery = (
            isinstance(self, Union) or (not isinstance(self, Aggregation) and self.heading.new_attributes) or self._top
        )
        if need_subquery:
            result = self.make_subquery()
        else:
            result = copy.copy(self)
            result._restriction = AndList(self.restriction)  # copy to preserve the original
        result.restriction.append(new_condition)
        result.restriction_attributes.update(attributes)
        return result

    def restrict_in_place(self, restriction):
        self.__dict__.update(self.restrict(restriction).__dict__)

    def __and__(self, restriction):
        """
        Restriction operator e.g. ``q1 & q2``.
        :return: a restricted copy of the input argument
        See QueryExpression.restrict for more detail.
        """
        return self.restrict(restriction)

    def __xor__(self, restriction):
        """The ^ operator has been removed in DataJoint 2.0."""
        raise DataJointError(
            "The ^ operator has been removed in DataJoint 2.0. "
            "Use .restrict(other, semantic_check=False) for restrictions without semantic checking."
        )

    def __sub__(self, restriction):
        """
        Inverted restriction e.g. ``q1 - q2``.
        :return: a restricted copy of the input argument
        See QueryExpression.restrict for more detail.
        """
        return self.restrict(Not(restriction))

    def __neg__(self):
        """
        Convert between restriction and inverted restriction e.g. ``-q1``.
        :return: target restriction
        See QueryExpression.restrict for more detail.
        """
        if isinstance(self, Not):
            return self.restriction
        return Not(self)

    def __mul__(self, other):
        """
        join of query expressions `self` and `other` e.g. ``q1 * q2``.
        """
        return self.join(other)

    def __matmul__(self, other):
        """The @ operator has been removed in DataJoint 2.0."""
        raise DataJointError(
            "The @ operator has been removed in DataJoint 2.0. "
            "Use .join(other, semantic_check=False) for joins without semantic checking."
        )

    def join(self, other, semantic_check=True, left=False, allow_nullable_pk=False):
        """
        Create the joined QueryExpression.

        :param other: QueryExpression to join with
        :param semantic_check: If True (default), use semantic matching - only match on
            homologous namesakes (same lineage) and error on non-homologous namesakes.
            If False, use natural join on all namesakes (no lineage checking).
        :param left: If True, perform a left join (retain all rows from self)
        :param allow_nullable_pk: If True, bypass the left join constraint that requires
            self to determine other. When bypassed, the result PK is the union of both
            operands' PKs, and PK attributes from the right operand could be NULL.
            Used internally by aggregation when exclude_nonmatching=False.
        :return: The joined QueryExpression

        a * b is short for a.join(b)
        """
        # Joining with U is no longer supported
        if isinstance(other, U):
            raise DataJointError(
                "table * dj.U(...) is no longer supported in DataJoint 2.0. "
                "This pattern is no longer necessary with the new semantic matching system."
            )
        if inspect.isclass(other) and issubclass(other, QueryExpression):
            other = other()  # instantiate
        if not isinstance(other, QueryExpression):
            raise DataJointError("The argument of join must be a QueryExpression")
        assert_join_compatibility(self, other, semantic_check=semantic_check)

        # Left join constraint: requires self → other (left operand determines right)
        # This ensures the result's PK (which is PK(self) for left joins) can't have NULLs
        nullable_pk = False
        if left and not self.heading.determines(other.heading):
            if allow_nullable_pk:
                nullable_pk = True
            else:
                undetermined = [attr for attr in other.heading.primary_key if attr not in self.heading.names]
                raise DataJointError(
                    "Left join requires the left operand to determine the right operand (A → B). "
                    f"The following attributes from the right operand's primary key are not "
                    f"in the left operand: {undetermined}. "
                    "Use an inner join or restructure the query."
                )

        # Always join on all non-hidden namesakes
        join_attributes = set(n for n in self.heading.names if n in other.heading.names)
        # needs subquery if self's FROM clause has common attributes with other's FROM clause
        need_subquery1 = need_subquery2 = bool(
            (set(self.original_heading.names) & set(other.original_heading.names)) - join_attributes
        )
        # need subquery if any of the join attributes are derived
        need_subquery1 = (
            need_subquery1
            or isinstance(self, Aggregation)
            or any(n in self.heading.new_attributes for n in join_attributes)
            or isinstance(self, Union)
        )
        need_subquery2 = (
            need_subquery2
            or isinstance(other, Aggregation)
            or any(n in other.heading.new_attributes for n in join_attributes)
            or isinstance(self, Union)
        )
        # With USING clause (instead of NATURAL JOIN), we need subqueries when
        # joining with multi-table expressions to ensure correct column matching
        if len(self.support) > 1 and join_attributes:
            need_subquery1 = True
        if len(other.support) > 1 and join_attributes:
            need_subquery2 = True
        if need_subquery1:
            self = self.make_subquery()
        if need_subquery2:
            other = other.make_subquery()
        result = QueryExpression()
        result._connection = self.connection
        result._support = self.support + other.support
        # Store join info: (is_left, using_attrs) - using_attrs excludes hidden attributes
        using_attrs = [n for n in self.heading.names if n in other.heading.names]
        result._joins = self._joins + [(left, using_attrs)] + other._joins
        result._heading = self.heading.join(other.heading, nullable_pk=nullable_pk)
        result._restriction = AndList(self.restriction)
        result._restriction.append(other.restriction)
        result._original_heading = self.original_heading.join(other.original_heading, nullable_pk=nullable_pk)
        assert len(result.support) == len(result._joins) + 1
        return result

    def extend(self, other, semantic_check=True):
        """
        Extend self with attributes from other.

        The extend operation adds attributes from `other` to `self` while preserving
        self's entity identity. It is semantically equivalent to `self.join(other, left=True)`
        but expresses a clearer intent: extending an entity set with additional attributes
        rather than combining two entity sets.

        Requirements:
            self → other: Every attribute in other's primary key must exist in self.
            This ensures:
            - All rows of self are preserved (no filtering)
            - Self's primary key remains the result's primary key (no NULL PKs)
            - The operation is a true extension, not a Cartesian product

        Conceptual model:
            Unlike a general join (Cartesian product restricted by matching attributes),
            extend is closer to projection—it adds new attributes to existing entities
            without changing which entities are in the result.

        Example:
            # Session determines Trial (session_id is in Trial's PK)
            # But Trial does NOT determine Session (trial_num not in Session)

            # Valid: extend trials with session info
            Trial.extend(Session)  # Adds 'date' from Session to each Trial

            # Invalid: Session cannot extend to Trial
            Session.extend(Trial)  # Error: trial_num not in Session

        :param other: QueryExpression whose attributes will extend self
        :param semantic_check: If True (default), require homologous namesakes.
            If False, match on all namesakes without lineage checking.
        :return: Extended QueryExpression with self's PK and combined attributes
        :raises DataJointError: If self does not determine other
        """
        return self.join(other, semantic_check=semantic_check, left=True)

    def __add__(self, other):
        """union e.g. ``q1 + q2``."""
        return Union.create(self, other)

    def proj(self, *attributes, **named_attributes):
        """
        Projection operator.

        :param attributes:  attributes to be included in the result. (The primary key is already included).
        :param named_attributes: new attributes computed or renamed from existing attributes.
        :return: the projected expression.
        Primary key attributes cannot be excluded but may be renamed.
        If the attribute list contains an Ellipsis ..., then all secondary attributes are included too
        Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.
        Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or
        self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)
        self.proj() -- include only primary key
        self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2
        self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2
        self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1
        self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'
        self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)
        from other attributes available before the projection.
        Each attribute name can only be used once.
        """
        named_attributes = {k: translate_attribute(v)[1] for k, v in named_attributes.items()}
        # new attributes in parentheses are included again with the new name without removing original
        duplication_pattern = re.compile(rf"^\s*\(\s*(?!{'|'.join(CONSTANT_LITERALS)})(?P<name>[a-zA-Z_]\w*)\s*\)\s*$")
        # attributes without parentheses renamed
        rename_pattern = re.compile(rf"^\s*(?!{'|'.join(CONSTANT_LITERALS)})(?P<name>[a-zA-Z_]\w*)\s*$")
        replicate_map = {
            k: m.group("name") for k, m in ((k, duplication_pattern.match(v)) for k, v in named_attributes.items()) if m
        }
        rename_map = {k: m.group("name") for k, m in ((k, rename_pattern.match(v)) for k, v in named_attributes.items()) if m}
        compute_map = {
            k: v for k, v in named_attributes.items() if not duplication_pattern.match(v) and not rename_pattern.match(v)
        }
        attributes = set(attributes)
        # include primary key
        attributes.update((k for k in self.primary_key if k not in rename_map.values()))
        # include all secondary attributes with Ellipsis
        if Ellipsis in attributes:
            attributes.discard(Ellipsis)
            attributes.update(
                (a for a in self.heading.secondary_attributes if a not in attributes and a not in rename_map.values())
            )
        try:
            raise DataJointError(
                "%s is not a valid data type for an attribute in .proj" % next(a for a in attributes if not isinstance(a, str))
            )
        except StopIteration:
            pass  # normal case
        # remove excluded attributes, specified as `-attr'
        excluded = set(a for a in attributes if a.strip().startswith("-"))
        attributes.difference_update(excluded)
        excluded = set(a.lstrip("-").strip() for a in excluded)
        attributes.difference_update(excluded)
        try:
            raise DataJointError(
                "Cannot exclude primary key attribute %s",
                next(a for a in excluded if a in self.primary_key),
            )
        except StopIteration:
            pass  # all ok
        # check that all attributes exist in heading
        try:
            raise DataJointError("Attribute `%s` not found." % next(a for a in attributes if a not in self.heading.names))
        except StopIteration:
            pass  # all ok

        # check that all mentioned names are present in heading
        mentions = attributes.union(replicate_map.values()).union(rename_map.values())
        try:
            raise DataJointError("Attribute '%s' not found." % next(a for a in mentions if not self.heading.names))
        except StopIteration:
            pass  # all ok

        # check that newly created attributes do not clash with any other selected attributes
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in rename_map if a in attributes.union(compute_map).union(replicate_map))
            )
        except StopIteration:
            pass  # all ok
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in compute_map if a in attributes.union(rename_map).union(replicate_map))
            )
        except StopIteration:
            pass  # all ok
        try:
            raise DataJointError(
                "Attribute `%s` already exists"
                % next(a for a in replicate_map if a in attributes.union(rename_map).union(compute_map))
            )
        except StopIteration:
            pass  # all ok

        # need a subquery if the projection remaps any remapped attributes
        used = set(q for v in compute_map.values() for q in extract_column_names(v))
        used.update(rename_map.values())
        used.update(replicate_map.values())
        used.intersection_update(self.heading.names)
        need_subquery = isinstance(self, Union) or any(self.heading[name].attribute_expression is not None for name in used)
        if not need_subquery and self.restriction:
            # need a subquery if the restriction applies to attributes that have been renamed
            need_subquery = any(name in self.restriction_attributes for name in self.heading.new_attributes)

        result = self.make_subquery() if need_subquery else copy.copy(self)
        result._original_heading = result.original_heading
        result._heading = result.heading.select(
            attributes,
            rename_map=dict(**rename_map, **replicate_map),
            compute_map=compute_map,
        )
        return result

    def aggr(self, group, *attributes, exclude_nonmatching=False, **named_attributes):
        """
        Aggregation/grouping operation, similar to proj but with computations over a grouped relation.

        By default, keeps all rows from self (like proj). Use exclude_nonmatching=True to
        keep only rows that have matches in group.

        :param group: The query expression to be aggregated.
        :param exclude_nonmatching: If True, exclude rows from self that have no matching
            entries in group (INNER JOIN). Default False keeps all rows (LEFT JOIN).
        :param named_attributes: computations of the form new_attribute="sql expression on attributes of group"
        :return: The derived query expression

        Example::

            # Count sessions per subject (keeps all subjects, even those with 0 sessions)
            Subject.aggr(Session, n="count(*)")

            # Count sessions per subject (only subjects with at least one session)
            Subject.aggr(Session, n="count(*)", exclude_nonmatching=True)
        """
        if Ellipsis in attributes:
            # expand ellipsis to include only attributes from the left table
            attributes = set(attributes)
            attributes.discard(Ellipsis)
            attributes.update(self.heading.secondary_attributes)
        keep_all_rows = not exclude_nonmatching
        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(*attributes, **named_attributes)

    aggregate = aggr  # alias for aggr

    # ---------- Fetch operators --------------------
    def fetch(
        self,
        *attrs,
        offset=None,
        limit=None,
        order_by=None,
        format=None,
        as_dict=None,
        squeeze=False,
    ):
        """
        Fetch data from the table (backward-compatible with DataJoint 0.14).

        .. deprecated:: 2.0
            Use the new explicit output methods instead:
            - ``to_dicts()`` for list of dictionaries
            - ``to_pandas()`` for pandas DataFrame
            - ``to_arrays()`` for numpy structured array
            - ``to_arrays('a', 'b')`` for tuple of arrays
            - ``keys()`` for primary keys

        Parameters
        ----------
        *attrs : str
            Attributes to fetch. If empty, fetches all.
        offset : int, optional
            Number of tuples to skip.
        limit : int, optional
            Maximum number of tuples to return.
        order_by : str or list, optional
            Attribute(s) for ordering results.
        format : str, optional
            Output format: 'array' or 'frame' (pandas DataFrame).
        as_dict : bool, optional
            Return as list of dicts instead of structured array.
        squeeze : bool, optional
            Remove extra dimensions from arrays. Default False.

        Returns
        -------
        np.recarray, list[dict], or pd.DataFrame
            Query results in requested format.
        """
        import warnings

        warnings.warn(
            "fetch() is deprecated in DataJoint 2.0. " "Use to_dicts(), to_pandas(), to_arrays(), or keys() instead.",
            DeprecationWarning,
            stacklevel=2,
        )

        # Handle format='frame' -> to_pandas()
        if format == "frame":
            if attrs or as_dict is not None:
                raise DataJointError("format='frame' cannot be combined with attrs or as_dict")
            return self.to_pandas(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)

        # Handle specific attributes requested
        if attrs:
            # Check for special 'KEY' attribute
            def is_key(attr):
                return attr == "KEY"

            has_key = any(is_key(a) for a in attrs)

            # Handle fetch('KEY') alone - return list of primary key dicts
            if has_key and len(attrs) == 1:
                return list(self.keys(order_by=order_by, limit=limit, offset=offset))

            if as_dict is True:
                # fetch('col1', 'col2', as_dict=True) -> list of dicts
                # Replace KEY with primary key columns
                proj_attrs = []
                for attr in attrs:
                    if is_key(attr):
                        proj_attrs.extend(self.primary_key)
                    else:
                        proj_attrs.append(attr)
                return self.proj(*proj_attrs).to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)
            else:
                # fetch('col1', 'col2') or fetch('col1', 'col2', as_dict=False) -> tuple of arrays
                # This matches DJ 1.x behavior where fetch('col') returns array(['alpha', 'beta'])
                if has_key:
                    # Need to handle KEY specially - it returns list of dicts, not array
                    proj_attrs = []
                    for attr in attrs:
                        if is_key(attr):
                            proj_attrs.extend(self.primary_key)
                        else:
                            proj_attrs.append(attr)
                    dicts = self.proj(*proj_attrs).to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)
                    # Build result, with KEY returning list of dicts
                    results = []
                    for attr in attrs:
                        if is_key(attr):
                            results.append([{k: d[k] for k in self.primary_key} for d in dicts])
                        else:
                            results.append(np.array([d[attr] for d in dicts]))
                    return results[0] if len(attrs) == 1 else tuple(results)
                return self.to_arrays(*attrs, order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)

        # Handle as_dict=True -> to_dicts()
        if as_dict:
            return self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)

        # Default: return structured array (legacy behavior)
        return self.to_arrays(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)

    def fetch1(self, *attrs, squeeze=False):
        """
        Fetch exactly one row from the query result.

        If no attributes are specified, returns the result as a dict.
        If attributes are specified, returns the corresponding values as a tuple.

        :param attrs: attribute names to fetch (if empty, fetch all as dict)
        :param squeeze: if True, remove extra dimensions from arrays
        :return: dict (no attrs) or tuple/value (with attrs)
        :raises DataJointError: if not exactly one row in result

        Examples::

            d = table.fetch1()              # returns dict with all attributes
            a, b = table.fetch1('a', 'b')   # returns tuple of attribute values
            value = table.fetch1('a')       # returns single value
        """
        heading = self.heading

        if not attrs:
            # Fetch all attributes, return as dict
            cursor = self.cursor(as_dict=True)
            row = cursor.fetchone()
            if not row or cursor.fetchone():
                raise DataJointError("fetch1 requires exactly one tuple in the input set.")
            return {name: decode_attribute(heading[name], row[name], squeeze=squeeze) for name in heading.names}
        else:
            # Handle "KEY" specially - it means primary key columns
            def is_key(attr):
                return attr == "KEY"

            has_key = any(is_key(a) for a in attrs)

            if has_key and len(attrs) == 1:
                # Just fetching KEY - return the primary key dict
                keys = self.keys()
                if len(keys) != 1:
                    raise DataJointError(f"fetch1 should only return one tuple. {len(keys)} tuples found")
                return keys[0]

            # Fetch specific attributes, return as tuple
            # Replace KEY with primary key columns for projection
            proj_attrs = []
            for attr in attrs:
                if is_key(attr):
                    proj_attrs.extend(self.primary_key)
                else:
                    proj_attrs.append(attr)

            dicts = self.proj(*proj_attrs).to_dicts(squeeze=squeeze)
            if len(dicts) != 1:
                raise DataJointError(f"fetch1 should only return one tuple. {len(dicts)} tuples found")
            row = dicts[0]

            # Build result values, handling KEY specially
            values = []
            for attr in attrs:
                if is_key(attr):
                    # Return dict of primary key columns
                    values.append({k: row[k] for k in self.primary_key})
                else:
                    values.append(row[attr])

            return values[0] if len(attrs) == 1 else tuple(values)

    def _apply_top(self, order_by=None, limit=None, offset=None):
        """Apply order_by, limit, offset if specified, return modified expression."""
        if order_by is not None or limit is not None or offset is not None:
            return self.restrict(Top(limit, order_by, offset))
        return self

    def to_dicts(self, order_by=None, limit=None, offset=None, squeeze=False):
        """
        Fetch all rows as a list of dictionaries.

        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :param squeeze: if True, remove extra dimensions from arrays
        :return: list of dictionaries, one per row

        For object storage types (attachments, filepaths), files are downloaded
        to config["download_path"]. Use config.override() to change::

            with dj.config.override(download_path="/data"):
                data = table.to_dicts()
        """
        expr = self._apply_top(order_by, limit, offset)
        cursor = expr.cursor(as_dict=True)
        heading = expr.heading
        return [{name: decode_attribute(heading[name], row[name], squeeze) for name in heading.names} for row in cursor]

    def to_pandas(self, order_by=None, limit=None, offset=None, squeeze=False):
        """
        Fetch all rows as a pandas DataFrame with primary key as index.

        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :param squeeze: if True, remove extra dimensions from arrays
        :return: pandas DataFrame with primary key columns as index
        """
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)
        df = pandas.DataFrame(dicts)
        if len(df) > 0 and self.primary_key:
            df = df.set_index(self.primary_key)
        return df

    def to_polars(self, order_by=None, limit=None, offset=None, squeeze=False):
        """
        Fetch all rows as a polars DataFrame.

        Requires polars: pip install datajoint[polars]

        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :param squeeze: if True, remove extra dimensions from arrays
        :return: polars DataFrame
        """
        try:
            import polars
        except ImportError:
            raise ImportError("polars is required for to_polars(). Install with: pip install datajoint[polars]")
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)
        return polars.DataFrame(dicts)

    def to_arrow(self, order_by=None, limit=None, offset=None, squeeze=False):
        """
        Fetch all rows as a PyArrow Table.

        Requires pyarrow: pip install datajoint[arrow]

        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :param squeeze: if True, remove extra dimensions from arrays
        :return: pyarrow Table
        """
        try:
            import pyarrow
        except ImportError:
            raise ImportError("pyarrow is required for to_arrow(). Install with: pip install datajoint[arrow]")
        dicts = self.to_dicts(order_by=order_by, limit=limit, offset=offset, squeeze=squeeze)
        if not dicts:
            return pyarrow.table({})
        return pyarrow.Table.from_pylist(dicts)

    def to_arrays(self, *attrs, include_key=False, order_by=None, limit=None, offset=None, squeeze=False):
        """
        Fetch data as numpy arrays.

        If no attrs specified, returns a numpy structured array (recarray) of all columns.
        If attrs specified, returns a tuple of numpy arrays (one per attribute).

        :param attrs: attribute names to fetch (if empty, fetch all)
        :param include_key: if True and attrs specified, prepend primary keys as list of dicts
        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :param squeeze: if True, remove extra dimensions from arrays
        :return: numpy recarray (no attrs) or tuple of arrays (with attrs).
            With include_key=True: (keys, *arrays) where keys is list[dict]

        Examples::

            # Fetch as structured array
            data = table.to_arrays()

            # Fetch specific columns as separate arrays
            a, b = table.to_arrays('a', 'b')

            # Fetch with primary keys for later restrictions
            keys, a, b = table.to_arrays('a', 'b', include_key=True)
            # keys = [{'id': 1}, {'id': 2}, ...]  # same format as table.keys()
        """
        from functools import partial

        expr = self._apply_top(order_by, limit, offset)
        heading = expr.heading

        if attrs:
            # Fetch specific attributes as tuple of arrays
            if include_key:
                fetch_attrs = list(expr.primary_key) + [a for a in attrs if a not in expr.primary_key]
            else:
                fetch_attrs = list(attrs)

            # Project to only needed columns
            projected = expr.proj(*fetch_attrs)
            dicts = projected.to_dicts(squeeze=squeeze)

            # Extract keys if requested
            if include_key:
                keys = [{k: d[k] for k in expr.primary_key} for d in dicts]

            # Extract arrays for requested attributes
            result_arrays = []
            for attr in attrs:
                values = [d[attr] for d in dicts]
                # Try to create a homogeneous array, fall back to object array for variable-size data
                try:
                    arr = np.array(values)
                except ValueError:
                    # Variable-size data (e.g., arrays of different shapes)
                    # Must assign individually to avoid numpy broadcasting issues
                    arr = np.empty(len(values), dtype=object)
                    for i, v in enumerate(values):
                        arr[i] = v
                result_arrays.append(arr)

            if include_key:
                return (keys, *result_arrays)
            return result_arrays[0] if len(attrs) == 1 else tuple(result_arrays)
        else:
            # Fetch all columns as structured array
            get = partial(decode_attribute, squeeze=squeeze)
            cursor = expr.cursor(as_dict=False)
            rows = list(cursor.fetchall())

            if not rows:
                return np.array([], dtype=heading.as_dtype)

            # Build dtype, detecting blob types from first row
            import numbers

            record_type = np.dtype(
                [
                    (name, type(value))
                    if heading[name].is_blob and isinstance(value, numbers.Number)
                    else (name, heading.as_dtype[name])
                    for value, name in zip(rows[0], heading.as_dtype.names)
                ]
            )

            ret = np.array(rows, dtype=record_type)
            # Decode blobs and codecs
            for name in heading:
                ret[name] = list(map(partial(get, heading[name]), ret[name]))
            return ret

    def keys(self, order_by=None, limit=None, offset=None):
        """
        Fetch primary key values as a list of dictionaries.

        :param order_by: attribute(s) to order by, or "KEY"/"KEY DESC"
        :param limit: maximum number of rows to return
        :param offset: number of rows to skip
        :return: list of dictionaries containing only primary key columns
        """
        return self.proj().to_dicts(order_by=order_by, limit=limit, offset=offset)

    def head(self, limit=25):
        """
        Preview the first few entries from query expression.

        :param limit: number of entries (default 25)
        :return: list of dictionaries
        """
        return self.to_dicts(order_by="KEY", limit=limit)

    def tail(self, limit=25):
        """
        Preview the last few entries from query expression.

        :param limit: number of entries (default 25)
        :return: list of dictionaries
        """
        return list(reversed(self.to_dicts(order_by="KEY DESC", limit=limit)))

    def __len__(self):
        """:return: number of elements in the result set e.g. ``len(q1)``."""
        result = self.make_subquery() if self._top else copy.copy(self)
        has_left_join = any(is_left for is_left, _ in result._joins)
        return result.connection.query(
            "SELECT {select_} FROM {from_}{where}".format(
                select_=(
                    "count(*)"
                    if has_left_join
                    else "count(DISTINCT {fields})".format(
                        fields=result.heading.as_sql(result.primary_key, include_aliases=False)
                    )
                ),
                from_=result.from_clause(),
                where=result.where_clause(),
            )
        ).fetchone()[0]

    def __bool__(self):
        """
        :return: True if the result is not empty. Equivalent to len(self) > 0 but often
            faster e.g. ``bool(q1)``.
        """
        return bool(
            self.connection.query(
                "SELECT EXISTS(SELECT 1 FROM {from_}{where})".format(from_=self.from_clause(), where=self.where_clause())
            ).fetchone()[0]
        )

    def __contains__(self, item):
        """
        returns True if the restriction in item matches any entries in self
            e.g. ``restriction in q1``.

        :param item: any restriction
        (item in query_expression) is equivalent to bool(query_expression & item) but may be
        executed more efficiently.
        """
        return bool(self & item)  # May be optimized e.g. using an EXISTS query

    def __iter__(self):
        """
        Lazy streaming iterator over rows as dictionaries.

        Yields one row at a time from a single database cursor, efficiently
        streaming data without loading all rows into memory.

        :yields: dict for each row
        """
        cursor = self.cursor(as_dict=True)
        heading = self.heading
        for row in cursor:
            yield {name: decode_attribute(heading[name], row[name], squeeze=False) for name in heading.names}

    def cursor(self, as_dict=False):
        """
        Execute the query and return a database cursor.

        :param as_dict: if True, rows are returned as dictionaries
        :return: database query cursor
        """
        sql = self.make_sql()
        logger.debug(sql)
        return self.connection.query(sql, as_dict=as_dict)

    def __repr__(self):
        """
        returns the string representation of a QueryExpression object e.g. ``str(q1)``.

        :param self: A query expression
        :type self: :class:`QueryExpression`
        :rtype: str
        """
        return super().__repr__() if config["loglevel"].lower() == "debug" else self.preview()

    def preview(self, limit=None, width=None):
        """:return: a string of preview of the contents of the query."""
        return preview(self, limit, width)

    def _repr_html_(self):
        """:return: HTML to display table in Jupyter notebook."""
        return repr_html(self)


class Aggregation(QueryExpression):
    """
    Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set
    with primary key from arg.
    The computed arguments comp1, ..., compn use aggregation calculations on the attributes of
    group or simple projections and calculations on the attributes of arg.
    Aggregation is used QueryExpression.aggr and U.aggr.
    Aggregation is a private class in DataJoint, not exposed to users.
    """

    _left_restrict = None  # the pre-GROUP BY conditions for the WHERE clause
    _subquery_alias_count = count()

    @classmethod
    def create(cls, groupby, group, keep_all_rows=False):
        """
        Create an aggregation expression.

        :param groupby: The expression to GROUP BY (determines the result's primary key)
        :param group: The expression to aggregate over
        :param keep_all_rows: If True, use left join to keep all rows from groupby
        """
        if inspect.isclass(group) and issubclass(group, QueryExpression):
            group = group()  # instantiate if a class
        assert isinstance(group, QueryExpression)

        # Aggregation requires group → groupby: every attribute in groupby's PK
        # must be in group, so we can GROUP BY groupby's PK.
        # Skip check for U (universal set) which doesn't have a heading until joined.
        if not isinstance(groupby, U) and not group.heading.determines(groupby.heading):
            missing = [attr for attr in groupby.heading.primary_key if attr not in group.heading.names]
            raise DataJointError(
                "Aggregation requires the group expression to contain all primary key "
                f"attributes of the grouping expression. Missing attributes: {missing}."
            )

        if keep_all_rows and len(group.support) > 1 or group.heading.new_attributes:
            group = group.make_subquery()  # subquery if left joining a join
        # When keep_all_rows=True, we use a left join which normally requires A → B.
        # Aggregation has the opposite requirement (B → A). We bypass the left join
        # constraint because GROUP BY resets the PK to groupby's PK (never NULL).
        join = groupby.join(group, left=keep_all_rows, allow_nullable_pk=keep_all_rows)
        result = cls()
        result._connection = join.connection
        result._heading = join.heading.set_primary_key(groupby.primary_key)
        result._support = join.support
        result._joins = join._joins
        result._left_restrict = join.restriction  # WHERE clause applied before GROUP BY
        result._grouping_attributes = result.primary_key

        return result

    def where_clause(self):
        return "" if not self._left_restrict else " WHERE (%s)" % ")AND(".join(str(s) for s in self._left_restrict)

    def make_sql(self, fields=None):
        fields = self.heading.as_sql(fields or self.heading.names)
        assert self._grouping_attributes or not self.restriction
        distinct = set(self.heading.names) == set(self.primary_key)
        return "SELECT {distinct}{fields} FROM {from_}{where}{group_by}{sorting}".format(
            distinct="DISTINCT " if distinct else "",
            fields=fields,
            from_=self.from_clause(),
            where=self.where_clause(),
            group_by=(
                ""
                if not self.primary_key
                else (
                    " GROUP BY `%s`" % "`,`".join(self._grouping_attributes)
                    + ("" if not self.restriction else " HAVING (%s)" % ")AND(".join(self.restriction))
                )
            ),
            sorting=self.sorting_clauses(),
        )

    def __len__(self):
        return self.connection.query(
            "SELECT count(1) FROM ({subquery}) `${alias:x}`".format(
                subquery=self.make_sql(), alias=next(self._subquery_alias_count)
            )
        ).fetchone()[0]

    def __bool__(self):
        return bool(self.connection.query("SELECT EXISTS({sql})".format(sql=self.make_sql())).fetchone()[0])


class Union(QueryExpression):
    """
    Union is the private DataJoint class that implements the union operator.
    """

    __count = count()

    @classmethod
    def create(cls, arg1, arg2):
        if inspect.isclass(arg2) and issubclass(arg2, QueryExpression):
            arg2 = arg2()  # instantiate if a class
        if not isinstance(arg2, QueryExpression):
            raise DataJointError("A QueryExpression can only be unioned with another QueryExpression")
        if arg1.connection != arg2.connection:
            raise DataJointError("Cannot operate on QueryExpressions originating from different connections.")
        if set(arg1.primary_key) != set(arg2.primary_key):
            raise DataJointError("The operands of a union must share the same primary key.")
        if set(arg1.heading.secondary_attributes) & set(arg2.heading.secondary_attributes):
            raise DataJointError("The operands of a union must not share any secondary attributes.")
        result = cls()
        result._connection = arg1.connection
        result._heading = arg1.heading.join(arg2.heading)
        result._support = [arg1, arg2]
        return result

    def make_sql(self):
        arg1, arg2 = self._support
        if not arg1.heading.secondary_attributes and not arg2.heading.secondary_attributes:
            # no secondary attributes: use UNION DISTINCT
            fields = arg1.primary_key
            return "SELECT * FROM (({sql1}) UNION ({sql2})) as `_u{alias}{sorting}`".format(
                sql1=(arg1.make_sql() if isinstance(arg1, Union) else arg1.make_sql(fields)),
                sql2=(arg2.make_sql() if isinstance(arg2, Union) else arg2.make_sql(fields)),
                alias=next(self.__count),
                sorting=self.sorting_clauses(),
            )
        # with secondary attributes, use union of left join with anti-restriction
        fields = self.heading.names
        sql1 = arg1.join(arg2, left=True).make_sql(fields)
        sql2 = (arg2 - arg1).proj(..., **{k: "NULL" for k in arg1.heading.secondary_attributes}).make_sql(fields)
        return "({sql1})  UNION ({sql2})".format(sql1=sql1, sql2=sql2)

    def from_clause(self):
        """The union does not use a FROM clause."""
        raise NotImplementedError("Union does not use a FROM clause")

    def where_clause(self):
        """The union does not use a WHERE clause."""
        raise NotImplementedError("Union does not use a WHERE clause")

    def __len__(self):
        return self.connection.query(
            "SELECT count(1) FROM ({subquery}) `${alias:x}`".format(
                subquery=self.make_sql(),
                alias=next(QueryExpression._subquery_alias_count),
            )
        ).fetchone()[0]

    def __bool__(self):
        return bool(self.connection.query("SELECT EXISTS({sql})".format(sql=self.make_sql())).fetchone()[0])


class U:
    """
    dj.U objects are the universal sets representing all possible values of their attributes.
    dj.U objects cannot be queried on their own but are useful for forming some queries.
    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.
    The universal set is the set of all possible combinations of values of the attributes.
    Without any attributes, dj.U() represents the set with one element that has no attributes.

    Restriction:

    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.

    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:

    >>> dj.U('contrast', 'brightness') & stimulus

    Aggregation:

    In aggregation, dj.U is used for summary calculation over an entire set:

    The following expression yields one element with one attribute `s` containing the total number of elements in
    query expression `expr`:

    >>> dj.U().aggr(expr, n='count(*)')

    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in
    query expression `expr`.

    >>> dj.U().aggr(expr, n='count(distinct attr)')
    >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')

    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`
    over entire result set of expression `expr`:

    >>> dj.U().aggr(expr, s='sum(attr)')

    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of
    their occurrences in the result set of query expression `expr`.

    >>> dj.U(attr1,attr2).aggr(expr, n='count(*)')

    Joins:

    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result
    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on
    non-primary key attributes.
    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw
    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first
    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.
    """

    def __init__(self, *primary_key):
        self._primary_key = primary_key

    @property
    def primary_key(self):
        return self._primary_key

    def __and__(self, other):
        if inspect.isclass(other) and issubclass(other, QueryExpression):
            other = other()  # instantiate if a class
        if not isinstance(other, QueryExpression):
            raise DataJointError("Set U can only be restricted with a QueryExpression.")
        result = copy.copy(other)
        result._distinct = True
        result._heading = result.heading.set_primary_key(self.primary_key)
        result = result.proj()
        return result

    def __mul__(self, other):
        """The * operator with dj.U has been removed in DataJoint 2.0."""
        raise DataJointError(
            "dj.U(...) * table is no longer supported in DataJoint 2.0. "
            "This pattern is no longer necessary with the new semantic matching system."
        )

    def __sub__(self, other):
        """Anti-restriction with dj.U produces an infinite set."""
        raise DataJointError(
            "dj.U(...) - table produces an infinite set and is not supported. "
            "Consider using a different approach for your query."
        )

    def aggr(self, group, **named_attributes):
        """
        Aggregation of the type U('attr1','attr2').aggr(group, computation="QueryExpression")
        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.

        Note: exclude_nonmatching is always True for dj.U (cannot keep all rows from infinite set).

        :param group:  The query expression to be aggregated.
        :param named_attributes: computations of the form new_attribute="sql expression on attributes of group"
        :return: The derived query expression
        """
        if named_attributes.pop("exclude_nonmatching", True) is False:
            raise DataJointError("Cannot set exclude_nonmatching=False when aggregating on a universal set.")

        if inspect.isclass(group) and issubclass(group, QueryExpression):
            group = group()
        if not isinstance(group, QueryExpression):
            raise DataJointError("dj.U.aggr requires a QueryExpression as the group argument.")

        # Verify U's primary key attributes exist in group
        missing = [attr for attr in self.primary_key if attr not in group.heading.names]
        if missing:
            raise DataJointError(f"Attributes {missing} not found in the group expression.")

        # Create Aggregation directly without join - just group by U's primary key
        result = Aggregation()
        result._connection = group.connection
        result._heading = group.heading.set_primary_key(list(self.primary_key))
        result._support = group.support
        result._joins = group._joins
        result._left_restrict = group.restriction
        result._grouping_attributes = list(self.primary_key)

        return result.proj(**named_attributes)

    aggregate = aggr  # alias for aggr


def _flatten_attribute_list(primary_key, attrs):
    """
    :param primary_key: list of attributes in primary key
    :param attrs: list of attribute names, which may include "KEY", "KEY DESC" or "KEY ASC"
    :return: generator of attributes where "KEY" is replaced with its component attributes
    """
    for a in attrs:
        if re.match(r"^\s*KEY(\s+[aA][Ss][Cc])?\s*$", a):
            if primary_key:
                yield from primary_key
        elif re.match(r"^\s*KEY\s+[Dd][Ee][Ss][Cc]\s*$", a):
            if primary_key:
                yield from (q + " DESC" for q in primary_key)
        else:
            yield a


def _wrap_attributes(attr):
    for entry in attr:  # wrap attribute names in backquotes
        yield re.sub(r"\b((?!asc|desc)\w+)\b", r"`\1`", entry, flags=re.IGNORECASE)
