{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"], "fields": {"title": {"boost": 1000.0}, "text": {"boost": 1.0}, "tags": {"boost": 1000000.0}}}, "docs": [{"location": "", "title": "Welcome to DataJoint for Python!", "text": "<p>DataJoint for Python is a framework for scientific workflow management based on relational principles. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, computing, and querying data.</p> <p>DataJoint was initially developed in 2009 by Dimitri Yatsenko in Andreas Tolias' Lab at Baylor College of Medicine for the distributed processing and management of large volumes of data streaming from regular experiments. Starting in 2011, DataJoint has been available as an open-source project adopted by other labs and improved through contributions from several developers. Presently, the primary developer of DataJoint open-source software is the company DataJoint.</p>"}, {"location": "#data-pipeline-example", "title": "Data Pipeline Example", "text": "<p>Yatsenko et al., bioRxiv 2021</p>"}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Install with Conda<pre><code>conda install -c conda-forge datajoint\n</code></pre> </li> </ul> <ul> <li>Install with pip<pre><code>pip install datajoint\n</code></pre> </li> </ul> <ul> <li>Quick Start Guide</li> </ul> <ul> <li>Interactive Tutorials on GitHub Codespaces</li> </ul> <ul> <li>DataJoint Elements - Catalog of example pipelines for neuroscience experiments</li> </ul> <ul> <li> <p>Contribute</p> <ul> <li>Development Environment</li> </ul> <ul> <li>Guidelines</li> </ul> </li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": ""}, {"location": "changelog/#release-notes", "title": "Release notes", "text": "<p>Note: This file is no longer updated. See the GitHub change log page for the latest release notes: https://github.com/datajoint/datajoint-python/releases.</p>"}, {"location": "changelog/#0143-sep-23-2024", "title": "0.14.3 -- Sep 23, 2024", "text": "<ul> <li>Added - <code>dj.Top</code> restriction - PR #1024) PR #1084</li> <li>Fixed - Added encapsulating double quotes to comply with DOT language - PR #1177</li> <li>Added - Datajoint python CLI (#940) - PR #1095</li> <li>Added - Ability to set hidden attributes on a table - PR #1091</li> <li>Added - Ability to specify a list of keys to populate - PR #989</li> <li>Fixed - fixed topological sort #1057 - PR #1184</li> <li>Fixed - .parts() not always returning parts #1103 - PR #1184</li> <li>Changed - replace <code>setup.py</code> with <code>pyproject.toml</code> - PR #1183</li> <li>Changed - disable <code>add_hidden_timestamp</code> configuration option by default - PR #1188</li> </ul>"}, {"location": "changelog/#0142-aug-19-2024", "title": "0.14.2 -- Aug 19, 2024", "text": "<ul> <li>Added - Migrate nosetests to pytest - PR #1142</li> <li>Added - Codespell GitHub Actions workflow</li> <li>Added - GitHub Actions workflow to manually release docs</li> <li>Changed - Update <code>datajoint/nginx</code> to <code>v0.2.6</code></li> <li>Changed - Migrate docs from <code>https://docs.datajoint.org/python</code> to <code>https://datajoint.com/docs/core/datajoint-python</code></li> <li>Fixed - DevContainer configuration - PR #1115</li> <li>Fixed - Updated set_password to work on MySQL 8 - PR #1106</li> <li>Added - Missing tests for set_password - PR #1106</li> <li>Changed - Returning success count after the .populate() call - PR #1050</li> <li>Fixed - <code>Autopopulate.populate</code> excludes <code>reserved</code> jobs in addition to <code>ignore</code> and <code>error</code> jobs</li> <li>Fixed - Issue #1159 (cascading delete) - PR #1160</li> <li>Changed - Minimum Python version for Datajoint-Python is now 3.8 PR #1163</li> <li>Fixed - <code>docker compose</code> commands in CI #1164</li> <li>Changed - Default delete behavior now includes masters of part tables - PR #1158</li> </ul>"}, {"location": "changelog/#0141-jun-02-2023", "title": "0.14.1 -- Jun 02, 2023", "text": "<ul> <li>Fixed - Fix altering a part table that uses the \"master\" keyword - PR #991</li> <li>Fixed - <code>.ipynb</code> output in tutorials is not visible in dark mode (#1078) PR #1080</li> <li>Fixed - preview table font for darkmode PR #1089</li> <li>Changed - Readme to update links and include example pipeline image</li> <li>Changed - Docs to add landing page and update navigation</li> <li>Changed - <code>.data</code> method to <code>.stream</code> in the <code>get()</code> method for S3 (external) objects PR #1085</li> <li>Fixed - Docs to rename <code>create_virtual_module</code> to <code>VirtualModule</code></li> <li>Added - Skeleton from <code>datajoint-company/datajoint-docs</code> repository for docs migration</li> <li>Added - Initial <code>pytest</code> for <code>test_connection</code></li> </ul>"}, {"location": "changelog/#0140-feb-13-2023", "title": "0.14.0 -- Feb 13, 2023", "text": "<ul> <li>Added - <code>json</code> data type (#245) PR #1051</li> <li>Fixed - Activating a schema requires all tables to exist even if <code>create_tables=False</code> PR #1058</li> <li>Changed - Populate call with <code>reserve_jobs=True</code> to exclude <code>error</code> and <code>ignore</code> keys - PR #1062</li> <li>Added - Support for inserting data with CSV files - PR #1067</li> <li>Changed - Switch testing image from <code>pydev</code> to <code>djtest</code> PR #1012</li> <li>Added - DevContainer development environment compatible with GH Codespaces PR 1071</li> <li>Fixed - Convert lingering prints by replacing with logs PR #1073</li> <li>Changed - <code>table.progress()</code> defaults to no stdout PR #1073</li> <li>Changed - <code>table.describe()</code> defaults to no stdout PR #1073</li> <li>Deprecated - <code>table._update()</code> PR #1073</li> <li>Deprecated - old-style foreign key syntax PR #1073</li> <li>Deprecated - <code>dj.migrate_dj011_external_blob_storage_to_dj012()</code> PR #1073</li> <li>Added - Method to set job keys to \"ignore\" status - PR #1068</li> </ul>"}, {"location": "changelog/#0138-sep-21-2022", "title": "0.13.8 -- Sep 21, 2022", "text": "<ul> <li>Added - New documentation structure based on markdown PR #1052</li> <li>Fixed - Fix queries with backslashes (#999) PR #1052</li> </ul>"}, {"location": "changelog/#0137-jul-13-2022", "title": "0.13.7 -- Jul 13, 2022", "text": "<ul> <li>Fixed - Fix networkx incompatible change by version pinning to 2.6.3 (#1035) PR #1036</li> <li>Added - Support for serializing numpy datetime64 types (#1022) PR #1036</li> <li>Changed - Add traceback to default logging PR #1036</li> </ul>"}, {"location": "changelog/#0136-jun-13-2022", "title": "0.13.6 -- Jun 13, 2022", "text": "<ul> <li>Added - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025</li> <li>Added - Unified package level logger for package (#667) PR #1031</li> <li>Changed - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031</li> <li>Fixed - Fix query caching deleting non-datajoint files PR #1027</li> <li>Changed - Minimum Python version for Datajoint-Python is now 3.7 PR #1027</li> </ul>"}, {"location": "changelog/#0135-may-19-2022", "title": "0.13.5 -- May 19, 2022", "text": "<ul> <li>Changed - Import ABC from collections.abc for Python 3.10 compatibility</li> <li>Fixed - Fix multiprocessing value error (#1013) PR #1026</li> </ul>"}, {"location": "changelog/#0134-mar-28-2022", "title": "0.13.4 -- Mar, 28 2022", "text": "<ul> <li>Added - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995</li> <li>Fixed - Add missing <code>jobs</code> argument for multiprocessing PR #997</li> <li>Added - Test for multiprocessing PR #1008</li> <li>Fixed - Fix external store key name doesn't allow '-' (#1005) PR #1006</li> <li>Added - Adopted black formatting into code base PR #998</li> </ul>"}, {"location": "changelog/#0133-feb-9-2022", "title": "0.13.3 -- Feb 9, 2022", "text": "<ul> <li>Fixed - Fix error in listing ancestors, descendants with part tables.</li> <li>Fixed - Fix Python 3.10 compatibility (#983) PR #972</li> <li>Fixed - Allow renaming non-conforming attributes in proj (#982) PR #972</li> <li>Added - Expose proxy feature for S3 external stores (#961) PR #962</li> <li>Added - implement multiprocessing in populate (#695) PR #704, #969</li> <li>Fixed - Dependencies not properly loaded on populate. (#902) PR #919</li> <li>Fixed - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939</li> <li>Fixed - Deletes and drops must include the master of each part. (#151, #374) PR #957</li> <li>Fixed - <code>ExternalTable.delete</code> should not remove row on error (#953) PR #956</li> <li>Fixed - Fix error handling of remove_object function in <code>s3.py</code> (#952) PR #955</li> <li>Fixed - Fix regression issue with <code>DISTINCT</code> clause and <code>GROUP_BY</code> (#914) PR #963</li> <li>Fixed - Fix sql code generation to comply with sql mode <code>ONLY_FULL_GROUP_BY</code> (#916) PR #965</li> <li>Fixed - Fix count for left-joined <code>QueryExpressions</code> (#951) PR #966</li> <li>Fixed - Fix assertion error when performing a union into a join (#930) PR #967</li> <li>Changed <code>~jobs.error_stack</code> from blob to mediumblob to allow error stacks &gt;64kB in jobs (#984) PR #986</li> <li>Fixed - Fix error when performing a union on multiple tables (#926) PR #964</li> <li>Added - Allow optional keyword arguments for <code>make()</code> in <code>populate()</code> PR #971</li> </ul>"}, {"location": "changelog/#0132-may-7-2021", "title": "0.13.2 -- May 7, 2021", "text": "<ul> <li>Changed <code>setuptools_certificate</code> dependency to new name <code>otumat</code></li> <li>Fixed - Explicit calls to <code>dj.Connection</code> throw error due to missing <code>host_input</code> (#895) PR #907</li> <li>Fixed - Correct count of deleted items. (#897) PR #912</li> </ul>"}, {"location": "changelog/#0131-apr-16-2021", "title": "0.13.1 -- Apr 16, 2021", "text": "<ul> <li>Added <code>None</code> as an alias for <code>IS NULL</code> comparison in <code>dict</code> restrictions (#824) PR #893</li> <li>Changed - Drop support for MySQL 5.6 since it has reached EOL PR #893</li> <li>Fixed - <code>schema.list_tables()</code> is not topologically sorted (#838) PR #893</li> <li>Fixed - Diagram part tables do not show proper class name (#882) PR #893</li> <li>Fixed - Error in complex restrictions (#892) PR #893</li> <li>Fixed - WHERE and GROUP BY classes are dropped on joins with aggregation (#898, #899) PR #893</li> </ul>"}, {"location": "changelog/#0130-mar-24-2021", "title": "0.13.0 -- Mar 24, 2021", "text": "<ul> <li>Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754</li> <li>Re-implement cascading deletes for better performance. PR #839</li> <li>Add support for deferred schema activation to allow for greater modularity. (#834) PR #839</li> <li>Add query caching mechanism for offline development (#550) PR #839</li> <li>Add table method <code>.update1</code> to update a row in the table with new values (#867) PR #763, #889</li> <li>Python datatypes are now enabled by default in blobs (#761). PR #859</li> <li>Added permissive join and restriction operators <code>@</code> and <code>^</code> (#785) PR #754</li> <li>Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735</li> <li>Add <code>dj.key_hash</code> alias to <code>dj.hash.key_hash</code> (#804) PR #862</li> <li>Default enable_python_native_blobs to True</li> <li>Bugfix - Regression error on joins with same attribute name (#857) PR #878</li> <li>Bugfix - Error when <code>fetch1('KEY')</code> when <code>dj.config['fetch_format']='frame'</code> set (#876) PR #880, #878</li> <li>Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839</li> <li>Add deprecation warning for <code>_update</code>. PR #889</li> <li>Add <code>purge_query_cache</code> utility. PR #889</li> <li>Add tests for query caching and permissive join and restriction. PR #889</li> <li>Drop support for Python 3.5 (#829) PR #861</li> </ul>"}, {"location": "changelog/#0129-mar-12-2021", "title": "0.12.9 -- Mar 12, 2021", "text": "<ul> <li>Fix bug with fetch1 with <code>dj.config['fetch_format']=\"frame\"</code>. (#876) PR #880</li> </ul>"}, {"location": "changelog/#0128-jan-12-2021", "title": "0.12.8 -- Jan 12, 2021", "text": "<ul> <li>table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833</li> <li>Load dependencies before querying dependencies. (#179) PR #833</li> <li>Fix display of part tables in <code>schema.save</code>. (#821) PR #833</li> <li>Add <code>schema.list_tables</code>. (#838) PR #844</li> <li>Fix minio new version regression. PR #847</li> <li>Add more S3 logging for debugging. (#831) PR #832</li> <li>Convert testing framework from TravisCI to GitHub Actions (#841) PR #840</li> </ul>"}, {"location": "changelog/#0127-oct-27-2020", "title": "0.12.7 -- Oct 27, 2020", "text": "<ul> <li>Fix case sensitivity issues to adapt to MySQL 8+. PR #819</li> <li>Fix pymysql regression bug (#814) PR #816</li> <li>Adapted attribute types now have dtype=object in all recarray results. PR #811</li> </ul>"}, {"location": "changelog/#0126-may-15-2020", "title": "0.12.6 -- May 15, 2020", "text": "<ul> <li>Add <code>order_by</code> to <code>dj.kill</code> (#668, #779) PR #775, #783</li> <li>Add explicit S3 bucket and file storage location existence checks (#748) PR #781</li> <li>Modify <code>_update</code> to allow nullable updates for strings/date (#664) PR #760</li> <li>Avoid logging events on auxiliary tables (#737) PR #753</li> <li>Add <code>kill_quick</code> and expand display to include host (#740) PR #741</li> <li>Bugfix - pandas insert fails due to additional <code>index</code> field (#666) PR #776</li> <li>Bugfix - <code>delete_external_files=True</code> does not remove from S3 (#686) PR #781</li> <li>Bugfix - pandas fetch throws error when <code>fetch_format='frame'</code> PR #774</li> </ul>"}, {"location": "changelog/#0125-feb-24-2020", "title": "0.12.5 -- Feb 24, 2020", "text": "<ul> <li>Rename module <code>dj.schema</code> into <code>dj.schemas</code>. <code>dj.schema</code> remains an alias for class <code>dj.Schema</code>. (#731) PR #732</li> <li><code>dj.create_virtual_module</code> is now called <code>dj.VirtualModule</code> (#731) PR #732</li> <li>Bugfix - SSL <code>KeyError</code> on failed connection (#716) PR #725</li> <li>Bugfix - Unable to run unit tests using nosetests (#723) PR #724</li> <li>Bugfix - <code>suppress_errors</code> does not suppress loss of connection error (#720) PR #721</li> </ul>"}, {"location": "changelog/#0124-jan-14-2020", "title": "0.12.4 -- Jan 14, 2020", "text": "<ul> <li>Support for simple scalar datatypes in blobs (#690) PR #709</li> <li>Add support for the <code>serial</code> data type in declarations: alias for <code>bigint unsigned auto_increment</code> PR #713</li> <li>Improve the log table to avoid primary key collisions PR #713</li> <li>Improve documentation in README PR #713</li> </ul>"}, {"location": "changelog/#0123-nov-22-2019", "title": "0.12.3 -- Nov 22, 2019", "text": "<ul> <li>Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705</li> <li>Bugfix - include table definition in doc string and help (#698, #699) PR #706</li> <li>Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702</li> </ul>"}, {"location": "changelog/#0122-nov-11-2019", "title": "0.12.2 -- Nov 11, 2019", "text": "<ul> <li>Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696</li> <li>Bugfix - Insert into external does not trim leading slash if defined in <code>dj.config['stores']['&lt;store&gt;']['location']</code> (#692) PR #693</li> </ul>"}, {"location": "changelog/#0121-nov-2-2019", "title": "0.12.1 -- Nov 2, 2019", "text": "<ul> <li>Bugfix - AttributeAdapter converts into a string (#684) PR #688</li> </ul>"}, {"location": "changelog/#0120-oct-31-2019", "title": "0.12.0 -- Oct 31, 2019", "text": "<ul> <li>Dropped support for Python 3.4</li> <li>Support secure connections with TLS (aka SSL) PR #620</li> <li>Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608</li> <li>Remove expression requirement to have additional attributes (#604) PR #604</li> <li>Support for filepath datatype (#481) PR #603, #659</li> <li>Support file attachment datatype (#480, #592, #637) PR #659</li> <li>Fetch return a dict array when specifying <code>as_dict=True</code> for specified attributes. (#595) PR #593</li> <li>Support of ellipsis in <code>proj</code>: <code>query_expression.proj(.., '-movie')</code> (#499) PR #578</li> <li>Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577</li> <li>Support for alter (#110) PR #573</li> <li>Support for <code>conda install datajoint</code> via <code>conda-forge</code> channel (#293)</li> <li><code>dj.conn()</code> accepts a <code>port</code> keyword argument (#563) PR #571</li> <li>Support for UUID datatype (#562) PR #567</li> <li><code>query_expr.fetch(\"KEY\", as_dict=False)</code> returns results as <code>np.recarray</code>(#414) PR #574</li> <li><code>dj.ERD</code> is now called <code>dj.Diagram</code> (#255, #546) PR #565</li> <li><code>dj.Diagram</code> underlines \"distinguished\" classes (#378) PR #557</li> <li>Accept alias for supported MySQL datatypes (#544) PR #545</li> <li>Support for pandas in <code>fetch</code> (#459, #537) PR #534</li> <li>Support for ordering by \"KEY\" in <code>fetch</code> (#541) PR #534</li> <li>Add config to enable python native blobs PR #672, #676</li> <li>Add secure option for external storage (#663) PR #674, #676</li> <li>Add blob migration utility from DJ011 to DJ012 PR #673</li> <li>Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532</li> <li>Increase default display rows (#523) PR #526</li> <li>Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656)</li> <li>Minor improvements (#538)</li> </ul>"}, {"location": "changelog/#0113-jul-26-2019", "title": "0.11.3 -- Jul 26, 2019", "text": "<ul> <li>Fix incompatibility with pyparsing 2.4.1 (#629) PR #631</li> </ul>"}, {"location": "changelog/#0112-jul-25-2019", "title": "0.11.2 -- Jul 25, 2019", "text": "<ul> <li>Fix #628 - incompatibility with pyparsing 2.4.1</li> </ul>"}, {"location": "changelog/#0111-nov-15-2018", "title": "0.11.1 -- Nov 15, 2018", "text": "<ul> <li>Fix ordering of attributes in proj (#483, #516)</li> <li>Prohibit direct insert into auto-populated tables (#511)</li> </ul>"}, {"location": "changelog/#0110-oct-25-2018", "title": "0.11.0 -- Oct 25, 2018", "text": "<ul> <li>Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507)</li> <li>Rename internal class and module names to comply with terminology in documentation (#494, #500)</li> <li>Full support of secondary indexes (#498, 500)</li> <li>ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500)</li> <li>Full support of unique and nullable dependencies (#254, #301, #493, #495, #500)</li> <li>Improve memory management in <code>populate</code> (#461, #486)</li> <li>Fix query errors and redundancies (#456, #463, #482)</li> </ul>"}, {"location": "changelog/#0101-aug-28-2018", "title": "0.10.1 -- Aug 28, 2018", "text": "<ul> <li>Fix ERD Tooltip message (#431)</li> <li>Networkx 2.0 support (#443)</li> <li>Fix insert from query with skip_duplicates=True (#451)</li> <li>Sped up queries (#458)</li> <li>Bugfix in restriction of the form (A &amp; B) * B (#463)</li> <li>Improved error messages (#466)</li> </ul>"}, {"location": "changelog/#0100-jan-10-2018", "title": "0.10.0 -- Jan 10, 2018", "text": "<ul> <li>Deletes are more efficient (#424)</li> <li>ERD shows table definition on tooltip hover in Jupyter (#422)</li> <li>S3 external storage</li> <li>Garbage collection for external sorage</li> <li>Most operators and methods of tables can be invoked as class methods rather than instance methods (#407)</li> <li>The schema decorator object no longer requires locals() to specify the context</li> <li>Compatibility with pymysql 0.8.0+</li> <li>More efficient loading of dependencies (#403)</li> </ul>"}, {"location": "changelog/#090-nov-17-2017", "title": "0.9.0 -- Nov 17, 2017", "text": "<ul> <li>Made graphviz installation optional</li> <li>Implement file-based external storage</li> <li>Implement union operator +</li> <li>Implement file-based external storage</li> </ul>"}, {"location": "changelog/#080-jul-26-2017", "title": "0.8.0 -- Jul 26, 2017", "text": "<p>Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io</p> <ul> <li>improved the ERD graphics and features using the graphviz libraries (#207, #333)</li> <li>improved password handling logic (#322, #321)</li> <li>the use of the <code>contents</code> property to populate tables now only works in <code>dj.Lookup</code> classes (#310).</li> <li>allow suppressing the display of size of query results through the <code>show_tuple_count</code> configuration option (#309)</li> <li>implemented renamed foreign keys to spec (#333)</li> <li>added the <code>limit</code> keyword argument to populate (#329)</li> <li>reduced the number of displayed messages (#308)</li> <li>added <code>size_on_disk</code> property for dj.Schema() objects (#323)</li> <li>job keys are entered in the jobs table (#316, #243)</li> <li>simplified the <code>fetch</code> and <code>fetch1</code> syntax, deprecating the <code>fetch[...]</code> syntax (#319)</li> <li>the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317)</li> </ul>"}, {"location": "changelog/#050-298-mar-8-2017", "title": "0.5.0 (#298) -- Mar 8, 2017", "text": "<ul> <li>All fetched integers are now 64-bit long and all fetched floats are double precision.</li> <li>Added <code>dj.create_virtual_module</code></li> </ul>"}, {"location": "changelog/#0410-286-feb-6-2017", "title": "0.4.10 (#286) -- Feb 6, 2017", "text": "<ul> <li>Removed Vagrant and Readthedocs support</li> <li>Explicit saving of configuration (issue #284)</li> </ul>"}, {"location": "changelog/#049-285-feb-2-2017", "title": "0.4.9 (#285) -- Feb 2, 2017", "text": "<ul> <li>Fixed setup.py for pip install</li> </ul>"}, {"location": "changelog/#047-281-jan-24-2017", "title": "0.4.7 (#281) -- Jan 24, 2017", "text": "<ul> <li>Fixed issues related to order of attributes in projection.</li> </ul>"}, {"location": "changelog/#046-277-dec-22-2016", "title": "0.4.6 (#277) -- Dec 22, 2016", "text": "<ul> <li>Proper handling of interruptions during populate</li> </ul>"}, {"location": "changelog/#045-274-dec-20-2016", "title": "0.4.5 (#274) -- Dec 20, 2016", "text": "<ul> <li>Populate reports how many keys remain to be populated at the start.</li> </ul>"}, {"location": "changelog/#043-271-dec-6-2016", "title": "0.4.3 (#271) -- Dec 6, 2016", "text": "<ul> <li>Fixed aggregation issues (#270)</li> <li>datajoint no longer attempts to connect to server at import time</li> <li>dropped support of view (reversed #257)</li> <li>more elegant handling of insufficient privileges (#268)</li> </ul>"}, {"location": "changelog/#042-267-dec-6-2016", "title": "0.4.2 (#267) -- Dec 6, 2016", "text": "<ul> <li>improved table appearance in Jupyter</li> </ul>"}, {"location": "changelog/#041-266-oct-28-2016", "title": "0.4.1 (#266) -- Oct 28, 2016", "text": "<ul> <li>bugfix for very long error messages</li> </ul>"}, {"location": "changelog/#039-sep-27-2016", "title": "0.3.9 -- Sep 27, 2016", "text": "<ul> <li>Added support for datatype <code>YEAR</code></li> <li>Fixed issues with <code>dj.U</code> and the <code>aggr</code> operator (#246, #247)</li> </ul>"}, {"location": "changelog/#038-aug-2-2016", "title": "0.3.8 -- Aug 2, 2016", "text": "<ul> <li>added the <code>_update</code> method in <code>base_relation</code>. It allows updating values in existing tuples.</li> <li>bugfix in reading values of type double. Previously it was cast as float32.</li> </ul>"}, {"location": "changelog/#037-jul-31-2016", "title": "0.3.7 -- Jul 31, 2016", "text": "<ul> <li>added parameter <code>ignore_extra_fields</code> in <code>insert</code></li> <li><code>insert(..., skip_duplicates=True)</code> now relies on <code>SELECT IGNORE</code>. Previously it explicitly checked if tuple already exists.</li> <li>table previews now include blob attributes displaying the string"}, {"location": "changelog/#036-jul-30-2016", "title": "0.3.6 -- Jul 30, 2016", "text": "<ul> <li>bugfix in <code>schema.spawn_missing_classes</code>. Previously, spawned part classes would not show in ERDs.</li> <li>dj.key now causes fetch to return as a list of dicts. Previously it was a recarray.</li> </ul>"}, {"location": "changelog/#035", "title": "0.3.5", "text": "<ul> <li><code>dj.set_password()</code> now asks for user confirmation before changing the password.</li> <li>fixed issue #228</li> </ul>"}, {"location": "changelog/#034", "title": "0.3.4", "text": "<ul> <li>Added method the <code>ERD.add_parts</code> method, which adds the part tables of all tables currently in the ERD.</li> <li><code>ERD() + arg</code> and <code>ERD() - arg</code> can now accept table classes as arg.</li> </ul>"}, {"location": "changelog/#033", "title": "0.3.3", "text": "<ul> <li>Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example.</li> <li>Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources</li> <li>ERD() no longer text the context argument.</li> <li>ERD.draw() now takes an optional context argument. By default uses the caller's locals.</li> </ul>"}, {"location": "changelog/#032", "title": "0.3.2", "text": "<ul> <li>Fixed issue #223: <code>insert</code> can insert relations without fetching.</li> <li>ERD() now takes the <code>context</code> argument, which specifies in which context to look for classes. The default is taken from the argument (schema or table).</li> <li>ERD.draw() no longer has the <code>prefix</code> argument: class names are shown as found in the context.</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the DataJoint for Python, please cite the following manuscript and Research Resource Identifier (RRID):</p> <ul> <li>Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. doi: https://doi.org/10.1101/031658</li> </ul> <ul> <li>DataJoint for Python - RRID:SCR_014543 - Version <code>Enter datajoint-python version you are using here</code></li> </ul>"}, {"location": "develop/", "title": "Developer Guide", "text": ""}, {"location": "develop/#table-of-contents", "title": "Table of Contents", "text": "<ul> <li>Contribute to DataJoint Python Documentation</li> <li>Setup Development Environment<ul> <li>Prerequisites</li> <li>With Virtual Environment</li> <li>With DevContainer</li> <li>Extra Efficiency, Optional But Recommended<ul> <li>Pre-commit Hooks</li> <li>Integration Tests</li> <li>VSCode<ul> <li>Jupyter Extension</li> <li>Debugger</li> </ul> </li> <li>MySQL CLI</li> </ul> </li> </ul> </li> </ul>"}, {"location": "develop/#contribute-to-datajoint-python-documentation", "title": "Contribute to DataJoint Python Documentation", "text": "<p>Contributions to documentations are equivalently important to any code for the community, please help us to resolve any confusions in documentations.</p> <p>Here is the instructions for contributing documentations, or you can find the same instructions at <code>$PROJECT_DIR/docs/README.md</code> in the repository.</p> <p>Back to top</p>"}, {"location": "develop/#setup-development-environment", "title": "Setup Development Environment", "text": "<p>We have DevContainer ready for contributors to develop without setting up their environment. If you are familiar with DevContainer, Docker or Github Codespace, this is the recommended development environment for you. If you have never used Docker, it might be easier for you to use a virtual environment through <code>conda/mamba/venv</code>, it is also very straightforward to set up.</p>"}, {"location": "develop/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>Clone datajoint-python repository</li> </ul> <p><pre><code># If you have your SSH key set up with GitHub, you can clone using SSH\ngit clone git@github.com:datajoint/datajoint-python.git\n# Otherwise, you can clone using HTTPS\ngit clone https://github.com/datajoint/datajoint-python.git\n</code></pre> - If you don't use DevContainer, then either install Anaconda/Miniconda/Mamba, or just use Python's built-in <code>venv</code> module without install anything else.</p>"}, {"location": "develop/#with-virtual-environment", "title": "With Virtual Environment", "text": "<pre><code># Check if you have Python 3.9 or higher, if not please upgrade\npython --version\n# Create a virtual environment with venv\npython -m venv .venv\nsource .venv/bin/activate\npip install -e .[dev]\n\n# Or create a virtual environment with conda\nconda create -n dj python=3.13 # any 3.9+ is fine\nconda activate dj\npip install -e .[dev]\n</code></pre> <p>Back to top</p>"}, {"location": "develop/#with-devcontainer", "title": "With DevContainer", "text": ""}, {"location": "develop/#launch-environment", "title": "Launch Environment", "text": "<p>Here are some options that provide a great developer experience:</p> <ul> <li>Cloud-based IDE: (recommended)<ul> <li>Launch using GitHub Codespaces using the option <code>Create codespace on master</code> in the codebase repository on your fork.</li> <li>Build time for a 2-Core codespace is ~6m. This is done infrequently and cached for convenience.</li> <li>Start time for a 2-Core codespace is ~2m. This will pull the built codespace from cache when you need it.</li> <li>Tip: GitHub auto names the codespace but you can rename the codespace so that it is easier to identify later.</li> </ul> </li> <li>Local IDE (VSCode - Dev Containers):<ul> <li>Ensure you have Git</li> <li>Ensure you have Docker</li> <li>Ensure you have VSCode</li> <li>Install the Dev Containers extension</li> <li><code>git clone</code> the codebase repository and open it in VSCode</li> <li>Use the <code>Dev Containers extension</code> to <code>Reopen in Container</code> (More info in the <code>Getting started</code> included with the extension)</li> <li>You will know your environment has finished loading once you see a terminal open related to <code>Running postStartCommand</code> with a final message: <code>Done. Press any key to close the terminal.</code>.</li> </ul> </li> <li>Local IDE (Docker Compose):<ul> <li>Ensure you have Git</li> <li>Ensure you have Docker</li> <li><code>git clone</code> the codebase repository and open it in VSCode</li> <li>Issue the following command in the terminal to build and run the Docker container: <code>HOST_UID=$(id -u) PY_VER=3.11 DJ_VERSION=$(grep -oP '\\d+\\.\\d+\\.\\d+' datajoint/version.py) docker compose --profile test run --rm -it djtest -- sh -c 'pip install -qe \".[dev]\" &amp;&amp; bash'</code></li> <li>Issue the following command in the terminal to stop the Docker compose stack: <code>docker compose --profile test down</code></li> </ul> </li> </ul> <p>Back to top</p>"}, {"location": "develop/#extra-efficiency-optional-but-recommended", "title": "Extra Efficiency, Optional But Recommended", "text": ""}, {"location": "develop/#pre-commit-hooks", "title": "Pre-commit Hooks", "text": "<p>We recommend using pre-commit to automatically run linters and formatters on your code before committing. To set up pre-commit, run the following command in your terminal:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>You can manually run pre-commit on all files with the following command:</p> <p><pre><code>pre-commit run --all-files\n</code></pre> This will run all the linters and formatters specified in the <code>.pre-commit-config.yaml</code> file. If all check passed, you can commit your code. Otherwise, you need to fix the failed checks and run the command again.</p> <p>Pre-commit will automatically run the linters and formatters on all staged files before committing. However, if your code doesn't follow the linters and formatters, the commit will fail. Some hooks will automatically fix your problem, and add the fixed files as git's <code>unstaged</code> files, you just need to add them(<code>git add .</code>) to git's <code>staged</code> files and commit again. Some hooks will not automatically fix your problem, so you need to check the pre-commit failed log to fix them manually and include the update to your <code>staged</code> files and commit again.</p> <p>If you really don't want to use pre-commit, or if you don't like it, you can uninstall it with the following command:</p> <pre><code>pre-commit uninstall\n</code></pre> <p>But when you issue a pull request, the same linter and formatter check will run against your contribution, you are going to have the same failure as well. So without pre-commit, you need to manually run these linters and formatters before committing your code:</p> <ul> <li>Syntax tests</li> </ul> <p>The following will verify that there are no syntax errors.</p> <pre><code>flake8 datajoint --count --select=E9,F63,F7,F82 --show-source --statistics\n</code></pre> <ul> <li>Style tests</li> </ul> <p>The following will verify that there are no code styling errors.</p> <pre><code>flake8 --ignore=E203,E722,W503 datajoint --count --max-complexity=62 --max-line-length=127 --statistics\n</code></pre> <p>The following will ensure the codebase has been formatted with black.</p> <pre><code>black datajoint --check -v --diff\n</code></pre> <p>The following will ensure the test suite has been formatted with black.</p> <pre><code>black tests --check -v --diff\n</code></pre> <p>Back to top</p>"}, {"location": "develop/#integration-tests", "title": "Integration Tests", "text": "<p>The following will verify there are no regression errors by running our test suite of unit and integration tests.</p> <ul> <li>Entire test suite:   <pre><code>pytest -sv --cov-report term-missing --cov=datajoint tests\n</code></pre></li> </ul> <ul> <li>A single functional test:   <pre><code>pytest -sv tests/test_connection.py::test_dj_conn\n</code></pre></li> <li>A single class test:   <pre><code>pytest -sv tests/test_aggr_regressions.py::TestIssue558\n</code></pre></li> </ul> <p>Back to top</p>"}, {"location": "develop/#vscode", "title": "VSCode", "text": ""}, {"location": "develop/#jupyter-extension", "title": "Jupyter Extension", "text": "<p>Be sure to go through this documentation if you are new to Running Jupyter Notebooks with VSCode.</p>"}, {"location": "develop/#debugger", "title": "Debugger", "text": "<p>VSCode Debugger is a powerful tool that can really accelerate fixes.</p> <p>Try it as follows:</p> <ul> <li>Create a python script of your choice</li> <li><code>import datajoint</code> (This will use the current state of the source)</li> <li>Add breakpoints by adding red dots next to line numbers</li> <li>Select the <code>Run and Debug</code> tab</li> <li>Start by clicking the button <code>Run and Debug</code></li> </ul> <p>Back to top</p>"}, {"location": "develop/#mysql-cli", "title": "MySQL CLI", "text": "<p>Installation instruction is in here</p> <p>It is often useful in development to connect to DataJoint's relational database backend directly using the MySQL CLI.</p> <p>Connect as follows to the database running within your developer environment:</p> <pre><code>mysql -hdb -uroot -ppassword\n</code></pre> <p>Back to top</p>"}, {"location": "faq/", "title": "Frequently Asked Questions", "text": ""}, {"location": "faq/#how-do-i-use-datajoint-with-a-gui", "title": "How do I use DataJoint with a GUI?", "text": "<p>It is common to enter data during experiments using a graphical user interface.</p> <ol> <li> <p>The DataJoint platform platform is a web-based,    end-to-end platform to host and execute data pipelines.</p> </li> <li> <p>DataJoint LabBook is an open source project for data entry but is no longer actively maintained.</p> </li> </ol>"}, {"location": "faq/#does-datajoint-support-other-programming-languages", "title": "Does DataJoint support other programming languages?", "text": "<p>DataJoint Python is the most up-to-date version and all future development will focus on the Python API. The Matlab API was actively developed through 2023. Previous projects implemented some DataJoint features in Julia and Rust. DataJoint's data model and data representation are largely language independent, which means that any language with a DataJoint client can work with a data pipeline defined in any other language. DataJoint clients for other programming languages will be implemented based on demand. All languages must comply to the same data model and computation approach as defined in DataJoint: a simpler relational data model.</p>"}, {"location": "faq/#can-i-use-datajoint-with-my-current-database", "title": "Can I use DataJoint with my current database?", "text": "<p>Researchers use many different tools to keep records, from simple formalized file hierarchies to complete software packages for colony management and standard file types like NWB. Existing projects have built interfaces with many such tools, such as PyRAT. The only requirement for interface is that tool has an open API. Contact support@datajoint.com with inquiries. The DataJoint team will consider development requests based on community demand.</p>"}, {"location": "faq/#is-datajoint-an-orm", "title": "Is DataJoint an ORM?", "text": "<p>Programmers are familiar with object-relational mappings (ORM) in various programming languages. Python in particular has several popular ORMs such as SQLAlchemy and Django ORM. The purpose of ORMs is to allow representations and manipulations of objects from the host programming language as data in a relational database. ORMs allow making objects persistent between program executions by creating a bridge (i.e., mapping) between the object model used by the host language and the relational model allowed by the database. The result is always a compromise, usually toward the object model. ORMs usually forgo key concepts, features, and capabilities of the relational model for the sake of convenient programming constructs in the language.</p> <p>In contrast, DataJoint implements a data model that is a refinement of the relational data model without compromising its core principles of data representation and queries. DataJoint supports data integrity (entity integrity, referential integrity, and group integrity) and provides a fully capable relational query language. DataJoint remains absolutely data-centric, with the primary focus on the structure and integrity of the data pipeline. Other ORMs are more application-centric, primarily focusing on the application design while the database plays a secondary role supporting the application with object persistence and sharing.</p>"}, {"location": "faq/#what-is-the-difference-between-datajoint-and-alyx", "title": "What is the difference between DataJoint and Alyx?", "text": "<p>Alyx is an experiment management database application developed in Kenneth Harris' lab at UCL.</p> <p>Alyx is an application with a fixed pipeline design with a nice graphical user interface. In contrast, DataJoint is a general-purpose library for designing and building data processing pipelines.</p> <p>Alyx is geared towards ease of data entry and tracking for a specific workflow (e.g. mouse colony information and some pre-specified experiments) and data types. DataJoint could be used as a more general purposes tool to design, implement, and execute processing on such workflows/pipelines from scratch, and DataJoint focuses on flexibility, data integrity, and ease of data analysis. The purposes are partly overlapping and complementary. The International Brain Lab project is developing a bridge from Alyx to DataJoint, hosted as an open-source project. It implements a DataJoint schema that replicates the major features of the Alyx application and a synchronization script from an existing Alyx database to its DataJoint counterpart.</p>"}, {"location": "faq/#where-is-my-data", "title": "Where is my data?", "text": "<p>New users often ask this question thinking of passive data repositories -- collections of files and folders and a separate collection of metadata -- information about how the files were collected and what they contain. Let's address metadata first, since the answer there is easy: Everything goes in the database! Any information about the experiment that would normally be stored in a lab notebook, in an Excel spreadsheet, or in a Word document is entered into tables in the database. These tables can accommodate numbers, strings, dates, or numerical arrays. The entry of metadata can be manual, or it can be an automated part of data acquisition (in this case the acquisition software itself is modified to enter information directly into the database).</p> <p>Depending on their size and contents, raw data files can be stored in a number of ways. In the simplest and most common scenario, raw data continues to be stored in either a local filesystem or in the cloud as collections of files and folders. The paths to these files are entered in the database (again, either manually or by automated processes). This is the point at which the notion of a data pipeline begins. Below these \"manual tables\" that contain metadata and file paths are a series of tables that load raw data from these files, process it in some way, and insert derived or summarized data directly into the database. For example, in an imaging application, the very large raw <code>.TIFF</code> stacks would reside on the filesystem, but the extracted fluorescent trace timeseries for each cell in the image would be stored as a numerical array directly in the database. Or the raw video used for animal tracking might be stored in a standard video format on the filesystem, but the computed X/Y positions of the animal would be stored in the database. Storing these intermediate computations in the database makes them easily available for downstream analyses and queries.</p>"}, {"location": "faq/#do-i-have-to-manually-enter-all-my-data-into-the-database", "title": "Do I have to manually enter all my data into the database?", "text": "<p>No! While some of the data will be manually entered (the same way that it would be manually recorded in a lab notebook), the advantage of DataJoint is that standard downstream processing steps can be run automatically on all new data with a single command. This is where the notion of a data pipeline comes into play. When the workflow of cleaning and processing the data, extracting important features, and performing basic analyses is all implemented in a DataJoint pipeline, minimal effort is required to analyze newly-collected data. Depending on the size of the raw files and the complexity of analysis, useful results may be available in a matter of minutes or hours. Because these results are stored in the database, they can be made available to anyone who is given access credentials for additional downstream analyses.</p>"}, {"location": "faq/#wont-the-database-get-too-big-if-all-my-data-are-there", "title": "Won't the database get too big if all my data are there?", "text": "<p>Typically, this is not a problem. If you find that your database is getting larger than a few dozen TB, DataJoint provides transparent solutions for storing very large chunks of data (larger than the 4 GB that can be natively stored as a LONGBLOB in MySQL). However, in many scenarios even long time series or images can be stored directly in the database with little effect on performance.</p>"}, {"location": "faq/#why-not-just-process-the-data-and-save-them-back-to-a-file", "title": "Why not just process the data and save them back to a file?", "text": "<p>There are two main advantages to storing results in the database. The first is data integrity. Because the relationships between data are enforced by the structure of the database, DataJoint ensures that the metadata in the upstream nodes always correctly describes the computed results downstream in the pipeline. If a specific experimental session is deleted, for example, all the data extracted from that session are automatically removed as well, so there is no chance of \"orphaned\" data. Likewise, the database ensures that computations are atomic. This means that any computation performed on a dataset is performed in an all-or-none fashion. Either all of the data are processed and inserted, or none at all. This ensures that there are no incomplete data. Neither of these important features of data integrity can be guaranteed by a file system.</p> <p>The second advantage of storing intermediate results in a data pipeline is flexible access. Accessing arbitrarily complex subsets of the data can be achieved with DataJoint's flexible query language. When data are stored in files, collecting the desired data requires trawling through the file hierarchy, finding and loading the files of interest, and selecting the interesting parts of the data.</p> <p>This brings us to the final important question:</p>"}, {"location": "faq/#how-do-i-get-my-data-out", "title": "How do I get my data out?", "text": "<p>This is the fun part. See queries for details of the DataJoint query language directly from Python.</p>"}, {"location": "faq/#interfaces", "title": "Interfaces", "text": "<p>Multiple interfaces may be used to get the data into and out of the pipeline.</p> <p>Some labs use third-party GUI applications such as HeidiSQL and Navicat, for example.  These applications allow entering and editing data in tables similarly to spreadsheets.</p> <p>The Helium Application (https://mattbdean.github.io/Helium/ and https://github.com/mattbdean/Helium) is web application for browsing DataJoint pipelines and entering new data. Matt Dean develops and maintains Helium under the direction of members of Karel Svoboda's lab at Janelia Research Campus and Vathes LLC.</p> <p>Data may also be imported or synchronized into a DataJoint pipeline from existing LIMS (laboratory information management systems). For example, the International Brain Lab synchronizes data from an Alyx database. For implementation details, see https://github.com/int-brain-lab/IBL-pipeline.</p> <p>Other labs (e.g. Sinz Lab) have developed GUI interfaces using the Flask web framework in Python.</p>"}, {"location": "publish-data/", "title": "Publishing Data", "text": "<p>DataJoint is a framework for building data pipelines that support rigorous flow of structured data between experimenters, data scientists, and computing agents during data acquisition and processing within a centralized project. Publishing final datasets for the outside world may require additional steps and conversion.</p>"}, {"location": "publish-data/#provide-access-to-a-datajoint-server", "title": "Provide access to a DataJoint server", "text": "<p>One approach for publishing data is to grant public access to an existing pipeline. Then public users will be able to query the data pipelines using DataJoint's query language and output interfaces just like any other users of the pipeline. For security, this may require synchronizing the data onto a separate read-only public server.</p>"}, {"location": "publish-data/#containerizing-as-a-datajoint-pipeline", "title": "Containerizing as a DataJoint pipeline", "text": "<p>Containerization platforms such as Docker allow convenient distribution of environments including database services and data. It is convenient to publish DataJoint pipelines as a docker container that deploys the populated DataJoint pipeline. One example of publishing a DataJoint pipeline as a docker container is</p> <p>Sinz, F., Ecker, A.S., Fahey, P., Walker, E., Cobos, E., Froudarakis, E., Yatsenko, D., Pitkow, Z., Reimer, J. and Tolias, A., 2018. Stimulus domain transfer in recurrent models for large scale cortical population prediction on video. In Advances in Neural Information Processing Systems (pp. 7198-7209).  https://www.biorxiv.org/content/early/2018/10/25/452672</p> <p>The code and the data can be found at https://github.com/sinzlab/Sinz2018_NIPS.</p>"}, {"location": "publish-data/#exporting-into-a-collection-of-files", "title": "Exporting into a collection of files", "text": "<p>Another option for publishing and archiving data is to export the data from the DataJoint pipeline into a collection of files. DataJoint provides features for exporting and importing sections of the pipeline. Several ongoing projects are implementing the capability to export from DataJoint pipelines into Neurodata Without Borders files.</p>"}, {"location": "quick-start/", "title": "Quick Start Guide", "text": ""}, {"location": "quick-start/#tutorials", "title": "Tutorials", "text": "<p>The easiest way to get started is through the DataJoint Tutorials. These tutorials are configured to run using GitHub Codespaces where the full environment including the database is already set up.</p> <p>Advanced users can install DataJoint locally. Please see the installation instructions below.</p>"}, {"location": "quick-start/#installation", "title": "Installation", "text": "<p>First, please install Python version 3.8 or later.</p> <p>Next, please install DataJoint via one of the following:</p> condapip + pip + pip +  <p>Pre-Requisites - Ensure you have conda installed.</p> <p>To add the <code>conda-forge</code> channel:</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>To install:</p> <pre><code>conda install -c conda-forge datajoint\n</code></pre> <p>Pre-Requisites - Ensure you have pip installed. - Install graphviz pre-requisite for     diagram visualization.</p> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre> <p>Pre-Requisites - Ensure you have pip installed. - Install graphviz pre-requisite for     diagram visualization.</p> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre> <p>Pre-Requisites - Ensure you have pip installed. - Install graphviz pre-requisite for     diagram visualization.</p> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre>"}, {"location": "quick-start/#connection", "title": "Connection", "text": "environment variablesmemoryfile <p>Before using <code>datajoint</code>, set the following environment variables like so:</p> <pre><code>DJ_HOST={host_address}\nDJ_USER={user}\nDJ_PASS={password}\n</code></pre> <p>To set connection settings within Python, perform:</p> <pre><code>import datajoint as dj\n\ndj.config[\"database.host\"] = \"{host_address}\"\ndj.config[\"database.user\"] = \"{user}\"\ndj.config[\"database.password\"] = \"{password}\"\n</code></pre> <p>These configuration settings can be saved either locally or system-wide using one of the following commands: <pre><code>dj.config.save_local()\ndj.config.save_global()\n</code></pre></p> <p>Before using <code>datajoint</code>, create a file named <code>dj_local_conf.json</code> in the current directory like so:</p> <pre><code>{\n    \"database.host\": \"{host_address}\",\n    \"database.user\": \"{user}\",\n    \"database.password\": \"{password}\"\n}\n</code></pre> <p>These settings will be loaded whenever a Python instance is launched from this directory. To configure settings globally, save a similar file as <code>.datajoint_config.json</code> in your home directory. A local config, if present, will take precedent over global settings.</p>"}, {"location": "quick-start/#data-pipeline-definition", "title": "Data Pipeline Definition", "text": "<p>Let's definite a simple data pipeline.</p> <pre><code>import datajoint as dj\nschema = dj.Schema(f\"{dj.config['database.user']}_shapes\") # This statement creates the database schema `{username}_shapes` on the server.\n\n@schema # The `@schema` decorator for DataJoint classes creates the table on the server.\nclass Rectangle(dj.Manual):\n    definition = \"\"\" # The table is defined by the the `definition` property.\n    shape_id: int\n    ---\n    shape_height: float\n    shape_width: float\n    \"\"\"\n\n@schema\nclass Area(dj.Computed):\n    definition = \"\"\"\n    -&gt; Rectangle\n    ---\n    shape_area: float\n    \"\"\"\n    def make(self, key):\n        rectangle = (Rectangle &amp; key).fetch1()\n        Area.insert1(\n            dict(\n                shape_id=rectangle[\"shape_id\"],\n                shape_area=rectangle[\"shape_height\"] * rectangle[\"shape_width\"],\n            )\n        )\n</code></pre> <p>It is a common practice to have a separate Python module for each schema. Therefore, each such module has only one <code>dj.Schema</code> object defined and is usually named <code>schema</code>.</p> <p>The <code>dj.Schema</code> constructor can take a number of optional parameters after the schema name.</p> <ul> <li><code>context</code> - Dictionary for looking up foreign key references.     Defaults to <code>None</code> to use local context.</li> <li><code>connection</code> - Specifies the DataJoint connection object. Defaults     to <code>dj.conn()</code>.</li> <li><code>create_schema</code> - When <code>False</code>, the schema object will not create a     schema on the database and will raise an error if one does not     already exist. Defaults to <code>True</code>.</li> <li><code>create_tables</code> - When <code>False</code>, the schema object will not create     tables on the database and will raise errors when accessing missing     tables. Defaults to <code>True</code>.</li> </ul> <p>The <code>@schema</code> decorator uses the class name and the data tier to check whether an appropriate table exists on the database. If a table does not already exist, the decorator creates one on the database using the definition property. The decorator attaches the information about the table to the class, and then returns the class.</p>"}, {"location": "quick-start/#diagram", "title": "Diagram", "text": ""}, {"location": "quick-start/#display", "title": "Display", "text": "<p>The diagram displays the relationship of the data model in the data pipeline.</p> <p>This can be done for an entire schema:</p> <pre><code>import datajoint as dj\nschema = dj.Schema('my_database')\ndj.Diagram(schema)\n</code></pre> <p></p> <p>Or for individual or sets of tables: <pre><code>dj.Diagram(schema.Rectangle)\ndj.Diagram(schema.Rectangle) + dj.Diagram(schema.Area)\n</code></pre></p> <p>What if I don't see the diagram?</p> <p>Some Python interfaces may require additional <code>draw</code> method.</p> <pre><code>dj.Diagram(schema).draw()\n</code></pre> <p>Calling the <code>.draw()</code> method is not necessary when working in a Jupyter notebook by entering <code>dj.Diagram(schema)</code> in a notebook cell. The Diagram will automatically render in the notebook by calling its <code>_repr_html_</code> method. A Diagram displayed without <code>.draw()</code> will be rendered as an SVG, and hovering the mouse over a table will reveal a compact version of the output of the <code>.describe()</code> method.</p> <p>For more information about diagrams, see this article.</p>"}, {"location": "quick-start/#customize", "title": "Customize", "text": "<p>Adding or subtracting a number to a diagram object adds nodes downstream or upstream, respectively, in the pipeline.</p> <pre><code>(dj.Diagram(schema.Rectangle)+1).draw() # Plot all the tables directly downstream from `schema.Rectangle`\n</code></pre> <pre><code>(dj.Diagram('my_schema')-1+1).draw() # Plot all tables directly downstream of those directly upstream of this schema.\n</code></pre>"}, {"location": "quick-start/#save", "title": "Save", "text": "<p>The diagram can be saved as either <code>png</code> or <code>svg</code>.</p> <pre><code>dj.Diagram(schema).save(filename='my-diagram', format='png')\n</code></pre>"}, {"location": "quick-start/#insert-data", "title": "Insert data", "text": "<p>Data entry is as easy as providing the appropriate data structure to a permitted table.</p> <p>Let's add data for a rectangle:</p> <pre><code>Rectangle.insert1(dict(shape_id=1, shape_height=2, shape_width=4))\n</code></pre> <p>Given the following table definition, we can insert data as tuples, dicts, pandas dataframes, or pathlib <code>Path</code> relative paths to local CSV files.</p> <pre><code>mouse_id: int            # unique mouse id\n---\ndob: date                # mouse date of birth\nsex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown\n</code></pre> TupleDictPandasCSV <pre><code>mouse.insert1( (0, '2017-03-01', 'M') ) # Single entry\ndata = [\n    (1, '2016-11-19', 'M'),\n    (2, '2016-11-20', 'U'),\n    (5, '2016-12-25', 'F')\n]\nmouse.insert(data) # Multi-entry\n</code></pre> <pre><code>mouse.insert1( dict(mouse_id=0, dob='2017-03-01', sex='M') ) # Single entry\ndata = [\n    {'mouse_id':1, 'dob':'2016-11-19', 'sex':'M'},\n    {'mouse_id':2, 'dob':'2016-11-20', 'sex':'U'},\n    {'mouse_id':5, 'dob':'2016-12-25', 'sex':'F'}\n]\nmouse.insert(data) # Multi-entry\n</code></pre> <pre><code>import pandas as pd\ndata = pd.DataFrame(\n    [[1, \"2016-11-19\", \"M\"], [2, \"2016-11-20\", \"U\"], [5, \"2016-12-25\", \"F\"]],\n    columns=[\"mouse_id\", \"dob\", \"sex\"],\n)\nmouse.insert(data)\n</code></pre> <p>Given the following CSV in the current working directory as <code>mice.csv</code></p> <pre><code>mouse_id,dob,sex\n1,2016-11-19,M\n2,2016-11-20,U\n5,2016-12-25,F\n</code></pre> <p>We can import as follows:</p> <pre><code>from pathlib import Path\nmouse.insert(Path('./mice.csv'))\n</code></pre>"}, {"location": "quick-start/#run-computation", "title": "Run computation", "text": "<p>Let's start the computations on our entity: <code>Area</code>.</p> <pre><code>Area.populate(display_progress=True)\n</code></pre> <p>The <code>make</code> method populates automated tables from inserted data. Read more in the full article here</p>"}, {"location": "quick-start/#query", "title": "Query", "text": "<p>Let's inspect the results.</p> <pre><code>Area &amp; \"shape_area &gt;= 8\"\n</code></pre> shaped_id shape_area 1 8.0"}, {"location": "quick-start/#fetch", "title": "Fetch", "text": "<p>Data queries in DataJoint comprise two distinct steps:</p> <ol> <li>Construct the <code>query</code> object to represent the required data using     tables and operators.</li> <li>Fetch the data from <code>query</code> into the workspace of the host language.</li> </ol> <p>Note that entities returned by <code>fetch</code> methods are not guaranteed to be sorted in any particular order unless specifically requested. Furthermore, the order is not guaranteed to be the same in any two queries, and the contents of two identical queries may change between two sequential invocations unless they are wrapped in a transaction. Therefore, if you wish to fetch matching pairs of attributes, do so in one <code>fetch</code> call.</p> <pre><code>data = query.fetch()\n</code></pre>"}, {"location": "quick-start/#entire-table", "title": "Entire table", "text": "<p>A <code>fetch</code> command can either retrieve table data as a NumPy recarray or a as a list of <code>dict</code></p> <pre><code>data = query.fetch() # NumPy recarray\ndata = query.fetch(as_dict=True) # List of `dict`\n</code></pre> <p>In some cases, the amount of data returned by fetch can be quite large; it can be useful to use the <code>size_on_disk</code> attribute to determine if running a bare fetch would be wise. Please note that it is only currently possible to query the size of entire tables stored directly in the database at this time.</p>"}, {"location": "quick-start/#separate-variables", "title": "Separate variables", "text": "<pre><code>name, img = query.fetch1('mouse_id', 'dob')  # when query has exactly one entity\nname, img = query.fetch('mouse_id', 'dob')   # [mouse_id, ...] [dob, ...]\n</code></pre>"}, {"location": "quick-start/#primary-key-values", "title": "Primary key values", "text": "<pre><code>keydict = tab.fetch1(\"KEY\")  # single key dict when tab has exactly one entity\nkeylist = tab.fetch(\"KEY\")   # list of key dictionaries [{}, ...]\n</code></pre> <p><code>KEY</code> can also used when returning attribute values as separate variables, such that one of the returned variables contains the entire primary keys.</p>"}, {"location": "quick-start/#sorting-results", "title": "Sorting results", "text": "<p>To sort the result, use the <code>order_by</code> keyword argument.</p> <pre><code>data = query.fetch(order_by='mouse_id')                # ascending order\ndata = query.fetch(order_by='mouse_id desc')           # descending order\ndata = query.fetch(order_by=('mouse_id', 'dob'))       # by ID first, dob second\ndata = query.fetch(order_by='KEY')                     # sort by the primary key\n</code></pre> <p>The <code>order_by</code> argument can be a string specifying the attribute to sort by. By default the sort is in ascending order. Use <code>'attr desc'</code> to sort in descending order by attribute <code>attr</code>. The value can also be a sequence of strings, in which case, the sort performed on all the attributes jointly in the order specified.</p> <p>The special attribute named <code>'KEY'</code> represents the primary key attributes in order that they appear in the index. Otherwise, this name can be used as any other argument.</p> <p>If an attribute happens to be a SQL reserved word, it needs to be enclosed in backquotes. For example:</p> <pre><code>data = query.fetch(order_by='`select` desc')\n</code></pre> <p>The <code>order_by</code> value is eventually passed to the <code>ORDER BY</code> clause.</p>"}, {"location": "quick-start/#limiting-results", "title": "Limiting results", "text": "<p>Similar to sorting, the <code>limit</code> and <code>offset</code> arguments can be used to limit the result to a subset of entities.</p> <pre><code>data = query.fetch(order_by='mouse_id', limit=10, offset=5)\n</code></pre> <p>Note that an <code>offset</code> cannot be used without specifying a <code>limit</code> as well.</p>"}, {"location": "quick-start/#usage-with-pandas", "title": "Usage with Pandas", "text": "<p>The <code>pandas</code> library is a popular library for data analysis in Python which can easily be used with DataJoint query results. Since the records returned by <code>fetch()</code> are contained within a <code>numpy.recarray</code>, they can be easily converted to <code>pandas.DataFrame</code> objects by passing them into the <code>pandas.DataFrame</code> constructor. For example:</p> <pre><code>import pandas as pd\nframe = pd.DataFrame(tab.fetch())\n</code></pre> <p>Calling <code>fetch()</code> with the argument <code>format=\"frame\"</code> returns results as <code>pandas.DataFrame</code> objects indexed by the table's primary key attributes.</p> <pre><code>frame = tab.fetch(format=\"frame\")\n</code></pre> <p>Returning results as a <code>DataFrame</code> is not possible when fetching a particular subset of attributes or when <code>as_dict</code> is set to <code>True</code>.</p>"}, {"location": "quick-start/#drop", "title": "Drop", "text": "<p>The <code>drop</code> method completely removes a table from the database, including its definition. It also removes all dependent tables, recursively. DataJoint will first display the tables being dropped and the number of entities in each before prompting the user for confirmation to proceed.</p> <p>The <code>drop</code> method is often used during initial design to allow altered table definitions to take effect.</p> <pre><code># drop the Person table from its schema\nPerson.drop()\n</code></pre>"}, {"location": "api/datajoint/__init__/", "title": "__init__.py", "text": "<p>DataJoint for Python is a framework for building data pipelines using MySQL databases to represent pipeline structure and bulk storage systems for large objects. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, and querying data.</p> <p>The DataJoint data model is described in https://arxiv.org/abs/1807.11104</p> <p>DataJoint is free software under the LGPL License. In addition, we request that any use of DataJoint leading to a publication be acknowledged in the publication.</p> <p>Please cite:</p> <p>- http://biorxiv.org/content/early/2015/11/14/031658   - http://dx.doi.org/10.1101/031658</p>"}, {"location": "api/datajoint/__init__/#datajoint.kill", "title": "<code>kill(restriction=None, connection=None, order_by=None)</code>", "text": "<p>view and kill database connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()</p> <code>None</code> <code>order_by</code> <p>order by a single attribute or the list of attributes. defaults to 'id'.  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill(restriction=None, connection=None, order_by=None):\n    \"\"\"\n    view and kill database connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n    :param order_by: order by a single attribute or the list of attributes. defaults to 'id'.\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\".\n        dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes\n    \"\"\"\n\n    if connection is None:\n        connection = conn()\n\n    if order_by is not None and not isinstance(order_by, str):\n        order_by = \",\".join(order_by)\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n        + (\" ORDER BY %s\" % (order_by or \"id\"))\n    )\n\n    while True:\n        print(\"  ID USER         HOST          STATE         TIME    INFO\")\n        print(\"+--+ +----------+ +-----------+ +-----------+ +-----+\")\n        cur = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in connection.query(query, as_dict=True)\n        )\n        for process in cur:\n            try:\n                print(\n                    \"{id:&gt;4d} {user:&lt;12s} {host:&lt;12s} {state:&lt;12s} {time:&gt;7d}  {info}\".format(\n                        **process\n                    )\n                )\n            except TypeError:\n                print(process)\n        response = input('process to kill or \"q\" to quit &gt; ')\n        if response == \"q\":\n            break\n        if response:\n            try:\n                pid = int(response)\n            except ValueError:\n                pass  # ignore non-numeric input\n            else:\n                try:\n                    connection.query(\"kill %d\" % pid)\n                except pymysql.err.InternalError:\n                    logger.warn(\"Process not found\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter", "title": "<code>AttributeAdapter</code>", "text": "<p>Base class for adapter objects for user-defined attribute types.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>class AttributeAdapter:\n    \"\"\"\n    Base class for adapter objects for user-defined attribute types.\n    \"\"\"\n\n    @property\n    def attribute_type(self):\n        \"\"\"\n        :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def get(self, value):\n        \"\"\"\n        convert value retrieved from the the attribute in a table into the adapted type\n\n        :param value: value from the database\n\n        :return: object of the adapted type\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def put(self, obj):\n        \"\"\"\n        convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n        :param obj: an object of the adapted type\n        :return: value to store in the database\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter.attribute_type", "title": "<code>attribute_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"</p>"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter.get", "title": "<code>get(value)</code>", "text": "<p>convert value retrieved from the the attribute in a table into the adapted type</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value from the database</p> required <p>Returns:</p> Type Description <p>object of the adapted type</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get(self, value):\n    \"\"\"\n    convert value retrieved from the the attribute in a table into the adapted type\n\n    :param value: value from the database\n\n    :return: object of the adapted type\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter.put", "title": "<code>put(obj)</code>", "text": "<p>convert an object of the adapted type into a value that DataJoint can store in a table attribute</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>an object of the adapted type</p> required <p>Returns:</p> Type Description <p>value to store in the database</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def put(self, obj):\n    \"\"\"\n    convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n    :param obj: an object of the adapted type\n    :return: value to store in the database\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.MatCell", "title": "<code>MatCell</code>", "text": "<p>               Bases: <code>ndarray</code></p> <p>a numpy ndarray representing a Matlab cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatCell(np.ndarray):\n    \"\"\"a numpy ndarray representing a Matlab cell array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.MatStruct", "title": "<code>MatStruct</code>", "text": "<p>               Bases: <code>recarray</code></p> <p>numpy.recarray representing a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatStruct(np.recarray):\n    \"\"\"numpy.recarray representing a Matlab struct array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection", "title": "<code>Connection</code>", "text": "<p>A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys).</p> <p>Most of the parameters below should be set in the local configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>host name, may include port number as hostname:port, in which case it overrides the value in port</p> required <code>user</code> <p>user name</p> required <code>password</code> <p>password</p> required <code>port</code> <p>port number</p> <code>None</code> <code>init_fun</code> <p>connection initialization function (SQL)</p> <code>None</code> <code>use_tls</code> <p>TLS encryption option</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>class Connection:\n    \"\"\"\n    A dj.Connection object manages a connection to a database server.\n    It also catalogues modules, schemas, tables, and their dependencies (foreign keys).\n\n    Most of the parameters below should be set in the local configuration file.\n\n    :param host: host name, may include port number as hostname:port, in which case it overrides the value in port\n    :param user: user name\n    :param password: password\n    :param port: port number\n    :param init_fun: connection initialization function (SQL)\n    :param use_tls: TLS encryption option\n    \"\"\"\n\n    def __init__(self, host, user, password, port=None, init_fun=None, use_tls=None):\n        host_input, host = (host, get_host_hook(host))\n        if \":\" in host:\n            # the port in the hostname overrides the port argument\n            host, port = host.split(\":\")\n            port = int(port)\n        elif port is None:\n            port = config[\"database.port\"]\n        self.conn_info = dict(host=host, port=port, user=user, passwd=password)\n        if use_tls is not False:\n            self.conn_info[\"ssl\"] = (\n                use_tls if isinstance(use_tls, dict) else {\"ssl\": {}}\n            )\n        self.conn_info[\"ssl_input\"] = use_tls\n        self.conn_info[\"host_input\"] = host_input\n        self.init_fun = init_fun\n        self._conn = None\n        self._query_cache = None\n        connect_host_hook(self)\n        if self.is_connected:\n            logger.info(\n                \"DataJoint {version} connected to {user}@{host}:{port}\".format(\n                    version=__version__, **self.conn_info\n                )\n            )\n            self.connection_id = self.query(\"SELECT connection_id()\").fetchone()[0]\n        else:\n            raise errors.LostConnectionError(\n                \"Connection failed {user}@{host}:{port}\".format(**self.conn_info)\n            )\n        self._in_transaction = False\n        self.schemas = dict()\n        self.dependencies = Dependencies(self)\n\n    def __eq__(self, other):\n        return self.conn_info == other.conn_info\n\n    def __repr__(self):\n        connected = \"connected\" if self.is_connected else \"disconnected\"\n        return \"DataJoint connection ({connected}) {user}@{host}:{port}\".format(\n            connected=connected, **self.conn_info\n        )\n\n    def connect(self):\n        \"\"\"Connect to the database server.\"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n            try:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if k not in [\"ssl_input\", \"host_input\"]\n                    },\n                )\n            except client.err.InternalError:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if not (\n                            k in [\"ssl_input\", \"host_input\"]\n                            or k == \"ssl\"\n                            and self.conn_info[\"ssl_input\"] is None\n                        )\n                    },\n                )\n        self._conn.autocommit(True)\n\n    def set_query_cache(self, query_cache=None):\n        \"\"\"\n        When query_cache is not None, the connection switches into the query caching mode, which entails:\n        1. Only SELECT queries are allowed.\n        2. The results of queries are cached under the path indicated by dj.config['query_cache']\n        3. query_cache is a string that differentiates different cache states.\n\n        :param query_cache: a string to initialize the hash for query results\n        \"\"\"\n        self._query_cache = query_cache\n\n    def purge_query_cache(self):\n        \"\"\"Purges all query cache.\"\"\"\n        if (\n            isinstance(config.get(cache_key), str)\n            and pathlib.Path(config[cache_key]).is_dir()\n        ):\n            for path in pathlib.Path(config[cache_key]).iterdir():\n                if not path.is_dir():\n                    path.unlink()\n\n    def close(self):\n        self._conn.close()\n\n    def register(self, schema):\n        self.schemas[schema.database] = schema\n        self.dependencies.clear()\n\n    def ping(self):\n        \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n        self._conn.ping(reconnect=False)\n\n    @property\n    def is_connected(self):\n        \"\"\"Return true if the object is connected to the database server.\"\"\"\n        try:\n            self.ping()\n        except:\n            return False\n        return True\n\n    @staticmethod\n    def _execute_query(cursor, query, args, suppress_warnings):\n        try:\n            with warnings.catch_warnings():\n                if suppress_warnings:\n                    # suppress all warnings arising from underlying SQL library\n                    warnings.simplefilter(\"ignore\")\n                cursor.execute(query, args)\n        except client.err.Error as err:\n            raise translate_query_error(err, query)\n\n    def query(\n        self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n    ):\n        \"\"\"\n        Execute the specified query and return the tuple generator (cursor).\n\n        :param query: SQL query\n        :param args: additional arguments for the client.cursor\n        :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                        query results as dictionary.\n        :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n        :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n        \"\"\"\n        # check cache first:\n        use_query_cache = bool(self._query_cache)\n        if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n            raise errors.DataJointError(\n                \"Only SELECT queries are allowed when query caching is on.\"\n            )\n        if use_query_cache:\n            if not config[cache_key]:\n                raise errors.DataJointError(\n                    f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n                )\n            hash_ = uuid_from_buffer(\n                (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n                + pack(args)\n            )\n            cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n            try:\n                buffer = cache_path.read_bytes()\n            except FileNotFoundError:\n                pass  # proceed to query the database\n            else:\n                return EmulatedCursor(unpack(buffer))\n\n        if reconnect is None:\n            reconnect = config[\"database.reconnect\"]\n        logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n        cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n        cursor = self._conn.cursor(cursor=cursor_class)\n        try:\n            self._execute_query(cursor, query, args, suppress_warnings)\n        except errors.LostConnectionError:\n            if not reconnect:\n                raise\n            logger.warning(\"Reconnecting to MySQL server.\")\n            connect_host_hook(self)\n            if self._in_transaction:\n                self.cancel_transaction()\n                raise errors.LostConnectionError(\n                    \"Connection was lost during a transaction.\"\n                )\n            logger.debug(\"Re-executing\")\n            cursor = self._conn.cursor(cursor=cursor_class)\n            self._execute_query(cursor, query, args, suppress_warnings)\n\n        if use_query_cache:\n            data = cursor.fetchall()\n            cache_path.write_bytes(pack(data))\n            return EmulatedCursor(data)\n\n        return cursor\n\n    def get_user(self):\n        \"\"\"\n        :return: the user name and host name provided by the client to the server.\n        \"\"\"\n        return self.query(\"SELECT user()\").fetchone()[0]\n\n    # ---------- transaction processing\n    @property\n    def in_transaction(self):\n        \"\"\"\n        :return: True if there is an open transaction.\n        \"\"\"\n        self._in_transaction = self._in_transaction and self.is_connected\n        return self._in_transaction\n\n    def start_transaction(self):\n        \"\"\"\n        Starts a transaction error.\n        \"\"\"\n        if self.in_transaction:\n            raise errors.DataJointError(\"Nested connections are not supported.\")\n        self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n        self._in_transaction = True\n        logger.debug(\"Transaction started\")\n\n    def cancel_transaction(self):\n        \"\"\"\n        Cancels the current transaction and rolls back all changes made during the transaction.\n        \"\"\"\n        self.query(\"ROLLBACK\")\n        self._in_transaction = False\n        logger.debug(\"Transaction cancelled. Rolling back ...\")\n\n    def commit_transaction(self):\n        \"\"\"\n        Commit all changes made during the transaction and close it.\n\n        \"\"\"\n        self.query(\"COMMIT\")\n        self._in_transaction = False\n        logger.debug(\"Transaction committed and closed.\")\n\n    # -------- context manager for transactions\n    @property\n    @contextmanager\n    def transaction(self):\n        \"\"\"\n        Context manager for transactions. Opens an transaction and closes it after the with statement.\n        If an error is caught during the transaction, the commits are automatically rolled back.\n        All errors are raised again.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.conn().transaction as conn:\n        &gt;&gt;&gt;     # transaction is open here\n        \"\"\"\n        try:\n            self.start_transaction()\n            yield self\n        except:\n            self.cancel_transaction()\n            raise\n        else:\n            self.commit_transaction()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.connect", "title": "<code>connect()</code>", "text": "<p>Connect to the database server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def connect(self):\n    \"\"\"Connect to the database server.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n        try:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if k not in [\"ssl_input\", \"host_input\"]\n                },\n            )\n        except client.err.InternalError:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if not (\n                        k in [\"ssl_input\", \"host_input\"]\n                        or k == \"ssl\"\n                        and self.conn_info[\"ssl_input\"] is None\n                    )\n                },\n            )\n    self._conn.autocommit(True)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.set_query_cache", "title": "<code>set_query_cache(query_cache=None)</code>", "text": "<p>When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states.</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <p>a string to initialize the hash for query results</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def set_query_cache(self, query_cache=None):\n    \"\"\"\n    When query_cache is not None, the connection switches into the query caching mode, which entails:\n    1. Only SELECT queries are allowed.\n    2. The results of queries are cached under the path indicated by dj.config['query_cache']\n    3. query_cache is a string that differentiates different cache states.\n\n    :param query_cache: a string to initialize the hash for query results\n    \"\"\"\n    self._query_cache = query_cache\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.purge_query_cache", "title": "<code>purge_query_cache()</code>", "text": "<p>Purges all query cache.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def purge_query_cache(self):\n    \"\"\"Purges all query cache.\"\"\"\n    if (\n        isinstance(config.get(cache_key), str)\n        and pathlib.Path(config[cache_key]).is_dir()\n    ):\n        for path in pathlib.Path(config[cache_key]).iterdir():\n            if not path.is_dir():\n                path.unlink()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.ping", "title": "<code>ping()</code>", "text": "<p>Ping the connection or raises an exception if the connection is closed.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def ping(self):\n    \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n    self._conn.ping(reconnect=False)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.is_connected", "title": "<code>is_connected</code>  <code>property</code>", "text": "<p>Return true if the object is connected to the database server.</p>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.query", "title": "<code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)</code>", "text": "<p>Execute the specified query and return the tuple generator (cursor).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>SQL query</p> required <code>args</code> <p>additional arguments for the client.cursor</p> <code>()</code> <code>as_dict</code> <p>If as_dict is set to True, the returned cursor objects returns query results as dictionary.</p> <code>False</code> <code>suppress_warnings</code> <p>If True, suppress all warnings arising from underlying query library</p> <code>True</code> <code>reconnect</code> <p>when None, get from config, when True, attempt to reconnect if disconnected</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def query(\n    self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n):\n    \"\"\"\n    Execute the specified query and return the tuple generator (cursor).\n\n    :param query: SQL query\n    :param args: additional arguments for the client.cursor\n    :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                    query results as dictionary.\n    :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n    :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n    \"\"\"\n    # check cache first:\n    use_query_cache = bool(self._query_cache)\n    if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n        raise errors.DataJointError(\n            \"Only SELECT queries are allowed when query caching is on.\"\n        )\n    if use_query_cache:\n        if not config[cache_key]:\n            raise errors.DataJointError(\n                f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n            )\n        hash_ = uuid_from_buffer(\n            (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n            + pack(args)\n        )\n        cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n        try:\n            buffer = cache_path.read_bytes()\n        except FileNotFoundError:\n            pass  # proceed to query the database\n        else:\n            return EmulatedCursor(unpack(buffer))\n\n    if reconnect is None:\n        reconnect = config[\"database.reconnect\"]\n    logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n    cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n    cursor = self._conn.cursor(cursor=cursor_class)\n    try:\n        self._execute_query(cursor, query, args, suppress_warnings)\n    except errors.LostConnectionError:\n        if not reconnect:\n            raise\n        logger.warning(\"Reconnecting to MySQL server.\")\n        connect_host_hook(self)\n        if self._in_transaction:\n            self.cancel_transaction()\n            raise errors.LostConnectionError(\n                \"Connection was lost during a transaction.\"\n            )\n        logger.debug(\"Re-executing\")\n        cursor = self._conn.cursor(cursor=cursor_class)\n        self._execute_query(cursor, query, args, suppress_warnings)\n\n    if use_query_cache:\n        data = cursor.fetchall()\n        cache_path.write_bytes(pack(data))\n        return EmulatedCursor(data)\n\n    return cursor\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.get_user", "title": "<code>get_user()</code>", "text": "<p>Returns:</p> Type Description <p>the user name and host name provided by the client to the server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def get_user(self):\n    \"\"\"\n    :return: the user name and host name provided by the client to the server.\n    \"\"\"\n    return self.query(\"SELECT user()\").fetchone()[0]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.in_transaction", "title": "<code>in_transaction</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True if there is an open transaction.</p>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.start_transaction", "title": "<code>start_transaction()</code>", "text": "<p>Starts a transaction error.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def start_transaction(self):\n    \"\"\"\n    Starts a transaction error.\n    \"\"\"\n    if self.in_transaction:\n        raise errors.DataJointError(\"Nested connections are not supported.\")\n    self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n    self._in_transaction = True\n    logger.debug(\"Transaction started\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.cancel_transaction", "title": "<code>cancel_transaction()</code>", "text": "<p>Cancels the current transaction and rolls back all changes made during the transaction.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def cancel_transaction(self):\n    \"\"\"\n    Cancels the current transaction and rolls back all changes made during the transaction.\n    \"\"\"\n    self.query(\"ROLLBACK\")\n    self._in_transaction = False\n    logger.debug(\"Transaction cancelled. Rolling back ...\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.commit_transaction", "title": "<code>commit_transaction()</code>", "text": "<p>Commit all changes made during the transaction and close it.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def commit_transaction(self):\n    \"\"\"\n    Commit all changes made during the transaction and close it.\n\n    \"\"\"\n    self.query(\"COMMIT\")\n    self._in_transaction = False\n    logger.debug(\"Transaction committed and closed.\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection.transaction", "title": "<code>transaction</code>  <code>property</code>", "text": "<p>Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again.</p> <p>Example:</p> <p>import datajoint as dj with dj.conn().transaction as conn:     # transaction is open here</p>"}, {"location": "api/datajoint/__init__/#datajoint.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS preferred, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n    \"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS preferred, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Diagram", "title": "<code>Diagram</code>", "text": "<p>               Bases: <code>DiGraph</code></p> <p>Schema diagram showing tables and foreign keys between in the form of a directed acyclic graph (DAG).  The diagram is derived from the connection.dependencies object.</p> <p>Usage:</p> <p>diag = Diagram(source)</p> <p>source can be a table object, a table class, a schema, or a module that has a schema.</p> <p>diag.draw()</p> <p>draws the diagram using pyplot</p> <p>diag1 + diag2  - combines the two diagrams. diag1 - diag2  - difference between diagrams diag1 * diag2  - intersection of diagrams diag + n   - expands n levels of successors diag - n   - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table</p> <p>Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed</p> Source code in <code>datajoint/diagram.py</code> <pre><code>class Diagram(nx.DiGraph):\n    \"\"\"\n    Schema diagram showing tables and foreign keys between in the form of a directed\n    acyclic graph (DAG).  The diagram is derived from the connection.dependencies object.\n\n    Usage:\n\n    &gt;&gt;&gt;  diag = Diagram(source)\n\n    source can be a table object, a table class, a schema, or a module that has a schema.\n\n    &gt;&gt;&gt; diag.draw()\n\n    draws the diagram using pyplot\n\n    diag1 + diag2  - combines the two diagrams.\n    diag1 - diag2  - difference between diagrams\n    diag1 * diag2  - intersection of diagrams\n    diag + n   - expands n levels of successors\n    diag - n   - expands n levels of predecessors\n    Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table\n\n    Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth.\n    Only those tables that are loaded in the connection object are displayed\n    \"\"\"\n\n    def __init__(self, source, context=None):\n\n        if isinstance(source, Diagram):\n            # copy constructor\n            self.nodes_to_show = set(source.nodes_to_show)\n            self.context = source.context\n            super().__init__(source)\n            return\n\n        # get the caller's context\n        if context is None:\n            frame = inspect.currentframe().f_back\n            self.context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        else:\n            self.context = context\n\n        # find connection in the source\n        try:\n            connection = source.connection\n        except AttributeError:\n            try:\n                connection = source.schema.connection\n            except AttributeError:\n                raise DataJointError(\n                    \"Could not find database connection in %s\" % repr(source[0])\n                )\n\n        # initialize graph from dependencies\n        connection.dependencies.load()\n        super().__init__(connection.dependencies)\n\n        # Enumerate nodes from all the items in the list\n        self.nodes_to_show = set()\n        try:\n            self.nodes_to_show.add(source.full_table_name)\n        except AttributeError:\n            try:\n                database = source.database\n            except AttributeError:\n                try:\n                    database = source.schema.database\n                except AttributeError:\n                    raise DataJointError(\n                        \"Cannot plot Diagram for %s\" % repr(source)\n                    )\n            for node in self:\n                if node.startswith(\"`%s`\" % database):\n                    self.nodes_to_show.add(node)\n\n    @classmethod\n    def from_sequence(cls, sequence):\n        \"\"\"\n        The join Diagram for all objects in sequence\n\n        :param sequence: a sequence (e.g. list, tuple)\n        :return: Diagram(arg1) + ... + Diagram(argn)\n        \"\"\"\n        return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n\n    def add_parts(self):\n        \"\"\"\n        Adds to the diagram the part tables of all master tables already in the diagram\n        :return:\n        \"\"\"\n\n        def is_part(part, master):\n            \"\"\"\n            :param part:  `database`.`table_name`\n            :param master:   `database`.`table_name`\n            :return: True if part is part of master.\n            \"\"\"\n            part = [s.strip(\"`\") for s in part.split(\".\")]\n            master = [s.strip(\"`\") for s in master.split(\".\")]\n            return (\n                master[0] == part[0]\n                and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n            )\n\n        self = Diagram(self)  # copy\n        self.nodes_to_show.update(\n            n\n            for n in self.nodes()\n            if any(is_part(n, m) for m in self.nodes_to_show)\n        )\n        return self\n\n    def __add__(self, arg):\n        \"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Union of the diagrams when arg is another Diagram\n                 or an expansion downstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.add(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    new = nx.algorithms.boundary.node_boundary(\n                        self, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            self, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __sub__(self, arg):\n        \"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Difference of the diagrams when arg is another Diagram or\n                 an expansion upstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.difference_update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.remove(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    graph = nx.DiGraph(self).reverse()\n                    new = nx.algorithms.boundary.node_boundary(\n                        graph, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            graph, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __mul__(self, arg):\n        \"\"\"\n        Intersection of two diagrams\n        :param arg: another Diagram\n        :return: a new Diagram comprising nodes that are present in both operands.\n        \"\"\"\n        self = Diagram(self)  # copy\n        self.nodes_to_show.intersection_update(arg.nodes_to_show)\n        return self\n\n    def topo_sort(self):\n        \"\"\"return nodes in lexicographical topological order\"\"\"\n        return topo_sort(self)\n\n    def _make_graph(self):\n        \"\"\"\n        Make the self.graph - a graph object ready for drawing\n        \"\"\"\n        # mark \"distinguished\" tables, i.e. those that introduce new primary key\n        # attributes\n        for name in self.nodes_to_show:\n            foreign_attributes = set(\n                attr\n                for p in self.in_edges(name, data=True)\n                for attr in p[2][\"attr_map\"]\n                if p[2][\"primary\"]\n            )\n            self.nodes[name][\"distinguished\"] = (\n                \"primary_key\" in self.nodes[name]\n                and foreign_attributes &lt; self.nodes[name][\"primary_key\"]\n            )\n        # include aliased nodes that are sandwiched between two displayed nodes\n        gaps = set(\n            nx.algorithms.boundary.node_boundary(self, self.nodes_to_show)\n        ).intersection(\n            nx.algorithms.boundary.node_boundary(\n                nx.DiGraph(self).reverse(), self.nodes_to_show\n            )\n        )\n        nodes = self.nodes_to_show.union(a for a in gaps if a.isdigit)\n        # construct subgraph and rename nodes to class names\n        graph = nx.DiGraph(nx.DiGraph(self).subgraph(nodes))\n        nx.set_node_attributes(\n            graph, name=\"node_type\", values={n: _get_tier(n) for n in graph}\n        )\n        # relabel nodes to class names\n        mapping = {\n            node: lookup_class_name(node, self.context) or node\n            for node in graph.nodes()\n        }\n        new_names = [mapping.values()]\n        if len(new_names) &gt; len(set(new_names)):\n            raise DataJointError(\n                \"Some classes have identical names. The Diagram cannot be plotted.\"\n            )\n        nx.relabel_nodes(graph, mapping, copy=False)\n        return graph\n\n    @staticmethod\n    def _encapsulate_edge_attributes(graph):\n        \"\"\"\n        Modifies the `nx.Graph`'s edge attribute `attr_map` to be a string representation\n        of the attribute map, and encapsulates the string in double quotes.\n        Changes the graph in place.\n\n        Implements workaround described in\n        https://github.com/pydot/pydot/issues/258#issuecomment-795798099\n        \"\"\"\n        for u, v, *_, edgedata in graph.edges(data=True):\n            if \"attr_map\" in edgedata:\n                graph.edges[u, v][\"attr_map\"] = '\"{0}\"'.format(edgedata[\"attr_map\"])\n\n    @staticmethod\n    def _encapsulate_node_names(graph):\n        \"\"\"\n        Modifies the `nx.Graph`'s node names string representations encapsulated in\n        double quotes.\n        Changes the graph in place.\n\n        Implements workaround described in\n        https://github.com/datajoint/datajoint-python/pull/1176\n        \"\"\"\n        nx.relabel_nodes(\n            graph,\n            {node: '\"{0}\"'.format(node) for node in graph.nodes()},\n            copy=False,\n        )\n\n    def make_dot(self):\n        graph = self._make_graph()\n        graph.nodes()\n\n        scale = 1.2  # scaling factor for fonts and boxes\n        label_props = {  # http://matplotlib.org/examples/color/named_colors.html\n            None: dict(\n                shape=\"circle\",\n                color=\"#FFFF0040\",\n                fontcolor=\"yellow\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            _AliasNode: dict(\n                shape=\"circle\",\n                color=\"#FF880080\",\n                fontcolor=\"#FF880080\",\n                fontsize=round(scale * 0),\n                size=0.05 * scale,\n                fixed=True,\n            ),\n            Manual: dict(\n                shape=\"box\",\n                color=\"#00FF0030\",\n                fontcolor=\"darkgreen\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Lookup: dict(\n                shape=\"plaintext\",\n                color=\"#00000020\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Computed: dict(\n                shape=\"ellipse\",\n                color=\"#FF000020\",\n                fontcolor=\"#7F0000A0\",\n                fontsize=round(scale * 10),\n                size=0.3 * scale,\n                fixed=True,\n            ),\n            Imported: dict(\n                shape=\"ellipse\",\n                color=\"#00007F40\",\n                fontcolor=\"#00007FA0\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Part: dict(\n                shape=\"plaintext\",\n                color=\"#0000000\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.1 * scale,\n                fixed=False,\n            ),\n        }\n        node_props = {\n            node: label_props[d[\"node_type\"]]\n            for node, d in dict(graph.nodes(data=True)).items()\n        }\n\n        self._encapsulate_node_names(graph)\n        self._encapsulate_edge_attributes(graph)\n        dot = nx.drawing.nx_pydot.to_pydot(graph)\n        for node in dot.get_nodes():\n            node.set_shape(\"circle\")\n            name = node.get_name().strip('\"')\n            props = node_props[name]\n            node.set_fontsize(props[\"fontsize\"])\n            node.set_fontcolor(props[\"fontcolor\"])\n            node.set_shape(props[\"shape\"])\n            node.set_fontname(\"arial\")\n            node.set_fixedsize(\"shape\" if props[\"fixed\"] else False)\n            node.set_width(props[\"size\"])\n            node.set_height(props[\"size\"])\n            if name.split(\".\")[0] in self.context:\n                cls = eval(name, self.context)\n                assert issubclass(cls, Table)\n                description = cls().describe(context=self.context).split(\"\\n\")\n                description = (\n                    (\n                        \"-\" * 30\n                        if q.startswith(\"---\")\n                        else (\n                            q.replace(\"-&gt;\", \"&amp;#8594;\")\n                            if \"-&gt;\" in q\n                            else q.split(\":\")[0]\n                        )\n                    )\n                    for q in description\n                    if not q.startswith(\"#\")\n                )\n                node.set_tooltip(\"&amp;#13;\".join(description))\n            node.set_label(\n                \"&lt;&lt;u&gt;\" + name + \"&lt;/u&gt;&gt;\"\n                if node.get(\"distinguished\") == \"True\"\n                else name\n            )\n            node.set_color(props[\"color\"])\n            node.set_style(\"filled\")\n\n        for edge in dot.get_edges():\n            # see https://graphviz.org/doc/info/attrs.html\n            src = edge.get_source()\n            dest = edge.get_destination()\n            props = graph.get_edge_data(src, dest)\n            if props is None:\n                raise DataJointError(\n                    \"Could not find edge with source \"\n                    \"'{}' and destination '{}'\".format(src, dest)\n                )\n            edge.set_color(\"#00000040\")\n            edge.set_style(\"solid\" if props[\"primary\"] else \"dashed\")\n            master_part = graph.nodes[dest][\n                \"node_type\"\n            ] is Part and dest.startswith(src + \".\")\n            edge.set_weight(3 if master_part else 1)\n            edge.set_arrowhead(\"none\")\n            edge.set_penwidth(0.75 if props[\"multi\"] else 2)\n\n        return dot\n\n    def make_svg(self):\n        from IPython.display import SVG\n\n        return SVG(self.make_dot().create_svg())\n\n    def make_png(self):\n        return io.BytesIO(self.make_dot().create_png())\n\n    def make_image(self):\n        if plot_active:\n            return plt.imread(self.make_png())\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def _repr_svg_(self):\n        return self.make_svg()._repr_svg_()\n\n    def draw(self):\n        if plot_active:\n            plt.imshow(self.make_image())\n            plt.gca().axis(\"off\")\n            plt.show()\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def save(self, filename, format=None):\n        if format is None:\n            if filename.lower().endswith(\".png\"):\n                format = \"png\"\n            elif filename.lower().endswith(\".svg\"):\n                format = \"svg\"\n        if format.lower() == \"png\":\n            with open(filename, \"wb\") as f:\n                f.write(self.make_png().getbuffer().tobytes())\n        elif format.lower() == \"svg\":\n            with open(filename, \"w\") as f:\n                f.write(self.make_svg().data)\n        else:\n            raise DataJointError(\"Unsupported file format\")\n\n    @staticmethod\n    def _layout(graph, **kwargs):\n        return pydot_layout(graph, prog=\"dot\", **kwargs)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Diagram.from_sequence", "title": "<code>from_sequence(sequence)</code>  <code>classmethod</code>", "text": "<p>The join Diagram for all objects in sequence</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <p>a sequence (e.g. list, tuple)</p> required <p>Returns:</p> Type Description <p>Diagram(arg1) + ... + Diagram(argn)</p> Source code in <code>datajoint/diagram.py</code> <pre><code>@classmethod\ndef from_sequence(cls, sequence):\n    \"\"\"\n    The join Diagram for all objects in sequence\n\n    :param sequence: a sequence (e.g. list, tuple)\n    :return: Diagram(arg1) + ... + Diagram(argn)\n    \"\"\"\n    return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Diagram.add_parts", "title": "<code>add_parts()</code>", "text": "<p>Adds to the diagram the part tables of all master tables already in the diagram</p> <p>Returns:</p> Type Description Source code in <code>datajoint/diagram.py</code> <pre><code>def add_parts(self):\n    \"\"\"\n    Adds to the diagram the part tables of all master tables already in the diagram\n    :return:\n    \"\"\"\n\n    def is_part(part, master):\n        \"\"\"\n        :param part:  `database`.`table_name`\n        :param master:   `database`.`table_name`\n        :return: True if part is part of master.\n        \"\"\"\n        part = [s.strip(\"`\") for s in part.split(\".\")]\n        master = [s.strip(\"`\") for s in master.split(\".\")]\n        return (\n            master[0] == part[0]\n            and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n        )\n\n    self = Diagram(self)  # copy\n    self.nodes_to_show.update(\n        n\n        for n in self.nodes()\n        if any(is_part(n, m) for m in self.nodes_to_show)\n    )\n    return self\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Diagram.topo_sort", "title": "<code>topo_sort()</code>", "text": "<p>return nodes in lexicographical topological order</p> Source code in <code>datajoint/diagram.py</code> <pre><code>def topo_sort(self):\n    \"\"\"return nodes in lexicographical topological order\"\"\"\n    return topo_sort(self)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>               Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n    \"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n        \"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n    \"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.AndList", "title": "<code>AndList</code>", "text": "<p>               Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n    \"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Not", "title": "<code>Not</code>", "text": "<p>invert restriction</p> Source code in <code>datajoint/condition.py</code> <pre><code>class Not:\n    \"\"\"invert restriction\"\"\"\n\n    def __init__(self, restriction):\n        self.restriction = restriction\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Top", "title": "<code>Top</code>  <code>dataclass</code>", "text": "<p>A restriction to the top entities of a query. In SQL, this corresponds to ORDER BY ... LIMIT ... OFFSET</p> Source code in <code>datajoint/condition.py</code> <pre><code>@dataclass\nclass Top:\n    \"\"\"\n    A restriction to the top entities of a query.\n    In SQL, this corresponds to ORDER BY ... LIMIT ... OFFSET\n    \"\"\"\n\n    limit: Union[int, None] = 1\n    order_by: Union[str, List[str]] = \"KEY\"\n    offset: int = 0\n\n    def __post_init__(self):\n        self.order_by = self.order_by or [\"KEY\"]\n        self.offset = self.offset or 0\n\n        if self.limit is not None and not isinstance(self.limit, int):\n            raise TypeError(\"Top limit must be an integer\")\n        if not isinstance(self.order_by, (str, collections.abc.Sequence)) or not all(\n            isinstance(r, str) for r in self.order_by\n        ):\n            raise TypeError(\"Top order_by attributes must all be strings\")\n        if not isinstance(self.offset, int):\n            raise TypeError(\"The offset argument must be an integer\")\n        if self.offset and self.limit is None:\n            self.limit = 999999999999  # arbitrary large number to allow query\n        if isinstance(self.order_by, str):\n            self.order_by = [self.order_by]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.U", "title": "<code>U</code>", "text": "<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expression <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class U:\n    \"\"\"\n    dj.U objects are the universal sets representing all possible values of their attributes.\n    dj.U objects cannot be queried on their own but are useful for forming some queries.\n    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.\n    The universal set is the set of all possible combinations of values of the attributes.\n    Without any attributes, dj.U() represents the set with one element that has no attributes.\n\n    Restriction:\n\n    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.\n\n    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:\n\n    &gt;&gt;&gt; dj.U('contrast', 'brightness') &amp; stimulus\n\n    Aggregation:\n\n    In aggregation, dj.U is used for summary calculation over an entire set:\n\n    The following expression yields one element with one attribute `s` containing the total number of elements in\n    query expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(*)')\n\n    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in\n    query expression `expr`.\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(distinct attr)')\n    &gt;&gt;&gt; dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')\n\n    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`\n    over entire result set of expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, s='sum(attr)')\n\n    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of\n    their occurrences in the result set of query expression `expr`.\n\n    &gt;&gt;&gt; dj.U(attr1,attr2).aggr(expr, n='count(*)')\n\n    Joins:\n\n    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result\n    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on\n    non-primary key attributes.\n    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw\n    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first\n    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.\n    \"\"\"\n\n    def __init__(self, *primary_key):\n        self._primary_key = primary_key\n\n    @property\n    def primary_key(self):\n        return self._primary_key\n\n    def __and__(self, other):\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be restricted with a QueryExpression.\")\n        result = copy.copy(other)\n        result._distinct = True\n        result._heading = result.heading.set_primary_key(self.primary_key)\n        result = result.proj()\n        return result\n\n    def join(self, other, left=False):\n        \"\"\"\n        Joining U with a query expression has the effect of promoting the attributes of U to\n        the primary key of the other query expression.\n\n        :param other: the other query expression to join with.\n        :param left: ignored. dj.U always acts as if left=False\n        :return: a copy of the other query expression with the primary key extended.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found\"\n                % next(k for k in self.primary_key if k not in other.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        result = copy.copy(other)\n        result._heading = result.heading.set_primary_key(\n            other.primary_key\n            + [k for k in self.primary_key if k not in other.primary_key]\n        )\n        return result\n\n    def __mul__(self, other):\n        \"\"\"shorthand for join\"\"\"\n        return self.join(other)\n\n    def aggr(self, group, **named_attributes):\n        \"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if named_attributes.get(\"keep_all_rows\", False):\n            raise DataJointError(\n                \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n            )\n        return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n            **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.U.join", "title": "<code>join(other, left=False)</code>", "text": "<p>Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>the other query expression to join with.</p> required <code>left</code> <p>ignored. dj.U always acts as if left=False</p> <code>False</code> <p>Returns:</p> Type Description <p>a copy of the other query expression with the primary key extended.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, left=False):\n    \"\"\"\n    Joining U with a query expression has the effect of promoting the attributes of U to\n    the primary key of the other query expression.\n\n    :param other: the other query expression to join with.\n    :param left: ignored. dj.U always acts as if left=False\n    :return: a copy of the other query expression with the primary key extended.\n    \"\"\"\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate if a class\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found\"\n            % next(k for k in self.primary_key if k not in other.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    result = copy.copy(other)\n    result._heading = result.heading.set_primary_key(\n        other.primary_key\n        + [k for k in self.primary_key if k not in other.primary_key]\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.U.aggr", "title": "<code>aggr(group, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, **named_attributes):\n    \"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if named_attributes.get(\"keep_all_rows\", False):\n        raise DataJointError(\n            \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n        )\n    return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n        **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.key", "title": "<code>key</code>", "text": "<p>object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key</p> Source code in <code>datajoint/fetch.py</code> <pre><code>class key:\n    \"\"\"\n    object that allows requesting the primary key as an argument in expression.fetch()\n    The string \"KEY\" can be used instead of the class key\n    \"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n    \"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema", "title": "<code>Schema</code>", "text": "<p>A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace <code>context</code> in which other UserTable classes are defined.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class Schema:\n    \"\"\"\n    A schema object is a decorator for UserTable classes that binds them to their database.\n    It also specifies the namespace `context` in which other UserTable classes are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_name=None,\n        context=None,\n        *,\n        connection=None,\n        create_schema=True,\n        create_tables=True,\n        add_objects=None,\n    ):\n        \"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        If the schema_name is omitted, then schema.activate(..) must be called later\n        to associate with the database.\n\n        :param schema_name: the database schema to associate.\n        :param context: dictionary for looking up foreign key references, leave None to use local context.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: When False, do not create the schema and raise an error if missing.\n        :param create_tables: When False, do not create tables and raise errors when accessing missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context in which table classes\n        are declared.\n        \"\"\"\n        self._log = None\n        self.connection = connection\n        self.database = None\n        self.context = context\n        self.create_schema = create_schema\n        self.create_tables = create_tables\n        self._jobs = None\n        self.external = ExternalMapping(self)\n        self.add_objects = add_objects\n        self.declare_list = []\n        if schema_name:\n            self.activate(schema_name)\n\n    def is_activated(self):\n        return self.database is not None\n\n    def activate(\n        self,\n        schema_name=None,\n        *,\n        connection=None,\n        create_schema=None,\n        create_tables=None,\n        add_objects=None,\n    ):\n        \"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        :param schema_name: the database schema to associate.\n            schema_name=None is used to assert that the schema has already been activated.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: If False, do not create the schema and raise an error if missing.\n        :param create_tables: If False, do not create tables and raise errors when attempting\n            to access missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context\n            in which table classes are declared.\n        \"\"\"\n        if schema_name is None:\n            if self.exists:\n                return\n            raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n        if self.database is not None and self.exists:\n            if self.database == schema_name:  # already activated\n                return\n            raise DataJointError(\n                \"The schema is already activated for schema {db}.\".format(\n                    db=self.database\n                )\n            )\n        if connection is not None:\n            self.connection = connection\n        if self.connection is None:\n            self.connection = conn()\n        self.database = schema_name\n        if create_schema is not None:\n            self.create_schema = create_schema\n        if create_tables is not None:\n            self.create_tables = create_tables\n        if add_objects:\n            self.add_objects = add_objects\n        if not self.exists:\n            if not self.create_schema or not self.database:\n                raise DataJointError(\n                    \"Database `{name}` has not yet been declared. \"\n                    \"Set argument create_schema=True to create it.\".format(\n                        name=schema_name\n                    )\n                )\n            # create database\n            logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n            try:\n                self.connection.query(\n                    \"CREATE DATABASE `{name}`\".format(name=schema_name)\n                )\n            except AccessError:\n                raise DataJointError(\n                    \"Schema `{name}` does not exist and could not be created. \"\n                    \"Check permissions.\".format(name=schema_name)\n                )\n            else:\n                self.log(\"created\")\n        self.connection.register(self)\n\n        # decorate all tables already decorated\n        for cls, context in self.declare_list:\n            if self.add_objects:\n                context = dict(context, **self.add_objects)\n            self._decorate_master(cls, context)\n\n    def _assert_exists(self, message=None):\n        if not self.exists:\n            raise DataJointError(\n                message\n                or \"Schema `{db}` has not been created.\".format(db=self.database)\n            )\n\n    def __call__(self, cls, *, context=None):\n        \"\"\"\n        Binds the supplied class to a schema. This is intended to be used as a decorator.\n\n        :param cls: class to decorate.\n        :param context: supplied when called from spawn_missing_classes\n        \"\"\"\n        context = context or self.context or inspect.currentframe().f_back.f_locals\n        if issubclass(cls, Part):\n            raise DataJointError(\n                \"The schema decorator should not be applied to Part tables.\"\n            )\n        if self.is_activated():\n            self._decorate_master(cls, context)\n        else:\n            self.declare_list.append((cls, context))\n        return cls\n\n    def _decorate_master(self, cls, context):\n        \"\"\"\n\n        :param cls: the master class to process\n        :param context: the class' declaration context\n        \"\"\"\n        self._decorate_table(\n            cls, context=dict(context, self=cls, **{cls.__name__: cls})\n        )\n        # Process part tables\n        for part in ordered_dir(cls):\n            if part[0].isupper():\n                part = getattr(cls, part)\n                if inspect.isclass(part) and issubclass(part, Part):\n                    part._master = cls\n                    # allow addressing master by name or keyword 'master'\n                    self._decorate_table(\n                        part,\n                        context=dict(\n                            context, master=cls, self=part, **{cls.__name__: cls}\n                        ),\n                    )\n\n    def _decorate_table(self, table_class, context, assert_declared=False):\n        \"\"\"\n        assign schema properties to the table class and declare the table\n        \"\"\"\n        table_class.database = self.database\n        table_class._connection = self.connection\n        table_class._heading = Heading(\n            table_info=dict(\n                conn=self.connection,\n                database=self.database,\n                table_name=table_class.table_name,\n                context=context,\n            )\n        )\n        table_class._support = [table_class.full_table_name]\n        table_class.declaration_context = context\n\n        # instantiate the class, declare the table if not already\n        instance = table_class()\n        is_declared = instance.is_declared\n        if not is_declared and not assert_declared and self.create_tables:\n            instance.declare(context)\n            self.connection.dependencies.clear()\n        is_declared = is_declared or instance.is_declared\n\n        # add table definition to the doc string\n        if isinstance(table_class.definition, str):\n            table_class.__doc__ = (\n                (table_class.__doc__ or \"\")\n                + \"\\nTable definition:\\n\\n\"\n                + table_class.definition\n            )\n\n        # fill values in Lookup tables from their contents property\n        if (\n            isinstance(instance, Lookup)\n            and hasattr(instance, \"contents\")\n            and is_declared\n        ):\n            contents = list(instance.contents)\n            if len(contents) &gt; len(instance):\n                if instance.heading.has_autoincrement:\n                    warnings.warn(\n                        (\n                            \"Contents has changed but cannot be inserted because \"\n                            \"{table} has autoincrement.\"\n                        ).format(table=instance.__class__.__name__)\n                    )\n                else:\n                    instance.insert(contents, skip_duplicates=True)\n\n    @property\n    def log(self):\n        self._assert_exists()\n        if self._log is None:\n            self._log = Log(self.connection, self.database)\n        return self._log\n\n    def __repr__(self):\n        return \"Schema `{name}`\\n\".format(name=self.database)\n\n    @property\n    def size_on_disk(self):\n        \"\"\"\n        :return: size of the entire schema in bytes\n        \"\"\"\n        self._assert_exists()\n        return int(\n            self.connection.query(\n                \"\"\"\n            SELECT SUM(data_length + index_length)\n            FROM information_schema.tables WHERE table_schema='{db}'\n            \"\"\".format(\n                    db=self.database\n                )\n            ).fetchone()[0]\n        )\n\n    def spawn_missing_classes(self, context=None):\n        \"\"\"\n        Creates the appropriate python user table classes from tables in the schema and places them\n        in the context.\n\n        :param context: alternative context to place the missing classes into, e.g. locals()\n        \"\"\"\n        self._assert_exists()\n        if context is None:\n            if self.context is not None:\n                context = self.context\n            else:\n                # if context is missing, use the calling namespace\n                frame = inspect.currentframe().f_back\n                context = frame.f_locals\n                del frame\n        tables = [\n            row[0]\n            for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n            if lookup_class_name(\n                \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n            )\n            is None\n        ]\n        master_classes = (Lookup, Manual, Imported, Computed)\n        part_tables = []\n        for table_name in tables:\n            class_name = to_camel_case(table_name)\n            if class_name not in context:\n                try:\n                    cls = next(\n                        cls\n                        for cls in master_classes\n                        if re.fullmatch(cls.tier_regexp, table_name)\n                    )\n                except StopIteration:\n                    if re.fullmatch(Part.tier_regexp, table_name):\n                        part_tables.append(table_name)\n                else:\n                    # declare and decorate master table classes\n                    context[class_name] = self(\n                        type(class_name, (cls,), dict()), context=context\n                    )\n\n        # attach parts to masters\n        for table_name in part_tables:\n            groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n            class_name = to_camel_case(groups[\"part\"])\n            try:\n                master_class = context[to_camel_case(groups[\"master\"])]\n            except KeyError:\n                raise DataJointError(\n                    \"The table %s does not follow DataJoint naming conventions\"\n                    % table_name\n                )\n            part_class = type(class_name, (Part,), dict(definition=...))\n            part_class._master = master_class\n            self._decorate_table(part_class, context=context, assert_declared=True)\n            setattr(master_class, class_name, part_class)\n\n    def drop(self, force=False):\n        \"\"\"\n        Drop the associated schema if it exists\n        \"\"\"\n        if not self.exists:\n            logger.info(\n                \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                    database=self.database\n                )\n            )\n        elif (\n            not config[\"safemode\"]\n            or force\n            or user_choice(\n                \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n            )\n            == \"yes\"\n        ):\n            logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n            try:\n                self.connection.query(\n                    \"DROP DATABASE `{database}`\".format(database=self.database)\n                )\n                logger.debug(\n                    \"Schema `{database}` was dropped successfully.\".format(\n                        database=self.database\n                    )\n                )\n            except AccessError:\n                raise AccessError(\n                    \"An attempt to drop schema `{database}` \"\n                    \"has failed. Check permissions.\".format(database=self.database)\n                )\n\n    @property\n    def exists(self):\n        \"\"\"\n        :return: true if the associated schema exists on the server\n        \"\"\"\n        if self.database is None:\n            raise DataJointError(\"Schema must be activated first.\")\n        return bool(\n            self.connection.query(\n                \"SELECT schema_name \"\n                \"FROM information_schema.schemata \"\n                \"WHERE schema_name = '{database}'\".format(database=self.database)\n            ).rowcount\n        )\n\n    @property\n    def jobs(self):\n        \"\"\"\n        schema.jobs provides a view of the job reservation table for the schema\n\n        :return: jobs table\n        \"\"\"\n        self._assert_exists()\n        if self._jobs is None:\n            self._jobs = JobTable(self.connection, self.database)\n        return self._jobs\n\n    @property\n    def code(self):\n        self._assert_exists()\n        return self.save()\n\n    def save(self, python_filename=None):\n        \"\"\"\n        Generate the code for a module that recreates the schema.\n        This method is in preparation for a future release and is not officially supported.\n\n        :return: a string containing the body of a complete Python module defining this schema.\n        \"\"\"\n        self.connection.dependencies.load()\n        self._assert_exists()\n        module_count = itertools.count()\n        # add virtual modules for referenced modules with names vmod0, vmod1, ...\n        module_lookup = collections.defaultdict(\n            lambda: \"vmod\" + str(next(module_count))\n        )\n        db = self.database\n\n        def make_class_definition(table):\n            tier = _get_tier(table).__name__\n            class_name = table.split(\".\")[1].strip(\"`\")\n            indent = \"\"\n            if tier == \"Part\":\n                class_name = class_name.split(\"__\")[-1]\n                indent += \"    \"\n            class_name = to_camel_case(class_name)\n\n            def replace(s):\n                d, tabs = s.group(1), s.group(2)\n                return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                    to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n                )\n\n            return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n                \"{indent}class {class_name}(dj.{tier}):\\n\"\n                '{indent}    definition = \"\"\"\\n'\n                '{indent}    {defi}\"\"\"'\n            ).format(\n                class_name=class_name,\n                indent=indent,\n                tier=tier,\n                defi=re.sub(\n                    r\"`([^`]+)`.`([^`]+)`\",\n                    replace,\n                    FreeTable(self.connection, table).describe(),\n                ).replace(\"\\n\", \"\\n    \" + indent),\n            )\n\n        tables = self.connection.dependencies.topo_sort()\n        body = \"\\n\\n\".join(make_class_definition(table) for table in tables)\n        python_code = \"\\n\\n\".join(\n            (\n                '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n                \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n                \"\\n\".join(\n                    \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                        module=v, schema_name=k\n                    )\n                    for k, v in module_lookup.items()\n                ),\n                body,\n            )\n        )\n        if python_filename is None:\n            return python_code\n        with open(python_filename, \"wt\") as f:\n            f.write(python_code)\n\n    def list_tables(self):\n        \"\"\"\n        Return a list of all tables in the schema except tables with ~ in first character such\n        as ~logs and ~job\n\n        :return: A list of table names from the database schema.\n        \"\"\"\n        self.connection.dependencies.load()\n        return [\n            t\n            for d, t in (\n                table_name.replace(\"`\", \"\").split(\".\")\n                for table_name in self.connection.dependencies.topo_sort()\n            )\n            if d == self.database\n        ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.activate", "title": "<code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)</code>", "text": "<p>Associate database schema <code>schema_name</code>. If the schema does not exist, attempt to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <p>the database schema to associate. schema_name=None is used to assert that the schema has already been activated.</p> <code>None</code> <code>connection</code> <p>Connection object. Defaults to datajoint.conn().</p> <code>None</code> <code>create_schema</code> <p>If False, do not create the schema and raise an error if missing.</p> <code>None</code> <code>create_tables</code> <p>If False, do not create tables and raise errors when attempting to access missing tables.</p> <code>None</code> <code>add_objects</code> <p>a mapping with additional objects to make available to the context in which table classes are declared.</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def activate(\n    self,\n    schema_name=None,\n    *,\n    connection=None,\n    create_schema=None,\n    create_tables=None,\n    add_objects=None,\n):\n    \"\"\"\n    Associate database schema `schema_name`. If the schema does not exist, attempt to\n    create it on the server.\n\n    :param schema_name: the database schema to associate.\n        schema_name=None is used to assert that the schema has already been activated.\n    :param connection: Connection object. Defaults to datajoint.conn().\n    :param create_schema: If False, do not create the schema and raise an error if missing.\n    :param create_tables: If False, do not create tables and raise errors when attempting\n        to access missing tables.\n    :param add_objects: a mapping with additional objects to make available to the context\n        in which table classes are declared.\n    \"\"\"\n    if schema_name is None:\n        if self.exists:\n            return\n        raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n    if self.database is not None and self.exists:\n        if self.database == schema_name:  # already activated\n            return\n        raise DataJointError(\n            \"The schema is already activated for schema {db}.\".format(\n                db=self.database\n            )\n        )\n    if connection is not None:\n        self.connection = connection\n    if self.connection is None:\n        self.connection = conn()\n    self.database = schema_name\n    if create_schema is not None:\n        self.create_schema = create_schema\n    if create_tables is not None:\n        self.create_tables = create_tables\n    if add_objects:\n        self.add_objects = add_objects\n    if not self.exists:\n        if not self.create_schema or not self.database:\n            raise DataJointError(\n                \"Database `{name}` has not yet been declared. \"\n                \"Set argument create_schema=True to create it.\".format(\n                    name=schema_name\n                )\n            )\n        # create database\n        logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n        try:\n            self.connection.query(\n                \"CREATE DATABASE `{name}`\".format(name=schema_name)\n            )\n        except AccessError:\n            raise DataJointError(\n                \"Schema `{name}` does not exist and could not be created. \"\n                \"Check permissions.\".format(name=schema_name)\n            )\n        else:\n            self.log(\"created\")\n    self.connection.register(self)\n\n    # decorate all tables already decorated\n    for cls, context in self.declare_list:\n        if self.add_objects:\n            context = dict(context, **self.add_objects)\n        self._decorate_master(cls, context)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of the entire schema in bytes</p>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.spawn_missing_classes", "title": "<code>spawn_missing_classes(context=None)</code>", "text": "<p>Creates the appropriate python user table classes from tables in the schema and places them in the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>alternative context to place the missing classes into, e.g. locals()</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def spawn_missing_classes(self, context=None):\n    \"\"\"\n    Creates the appropriate python user table classes from tables in the schema and places them\n    in the context.\n\n    :param context: alternative context to place the missing classes into, e.g. locals()\n    \"\"\"\n    self._assert_exists()\n    if context is None:\n        if self.context is not None:\n            context = self.context\n        else:\n            # if context is missing, use the calling namespace\n            frame = inspect.currentframe().f_back\n            context = frame.f_locals\n            del frame\n    tables = [\n        row[0]\n        for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n        if lookup_class_name(\n            \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n        )\n        is None\n    ]\n    master_classes = (Lookup, Manual, Imported, Computed)\n    part_tables = []\n    for table_name in tables:\n        class_name = to_camel_case(table_name)\n        if class_name not in context:\n            try:\n                cls = next(\n                    cls\n                    for cls in master_classes\n                    if re.fullmatch(cls.tier_regexp, table_name)\n                )\n            except StopIteration:\n                if re.fullmatch(Part.tier_regexp, table_name):\n                    part_tables.append(table_name)\n            else:\n                # declare and decorate master table classes\n                context[class_name] = self(\n                    type(class_name, (cls,), dict()), context=context\n                )\n\n    # attach parts to masters\n    for table_name in part_tables:\n        groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n        class_name = to_camel_case(groups[\"part\"])\n        try:\n            master_class = context[to_camel_case(groups[\"master\"])]\n        except KeyError:\n            raise DataJointError(\n                \"The table %s does not follow DataJoint naming conventions\"\n                % table_name\n            )\n        part_class = type(class_name, (Part,), dict(definition=...))\n        part_class._master = master_class\n        self._decorate_table(part_class, context=context, assert_declared=True)\n        setattr(master_class, class_name, part_class)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.drop", "title": "<code>drop(force=False)</code>", "text": "<p>Drop the associated schema if it exists</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def drop(self, force=False):\n    \"\"\"\n    Drop the associated schema if it exists\n    \"\"\"\n    if not self.exists:\n        logger.info(\n            \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                database=self.database\n            )\n        )\n    elif (\n        not config[\"safemode\"]\n        or force\n        or user_choice(\n            \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n        )\n        == \"yes\"\n    ):\n        logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n        try:\n            self.connection.query(\n                \"DROP DATABASE `{database}`\".format(database=self.database)\n            )\n            logger.debug(\n                \"Schema `{database}` was dropped successfully.\".format(\n                    database=self.database\n                )\n            )\n        except AccessError:\n            raise AccessError(\n                \"An attempt to drop schema `{database}` \"\n                \"has failed. Check permissions.\".format(database=self.database)\n            )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.exists", "title": "<code>exists</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>true if the associated schema exists on the server</p>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.jobs", "title": "<code>jobs</code>  <code>property</code>", "text": "<p>schema.jobs provides a view of the job reservation table for the schema</p> <p>Returns:</p> Type Description <p>jobs table</p>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.save", "title": "<code>save(python_filename=None)</code>", "text": "<p>Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported.</p> <p>Returns:</p> Type Description <p>a string containing the body of a complete Python module defining this schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def save(self, python_filename=None):\n    \"\"\"\n    Generate the code for a module that recreates the schema.\n    This method is in preparation for a future release and is not officially supported.\n\n    :return: a string containing the body of a complete Python module defining this schema.\n    \"\"\"\n    self.connection.dependencies.load()\n    self._assert_exists()\n    module_count = itertools.count()\n    # add virtual modules for referenced modules with names vmod0, vmod1, ...\n    module_lookup = collections.defaultdict(\n        lambda: \"vmod\" + str(next(module_count))\n    )\n    db = self.database\n\n    def make_class_definition(table):\n        tier = _get_tier(table).__name__\n        class_name = table.split(\".\")[1].strip(\"`\")\n        indent = \"\"\n        if tier == \"Part\":\n            class_name = class_name.split(\"__\")[-1]\n            indent += \"    \"\n        class_name = to_camel_case(class_name)\n\n        def replace(s):\n            d, tabs = s.group(1), s.group(2)\n            return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n            )\n\n        return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n            \"{indent}class {class_name}(dj.{tier}):\\n\"\n            '{indent}    definition = \"\"\"\\n'\n            '{indent}    {defi}\"\"\"'\n        ).format(\n            class_name=class_name,\n            indent=indent,\n            tier=tier,\n            defi=re.sub(\n                r\"`([^`]+)`.`([^`]+)`\",\n                replace,\n                FreeTable(self.connection, table).describe(),\n            ).replace(\"\\n\", \"\\n    \" + indent),\n        )\n\n    tables = self.connection.dependencies.topo_sort()\n    body = \"\\n\\n\".join(make_class_definition(table) for table in tables)\n    python_code = \"\\n\\n\".join(\n        (\n            '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n            \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n            \"\\n\".join(\n                \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                    module=v, schema_name=k\n                )\n                for k, v in module_lookup.items()\n            ),\n            body,\n        )\n    )\n    if python_filename is None:\n        return python_code\n    with open(python_filename, \"wt\") as f:\n        f.write(python_code)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema.list_tables", "title": "<code>list_tables()</code>", "text": "<p>Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job</p> <p>Returns:</p> Type Description <p>A list of table names from the database schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_tables(self):\n    \"\"\"\n    Return a list of all tables in the schema except tables with ~ in first character such\n    as ~logs and ~job\n\n    :return: A list of table names from the database schema.\n    \"\"\"\n    self.connection.dependencies.load()\n    return [\n        t\n        for d, t in (\n            table_name.replace(\"`\", \"\").split(\".\")\n            for table_name in self.connection.dependencies.topo_sort()\n        )\n        if d == self.database\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.VirtualModule", "title": "<code>VirtualModule</code>", "text": "<p>               Bases: <code>ModuleType</code></p> <p>A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class VirtualModule(types.ModuleType):\n    \"\"\"\n    A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database.\n    It declares the schema objects and a class for each table.\n    \"\"\"\n\n    def __init__(\n        self,\n        module_name,\n        schema_name,\n        *,\n        create_schema=False,\n        create_tables=False,\n        connection=None,\n        add_objects=None,\n    ):\n        \"\"\"\n        Creates a python module with the given name from the name of a schema on the server and\n        automatically adds classes to it corresponding to the tables in the schema.\n\n        :param module_name: displayed module name\n        :param schema_name: name of the database in mysql\n        :param create_schema: if True, create the schema on the database server\n        :param create_tables: if True, module.schema can be used as the decorator for declaring new\n        :param connection: a dj.Connection object to pass into the schema\n        :param add_objects: additional objects to add to the module\n        :return: the python module containing classes from the schema object and the table classes\n        \"\"\"\n        super(VirtualModule, self).__init__(name=module_name)\n        _schema = Schema(\n            schema_name,\n            create_schema=create_schema,\n            create_tables=create_tables,\n            connection=connection,\n        )\n        if add_objects:\n            self.__dict__.update(add_objects)\n        self.__dict__[\"schema\"] = _schema\n        _schema.spawn_missing_classes(context=self.__dict__)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.list_schemas", "title": "<code>list_schemas(connection=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>connection</code> <p>a dj.Connection object</p> <code>None</code> <p>Returns:</p> Type Description <p>list of all accessible schemas on the server</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_schemas(connection=None):\n    \"\"\"\n    :param connection: a dj.Connection object\n    :return: list of all accessible schemas on the server\n    \"\"\"\n    return [\n        r[0]\n        for r in (connection or conn()).query(\n            \"SELECT schema_name \"\n            \"FROM information_schema.schemata \"\n            'WHERE schema_name &lt;&gt; \"information_schema\"'\n        )\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>               Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n    \"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table", "title": "<code>Table</code>", "text": "<p>               Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n    \"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def class_name(self):\n        return self.__class__.__name__\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n        \"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        # Enforce strict CamelCase #1150\n        if not is_camel_case(self.class_name):\n            raise DataJointError(\n                \"Table class name `{name}` is invalid. Please use CamelCase. \".format(\n                    name=self.class_name\n                )\n                + \"Classes defining tables should be formatted in strict CamelCase.\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n        \"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warning(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n        \"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n        \"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n        \"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n        \"\"\"\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n        \"\"\"\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n        \"\"\"\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n        \"\"\"\n        return part tables either as entries in a dict with foreign key information or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        self.connection.dependencies.load(force=False)\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n        \"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n        \"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n        \"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n        \"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n        \"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        # collects the field list from first row (passed by reference)\n        field_list = []\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n        \"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n        force_masters: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n            force_masters: If `True`, include part/master pairs in the cascade.\n                Default is `False`.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n        visited_masters = set()\n\n        def cascade(table):\n            \"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0])\n                    if match is None:\n                        raise DataJointError(\n                            \"Cascading deletes failed because the error message is missing foreign key information.\"\n                            \"Make sure you have REFERENCES privilege to all dependent tables.\"\n                        ) from None\n                    match = match.groupdict()\n                    # if schema name missing, use table\n                    if \"`.`\" not in match[\"child\"]:\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                        child._restriction_attributes = table.restriction_attributes\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n\n                    master_name = get_master(child.full_table_name)\n                    if (\n                        force_masters\n                        and master_name\n                        and master_name != table.full_table_name\n                        and master_name not in visited_masters\n                    ):\n                        master = FreeTable(table.connection, master_name)\n                        master._restriction_attributes = set()\n                        master._restriction = [\n                            make_condition(  # &amp;= may cause in target tables in subquery\n                                master,\n                                (master.proj() &amp; child.proj()).fetch(),\n                                master._restriction_attributes,\n                            )\n                        ]\n                        visited_masters.add(master_name)\n                        cascade(master)\n                    else:\n                        cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warning(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        elif not transaction:\n            logger.info(\"Delete completed\")\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Delete committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warning(\"Delete cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n        \"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n        \"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n        \"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def describe(self, context=None, printout=False):\n        \"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    (\n                        attr.name\n                        if attr.default is None\n                        else \"%s=%s\" % (attr.name, attr.default)\n                    ),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n        \"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n        \"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n            \"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n    \"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    # Enforce strict CamelCase #1150\n    if not is_camel_case(self.class_name):\n        raise DataJointError(\n            \"Table class name `{name}` is invalid. Please use CamelCase. \".format(\n                name=self.class_name\n            )\n            + \"Classes defining tables should be formatted in strict CamelCase.\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n    \"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warning(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n    \"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n    \"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n    \"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n    \"\"\"\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n    \"\"\"\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n    \"\"\"\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key information or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n    \"\"\"\n    return part tables either as entries in a dict with foreign key information or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    self.connection.dependencies.load(force=False)\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/__init__/#datajoint.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/__init__/#datajoint.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n    \"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n    \"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n    \"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    # collects the field list from first row (passed by reference)\n    field_list = []\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n    \"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False, force_masters=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.     force_masters: If <code>True</code>, include part/master pairs in the cascade.         Default is <code>False</code>.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n    force_masters: bool = False,\n) -&gt; int:\n    \"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n        force_masters: If `True`, include part/master pairs in the cascade.\n            Default is `False`.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n    visited_masters = set()\n\n    def cascade(table):\n        \"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0])\n                if match is None:\n                    raise DataJointError(\n                        \"Cascading deletes failed because the error message is missing foreign key information.\"\n                        \"Make sure you have REFERENCES privilege to all dependent tables.\"\n                    ) from None\n                match = match.groupdict()\n                # if schema name missing, use table\n                if \"`.`\" not in match[\"child\"]:\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                    child._restriction_attributes = table.restriction_attributes\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n\n                master_name = get_master(child.full_table_name)\n                if (\n                    force_masters\n                    and master_name\n                    and master_name != table.full_table_name\n                    and master_name not in visited_masters\n                ):\n                    master = FreeTable(table.connection, master_name)\n                    master._restriction_attributes = set()\n                    master._restriction = [\n                        make_condition(  # &amp;= may cause in target tables in subquery\n                            master,\n                            (master.proj() &amp; child.proj()).fetch(),\n                            master._restriction_attributes,\n                        )\n                    ]\n                    visited_masters.add(master_name)\n                    cascade(master)\n                else:\n                    cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warning(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    elif not transaction:\n        logger.info(\"Delete completed\")\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Delete committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warning(\"Delete cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n    \"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n    \"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/__init__/#datajoint.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n    \"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default)\n                ),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Computed", "title": "<code>Computed</code>", "text": "<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n    \"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Imported", "title": "<code>Imported</code>", "text": "<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n    \"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Lookup", "title": "<code>Lookup</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Manual", "title": "<code>Manual</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Part", "title": "<code>Part</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n        \"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n        \"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n    \"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n    \"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/admin/", "title": "admin.py", "text": ""}, {"location": "api/datajoint/admin/#datajoint.admin.kill", "title": "<code>kill(restriction=None, connection=None, order_by=None)</code>", "text": "<p>view and kill database connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()</p> <code>None</code> <code>order_by</code> <p>order by a single attribute or the list of attributes. defaults to 'id'.  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill(restriction=None, connection=None, order_by=None):\n    \"\"\"\n    view and kill database connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n    :param order_by: order by a single attribute or the list of attributes. defaults to 'id'.\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\".\n        dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes\n    \"\"\"\n\n    if connection is None:\n        connection = conn()\n\n    if order_by is not None and not isinstance(order_by, str):\n        order_by = \",\".join(order_by)\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n        + (\" ORDER BY %s\" % (order_by or \"id\"))\n    )\n\n    while True:\n        print(\"  ID USER         HOST          STATE         TIME    INFO\")\n        print(\"+--+ +----------+ +-----------+ +-----------+ +-----+\")\n        cur = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in connection.query(query, as_dict=True)\n        )\n        for process in cur:\n            try:\n                print(\n                    \"{id:&gt;4d} {user:&lt;12s} {host:&lt;12s} {state:&lt;12s} {time:&gt;7d}  {info}\".format(\n                        **process\n                    )\n                )\n            except TypeError:\n                print(process)\n        response = input('process to kill or \"q\" to quit &gt; ')\n        if response == \"q\":\n            break\n        if response:\n            try:\n                pid = int(response)\n            except ValueError:\n                pass  # ignore non-numeric input\n            else:\n                try:\n                    connection.query(\"kill %d\" % pid)\n                except pymysql.err.InternalError:\n                    logger.warn(\"Process not found\")\n</code></pre>"}, {"location": "api/datajoint/admin/#datajoint.admin.kill_quick", "title": "<code>kill_quick(restriction=None, connection=None)</code>", "text": "<p>Kill database connections without prompting. Returns number of terminated connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\".</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill_quick(restriction=None, connection=None):\n    \"\"\"\n    Kill database connections without prompting. Returns number of terminated connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\".\n    \"\"\"\n    if connection is None:\n        connection = conn()\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n    )\n\n    cur = (\n        {k.lower(): v for k, v in elem.items()}\n        for elem in connection.query(query, as_dict=True)\n    )\n    nkill = 0\n    for process in cur:\n        connection.query(\"kill %d\" % process[\"id\"])\n        nkill += 1\n    return nkill\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/", "title": "attribute_adapter.py", "text": ""}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter", "title": "<code>AttributeAdapter</code>", "text": "<p>Base class for adapter objects for user-defined attribute types.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>class AttributeAdapter:\n    \"\"\"\n    Base class for adapter objects for user-defined attribute types.\n    \"\"\"\n\n    @property\n    def attribute_type(self):\n        \"\"\"\n        :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def get(self, value):\n        \"\"\"\n        convert value retrieved from the the attribute in a table into the adapted type\n\n        :param value: value from the database\n\n        :return: object of the adapted type\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def put(self, obj):\n        \"\"\"\n        convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n        :param obj: an object of the adapted type\n        :return: value to store in the database\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "title": "<code>attribute_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"</p>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.get", "title": "<code>get(value)</code>", "text": "<p>convert value retrieved from the the attribute in a table into the adapted type</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value from the database</p> required <p>Returns:</p> Type Description <p>object of the adapted type</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get(self, value):\n    \"\"\"\n    convert value retrieved from the the attribute in a table into the adapted type\n\n    :param value: value from the database\n\n    :return: object of the adapted type\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.put", "title": "<code>put(obj)</code>", "text": "<p>convert an object of the adapted type into a value that DataJoint can store in a table attribute</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>an object of the adapted type</p> required <p>Returns:</p> Type Description <p>value to store in the database</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def put(self, obj):\n    \"\"\"\n    convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n    :param obj: an object of the adapted type\n    :return: value to store in the database\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.get_adapter", "title": "<code>get_adapter(context, adapter_name)</code>", "text": "<p>Extract the AttributeAdapter object by its name from the context and validate.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get_adapter(context, adapter_name):\n    \"\"\"\n    Extract the AttributeAdapter object by its name from the context and validate.\n    \"\"\"\n    if not _support_adapted_types():\n        raise DataJointError(\"Support for Adapted Attribute types is disabled.\")\n    adapter_name = adapter_name.lstrip(\"&lt;\").rstrip(\"&gt;\")\n    try:\n        adapter = (\n            context[adapter_name]\n            if adapter_name in context\n            else type_plugins[adapter_name][\"object\"].load()\n        )\n    except KeyError:\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' is not defined.\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter, AttributeAdapter):\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' must be an instance of datajoint.AttributeAdapter\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter.attribute_type, str) or not re.match(\n        r\"^\\w\", adapter.attribute_type\n    ):\n        raise DataJointError(\n            \"Invalid attribute type {type} in attribute adapter '{adapter_name}'\".format(\n                type=adapter.attribute_type, adapter_name=adapter_name\n            )\n        )\n    return adapter\n</code></pre>"}, {"location": "api/datajoint/autopopulate/", "title": "autopopulate.py", "text": "<p>This module defines class dj.AutoPopulate</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate", "title": "<code>AutoPopulate</code>", "text": "<p>AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property <code>key_source</code>, and must define the callback method <code>make</code>.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>class AutoPopulate:\n    \"\"\"\n    AutoPopulate is a mixin class that adds the method populate() to a Table class.\n    Auto-populated tables must inherit from both Table and AutoPopulate,\n    must define the property `key_source`, and must define the callback method `make`.\n    \"\"\"\n\n    _key_source = None\n    _allow_insert = False\n\n    @property\n    def key_source(self):\n        \"\"\"\n        :return: the query expression that yields primary key values to be passed,\n        sequentially, to the ``make`` method when populate() is called.\n        The default value is the join of the parent tables references from the primary key.\n        Subclasses may override they key_source to change the scope or the granularity\n        of the make calls.\n        \"\"\"\n\n        def _rename_attributes(table, props):\n            return (\n                table.proj(\n                    **{\n                        attr: ref\n                        for attr, ref in props[\"attr_map\"].items()\n                        if attr != ref\n                    }\n                )\n                if props[\"aliased\"]\n                else table.proj()\n            )\n\n        if self._key_source is None:\n            parents = self.target.parents(\n                primary=True, as_objects=True, foreign_key_info=True\n            )\n            if not parents:\n                raise DataJointError(\n                    \"A table must have dependencies \"\n                    \"from its primary key for auto-populate to work\"\n                )\n            self._key_source = _rename_attributes(*parents[0])\n            for q in parents[1:]:\n                self._key_source *= _rename_attributes(*q)\n        return self._key_source\n\n    def make(self, key, **kwargs):\n        \"\"\"\n        This method must be implemented by derived classes to perform automated computation.\n        The method must implement the following three steps:\n\n        1. Fetch data from tables above in the dependency hierarchy, restricted by the given key.\n        2. Compute secondary attributes based on the fetched data.\n        3. Insert the new tuple(s) into the current table.\n\n        The method can be implemented either as:\n        (a) Regular method: All three steps are performed in a single database transaction.\n            The method must return None.\n        (b) Generator method:\n            The make method is split into three functions:\n            - `make_fetch`: Fetches data from the parent tables.\n            - `make_compute`: Computes secondary attributes based on the fetched data.\n            - `make_insert`: Inserts the computed data into the current table.\n\n            Then populate logic is executes as follows:\n\n            &lt;pseudocode&gt;\n            fetched_data1 = self.make_fetch(key)\n            computed_result = self.make_compute(key, *fetched_data1)\n            begin transaction:\n                fetched_data2 = self.make_fetch(key)\n                if fetched_data1 != fetched_data2:\n                    cancel transaction\n                else:\n                    self.make_insert(key, *computed_result)\n                    commit_transaction\n            &lt;pseudocode&gt;\n\n        Importantly, the output of make_fetch is a tuple that serves as the input into `make_compute`.\n        The output of `make_compute` is a tuple that serves as the input into `make_insert`.\n\n        The functionality must be strictly divided between these three methods:\n        - All database queries must be completed in `make_fetch`.\n        - All computation must be completed in `make_compute`.\n        - All database inserts must be completed in `make_insert`.\n\n        DataJoint may programmatically enforce this separation in the future.\n\n        :param key: The primary key value used to restrict the data fetching.\n        :param kwargs: Keyword arguments passed from populate(make_kwargs=...).\n            These are passed to make_fetch for the tripartite pattern.\n        :raises NotImplementedError: If the derived class does not implement the required methods.\n        \"\"\"\n\n        if not (\n            hasattr(self, \"make_fetch\")\n            and hasattr(self, \"make_insert\")\n            and hasattr(self, \"make_compute\")\n        ):\n            # user must implement `make`\n            raise NotImplementedError(\n                \"Subclasses of AutoPopulate must implement the method `make` \"\n                \"or (`make_fetch` + `make_compute` + `make_insert`)\"\n            )\n\n        # User has implemented `_fetch`, `_compute`, and `_insert` methods instead\n\n        # Step 1: Fetch data from parent tables\n        fetched_data = self.make_fetch(key, **kwargs)  # fetched_data is a tuple\n        computed_result = yield fetched_data  # passed as input into make_compute\n\n        # Step 2: If computed result is not passed in, compute the result\n        if computed_result is None:\n            # this is only executed in the first invocation\n            computed_result = self.make_compute(key, *fetched_data)\n            yield computed_result  # this is passed to the second invocation of make\n\n        # Step 3: Insert the computed result into the current table.\n        self.make_insert(key, *computed_result)\n        yield\n\n    @property\n    def target(self):\n        \"\"\"\n        :return: table to be populated.\n        In the typical case, dj.AutoPopulate is mixed into a dj.Table class by\n        inheritance and the target is self.\n        \"\"\"\n        return self\n\n    def _job_key(self, key):\n        \"\"\"\n        :param key:  they key returned for the job from the key source\n        :return: the dict to use to generate the job reservation hash\n        This method allows subclasses to control the job reservation granularity.\n        \"\"\"\n        return key\n\n    def _jobs_to_do(self, restrictions):\n        \"\"\"\n        :return: the query yielding the keys to be computed (derived from self.key_source)\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"Cannot call populate on a restricted table. \"\n                \"Instead, pass conditions to populate() as arguments.\"\n            )\n        todo = self.key_source\n\n        # key_source is a QueryExpression subclass -- trigger instantiation\n        if inspect.isclass(todo) and issubclass(todo, QueryExpression):\n            todo = todo()\n\n        if not isinstance(todo, QueryExpression):\n            raise DataJointError(\"Invalid key_source value\")\n\n        try:\n            # check if target lacks any attributes from the primary key of key_source\n            raise DataJointError(\n                \"The populate target lacks attribute %s \"\n                \"from the primary key of key_source\"\n                % next(\n                    name\n                    for name in todo.heading.primary_key\n                    if name not in self.target.heading\n                )\n            )\n        except StopIteration:\n            pass\n        return (todo &amp; AndList(restrictions)).proj()\n\n    def populate(\n        self,\n        *restrictions,\n        keys=None,\n        suppress_errors=False,\n        return_exception_objects=False,\n        reserve_jobs=False,\n        order=\"original\",\n        limit=None,\n        max_calls=None,\n        display_progress=False,\n        processes=1,\n        make_kwargs=None,\n    ):\n        \"\"\"\n        ``table.populate()`` calls ``table.make(key)`` for every primary key in\n        ``self.key_source`` for which there is not already a tuple in table.\n\n        :param restrictions: a list of restrictions each restrict\n            (table.key_source - target.proj())\n        :param keys: The list of keys (dicts) to send to self.make().\n            If None (default), then use self.key_source to query they keys.\n        :param suppress_errors: if True, do not terminate execution.\n        :param return_exception_objects: return error objects instead of just error messages\n        :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n        :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n        :param limit: if not None, check at most this many keys\n        :param max_calls: if not None, populate at most this many keys\n        :param display_progress: if True, report progress_bar\n        :param processes: number of processes to use. Set to None to use all cores\n        :param make_kwargs: Keyword arguments which do not affect the result of computation\n            to be passed down to each ``make()`` call. Computation arguments should be\n            specified within the pipeline e.g. using a `dj.Lookup` table.\n        :type make_kwargs: dict, optional\n        :return: a dict with two keys\n            \"success_count\": the count of successful ``make()`` calls in this ``populate()`` call\n            \"error_list\": the error list that is filled if `suppress_errors` is True\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n        valid_order = [\"original\", \"reverse\", \"random\"]\n        if order not in valid_order:\n            raise DataJointError(\n                \"The order argument must be one of %s\" % str(valid_order)\n            )\n        jobs = (\n            self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n        )\n\n        if reserve_jobs:\n            # Define a signal handler for SIGTERM\n            def handler(signum, frame):\n                logger.info(\"Populate terminated by SIGTERM\")\n                raise SystemExit(\"SIGTERM received\")\n\n            old_handler = signal.signal(signal.SIGTERM, handler)\n\n        if keys is None:\n            keys = (self._jobs_to_do(restrictions) - self.target).fetch(\n                \"KEY\", limit=limit\n            )\n\n        # exclude \"error\", \"ignore\" or \"reserved\" jobs\n        if reserve_jobs:\n            exclude_key_hashes = (\n                jobs\n                &amp; {\"table_name\": self.target.table_name}\n                &amp; 'status in (\"error\", \"ignore\", \"reserved\")'\n            ).fetch(\"key_hash\")\n            keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n        if order == \"reverse\":\n            keys.reverse()\n        elif order == \"random\":\n            random.shuffle(keys)\n\n        logger.debug(\"Found %d keys to populate\" % len(keys))\n\n        keys = keys[:max_calls]\n        nkeys = len(keys)\n\n        error_list = []\n        success_list = []\n\n        if nkeys:\n            processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n            populate_kwargs = dict(\n                suppress_errors=suppress_errors,\n                return_exception_objects=return_exception_objects,\n                make_kwargs=make_kwargs,\n            )\n\n            if processes == 1:\n                for key in (\n                    tqdm(keys, desc=self.__class__.__name__)\n                    if display_progress\n                    else keys\n                ):\n                    status = self._populate1(key, jobs, **populate_kwargs)\n                    if status is True:\n                        success_list.append(1)\n                    elif isinstance(status, tuple):\n                        error_list.append(status)\n                    else:\n                        assert status is False\n            else:\n                # spawn multiple processes\n                self.connection.close()  # disconnect parent process from MySQL server\n                del self.connection._conn.ctx  # SSLContext is not pickleable\n                with (\n                    mp.Pool(\n                        processes, _initialize_populate, (self, jobs, populate_kwargs)\n                    ) as pool,\n                    (\n                        tqdm(desc=\"Processes: \", total=nkeys)\n                        if display_progress\n                        else contextlib.nullcontext()\n                    ) as progress_bar,\n                ):\n                    for status in pool.imap(_call_populate1, keys, chunksize=1):\n                        if status is True:\n                            success_list.append(1)\n                        elif isinstance(status, tuple):\n                            error_list.append(status)\n                        else:\n                            assert status is False\n                        if display_progress:\n                            progress_bar.update()\n                self.connection.connect()  # reconnect parent process to MySQL server\n\n        # restore original signal handler:\n        if reserve_jobs:\n            signal.signal(signal.SIGTERM, old_handler)\n\n        return {\n            \"success_count\": sum(success_list),\n            \"error_list\": error_list,\n        }\n\n    def _populate1(\n        self, key, jobs, suppress_errors, return_exception_objects, make_kwargs=None\n    ):\n        \"\"\"\n        populates table for one source key, calling self.make inside a transaction.\n        :param jobs: the jobs table or None if not reserve_jobs\n        :param key: dict specifying job to populate\n        :param suppress_errors: bool if errors should be suppressed and returned\n        :param return_exception_objects: if True, errors must be returned as objects\n        :return: (key, error) when suppress_errors=True,\n            True if successfully invoke one `make()` call, otherwise False\n        \"\"\"\n        # use the legacy `_make_tuples` callback.\n        make = self._make_tuples if hasattr(self, \"_make_tuples\") else self.make\n\n        if jobs is not None and not jobs.reserve(\n            self.target.table_name, self._job_key(key)\n        ):\n            return False\n\n        # if make is a generator, it transaction can be delayed until the final stage\n        is_generator = inspect.isgeneratorfunction(make)\n        if not is_generator:\n            self.connection.start_transaction()\n\n        if key in self.target:  # already populated\n            if not is_generator:\n                self.connection.cancel_transaction()\n            if jobs is not None:\n                jobs.complete(self.target.table_name, self._job_key(key))\n            return False\n\n        logger.debug(f\"Making {key} -&gt; {self.target.full_table_name}\")\n        self.__class__._allow_insert = True\n\n        try:\n            if not is_generator:\n                make(dict(key), **(make_kwargs or {}))\n            else:\n                # tripartite make - transaction is delayed until the final stage\n                gen = make(dict(key), **(make_kwargs or {}))\n                fetched_data = next(gen)\n                fetch_hash = deepdiff.DeepHash(\n                    fetched_data, ignore_iterable_order=False\n                )[fetched_data]\n                computed_result = next(gen)  # perform the computation\n                # fetch and insert inside a transaction\n                self.connection.start_transaction()\n                gen = make(dict(key), **(make_kwargs or {}))  # restart make\n                fetched_data = next(gen)\n                if (\n                    fetch_hash\n                    != deepdiff.DeepHash(fetched_data, ignore_iterable_order=False)[\n                        fetched_data\n                    ]\n                ):  # raise error if fetched data has changed\n                    raise DataJointError(\n                        \"Referential integrity failed! The `make_fetch` data has changed\"\n                    )\n                gen.send(computed_result)  # insert\n\n        except (KeyboardInterrupt, SystemExit, Exception) as error:\n            try:\n                self.connection.cancel_transaction()\n            except LostConnectionError:\n                pass\n            error_message = \"{exception}{msg}\".format(\n                exception=error.__class__.__name__,\n                msg=\": \" + str(error) if str(error) else \"\",\n            )\n            logger.debug(\n                f\"Error making {key} -&gt; {self.target.full_table_name} - {error_message}\"\n            )\n            if jobs is not None:\n                # show error name and error message (if any)\n                jobs.error(\n                    self.target.table_name,\n                    self._job_key(key),\n                    error_message=error_message,\n                    error_stack=traceback.format_exc(),\n                )\n            if not suppress_errors or isinstance(error, SystemExit):\n                raise\n            else:\n                logger.error(error)\n                return key, error if return_exception_objects else error_message\n        else:\n            self.connection.commit_transaction()\n            logger.debug(f\"Success making {key} -&gt; {self.target.full_table_name}\")\n            if jobs is not None:\n                jobs.complete(self.target.table_name, self._job_key(key))\n            return True\n        finally:\n            self.__class__._allow_insert = False\n\n    def progress(self, *restrictions, display=False):\n        \"\"\"\n        Report the progress of populating the table.\n        :return: (remaining, total) -- numbers of tuples to be populated\n        \"\"\"\n        todo = self._jobs_to_do(restrictions)\n        total = len(todo)\n        remaining = len(todo - self.target)\n        if display:\n            logger.info(\n                \"%-20s\" % self.__class__.__name__\n                + \" Completed %d of %d (%2.1f%%)   %s\"\n                % (\n                    total - remaining,\n                    total,\n                    100 - 100 * remaining / (total + 1e-12),\n                    datetime.datetime.strftime(\n                        datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                    ),\n                ),\n            )\n        return remaining, total\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>the query expression that yields primary key values to be passed, sequentially, to the <code>make</code> method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls.</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.make", "title": "<code>make(key, **kwargs)</code>", "text": "<p>This method must be implemented by derived classes to perform automated computation. The method must implement the following three steps:</p> <ol> <li>Fetch data from tables above in the dependency hierarchy, restricted by the given key.</li> <li>Compute secondary attributes based on the fetched data.</li> <li>Insert the new tuple(s) into the current table.</li> </ol> <p>The method can be implemented either as: (a) Regular method: All three steps are performed in a single database transaction.     The method must return None. (b) Generator method:     The make method is split into three functions:     - <code>make_fetch</code>: Fetches data from the parent tables.     - <code>make_compute</code>: Computes secondary attributes based on the fetched data.     - <code>make_insert</code>: Inserts the computed data into the current table.</p> <pre><code>Then populate logic is executes as follows:\n\n&lt;pseudocode&gt;\nfetched_data1 = self.make_fetch(key)\ncomputed_result = self.make_compute(key, *fetched_data1)\nbegin transaction:\n    fetched_data2 = self.make_fetch(key)\n    if fetched_data1 != fetched_data2:\n        cancel transaction\n    else:\n        self.make_insert(key, *computed_result)\n        commit_transaction\n&lt;pseudocode&gt;\n</code></pre> <p>Importantly, the output of make_fetch is a tuple that serves as the input into <code>make_compute</code>. The output of <code>make_compute</code> is a tuple that serves as the input into <code>make_insert</code>.</p> <p>The functionality must be strictly divided between these three methods: - All database queries must be completed in <code>make_fetch</code>. - All computation must be completed in <code>make_compute</code>. - All database inserts must be completed in <code>make_insert</code>.</p> <p>DataJoint may programmatically enforce this separation in the future.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The primary key value used to restrict the data fetching.</p> required <code>kwargs</code> <p>Keyword arguments passed from populate(make_kwargs=...). These are passed to make_fetch for the tripartite pattern.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the derived class does not implement the required methods.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def make(self, key, **kwargs):\n    \"\"\"\n    This method must be implemented by derived classes to perform automated computation.\n    The method must implement the following three steps:\n\n    1. Fetch data from tables above in the dependency hierarchy, restricted by the given key.\n    2. Compute secondary attributes based on the fetched data.\n    3. Insert the new tuple(s) into the current table.\n\n    The method can be implemented either as:\n    (a) Regular method: All three steps are performed in a single database transaction.\n        The method must return None.\n    (b) Generator method:\n        The make method is split into three functions:\n        - `make_fetch`: Fetches data from the parent tables.\n        - `make_compute`: Computes secondary attributes based on the fetched data.\n        - `make_insert`: Inserts the computed data into the current table.\n\n        Then populate logic is executes as follows:\n\n        &lt;pseudocode&gt;\n        fetched_data1 = self.make_fetch(key)\n        computed_result = self.make_compute(key, *fetched_data1)\n        begin transaction:\n            fetched_data2 = self.make_fetch(key)\n            if fetched_data1 != fetched_data2:\n                cancel transaction\n            else:\n                self.make_insert(key, *computed_result)\n                commit_transaction\n        &lt;pseudocode&gt;\n\n    Importantly, the output of make_fetch is a tuple that serves as the input into `make_compute`.\n    The output of `make_compute` is a tuple that serves as the input into `make_insert`.\n\n    The functionality must be strictly divided between these three methods:\n    - All database queries must be completed in `make_fetch`.\n    - All computation must be completed in `make_compute`.\n    - All database inserts must be completed in `make_insert`.\n\n    DataJoint may programmatically enforce this separation in the future.\n\n    :param key: The primary key value used to restrict the data fetching.\n    :param kwargs: Keyword arguments passed from populate(make_kwargs=...).\n        These are passed to make_fetch for the tripartite pattern.\n    :raises NotImplementedError: If the derived class does not implement the required methods.\n    \"\"\"\n\n    if not (\n        hasattr(self, \"make_fetch\")\n        and hasattr(self, \"make_insert\")\n        and hasattr(self, \"make_compute\")\n    ):\n        # user must implement `make`\n        raise NotImplementedError(\n            \"Subclasses of AutoPopulate must implement the method `make` \"\n            \"or (`make_fetch` + `make_compute` + `make_insert`)\"\n        )\n\n    # User has implemented `_fetch`, `_compute`, and `_insert` methods instead\n\n    # Step 1: Fetch data from parent tables\n    fetched_data = self.make_fetch(key, **kwargs)  # fetched_data is a tuple\n    computed_result = yield fetched_data  # passed as input into make_compute\n\n    # Step 2: If computed result is not passed in, compute the result\n    if computed_result is None:\n        # this is only executed in the first invocation\n        computed_result = self.make_compute(key, *fetched_data)\n        yield computed_result  # this is passed to the second invocation of make\n\n    # Step 3: Insert the computed result into the current table.\n    self.make_insert(key, *computed_result)\n    yield\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.target", "title": "<code>target</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self.</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.populate", "title": "<code>populate(*restrictions, keys=None, suppress_errors=False, return_exception_objects=False, reserve_jobs=False, order='original', limit=None, max_calls=None, display_progress=False, processes=1, make_kwargs=None)</code>", "text": "<p><code>table.populate()</code> calls <code>table.make(key)</code> for every primary key in <code>self.key_source</code> for which there is not already a tuple in table.</p> <p>Parameters:</p> Name Type Description Default <code>restrictions</code> <p>a list of restrictions each restrict (table.key_source - target.proj())</p> <code>()</code> <code>keys</code> <p>The list of keys (dicts) to send to self.make(). If None (default), then use self.key_source to query they keys.</p> <code>None</code> <code>suppress_errors</code> <p>if True, do not terminate execution.</p> <code>False</code> <code>return_exception_objects</code> <p>return error objects instead of just error messages</p> <code>False</code> <code>reserve_jobs</code> <p>if True, reserve jobs to populate in asynchronous fashion</p> <code>False</code> <code>order</code> <p>\"original\"|\"reverse\"|\"random\"  - the order of execution</p> <code>'original'</code> <code>limit</code> <p>if not None, check at most this many keys</p> <code>None</code> <code>max_calls</code> <p>if not None, populate at most this many keys</p> <code>None</code> <code>display_progress</code> <p>if True, report progress_bar</p> <code>False</code> <code>processes</code> <p>number of processes to use. Set to None to use all cores</p> <code>1</code> <code>make_kwargs</code> <code>(dict, optional)</code> <p>Keyword arguments which do not affect the result of computation to be passed down to each <code>make()</code> call. Computation arguments should be specified within the pipeline e.g. using a <code>dj.Lookup</code> table.</p> <code>None</code> <p>Returns:</p> Type Description <p>a dict with two keys \"success_count\": the count of successful <code>make()</code> calls in this <code>populate()</code> call \"error_list\": the error list that is filled if <code>suppress_errors</code> is True</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def populate(\n    self,\n    *restrictions,\n    keys=None,\n    suppress_errors=False,\n    return_exception_objects=False,\n    reserve_jobs=False,\n    order=\"original\",\n    limit=None,\n    max_calls=None,\n    display_progress=False,\n    processes=1,\n    make_kwargs=None,\n):\n    \"\"\"\n    ``table.populate()`` calls ``table.make(key)`` for every primary key in\n    ``self.key_source`` for which there is not already a tuple in table.\n\n    :param restrictions: a list of restrictions each restrict\n        (table.key_source - target.proj())\n    :param keys: The list of keys (dicts) to send to self.make().\n        If None (default), then use self.key_source to query they keys.\n    :param suppress_errors: if True, do not terminate execution.\n    :param return_exception_objects: return error objects instead of just error messages\n    :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n    :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n    :param limit: if not None, check at most this many keys\n    :param max_calls: if not None, populate at most this many keys\n    :param display_progress: if True, report progress_bar\n    :param processes: number of processes to use. Set to None to use all cores\n    :param make_kwargs: Keyword arguments which do not affect the result of computation\n        to be passed down to each ``make()`` call. Computation arguments should be\n        specified within the pipeline e.g. using a `dj.Lookup` table.\n    :type make_kwargs: dict, optional\n    :return: a dict with two keys\n        \"success_count\": the count of successful ``make()`` calls in this ``populate()`` call\n        \"error_list\": the error list that is filled if `suppress_errors` is True\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n    valid_order = [\"original\", \"reverse\", \"random\"]\n    if order not in valid_order:\n        raise DataJointError(\n            \"The order argument must be one of %s\" % str(valid_order)\n        )\n    jobs = (\n        self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n    )\n\n    if reserve_jobs:\n        # Define a signal handler for SIGTERM\n        def handler(signum, frame):\n            logger.info(\"Populate terminated by SIGTERM\")\n            raise SystemExit(\"SIGTERM received\")\n\n        old_handler = signal.signal(signal.SIGTERM, handler)\n\n    if keys is None:\n        keys = (self._jobs_to_do(restrictions) - self.target).fetch(\n            \"KEY\", limit=limit\n        )\n\n    # exclude \"error\", \"ignore\" or \"reserved\" jobs\n    if reserve_jobs:\n        exclude_key_hashes = (\n            jobs\n            &amp; {\"table_name\": self.target.table_name}\n            &amp; 'status in (\"error\", \"ignore\", \"reserved\")'\n        ).fetch(\"key_hash\")\n        keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n    if order == \"reverse\":\n        keys.reverse()\n    elif order == \"random\":\n        random.shuffle(keys)\n\n    logger.debug(\"Found %d keys to populate\" % len(keys))\n\n    keys = keys[:max_calls]\n    nkeys = len(keys)\n\n    error_list = []\n    success_list = []\n\n    if nkeys:\n        processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n        populate_kwargs = dict(\n            suppress_errors=suppress_errors,\n            return_exception_objects=return_exception_objects,\n            make_kwargs=make_kwargs,\n        )\n\n        if processes == 1:\n            for key in (\n                tqdm(keys, desc=self.__class__.__name__)\n                if display_progress\n                else keys\n            ):\n                status = self._populate1(key, jobs, **populate_kwargs)\n                if status is True:\n                    success_list.append(1)\n                elif isinstance(status, tuple):\n                    error_list.append(status)\n                else:\n                    assert status is False\n        else:\n            # spawn multiple processes\n            self.connection.close()  # disconnect parent process from MySQL server\n            del self.connection._conn.ctx  # SSLContext is not pickleable\n            with (\n                mp.Pool(\n                    processes, _initialize_populate, (self, jobs, populate_kwargs)\n                ) as pool,\n                (\n                    tqdm(desc=\"Processes: \", total=nkeys)\n                    if display_progress\n                    else contextlib.nullcontext()\n                ) as progress_bar,\n            ):\n                for status in pool.imap(_call_populate1, keys, chunksize=1):\n                    if status is True:\n                        success_list.append(1)\n                    elif isinstance(status, tuple):\n                        error_list.append(status)\n                    else:\n                        assert status is False\n                    if display_progress:\n                        progress_bar.update()\n            self.connection.connect()  # reconnect parent process to MySQL server\n\n    # restore original signal handler:\n    if reserve_jobs:\n        signal.signal(signal.SIGTERM, old_handler)\n\n    return {\n        \"success_count\": sum(success_list),\n        \"error_list\": error_list,\n    }\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.progress", "title": "<code>progress(*restrictions, display=False)</code>", "text": "<p>Report the progress of populating the table.</p> <p>Returns:</p> Type Description <p>(remaining, total) -- numbers of tuples to be populated</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def progress(self, *restrictions, display=False):\n    \"\"\"\n    Report the progress of populating the table.\n    :return: (remaining, total) -- numbers of tuples to be populated\n    \"\"\"\n    todo = self._jobs_to_do(restrictions)\n    total = len(todo)\n    remaining = len(todo - self.target)\n    if display:\n        logger.info(\n            \"%-20s\" % self.__class__.__name__\n            + \" Completed %d of %d (%2.1f%%)   %s\"\n            % (\n                total - remaining,\n                total,\n                100 - 100 * remaining / (total + 1e-12),\n                datetime.datetime.strftime(\n                    datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                ),\n            ),\n        )\n    return remaining, total\n</code></pre>"}, {"location": "api/datajoint/blob/", "title": "blob.py", "text": "<p>(De)serialization methods for basic datatypes and numpy.ndarrays with provisions for mutual compatibility with Matlab-based serialization implemented by mYm.</p>"}, {"location": "api/datajoint/blob/#datajoint.blob.MatCell", "title": "<code>MatCell</code>", "text": "<p>               Bases: <code>ndarray</code></p> <p>a numpy ndarray representing a Matlab cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatCell(np.ndarray):\n    \"\"\"a numpy ndarray representing a Matlab cell array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.MatStruct", "title": "<code>MatStruct</code>", "text": "<p>               Bases: <code>recarray</code></p> <p>numpy.recarray representing a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatStruct(np.recarray):\n    \"\"\"numpy.recarray representing a Matlab struct array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob", "title": "<code>Blob</code>", "text": "Source code in <code>datajoint/blob.py</code> <pre><code>class Blob:\n    def __init__(self, squeeze=False):\n        self._squeeze = squeeze\n        self._blob = None\n        self._pos = 0\n        self.protocol = None\n\n    def set_dj0(self):\n        if not config.get(\"enable_python_native_blobs\"):\n            raise DataJointError(\n                \"\"\"v0.12+ python native blobs disabled.\n                See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\"\n            )\n\n        self.protocol = b\"dj0\\0\"  # when using new blob features\n\n    def squeeze(self, array, convert_to_scalar=True):\n        \"\"\"\n        Simplify the input array - squeeze out all singleton dimensions.\n        If convert_to_scalar, then convert zero-dimensional arrays to scalars\n        \"\"\"\n        if not self._squeeze:\n            return array\n        array = array.squeeze()\n        return array.item() if array.ndim == 0 and convert_to_scalar else array\n\n    def unpack(self, blob):\n        self._blob = blob\n        try:\n            # decompress\n            prefix = next(\n                p for p in compression if self._blob[self._pos :].startswith(p)\n            )\n        except StopIteration:\n            pass  # assume uncompressed but could be unrecognized compression\n        else:\n            self._pos += len(prefix)\n            blob_size = self.read_value()\n            blob = compression[prefix](self._blob[self._pos :])\n            assert len(blob) == blob_size\n            self._blob = blob\n            self._pos = 0\n        blob_format = self.read_zero_terminated_string()\n        if blob_format in (\"mYm\", \"dj0\"):\n            return self.read_blob(n_bytes=len(self._blob) - self._pos)\n\n    def read_blob(self, n_bytes=None):\n        start = self._pos\n        data_structure_code = chr(self.read_value(\"uint8\"))\n        try:\n            call = {\n                # MATLAB-compatible, inherited from original mYm\n                \"A\": self.read_array,  # matlab-compatible numeric arrays and scalars with ndim==0\n                \"P\": self.read_sparse_array,  # matlab sparse array -- not supported yet\n                \"S\": self.read_struct,  # matlab struct array\n                \"C\": self.read_cell_array,  # matlab cell array\n                # basic data types\n                \"\\xff\": self.read_none,  # None\n                \"\\x01\": self.read_tuple,  # a Sequence (e.g. tuple)\n                \"\\x02\": self.read_list,  # a MutableSequence (e.g. list)\n                \"\\x03\": self.read_set,  # a Set\n                \"\\x04\": self.read_dict,  # a Mapping (e.g. dict)\n                \"\\x05\": self.read_string,  # a UTF8-encoded string\n                \"\\x06\": self.read_bytes,  # a ByteString\n                \"\\x0a\": self.read_int,  # unbounded scalar int\n                \"\\x0b\": self.read_bool,  # scalar boolean\n                \"\\x0c\": self.read_complex,  # scalar 128-bit complex number\n                \"\\x0d\": self.read_float,  # scalar 64-bit float\n                \"F\": self.read_recarray,  # numpy array with fields, including recarrays\n                \"d\": self.read_decimal,  # a decimal\n                \"t\": self.read_datetime,  # date, time, or datetime\n                \"u\": self.read_uuid,  # UUID\n            }[data_structure_code]\n        except KeyError:\n            raise DataJointError(\n                'Unknown data structure code \"%s\". Upgrade datajoint.'\n                % data_structure_code\n            )\n        v = call()\n        if n_bytes is not None and self._pos - start != n_bytes:\n            raise DataJointError(\"Blob length check failed! Invalid blob\")\n        return v\n\n    def pack_blob(self, obj):\n        # original mYm-based serialization from datajoint-matlab\n        if isinstance(obj, MatCell):\n            return self.pack_cell_array(obj)\n        if isinstance(obj, MatStruct):\n            return self.pack_struct(obj)\n        if isinstance(obj, np.ndarray) and obj.dtype.fields is None:\n            return self.pack_array(obj)\n\n        # blob types in the expanded dj0 blob format\n        self.set_dj0()\n        if not isinstance(obj, (np.ndarray, np.number)):\n            # python built-in data types\n            if isinstance(obj, bool):\n                return self.pack_bool(obj)\n            if isinstance(obj, int):\n                return self.pack_int(obj)\n            if isinstance(obj, complex):\n                return self.pack_complex(obj)\n            if isinstance(obj, float):\n                return self.pack_float(obj)\n        if isinstance(obj, np.ndarray) and obj.dtype.fields:\n            return self.pack_recarray(np.array(obj))\n        if isinstance(obj, (np.number, np.datetime64)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (bool, np.bool_)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (float, int, complex)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n            return self.pack_datetime(obj)\n        if isinstance(obj, Decimal):\n            return self.pack_decimal(obj)\n        if isinstance(obj, uuid.UUID):\n            return self.pack_uuid(obj)\n        if isinstance(obj, collections.abc.Mapping):\n            return self.pack_dict(obj)\n        if isinstance(obj, str):\n            return self.pack_string(obj)\n        if isinstance(obj, (bytes, bytearray)):\n            return self.pack_bytes(obj)\n        if isinstance(obj, collections.abc.MutableSequence):\n            return self.pack_list(obj)\n        if isinstance(obj, collections.abc.Sequence):\n            return self.pack_tuple(obj)\n        if isinstance(obj, collections.abc.Set):\n            return self.pack_set(obj)\n        if obj is None:\n            return self.pack_none()\n        raise DataJointError(\n            \"Packing object of type %s currently not supported!\" % type(obj)\n        )\n\n    def read_array(self):\n        n_dims = int(self.read_value())\n        shape = self.read_value(count=n_dims)\n        n_elem = np.prod(shape, dtype=int)\n        dtype_id, is_complex = self.read_value(\"uint32\", 2)\n\n        # Get dtype from type id\n        dtype = deserialize_lookup[dtype_id][\"dtype\"]\n\n        # Check if name is void\n        if deserialize_lookup[dtype_id][\"scalar_type\"] == \"VOID\":\n            data = np.array(\n                list(self.read_blob(self.read_value()) for _ in range(n_elem)),\n                dtype=np.dtype(\"O\"),\n            )\n        # Check if name is char\n        elif deserialize_lookup[dtype_id][\"scalar_type\"] == \"CHAR\":\n            # compensate for MATLAB packing of char arrays\n            data = self.read_value(dtype, count=2 * n_elem)\n            data = data[::2].astype(\"U1\")\n            if n_dims == 2 and shape[0] == 1 or n_dims == 1:\n                compact = data.squeeze()\n                data = (\n                    compact\n                    if compact.shape == ()\n                    else np.array(\"\".join(data.squeeze()))\n                )\n                shape = (1,)\n        else:\n            data = self.read_value(dtype, count=n_elem)\n            if is_complex:\n                data = data + 1j * self.read_value(dtype, count=n_elem)\n        return self.squeeze(data.reshape(shape, order=\"F\"))\n\n    def pack_array(self, array):\n        \"\"\"\n        Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.\n        \"\"\"\n        if \"datetime64\" in array.dtype.name:\n            self.set_dj0()\n        blob = (\n            b\"A\"\n            + np.uint64(array.ndim).tobytes()\n            + np.array(array.shape, dtype=np.uint64).tobytes()\n        )\n        is_complex = np.iscomplexobj(array)\n        if is_complex:\n            array, imaginary = np.real(array), np.imag(array)\n        try:\n            type_id = serialize_lookup[array.dtype][\"type_id\"]\n        except KeyError:\n            # U is for unicode string\n            if array.dtype.char == \"U\":\n                type_id = serialize_lookup[np.dtype(\"O\")][\"type_id\"]\n            else:\n                raise DataJointError(f\"Type {array.dtype} is ambiguous or unknown\")\n\n        blob += np.array([type_id, is_complex], dtype=np.uint32).tobytes()\n        if (\n            array.dtype.char == \"U\"\n            or serialize_lookup[array.dtype][\"scalar_type\"] == \"VOID\"\n        ):\n            blob += b\"\".join(\n                len_u64(it) + it\n                for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n            )\n            self.set_dj0()  # not supported by original mym\n        elif serialize_lookup[array.dtype][\"scalar_type\"] == \"CHAR\":\n            blob += (\n                array.view(np.uint8).astype(np.uint16).tobytes()\n            )  # convert to 16-bit chars for MATLAB\n        else:  # numeric arrays\n            if array.ndim == 0:  # not supported by original mym\n                self.set_dj0()\n            blob += array.tobytes(order=\"F\")\n            if is_complex:\n                blob += imaginary.tobytes(order=\"F\")\n        return blob\n\n    def read_recarray(self):\n        \"\"\"\n        Serialize an np.ndarray with fields, including recarrays\n        \"\"\"\n        n_fields = self.read_value(\"uint32\")\n        if not n_fields:\n            return np.array(None)  # empty array\n        field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n        arrays = [self.read_blob() for _ in range(n_fields)]\n        rec = np.empty(\n            arrays[0].shape,\n            np.dtype([(f, t.dtype) for f, t in zip(field_names, arrays)]),\n        )\n        for f, t in zip(field_names, arrays):\n            rec[f] = t\n        return rec.view(np.recarray)\n\n    def pack_recarray(self, array):\n        \"\"\"Serialize a Matlab struct array\"\"\"\n        return (\n            b\"F\"\n            + len_u32(array.dtype)\n            + \"\\0\".join(array.dtype.names).encode()  # number of fields\n            + b\"\\0\"\n            + b\"\".join(  # field names\n                (\n                    self.pack_recarray(array[f])\n                    if array[f].dtype.fields\n                    else self.pack_array(array[f])\n                )\n                for f in array.dtype.names\n            )\n        )\n\n    def read_sparse_array(self):\n        raise DataJointError(\n            \"datajoint-python does not yet support sparse arrays. Issue (#590)\"\n        )\n\n    def read_int(self):\n        return int.from_bytes(\n            self.read_binary(self.read_value(\"uint16\")), byteorder=\"little\", signed=True\n        )\n\n    @staticmethod\n    def pack_int(v):\n        n_bytes = v.bit_length() // 8 + 1\n        assert 0 &lt; n_bytes &lt;= 0xFFFF, \"Integers are limited to 65535 bytes\"\n        return (\n            b\"\\x0a\"\n            + np.uint16(n_bytes).tobytes()\n            + v.to_bytes(n_bytes, byteorder=\"little\", signed=True)\n        )\n\n    def read_bool(self):\n        return bool(self.read_value(\"bool\"))\n\n    @staticmethod\n    def pack_bool(v):\n        return b\"\\x0b\" + np.array(v, dtype=\"bool\").tobytes()\n\n    def read_complex(self):\n        return complex(self.read_value(\"complex128\"))\n\n    @staticmethod\n    def pack_complex(v):\n        return b\"\\x0c\" + np.array(v, dtype=\"complex128\").tobytes()\n\n    def read_float(self):\n        return float(self.read_value(\"float64\"))\n\n    @staticmethod\n    def pack_float(v):\n        return b\"\\x0d\" + np.array(v, dtype=\"float64\").tobytes()\n\n    def read_decimal(self):\n        return Decimal(self.read_string())\n\n    @staticmethod\n    def pack_decimal(d):\n        s = str(d)\n        return b\"d\" + len_u64(s) + s.encode()\n\n    def read_string(self):\n        return self.read_binary(self.read_value()).decode()\n\n    @staticmethod\n    def pack_string(s):\n        blob = s.encode()\n        return b\"\\5\" + len_u64(blob) + blob\n\n    def read_bytes(self):\n        return self.read_binary(self.read_value())\n\n    @staticmethod\n    def pack_bytes(s):\n        return b\"\\6\" + len_u64(s) + s\n\n    def read_none(self):\n        pass\n\n    @staticmethod\n    def pack_none():\n        return b\"\\xff\"\n\n    def read_tuple(self):\n        return tuple(\n            self.read_blob(self.read_value()) for _ in range(self.read_value())\n        )\n\n    def pack_tuple(self, t):\n        return (\n            b\"\\1\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_list(self):\n        return list(self.read_blob(self.read_value()) for _ in range(self.read_value()))\n\n    def pack_list(self, t):\n        return (\n            b\"\\2\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_set(self):\n        return set(self.read_blob(self.read_value()) for _ in range(self.read_value()))\n\n    def pack_set(self, t):\n        return (\n            b\"\\3\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_dict(self):\n        return dict(\n            (self.read_blob(self.read_value()), self.read_blob(self.read_value()))\n            for _ in range(self.read_value())\n        )\n\n    def pack_dict(self, d):\n        return (\n            b\"\\4\"\n            + len_u64(d)\n            + b\"\".join(\n                b\"\".join((len_u64(it) + it) for it in packed)\n                for packed in (map(self.pack_blob, pair) for pair in d.items())\n            )\n        )\n\n    def read_struct(self):\n        \"\"\"deserialize matlab struct\"\"\"\n        n_dims = self.read_value()\n        shape = self.read_value(count=n_dims)\n        n_elem = np.prod(shape, dtype=int)\n        n_fields = self.read_value(\"uint32\")\n        if not n_fields:\n            return np.array(None)  # empty array\n        field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n        raw_data = [\n            tuple(\n                self.read_blob(n_bytes=int(self.read_value())) for _ in range(n_fields)\n            )\n            for __ in range(n_elem)\n        ]\n        data = np.array(raw_data, dtype=list(zip(field_names, repeat(object))))\n        return self.squeeze(\n            data.reshape(shape, order=\"F\"), convert_to_scalar=False\n        ).view(MatStruct)\n\n    def pack_struct(self, array):\n        \"\"\"Serialize a Matlab struct array\"\"\"\n        return (\n            b\"S\"\n            + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n            + len_u32(array.dtype.names)  # dimensionality\n            + \"\\0\".join(array.dtype.names).encode()  # number of fields\n            + b\"\\0\"\n            + b\"\".join(  # field names\n                len_u64(it) + it\n                for it in (\n                    self.pack_blob(e) for rec in array.flatten(order=\"F\") for e in rec\n                )\n            )\n        )  # values\n\n    def read_cell_array(self):\n        \"\"\"deserialize MATLAB cell array\"\"\"\n        n_dims = self.read_value()\n        shape = self.read_value(count=n_dims)\n        n_elem = int(np.prod(shape))\n        result = [self.read_blob(n_bytes=self.read_value()) for _ in range(n_elem)]\n        return (\n            self.squeeze(\n                np.array(result).reshape(shape, order=\"F\"), convert_to_scalar=False\n            )\n        ).view(MatCell)\n\n    def pack_cell_array(self, array):\n        return (\n            b\"C\"\n            + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n            + b\"\".join(\n                len_u64(it) + it\n                for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n            )\n        )\n\n    def read_datetime(self):\n        \"\"\"deserialize datetime.date, .time, or .datetime\"\"\"\n        date, time = self.read_value(\"int32\"), self.read_value(\"int64\")\n        date = (\n            datetime.date(year=date // 10000, month=(date // 100) % 100, day=date % 100)\n            if date &gt;= 0\n            else None\n        )\n        time = (\n            datetime.time(\n                hour=(time // 10000000000) % 100,\n                minute=(time // 100000000) % 100,\n                second=(time // 1000000) % 100,\n                microsecond=time % 1000000,\n            )\n            if time &gt;= 0\n            else None\n        )\n        return time and date and datetime.datetime.combine(date, time) or time or date\n\n    @staticmethod\n    def pack_datetime(d):\n        if isinstance(d, datetime.datetime):\n            date, time = d.date(), d.time()\n        elif isinstance(d, datetime.date):\n            date, time = d, None\n        else:\n            date, time = None, d\n        return b\"t\" + (\n            np.int32(\n                -1 if date is None else (date.year * 100 + date.month) * 100 + date.day\n            ).tobytes()\n            + np.int64(\n                -1\n                if time is None\n                else ((time.hour * 100 + time.minute) * 100 + time.second) * 1000000\n                + time.microsecond\n            ).tobytes()\n        )\n\n    def read_uuid(self):\n        q = self.read_binary(16)\n        return uuid.UUID(bytes=q)\n\n    @staticmethod\n    def pack_uuid(obj):\n        return b\"u\" + obj.bytes\n\n    def read_zero_terminated_string(self):\n        target = self._blob.find(b\"\\0\", self._pos)\n        data = self._blob[self._pos : target].decode()\n        self._pos = target + 1\n        return data\n\n    def read_value(self, dtype=None, count=1):\n        if dtype is None:\n            dtype = \"uint32\" if use_32bit_dims else \"uint64\"\n        data = np.frombuffer(self._blob, dtype=dtype, count=count, offset=self._pos)\n        self._pos += data.dtype.itemsize * data.size\n        return data[0] if count == 1 else data\n\n    def read_binary(self, size):\n        self._pos += int(size)\n        return self._blob[self._pos - int(size) : self._pos]\n\n    def pack(self, obj, compress):\n        self.protocol = b\"mYm\\0\"  # will be replaced with dj0 if new features are used\n        blob = self.pack_blob(\n            obj\n        )  # this may reset the protocol and must precede protocol evaluation\n        blob = self.protocol + blob\n        if compress and len(blob) &gt; 1000:\n            compressed = b\"ZL123\\0\" + len_u64(blob) + zlib.compress(blob)\n            if len(compressed) &lt; len(blob):\n                blob = compressed\n        return blob\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.squeeze", "title": "<code>squeeze(array, convert_to_scalar=True)</code>", "text": "<p>Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars</p> Source code in <code>datajoint/blob.py</code> <pre><code>def squeeze(self, array, convert_to_scalar=True):\n    \"\"\"\n    Simplify the input array - squeeze out all singleton dimensions.\n    If convert_to_scalar, then convert zero-dimensional arrays to scalars\n    \"\"\"\n    if not self._squeeze:\n        return array\n    array = array.squeeze()\n    return array.item() if array.ndim == 0 and convert_to_scalar else array\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_array", "title": "<code>pack_array(array)</code>", "text": "<p>Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_array(self, array):\n    \"\"\"\n    Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.\n    \"\"\"\n    if \"datetime64\" in array.dtype.name:\n        self.set_dj0()\n    blob = (\n        b\"A\"\n        + np.uint64(array.ndim).tobytes()\n        + np.array(array.shape, dtype=np.uint64).tobytes()\n    )\n    is_complex = np.iscomplexobj(array)\n    if is_complex:\n        array, imaginary = np.real(array), np.imag(array)\n    try:\n        type_id = serialize_lookup[array.dtype][\"type_id\"]\n    except KeyError:\n        # U is for unicode string\n        if array.dtype.char == \"U\":\n            type_id = serialize_lookup[np.dtype(\"O\")][\"type_id\"]\n        else:\n            raise DataJointError(f\"Type {array.dtype} is ambiguous or unknown\")\n\n    blob += np.array([type_id, is_complex], dtype=np.uint32).tobytes()\n    if (\n        array.dtype.char == \"U\"\n        or serialize_lookup[array.dtype][\"scalar_type\"] == \"VOID\"\n    ):\n        blob += b\"\".join(\n            len_u64(it) + it\n            for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n        )\n        self.set_dj0()  # not supported by original mym\n    elif serialize_lookup[array.dtype][\"scalar_type\"] == \"CHAR\":\n        blob += (\n            array.view(np.uint8).astype(np.uint16).tobytes()\n        )  # convert to 16-bit chars for MATLAB\n    else:  # numeric arrays\n        if array.ndim == 0:  # not supported by original mym\n            self.set_dj0()\n        blob += array.tobytes(order=\"F\")\n        if is_complex:\n            blob += imaginary.tobytes(order=\"F\")\n    return blob\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_recarray", "title": "<code>read_recarray()</code>", "text": "<p>Serialize an np.ndarray with fields, including recarrays</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_recarray(self):\n    \"\"\"\n    Serialize an np.ndarray with fields, including recarrays\n    \"\"\"\n    n_fields = self.read_value(\"uint32\")\n    if not n_fields:\n        return np.array(None)  # empty array\n    field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n    arrays = [self.read_blob() for _ in range(n_fields)]\n    rec = np.empty(\n        arrays[0].shape,\n        np.dtype([(f, t.dtype) for f, t in zip(field_names, arrays)]),\n    )\n    for f, t in zip(field_names, arrays):\n        rec[f] = t\n    return rec.view(np.recarray)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_recarray", "title": "<code>pack_recarray(array)</code>", "text": "<p>Serialize a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_recarray(self, array):\n    \"\"\"Serialize a Matlab struct array\"\"\"\n    return (\n        b\"F\"\n        + len_u32(array.dtype)\n        + \"\\0\".join(array.dtype.names).encode()  # number of fields\n        + b\"\\0\"\n        + b\"\".join(  # field names\n            (\n                self.pack_recarray(array[f])\n                if array[f].dtype.fields\n                else self.pack_array(array[f])\n            )\n            for f in array.dtype.names\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_struct", "title": "<code>read_struct()</code>", "text": "<p>deserialize matlab struct</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_struct(self):\n    \"\"\"deserialize matlab struct\"\"\"\n    n_dims = self.read_value()\n    shape = self.read_value(count=n_dims)\n    n_elem = np.prod(shape, dtype=int)\n    n_fields = self.read_value(\"uint32\")\n    if not n_fields:\n        return np.array(None)  # empty array\n    field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n    raw_data = [\n        tuple(\n            self.read_blob(n_bytes=int(self.read_value())) for _ in range(n_fields)\n        )\n        for __ in range(n_elem)\n    ]\n    data = np.array(raw_data, dtype=list(zip(field_names, repeat(object))))\n    return self.squeeze(\n        data.reshape(shape, order=\"F\"), convert_to_scalar=False\n    ).view(MatStruct)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_struct", "title": "<code>pack_struct(array)</code>", "text": "<p>Serialize a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_struct(self, array):\n    \"\"\"Serialize a Matlab struct array\"\"\"\n    return (\n        b\"S\"\n        + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n        + len_u32(array.dtype.names)  # dimensionality\n        + \"\\0\".join(array.dtype.names).encode()  # number of fields\n        + b\"\\0\"\n        + b\"\".join(  # field names\n            len_u64(it) + it\n            for it in (\n                self.pack_blob(e) for rec in array.flatten(order=\"F\") for e in rec\n            )\n        )\n    )  # values\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_cell_array", "title": "<code>read_cell_array()</code>", "text": "<p>deserialize MATLAB cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_cell_array(self):\n    \"\"\"deserialize MATLAB cell array\"\"\"\n    n_dims = self.read_value()\n    shape = self.read_value(count=n_dims)\n    n_elem = int(np.prod(shape))\n    result = [self.read_blob(n_bytes=self.read_value()) for _ in range(n_elem)]\n    return (\n        self.squeeze(\n            np.array(result).reshape(shape, order=\"F\"), convert_to_scalar=False\n        )\n    ).view(MatCell)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_datetime", "title": "<code>read_datetime()</code>", "text": "<p>deserialize datetime.date, .time, or .datetime</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_datetime(self):\n    \"\"\"deserialize datetime.date, .time, or .datetime\"\"\"\n    date, time = self.read_value(\"int32\"), self.read_value(\"int64\")\n    date = (\n        datetime.date(year=date // 10000, month=(date // 100) % 100, day=date % 100)\n        if date &gt;= 0\n        else None\n    )\n    time = (\n        datetime.time(\n            hour=(time // 10000000000) % 100,\n            minute=(time // 100000000) % 100,\n            second=(time // 1000000) % 100,\n            microsecond=time % 1000000,\n        )\n        if time &gt;= 0\n        else None\n    )\n    return time and date and datetime.datetime.combine(date, time) or time or date\n</code></pre>"}, {"location": "api/datajoint/cli/", "title": "cli.py", "text": ""}, {"location": "api/datajoint/cli/#datajoint.cli.cli", "title": "<code>cli(args=None)</code>", "text": "<p>Console interface for DataJoint Python</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list</code> <p>List of arguments to be passed in, defaults to reading stdin</p> <code>None</code> Source code in <code>datajoint/cli.py</code> <pre><code>def cli(args: list = None):\n    \"\"\"\n    Console interface for DataJoint Python\n\n    :param args: List of arguments to be passed in, defaults to reading stdin\n    :type args: list, optional\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"datajoint\",\n        description=\"DataJoint console interface.\",\n        conflict_handler=\"resolve\",\n    )\n    parser.add_argument(\n        \"-V\", \"--version\", action=\"version\", version=f\"{dj.__name__} {dj.__version__}\"\n    )\n    parser.add_argument(\n        \"-u\",\n        \"--user\",\n        type=str,\n        default=dj.config[\"database.user\"],\n        required=False,\n        help=\"Datajoint username\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--password\",\n        type=str,\n        default=dj.config[\"database.password\"],\n        required=False,\n        help=\"Datajoint password\",\n    )\n    parser.add_argument(\n        \"-h\",\n        \"--host\",\n        type=str,\n        default=dj.config[\"database.host\"],\n        required=False,\n        help=\"Datajoint host\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--schemas\",\n        nargs=\"+\",\n        type=str,\n        required=False,\n        help=\"A list of virtual module mappings in `db:schema ...` format\",\n    )\n    kwargs = vars(parser.parse_args(args))\n    mods = {}\n    if kwargs[\"user\"]:\n        dj.config[\"database.user\"] = kwargs[\"user\"]\n    if kwargs[\"password\"]:\n        dj.config[\"database.password\"] = kwargs[\"password\"]\n    if kwargs[\"host\"]:\n        dj.config[\"database.host\"] = kwargs[\"host\"]\n    if kwargs[\"schemas\"]:\n        for vm in kwargs[\"schemas\"]:\n            d, m = vm.split(\":\")\n            mods[m] = dj.create_virtual_module(m, d)\n\n    banner = \"dj repl\\n\"\n    if mods:\n        modstr = \"\\n\".join(\"  - {}\".format(m) for m in mods)\n        banner += \"\\nschema modules:\\n\\n\" + modstr + \"\\n\"\n    interact(banner, local=dict(ChainMap(mods, locals(), globals())))\n\n    raise SystemExit\n</code></pre>"}, {"location": "api/datajoint/condition/", "title": "condition.py", "text": "<p>methods for generating SQL WHERE clauses from datajoint restriction conditions</p>"}, {"location": "api/datajoint/condition/#datajoint.condition.PromiscuousOperand", "title": "<code>PromiscuousOperand</code>", "text": "<p>A container for an operand to ignore join compatibility</p> Source code in <code>datajoint/condition.py</code> <pre><code>class PromiscuousOperand:\n    \"\"\"\n    A container for an operand to ignore join compatibility\n    \"\"\"\n\n    def __init__(self, operand):\n        self.operand = operand\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.AndList", "title": "<code>AndList</code>", "text": "<p>               Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n    \"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.Top", "title": "<code>Top</code>  <code>dataclass</code>", "text": "<p>A restriction to the top entities of a query. In SQL, this corresponds to ORDER BY ... LIMIT ... OFFSET</p> Source code in <code>datajoint/condition.py</code> <pre><code>@dataclass\nclass Top:\n    \"\"\"\n    A restriction to the top entities of a query.\n    In SQL, this corresponds to ORDER BY ... LIMIT ... OFFSET\n    \"\"\"\n\n    limit: Union[int, None] = 1\n    order_by: Union[str, List[str]] = \"KEY\"\n    offset: int = 0\n\n    def __post_init__(self):\n        self.order_by = self.order_by or [\"KEY\"]\n        self.offset = self.offset or 0\n\n        if self.limit is not None and not isinstance(self.limit, int):\n            raise TypeError(\"Top limit must be an integer\")\n        if not isinstance(self.order_by, (str, collections.abc.Sequence)) or not all(\n            isinstance(r, str) for r in self.order_by\n        ):\n            raise TypeError(\"Top order_by attributes must all be strings\")\n        if not isinstance(self.offset, int):\n            raise TypeError(\"The offset argument must be an integer\")\n        if self.offset and self.limit is None:\n            self.limit = 999999999999  # arbitrary large number to allow query\n        if isinstance(self.order_by, str):\n            self.order_by = [self.order_by]\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.Not", "title": "<code>Not</code>", "text": "<p>invert restriction</p> Source code in <code>datajoint/condition.py</code> <pre><code>class Not:\n    \"\"\"invert restriction\"\"\"\n\n    def __init__(self, restriction):\n        self.restriction = restriction\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.assert_join_compatibility", "title": "<code>assert_join_compatibility(expr1, expr2)</code>", "text": "<p>Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible.</p> <p>Parameters:</p> Name Type Description Default <code>expr1</code> <p>A QueryExpression object</p> required <code>expr2</code> <p>A QueryExpression object</p> required Source code in <code>datajoint/condition.py</code> <pre><code>def assert_join_compatibility(expr1, expr2):\n    \"\"\"\n    Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible,\n    the matching attributes in the two expressions must be in the primary key of one or the\n    other expression.\n    Raises an exception if not compatible.\n\n    :param expr1: A QueryExpression object\n    :param expr2: A QueryExpression object\n    \"\"\"\n    from .expression import QueryExpression, U\n\n    for rel in (expr1, expr2):\n        if not isinstance(rel, (U, QueryExpression)):\n            raise DataJointError(\n                \"Object %r is not a QueryExpression and cannot be joined.\" % rel\n            )\n    if not isinstance(expr1, U) and not isinstance(\n        expr2, U\n    ):  # dj.U is always compatible\n        try:\n            raise DataJointError(\n                \"Cannot join query expressions on dependent attribute `%s`\"\n                % next(\n                    r\n                    for r in set(expr1.heading.secondary_attributes).intersection(\n                        expr2.heading.secondary_attributes\n                    )\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.make_condition", "title": "<code>make_condition(query_expression, condition, columns)</code>", "text": "<p>Translate the input condition into the equivalent SQL condition (a string)</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <p>a dj.QueryExpression object to apply condition</p> required <code>condition</code> <p>any valid restriction object.</p> required <code>columns</code> <p>a set passed by reference to collect all column names used in the condition.</p> required <p>Returns:</p> Type Description <p>an SQL condition string or a boolean value.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def make_condition(query_expression, condition, columns):\n    \"\"\"\n    Translate the input condition into the equivalent SQL condition (a string)\n\n    :param query_expression: a dj.QueryExpression object to apply condition\n    :param condition: any valid restriction object.\n    :param columns: a set passed by reference to collect all column names used in the\n        condition.\n    :return: an SQL condition string or a boolean value.\n    \"\"\"\n    from .expression import Aggregation, QueryExpression, U\n\n    def prep_value(k, v):\n        \"\"\"prepare SQL condition\"\"\"\n        key_match, k = translate_attribute(k)\n        if key_match[\"path\"] is None:\n            k = f\"`{k}`\"\n        if (\n            query_expression.heading[key_match[\"attr\"]].json\n            and key_match[\"path\"] is not None\n            and isinstance(v, dict)\n        ):\n            return f\"{k}='{json.dumps(v)}'\"\n        if v is None:\n            return f\"{k} IS NULL\"\n        if query_expression.heading[key_match[\"attr\"]].uuid:\n            if not isinstance(v, uuid.UUID):\n                try:\n                    v = uuid.UUID(v)\n                except (AttributeError, ValueError):\n                    raise DataJointError(\n                        \"Badly formed UUID {v} in restriction by `{k}`\".format(k=k, v=v)\n                    )\n            return f\"{k}=X'{v.bytes.hex()}'\"\n        if isinstance(\n            v,\n            (\n                datetime.date,\n                datetime.datetime,\n                datetime.time,\n                decimal.Decimal,\n                list,\n            ),\n        ):\n            return f'{k}=\"{v}\"'\n        if isinstance(v, str):\n            v = v.replace(\"%\", \"%%\").replace(\"\\\\\", \"\\\\\\\\\")\n            return f'{k}=\"{v}\"'\n        return f\"{k}={v}\"\n\n    def combine_conditions(negate, conditions):\n        return f\"{'NOT ' if negate else ''} ({')AND('.join(conditions)})\"\n\n    negate = False\n    while isinstance(condition, Not):\n        negate = not negate\n        condition = condition.restriction\n\n    # restrict by string\n    if isinstance(condition, str):\n        columns.update(extract_column_names(condition))\n        return combine_conditions(\n            negate, conditions=[condition.strip().replace(\"%\", \"%%\")]\n        )  # escape %, see issue #376\n\n    # restrict by AndList\n    if isinstance(condition, AndList):\n        # omit all conditions that evaluate to True\n        items = [\n            item\n            for item in (\n                make_condition(query_expression, cond, columns) for cond in condition\n            )\n            if item is not True\n        ]\n        if any(item is False for item in items):\n            return negate  # if any item is False, the whole thing is False\n        if not items:\n            return not negate  # and empty AndList is True\n        return combine_conditions(negate, conditions=items)\n\n    # restriction by dj.U evaluates to True\n    if isinstance(condition, U):\n        return not negate\n\n    # restrict by boolean\n    if isinstance(condition, bool):\n        return negate != condition\n\n    # restrict by a mapping/dict -- convert to an AndList of string equality conditions\n    if isinstance(condition, collections.abc.Mapping):\n        common_attributes = set(c.split(\".\", 1)[0] for c in condition).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluates to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[\n                prep_value(k, v)\n                for k, v in condition.items()\n                if k.split(\".\", 1)[0] in common_attributes  # handle json indexing\n            ],\n        )\n\n    # restrict by a numpy record -- convert to an AndList of string equality conditions\n    if isinstance(condition, numpy.void):\n        common_attributes = set(condition.dtype.fields).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluate to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[prep_value(k, condition[k]) for k in common_attributes],\n        )\n\n    # restrict by a QueryExpression subclass -- trigger instantiation and move on\n    if inspect.isclass(condition) and issubclass(condition, QueryExpression):\n        condition = condition()\n\n    # restrict by another expression (aka semijoin and antijoin)\n    check_compatibility = True\n    if isinstance(condition, PromiscuousOperand):\n        condition = condition.operand\n        check_compatibility = False\n\n    if isinstance(condition, QueryExpression):\n        if check_compatibility:\n            assert_join_compatibility(query_expression, condition)\n        common_attributes = [\n            q for q in condition.heading.names if q in query_expression.heading.names\n        ]\n        columns.update(common_attributes)\n        if isinstance(condition, Aggregation):\n            condition = condition.make_subquery()\n        return (\n            # without common attributes, any non-empty set matches everything\n            (not negate if condition else negate)\n            if not common_attributes\n            else \"({fields}) {not_}in ({subquery})\".format(\n                fields=\"`\" + \"`,`\".join(common_attributes) + \"`\",\n                not_=\"not \" if negate else \"\",\n                subquery=condition.make_sql(common_attributes),\n            )\n        )\n\n    # restrict by pandas.DataFrames\n    if isinstance(condition, pandas.DataFrame):\n        condition = condition.to_records()  # convert to numpy.recarray and move on\n\n    # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList\n    try:\n        or_list = [make_condition(query_expression, q, columns) for q in condition]\n    except TypeError:\n        raise DataJointError(\"Invalid restriction type %r\" % condition)\n    else:\n        or_list = [\n            item for item in or_list if item is not False\n        ]  # ignore False conditions\n        if any(item is True for item in or_list):  # if any item is True, entirely True\n            return not negate\n        return (\n            f\"{'NOT ' if negate else ''} ({' OR '.join(or_list)})\"\n            if or_list\n            else negate\n        )\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.extract_column_names", "title": "<code>extract_column_names(sql_expression)</code>", "text": "<p>extract all presumed column names from an sql expression such as the WHERE clause, for example.</p> <p>Parameters:</p> Name Type Description Default <code>sql_expression</code> <p>a string containing an SQL expression</p> required <p>Returns:</p> Type Description <p>set of extracted column names This may be MySQL-specific for now.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def extract_column_names(sql_expression):\n    \"\"\"\n    extract all presumed column names from an sql expression such as the WHERE clause,\n    for example.\n\n    :param sql_expression: a string containing an SQL expression\n    :return: set of extracted column names\n    This may be MySQL-specific for now.\n    \"\"\"\n    assert isinstance(sql_expression, str)\n    result = set()\n    s = sql_expression  # for terseness\n    # remove escaped quotes\n    s = re.sub(r\"(\\\\\\\")|(\\\\\\')\", \"\", s)\n    # remove quoted text\n    s = re.sub(r\"'[^']*'\", \"\", s)\n    s = re.sub(r'\"[^\"]*\"', \"\", s)\n    # find all tokens in back quotes and remove them\n    result.update(re.findall(r\"`([a-z][a-z_0-9]*)`\", s))\n    s = re.sub(r\"`[a-z][a-z_0-9]*`\", \"\", s)\n    # remove space before parentheses\n    s = re.sub(r\"\\s*\\(\", \"(\", s)\n    # remove tokens followed by ( since they must be functions\n    s = re.sub(r\"(\\b[a-z][a-z_0-9]*)\\(\", \"(\", s)\n    remaining_tokens = set(re.findall(r\"\\b[a-z][a-z_0-9]*\\b\", s))\n    # update result removing reserved words\n    result.update(\n        remaining_tokens\n        - {\n            \"is\",\n            \"in\",\n            \"between\",\n            \"like\",\n            \"and\",\n            \"or\",\n            \"null\",\n            \"not\",\n            \"interval\",\n            \"second\",\n            \"minute\",\n            \"hour\",\n            \"day\",\n            \"month\",\n            \"week\",\n            \"year\",\n        }\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/connection/", "title": "connection.py", "text": "<p>This module contains the Connection class that manages the connection to the database, and the <code>conn</code> function that provides access to a persistent connection in datajoint.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.translate_query_error", "title": "<code>translate_query_error(client_error, query)</code>", "text": "<p>Take client error and original query and return the corresponding DataJoint exception.</p> <p>Parameters:</p> Name Type Description Default <code>client_error</code> <p>the exception raised by the client interface</p> required <code>query</code> <p>sql query with placeholders</p> required <p>Returns:</p> Type Description <p>an instance of the corresponding subclass of datajoint.errors.DataJointError</p> Source code in <code>datajoint/connection.py</code> <pre><code>def translate_query_error(client_error, query):\n    \"\"\"\n    Take client error and original query and return the corresponding DataJoint exception.\n\n    :param client_error: the exception raised by the client interface\n    :param query: sql query with placeholders\n    :return: an instance of the corresponding subclass of datajoint.errors.DataJointError\n    \"\"\"\n    logger.debug(\"type: {}, args: {}\".format(type(client_error), client_error.args))\n\n    err, *args = client_error.args\n\n    # Loss of connection errors\n    if err in (0, \"(0, '')\"):\n        return errors.LostConnectionError(\n            \"Server connection lost due to an interface error.\", *args\n        )\n    if err == 2006:\n        return errors.LostConnectionError(\"Connection timed out\", *args)\n    if err == 2013:\n        return errors.LostConnectionError(\"Server connection lost\", *args)\n    # Access errors\n    if err in (1044, 1142):\n        return errors.AccessError(\"Insufficient privileges.\", args[0], query)\n    # Integrity errors\n    if err == 1062:\n        return errors.DuplicateError(*args)\n    if err == 1217:  # MySQL 8 error code\n        return errors.IntegrityError(*args)\n    if err == 1451:\n        return errors.IntegrityError(*args)\n    if err == 1452:\n        return errors.IntegrityError(*args)\n    # Syntax errors\n    if err == 1064:\n        return errors.QuerySyntaxError(args[0], query)\n    # Existence errors\n    if err == 1146:\n        return errors.MissingTableError(args[0], query)\n    if err == 1364:\n        return errors.MissingAttributeError(*args)\n    if err == 1054:\n        return errors.UnknownAttributeError(*args)\n    # all the other errors are re-raised in original form\n    return client_error\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS preferred, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n    \"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS preferred, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.EmulatedCursor", "title": "<code>EmulatedCursor</code>", "text": "<p>acts like a cursor</p> Source code in <code>datajoint/connection.py</code> <pre><code>class EmulatedCursor:\n    \"\"\"acts like a cursor\"\"\"\n\n    def __init__(self, data):\n        self._data = data\n        self._iter = iter(self._data)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._iter)\n\n    def fetchall(self):\n        return self._data\n\n    def fetchone(self):\n        return next(self._iter)\n\n    @property\n    def rowcount(self):\n        return len(self._data)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection", "title": "<code>Connection</code>", "text": "<p>A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys).</p> <p>Most of the parameters below should be set in the local configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>host name, may include port number as hostname:port, in which case it overrides the value in port</p> required <code>user</code> <p>user name</p> required <code>password</code> <p>password</p> required <code>port</code> <p>port number</p> <code>None</code> <code>init_fun</code> <p>connection initialization function (SQL)</p> <code>None</code> <code>use_tls</code> <p>TLS encryption option</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>class Connection:\n    \"\"\"\n    A dj.Connection object manages a connection to a database server.\n    It also catalogues modules, schemas, tables, and their dependencies (foreign keys).\n\n    Most of the parameters below should be set in the local configuration file.\n\n    :param host: host name, may include port number as hostname:port, in which case it overrides the value in port\n    :param user: user name\n    :param password: password\n    :param port: port number\n    :param init_fun: connection initialization function (SQL)\n    :param use_tls: TLS encryption option\n    \"\"\"\n\n    def __init__(self, host, user, password, port=None, init_fun=None, use_tls=None):\n        host_input, host = (host, get_host_hook(host))\n        if \":\" in host:\n            # the port in the hostname overrides the port argument\n            host, port = host.split(\":\")\n            port = int(port)\n        elif port is None:\n            port = config[\"database.port\"]\n        self.conn_info = dict(host=host, port=port, user=user, passwd=password)\n        if use_tls is not False:\n            self.conn_info[\"ssl\"] = (\n                use_tls if isinstance(use_tls, dict) else {\"ssl\": {}}\n            )\n        self.conn_info[\"ssl_input\"] = use_tls\n        self.conn_info[\"host_input\"] = host_input\n        self.init_fun = init_fun\n        self._conn = None\n        self._query_cache = None\n        connect_host_hook(self)\n        if self.is_connected:\n            logger.info(\n                \"DataJoint {version} connected to {user}@{host}:{port}\".format(\n                    version=__version__, **self.conn_info\n                )\n            )\n            self.connection_id = self.query(\"SELECT connection_id()\").fetchone()[0]\n        else:\n            raise errors.LostConnectionError(\n                \"Connection failed {user}@{host}:{port}\".format(**self.conn_info)\n            )\n        self._in_transaction = False\n        self.schemas = dict()\n        self.dependencies = Dependencies(self)\n\n    def __eq__(self, other):\n        return self.conn_info == other.conn_info\n\n    def __repr__(self):\n        connected = \"connected\" if self.is_connected else \"disconnected\"\n        return \"DataJoint connection ({connected}) {user}@{host}:{port}\".format(\n            connected=connected, **self.conn_info\n        )\n\n    def connect(self):\n        \"\"\"Connect to the database server.\"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n            try:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if k not in [\"ssl_input\", \"host_input\"]\n                    },\n                )\n            except client.err.InternalError:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if not (\n                            k in [\"ssl_input\", \"host_input\"]\n                            or k == \"ssl\"\n                            and self.conn_info[\"ssl_input\"] is None\n                        )\n                    },\n                )\n        self._conn.autocommit(True)\n\n    def set_query_cache(self, query_cache=None):\n        \"\"\"\n        When query_cache is not None, the connection switches into the query caching mode, which entails:\n        1. Only SELECT queries are allowed.\n        2. The results of queries are cached under the path indicated by dj.config['query_cache']\n        3. query_cache is a string that differentiates different cache states.\n\n        :param query_cache: a string to initialize the hash for query results\n        \"\"\"\n        self._query_cache = query_cache\n\n    def purge_query_cache(self):\n        \"\"\"Purges all query cache.\"\"\"\n        if (\n            isinstance(config.get(cache_key), str)\n            and pathlib.Path(config[cache_key]).is_dir()\n        ):\n            for path in pathlib.Path(config[cache_key]).iterdir():\n                if not path.is_dir():\n                    path.unlink()\n\n    def close(self):\n        self._conn.close()\n\n    def register(self, schema):\n        self.schemas[schema.database] = schema\n        self.dependencies.clear()\n\n    def ping(self):\n        \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n        self._conn.ping(reconnect=False)\n\n    @property\n    def is_connected(self):\n        \"\"\"Return true if the object is connected to the database server.\"\"\"\n        try:\n            self.ping()\n        except:\n            return False\n        return True\n\n    @staticmethod\n    def _execute_query(cursor, query, args, suppress_warnings):\n        try:\n            with warnings.catch_warnings():\n                if suppress_warnings:\n                    # suppress all warnings arising from underlying SQL library\n                    warnings.simplefilter(\"ignore\")\n                cursor.execute(query, args)\n        except client.err.Error as err:\n            raise translate_query_error(err, query)\n\n    def query(\n        self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n    ):\n        \"\"\"\n        Execute the specified query and return the tuple generator (cursor).\n\n        :param query: SQL query\n        :param args: additional arguments for the client.cursor\n        :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                        query results as dictionary.\n        :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n        :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n        \"\"\"\n        # check cache first:\n        use_query_cache = bool(self._query_cache)\n        if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n            raise errors.DataJointError(\n                \"Only SELECT queries are allowed when query caching is on.\"\n            )\n        if use_query_cache:\n            if not config[cache_key]:\n                raise errors.DataJointError(\n                    f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n                )\n            hash_ = uuid_from_buffer(\n                (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n                + pack(args)\n            )\n            cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n            try:\n                buffer = cache_path.read_bytes()\n            except FileNotFoundError:\n                pass  # proceed to query the database\n            else:\n                return EmulatedCursor(unpack(buffer))\n\n        if reconnect is None:\n            reconnect = config[\"database.reconnect\"]\n        logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n        cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n        cursor = self._conn.cursor(cursor=cursor_class)\n        try:\n            self._execute_query(cursor, query, args, suppress_warnings)\n        except errors.LostConnectionError:\n            if not reconnect:\n                raise\n            logger.warning(\"Reconnecting to MySQL server.\")\n            connect_host_hook(self)\n            if self._in_transaction:\n                self.cancel_transaction()\n                raise errors.LostConnectionError(\n                    \"Connection was lost during a transaction.\"\n                )\n            logger.debug(\"Re-executing\")\n            cursor = self._conn.cursor(cursor=cursor_class)\n            self._execute_query(cursor, query, args, suppress_warnings)\n\n        if use_query_cache:\n            data = cursor.fetchall()\n            cache_path.write_bytes(pack(data))\n            return EmulatedCursor(data)\n\n        return cursor\n\n    def get_user(self):\n        \"\"\"\n        :return: the user name and host name provided by the client to the server.\n        \"\"\"\n        return self.query(\"SELECT user()\").fetchone()[0]\n\n    # ---------- transaction processing\n    @property\n    def in_transaction(self):\n        \"\"\"\n        :return: True if there is an open transaction.\n        \"\"\"\n        self._in_transaction = self._in_transaction and self.is_connected\n        return self._in_transaction\n\n    def start_transaction(self):\n        \"\"\"\n        Starts a transaction error.\n        \"\"\"\n        if self.in_transaction:\n            raise errors.DataJointError(\"Nested connections are not supported.\")\n        self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n        self._in_transaction = True\n        logger.debug(\"Transaction started\")\n\n    def cancel_transaction(self):\n        \"\"\"\n        Cancels the current transaction and rolls back all changes made during the transaction.\n        \"\"\"\n        self.query(\"ROLLBACK\")\n        self._in_transaction = False\n        logger.debug(\"Transaction cancelled. Rolling back ...\")\n\n    def commit_transaction(self):\n        \"\"\"\n        Commit all changes made during the transaction and close it.\n\n        \"\"\"\n        self.query(\"COMMIT\")\n        self._in_transaction = False\n        logger.debug(\"Transaction committed and closed.\")\n\n    # -------- context manager for transactions\n    @property\n    @contextmanager\n    def transaction(self):\n        \"\"\"\n        Context manager for transactions. Opens an transaction and closes it after the with statement.\n        If an error is caught during the transaction, the commits are automatically rolled back.\n        All errors are raised again.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.conn().transaction as conn:\n        &gt;&gt;&gt;     # transaction is open here\n        \"\"\"\n        try:\n            self.start_transaction()\n            yield self\n        except:\n            self.cancel_transaction()\n            raise\n        else:\n            self.commit_transaction()\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.connect", "title": "<code>connect()</code>", "text": "<p>Connect to the database server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def connect(self):\n    \"\"\"Connect to the database server.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n        try:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if k not in [\"ssl_input\", \"host_input\"]\n                },\n            )\n        except client.err.InternalError:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if not (\n                        k in [\"ssl_input\", \"host_input\"]\n                        or k == \"ssl\"\n                        and self.conn_info[\"ssl_input\"] is None\n                    )\n                },\n            )\n    self._conn.autocommit(True)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.set_query_cache", "title": "<code>set_query_cache(query_cache=None)</code>", "text": "<p>When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states.</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <p>a string to initialize the hash for query results</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def set_query_cache(self, query_cache=None):\n    \"\"\"\n    When query_cache is not None, the connection switches into the query caching mode, which entails:\n    1. Only SELECT queries are allowed.\n    2. The results of queries are cached under the path indicated by dj.config['query_cache']\n    3. query_cache is a string that differentiates different cache states.\n\n    :param query_cache: a string to initialize the hash for query results\n    \"\"\"\n    self._query_cache = query_cache\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.purge_query_cache", "title": "<code>purge_query_cache()</code>", "text": "<p>Purges all query cache.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def purge_query_cache(self):\n    \"\"\"Purges all query cache.\"\"\"\n    if (\n        isinstance(config.get(cache_key), str)\n        and pathlib.Path(config[cache_key]).is_dir()\n    ):\n        for path in pathlib.Path(config[cache_key]).iterdir():\n            if not path.is_dir():\n                path.unlink()\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.ping", "title": "<code>ping()</code>", "text": "<p>Ping the connection or raises an exception if the connection is closed.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def ping(self):\n    \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n    self._conn.ping(reconnect=False)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.is_connected", "title": "<code>is_connected</code>  <code>property</code>", "text": "<p>Return true if the object is connected to the database server.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.query", "title": "<code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)</code>", "text": "<p>Execute the specified query and return the tuple generator (cursor).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>SQL query</p> required <code>args</code> <p>additional arguments for the client.cursor</p> <code>()</code> <code>as_dict</code> <p>If as_dict is set to True, the returned cursor objects returns query results as dictionary.</p> <code>False</code> <code>suppress_warnings</code> <p>If True, suppress all warnings arising from underlying query library</p> <code>True</code> <code>reconnect</code> <p>when None, get from config, when True, attempt to reconnect if disconnected</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def query(\n    self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n):\n    \"\"\"\n    Execute the specified query and return the tuple generator (cursor).\n\n    :param query: SQL query\n    :param args: additional arguments for the client.cursor\n    :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                    query results as dictionary.\n    :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n    :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n    \"\"\"\n    # check cache first:\n    use_query_cache = bool(self._query_cache)\n    if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n        raise errors.DataJointError(\n            \"Only SELECT queries are allowed when query caching is on.\"\n        )\n    if use_query_cache:\n        if not config[cache_key]:\n            raise errors.DataJointError(\n                f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n            )\n        hash_ = uuid_from_buffer(\n            (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n            + pack(args)\n        )\n        cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n        try:\n            buffer = cache_path.read_bytes()\n        except FileNotFoundError:\n            pass  # proceed to query the database\n        else:\n            return EmulatedCursor(unpack(buffer))\n\n    if reconnect is None:\n        reconnect = config[\"database.reconnect\"]\n    logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n    cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n    cursor = self._conn.cursor(cursor=cursor_class)\n    try:\n        self._execute_query(cursor, query, args, suppress_warnings)\n    except errors.LostConnectionError:\n        if not reconnect:\n            raise\n        logger.warning(\"Reconnecting to MySQL server.\")\n        connect_host_hook(self)\n        if self._in_transaction:\n            self.cancel_transaction()\n            raise errors.LostConnectionError(\n                \"Connection was lost during a transaction.\"\n            )\n        logger.debug(\"Re-executing\")\n        cursor = self._conn.cursor(cursor=cursor_class)\n        self._execute_query(cursor, query, args, suppress_warnings)\n\n    if use_query_cache:\n        data = cursor.fetchall()\n        cache_path.write_bytes(pack(data))\n        return EmulatedCursor(data)\n\n    return cursor\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.get_user", "title": "<code>get_user()</code>", "text": "<p>Returns:</p> Type Description <p>the user name and host name provided by the client to the server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def get_user(self):\n    \"\"\"\n    :return: the user name and host name provided by the client to the server.\n    \"\"\"\n    return self.query(\"SELECT user()\").fetchone()[0]\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.in_transaction", "title": "<code>in_transaction</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True if there is an open transaction.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.start_transaction", "title": "<code>start_transaction()</code>", "text": "<p>Starts a transaction error.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def start_transaction(self):\n    \"\"\"\n    Starts a transaction error.\n    \"\"\"\n    if self.in_transaction:\n        raise errors.DataJointError(\"Nested connections are not supported.\")\n    self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n    self._in_transaction = True\n    logger.debug(\"Transaction started\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.cancel_transaction", "title": "<code>cancel_transaction()</code>", "text": "<p>Cancels the current transaction and rolls back all changes made during the transaction.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def cancel_transaction(self):\n    \"\"\"\n    Cancels the current transaction and rolls back all changes made during the transaction.\n    \"\"\"\n    self.query(\"ROLLBACK\")\n    self._in_transaction = False\n    logger.debug(\"Transaction cancelled. Rolling back ...\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.commit_transaction", "title": "<code>commit_transaction()</code>", "text": "<p>Commit all changes made during the transaction and close it.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def commit_transaction(self):\n    \"\"\"\n    Commit all changes made during the transaction and close it.\n\n    \"\"\"\n    self.query(\"COMMIT\")\n    self._in_transaction = False\n    logger.debug(\"Transaction committed and closed.\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.transaction", "title": "<code>transaction</code>  <code>property</code>", "text": "<p>Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again.</p> <p>Example:</p> <p>import datajoint as dj with dj.conn().transaction as conn:     # transaction is open here</p>"}, {"location": "api/datajoint/declare/", "title": "declare.py", "text": "<p>This module hosts functions to convert DataJoint table definitions into mysql table definitions, and to declare the corresponding mysql tables.</p>"}, {"location": "api/datajoint/declare/#datajoint.declare.is_foreign_key", "title": "<code>is_foreign_key(line)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>line</code> <p>a line from the table definition</p> required <p>Returns:</p> Type Description <p>true if the line appears to be a foreign key definition</p> Source code in <code>datajoint/declare.py</code> <pre><code>def is_foreign_key(line):\n    \"\"\"\n\n    :param line: a line from the table definition\n    :return: true if the line appears to be a foreign key definition\n    \"\"\"\n    arrow_position = line.find(\"-&gt;\")\n    return arrow_position &gt;= 0 and not any(c in line[:arrow_position] for c in \"\\\"#'\")\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_foreign_key", "title": "<code>compile_foreign_key(line, context, attributes, primary_key, attr_sql, foreign_key_sql, index_sql)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>line</code> <p>a line from a table definition</p> required <code>context</code> <p>namespace containing referenced objects</p> required <code>attributes</code> <p>list of attribute names already in the declaration -- to be updated by this function</p> required <code>primary_key</code> <p>None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function</p> required <code>attr_sql</code> <p>list of sql statements defining attributes -- to be updated by this function.</p> required <code>foreign_key_sql</code> <p>list of sql statements specifying foreign key constraints -- to be updated by this function.</p> required <code>index_sql</code> <p>list of INDEX declaration statements, duplicate or redundant indexes are ok.</p> required Source code in <code>datajoint/declare.py</code> <pre><code>def compile_foreign_key(\n    line, context, attributes, primary_key, attr_sql, foreign_key_sql, index_sql\n):\n    \"\"\"\n    :param line: a line from a table definition\n    :param context: namespace containing referenced objects\n    :param attributes: list of attribute names already in the declaration -- to be updated by this function\n    :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list\n        of primary key attributes thus far -- to be updated by the function\n    :param attr_sql: list of sql statements defining attributes -- to be updated by this function.\n    :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function.\n    :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok.\n    \"\"\"\n    # Parse and validate\n    from .expression import QueryExpression\n    from .table import Table\n\n    try:\n        result = foreign_key_parser.parseString(line)\n    except pp.ParseException as err:\n        raise DataJointError('Parsing error in line \"%s\". %s.' % (line, err))\n\n    try:\n        ref = eval(result.ref_table, context)\n    except Exception:\n        raise DataJointError(\n            \"Foreign key reference %s could not be resolved\" % result.ref_table\n        )\n\n    options = [opt.upper() for opt in result.options]\n    for opt in options:  # check for invalid options\n        if opt not in {\"NULLABLE\", \"UNIQUE\"}:\n            raise DataJointError('Invalid foreign key option \"{opt}\"'.format(opt=opt))\n    is_nullable = \"NULLABLE\" in options\n    is_unique = \"UNIQUE\" in options\n    if is_nullable and primary_key is not None:\n        raise DataJointError(\n            'Primary dependencies cannot be nullable in line \"{line}\"'.format(line=line)\n        )\n\n    if isinstance(ref, type) and issubclass(ref, Table):\n        ref = ref()\n\n    # check that dependency is of a supported type\n    if (\n        not isinstance(ref, QueryExpression)\n        or len(ref.restriction)\n        or len(ref.support) != 1\n        or not isinstance(ref.support[0], str)\n    ):\n        raise DataJointError(\n            'Dependency \"%s\" is not supported (yet). Use a base table or its projection.'\n            % result.ref_table\n        )\n\n    # declare new foreign key attributes\n    for attr in ref.primary_key:\n        if attr not in attributes:\n            attributes.append(attr)\n            if primary_key is not None:\n                primary_key.append(attr)\n            attr_sql.append(\n                ref.heading[attr].sql.replace(\"NOT NULL \", \"\", int(is_nullable))\n            )\n\n    # declare the foreign key\n    foreign_key_sql.append(\n        \"FOREIGN KEY (`{fk}`) REFERENCES {ref} (`{pk}`) ON UPDATE CASCADE ON DELETE RESTRICT\".format(\n            fk=\"`,`\".join(ref.primary_key),\n            pk=\"`,`\".join(ref.heading[name].original_name for name in ref.primary_key),\n            ref=ref.support[0],\n        )\n    )\n\n    # declare unique index\n    if is_unique:\n        index_sql.append(\n            \"UNIQUE INDEX ({attrs})\".format(\n                attrs=\",\".join(\"`%s`\" % attr for attr in ref.primary_key)\n            )\n        )\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.declare", "title": "<code>declare(full_table_name, definition, context)</code>", "text": "<p>Parse declaration and generate the SQL CREATE TABLE code</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>full name of the table</p> required <code>definition</code> <p>DataJoint table definition</p> required <code>context</code> <p>dictionary of objects that might be referred to in the table</p> required <p>Returns:</p> Type Description <p>SQL CREATE TABLE statement, list of external stores used</p> Source code in <code>datajoint/declare.py</code> <pre><code>def declare(full_table_name, definition, context):\n    \"\"\"\n    Parse declaration and generate the SQL CREATE TABLE code\n\n    :param full_table_name: full name of the table\n    :param definition: DataJoint table definition\n    :param context: dictionary of objects that might be referred to in the table\n    :return: SQL CREATE TABLE statement, list of external stores used\n    \"\"\"\n    table_name = full_table_name.strip(\"`\").split(\".\")[1]\n    if len(table_name) &gt; MAX_TABLE_NAME_LENGTH:\n        raise DataJointError(\n            \"Table name `{name}` exceeds the max length of {max_length}\".format(\n                name=table_name, max_length=MAX_TABLE_NAME_LENGTH\n            )\n        )\n\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n\n    if config.get(\"add_hidden_timestamp\", False):\n        metadata_attr_sql = [\n            \"`_{full_table_name}_timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP\"\n        ]\n        attribute_sql.extend(\n            attr.format(\n                full_table_name=sha1(\n                    full_table_name.replace(\"`\", \"\").encode(\"utf-8\")\n                ).hexdigest()\n            )\n            for attr in metadata_attr_sql\n        )\n\n    if not primary_key:\n        raise DataJointError(\"Table must have a primary key\")\n\n    return (\n        \"CREATE TABLE IF NOT EXISTS %s (\\n\" % full_table_name\n        + \",\\n\".join(\n            attribute_sql\n            + [\"PRIMARY KEY (`\" + \"`,`\".join(primary_key) + \"`)\"]\n            + foreign_key_sql\n            + index_sql\n        )\n        + '\\n) ENGINE=InnoDB, COMMENT \"%s\"' % table_comment\n    ), external_stores\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.alter", "title": "<code>alter(definition, old_definition, context)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>definition</code> <p>new table definition</p> required <code>old_definition</code> <p>current table definition</p> required <code>context</code> <p>the context in which to evaluate foreign key definitions</p> required <p>Returns:</p> Type Description <p>string SQL ALTER command, list of new stores used for external storage</p> Source code in <code>datajoint/declare.py</code> <pre><code>def alter(definition, old_definition, context):\n    \"\"\"\n    :param definition: new table definition\n    :param old_definition: current table definition\n    :param context: the context in which to evaluate foreign key definitions\n    :return: string SQL ALTER command, list of new stores used for external storage\n    \"\"\"\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n    (\n        table_comment_,\n        primary_key_,\n        attribute_sql_,\n        foreign_key_sql_,\n        index_sql_,\n        external_stores_,\n    ) = prepare_declare(old_definition, context)\n\n    # analyze differences between declarations\n    sql = list()\n    if primary_key != primary_key_:\n        raise NotImplementedError(\"table.alter cannot alter the primary key (yet).\")\n    if foreign_key_sql != foreign_key_sql_:\n        raise NotImplementedError(\"table.alter cannot alter foreign keys (yet).\")\n    if index_sql != index_sql_:\n        raise NotImplementedError(\"table.alter cannot alter indexes (yet)\")\n    if attribute_sql != attribute_sql_:\n        sql.extend(_make_attribute_alter(attribute_sql, attribute_sql_, primary_key))\n    if table_comment != table_comment_:\n        sql.append('COMMENT=\"%s\"' % table_comment)\n    return sql, [e for e in external_stores if e not in external_stores_]\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.substitute_special_type", "title": "<code>substitute_special_type(match, category, foreign_key_sql, context)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>match</code> <p>dict containing with keys \"type\" and \"comment\" -- will be modified in place</p> required <code>category</code> <p>attribute type category from TYPE_PATTERN</p> required <code>foreign_key_sql</code> <p>list of foreign key declarations to add to</p> required <code>context</code> <p>context for looking up user-defined attribute_type adapters</p> required Source code in <code>datajoint/declare.py</code> <pre><code>def substitute_special_type(match, category, foreign_key_sql, context):\n    \"\"\"\n    :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place\n    :param category: attribute type category from TYPE_PATTERN\n    :param foreign_key_sql: list of foreign key declarations to add to\n    :param context: context for looking up user-defined attribute_type adapters\n    \"\"\"\n    if category == \"UUID\":\n        match[\"type\"] = UUID_DATA_TYPE\n    elif category == \"INTERNAL_ATTACH\":\n        match[\"type\"] = \"LONGBLOB\"\n    elif category in EXTERNAL_TYPES:\n        if category == \"FILEPATH\" and not _support_filepath_types():\n            raise DataJointError(\n                \"\"\"\n            The filepath data type is disabled until complete validation.\n            To turn it on as experimental feature, set the environment variable\n            {env} = TRUE or upgrade datajoint.\n            \"\"\".format(\n                    env=FILEPATH_FEATURE_SWITCH\n                )\n            )\n        match[\"store\"] = match[\"type\"].split(\"@\", 1)[1]\n        match[\"type\"] = UUID_DATA_TYPE\n        foreign_key_sql.append(\n            \"FOREIGN KEY (`{name}`) REFERENCES `{{database}}`.`{external_table_root}_{store}` (`hash`) \"\n            \"ON UPDATE RESTRICT ON DELETE RESTRICT\".format(\n                external_table_root=EXTERNAL_TABLE_ROOT, **match\n            )\n        )\n    elif category == \"ADAPTED\":\n        adapter = get_adapter(context, match[\"type\"])\n        match[\"type\"] = adapter.attribute_type\n        category = match_type(match[\"type\"])\n        if category in SPECIAL_TYPES:\n            # recursive redefinition from user-defined datatypes.\n            substitute_special_type(match, category, foreign_key_sql, context)\n    else:\n        assert False, \"Unknown special type\"\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_attribute", "title": "<code>compile_attribute(line, in_key, foreign_key_sql, context)</code>", "text": "<p>Convert attribute definition from DataJoint format to SQL</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <p>attribution line</p> required <code>in_key</code> <p>set to True if attribute is in primary key set</p> required <code>foreign_key_sql</code> <p>the list of foreign key declarations to add to</p> required <code>context</code> <p>context in which to look up user-defined attribute type adapterss</p> required <p>Returns:</p> Type Description <p>(name, sql, is_external) -- attribute name and sql code for its declaration</p> Source code in <code>datajoint/declare.py</code> <pre><code>def compile_attribute(line, in_key, foreign_key_sql, context):\n    \"\"\"\n    Convert attribute definition from DataJoint format to SQL\n\n    :param line: attribution line\n    :param in_key: set to True if attribute is in primary key set\n    :param foreign_key_sql: the list of foreign key declarations to add to\n    :param context: context in which to look up user-defined attribute type adapterss\n    :returns: (name, sql, is_external) -- attribute name and sql code for its declaration\n    \"\"\"\n    try:\n        match = attribute_parser.parseString(line + \"#\", parseAll=True)\n    except pp.ParseException as err:\n        raise DataJointError(\n            \"Declaration error in position {pos} in line:\\n  {line}\\n{msg}\".format(\n                line=err.args[0], pos=err.args[1], msg=err.args[2]\n            )\n        )\n    match[\"comment\"] = match[\"comment\"].rstrip(\"#\")\n    if \"default\" not in match:\n        match[\"default\"] = \"\"\n    match = {k: v.strip() for k, v in match.items()}\n    match[\"nullable\"] = match[\"default\"].lower() == \"null\"\n\n    if match[\"nullable\"]:\n        if in_key:\n            raise DataJointError(\n                'Primary key attributes cannot be nullable in line \"%s\"' % line\n            )\n        match[\"default\"] = \"DEFAULT NULL\"  # nullable attributes default to null\n    else:\n        if match[\"default\"]:\n            quote = (\n                match[\"default\"].split(\"(\")[0].upper() not in CONSTANT_LITERALS\n                and match[\"default\"][0] not in \"\\\"'\"\n            )\n            match[\"default\"] = (\n                \"NOT NULL DEFAULT \" + ('\"%s\"' if quote else \"%s\") % match[\"default\"]\n            )\n        else:\n            match[\"default\"] = \"NOT NULL\"\n\n    match[\"comment\"] = match[\"comment\"].replace(\n        '\"', '\\\\\"'\n    )  # escape double quotes in comment\n\n    if match[\"comment\"].startswith(\":\"):\n        raise DataJointError(\n            'An attribute comment must not start with a colon in comment \"{comment}\"'.format(\n                **match\n            )\n        )\n\n    category = match_type(match[\"type\"])\n    if category in SPECIAL_TYPES:\n        match[\"comment\"] = \":{type}:{comment}\".format(\n            **match\n        )  # insert custom type into comment\n        substitute_special_type(match, category, foreign_key_sql, context)\n\n    if category in SERIALIZED_TYPES and match[\"default\"] not in {\n        \"DEFAULT NULL\",\n        \"NOT NULL\",\n    }:\n        raise DataJointError(\n            \"The default value for a blob or attachment attributes can only be NULL in:\\n{line}\".format(\n                line=line\n            )\n        )\n\n    sql = (\n        \"`{name}` {type} {default}\"\n        + (' COMMENT \"{comment}\"' if match[\"comment\"] else \"\")\n    ).format(**match)\n    return match[\"name\"], sql, match.get(\"store\")\n</code></pre>"}, {"location": "api/datajoint/dependencies/", "title": "dependencies.py", "text": ""}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.extract_master", "title": "<code>extract_master(part_table)</code>", "text": "<p>given a part table name, return master part. None if not a part table</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def extract_master(part_table):\n    \"\"\"\n    given a part table name, return master part. None if not a part table\n    \"\"\"\n    match = re.match(r\"(?P&lt;master&gt;`\\w+`.`#?\\w+)__\\w+`\", part_table)\n    return match[\"master\"] + \"`\" if match else None\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.topo_sort", "title": "<code>topo_sort(graph)</code>", "text": "<p>topological sort of a dependency graph that keeps part tables together with their masters</p> <p>Returns:</p> Type Description <p>list of table names in topological order</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def topo_sort(graph):\n    \"\"\"\n    topological sort of a dependency graph that keeps part tables together with their masters\n    :return: list of table names in topological order\n    \"\"\"\n\n    graph = nx.DiGraph(graph)  # make a copy\n\n    # collapse alias nodes\n    alias_nodes = [node for node in graph if node.isdigit()]\n    for node in alias_nodes:\n        try:\n            direct_edge = (\n                next(x for x in graph.in_edges(node))[0],\n                next(x for x in graph.out_edges(node))[1],\n            )\n        except StopIteration:\n            pass  # a disconnected alias node\n        else:\n            graph.add_edge(*direct_edge)\n    graph.remove_nodes_from(alias_nodes)\n\n    # Add parts' dependencies to their masters' dependencies\n    # to ensure correct topological ordering of the masters.\n    for part in graph:\n        # find the part's master\n        if (master := extract_master(part)) in graph:\n            for edge in graph.in_edges(part):\n                parent = edge[0]\n                if master not in (parent, extract_master(parent)):\n                    # if parent is neither master nor part of master\n                    graph.add_edge(parent, master)\n    sorted_nodes = list(nx.topological_sort(graph))\n\n    # bring parts up to their masters\n    pos = len(sorted_nodes) - 1\n    placed = set()\n    while pos &gt; 1:\n        part = sorted_nodes[pos]\n        if (master := extract_master(part)) not in graph or part in placed:\n            pos -= 1\n        else:\n            placed.add(part)\n            insert_pos = sorted_nodes.index(master) + 1\n            if pos &gt; insert_pos:\n                # move the part to the position immediately after its master\n                del sorted_nodes[pos]\n                sorted_nodes.insert(insert_pos, part)\n\n    return sorted_nodes\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies", "title": "<code>Dependencies</code>", "text": "<p>               Bases: <code>DiGraph</code></p> <p>The graph of dependencies (foreign keys) between loaded tables.</p> <p>Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>class Dependencies(nx.DiGraph):\n    \"\"\"\n    The graph of dependencies (foreign keys) between loaded tables.\n\n    Note: the 'connection' argument should normally be supplied;\n    Empty use is permitted to facilitate use of networkx algorithms which\n    internally create objects with the expectation of empty constructors.\n    See also: https://github.com/datajoint/datajoint-python/pull/443\n    \"\"\"\n\n    def __init__(self, connection=None):\n        self._conn = connection\n        self._node_alias_count = itertools.count()\n        self._loaded = False\n        super().__init__(self)\n\n    def clear(self):\n        self._loaded = False\n        super().clear()\n\n    def load(self, force=True):\n        \"\"\"\n        Load dependencies for all loaded schemas.\n        This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n        \"\"\"\n        # reload from scratch to prevent duplication of renamed edges\n        if self._loaded and not force:\n            return\n\n        self.clear()\n\n        # load primary key info\n        keys = self._conn.query(\n            \"\"\"\n                SELECT\n                    concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n                FROM information_schema.key_column_usage\n                WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n                \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            )\n        )\n        pks = defaultdict(set)\n        for key in keys:\n            pks[key[0]].add(key[1])\n\n        # add nodes to the graph\n        for n, pk in pks.items():\n            self.add_node(n, primary_key=pk)\n\n        # load foreign keys\n        keys = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in self._conn.query(\n                \"\"\"\n        SELECT constraint_name,\n            concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n            concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n            column_name, referenced_column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n            referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n        \"\"\".format(\n                    schemas=\"','\".join(self._conn.schemas)\n                ),\n                as_dict=True,\n            )\n        )\n        fks = defaultdict(lambda: dict(attr_map=dict()))\n        for key in keys:\n            d = fks[\n                (\n                    key[\"constraint_name\"],\n                    key[\"referencing_table\"],\n                    key[\"referenced_table\"],\n                )\n            ]\n            d[\"referencing_table\"] = key[\"referencing_table\"]\n            d[\"referenced_table\"] = key[\"referenced_table\"]\n            d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n        # add edges to the graph\n        for fk in fks.values():\n            props = dict(\n                primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n                attr_map=fk[\"attr_map\"],\n                aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n                multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n            )\n            if not props[\"aliased\"]:\n                self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n            else:\n                # for aliased dependencies, add an extra node in the format '1', '2', etc\n                alias_node = \"%d\" % next(self._node_alias_count)\n                self.add_node(alias_node)\n                self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n                self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n        if not nx.is_directed_acyclic_graph(self):\n            raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n        self._loaded = True\n\n    def topo_sort(self):\n        \"\"\":return: list of tables names in topological order\"\"\"\n        return topo_sort(self)\n\n    def parents(self, table_name, primary=None):\n        \"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referenced by the foreign keys of table\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[0]: p[2]\n            for p in self.in_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def children(self, table_name, primary=None):\n        \"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referencing the table through foreign keys\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[1]: p[2]\n            for p in self.out_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def descendants(self, full_table_name):\n        \"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.descendants(self, full_table_name))\n        return [full_table_name] + nodes.topo_sort()\n\n    def ancestors(self, full_table_name):\n        \"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.ancestors(self, full_table_name))\n        return reversed(nodes.topo_sort() + [full_table_name])\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.load", "title": "<code>load(force=True)</code>", "text": "<p>Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def load(self, force=True):\n    \"\"\"\n    Load dependencies for all loaded schemas.\n    This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n    \"\"\"\n    # reload from scratch to prevent duplication of renamed edges\n    if self._loaded and not force:\n        return\n\n    self.clear()\n\n    # load primary key info\n    keys = self._conn.query(\n        \"\"\"\n            SELECT\n                concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n            FROM information_schema.key_column_usage\n            WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n            \"\"\".format(\n            schemas=\"','\".join(self._conn.schemas)\n        )\n    )\n    pks = defaultdict(set)\n    for key in keys:\n        pks[key[0]].add(key[1])\n\n    # add nodes to the graph\n    for n, pk in pks.items():\n        self.add_node(n, primary_key=pk)\n\n    # load foreign keys\n    keys = (\n        {k.lower(): v for k, v in elem.items()}\n        for elem in self._conn.query(\n            \"\"\"\n    SELECT constraint_name,\n        concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n        concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n        column_name, referenced_column_name\n    FROM information_schema.key_column_usage\n    WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n        referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n    \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            ),\n            as_dict=True,\n        )\n    )\n    fks = defaultdict(lambda: dict(attr_map=dict()))\n    for key in keys:\n        d = fks[\n            (\n                key[\"constraint_name\"],\n                key[\"referencing_table\"],\n                key[\"referenced_table\"],\n            )\n        ]\n        d[\"referencing_table\"] = key[\"referencing_table\"]\n        d[\"referenced_table\"] = key[\"referenced_table\"]\n        d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n    # add edges to the graph\n    for fk in fks.values():\n        props = dict(\n            primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n            attr_map=fk[\"attr_map\"],\n            aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n            multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n        )\n        if not props[\"aliased\"]:\n            self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n        else:\n            # for aliased dependencies, add an extra node in the format '1', '2', etc\n            alias_node = \"%d\" % next(self._node_alias_count)\n            self.add_node(alias_node)\n            self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n            self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n    if not nx.is_directed_acyclic_graph(self):\n        raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n    self._loaded = True\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.topo_sort", "title": "<code>topo_sort()</code>", "text": "<p>Returns:</p> Type Description <p>list of tables names in topological order</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def topo_sort(self):\n    \"\"\":return: list of tables names in topological order\"\"\"\n    return topo_sort(self)\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.parents", "title": "<code>parents(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referenced by the foreign keys of table</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def parents(self, table_name, primary=None):\n    \"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referenced by the foreign keys of table\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[0]: p[2]\n        for p in self.in_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.children", "title": "<code>children(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referencing the table through foreign keys</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def children(self, table_name, primary=None):\n    \"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referencing the table through foreign keys\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[1]: p[2]\n        for p in self.out_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.descendants", "title": "<code>descendants(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def descendants(self, full_table_name):\n    \"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.descendants(self, full_table_name))\n    return [full_table_name] + nodes.topo_sort()\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.ancestors", "title": "<code>ancestors(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def ancestors(self, full_table_name):\n    \"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.ancestors(self, full_table_name))\n    return reversed(nodes.topo_sort() + [full_table_name])\n</code></pre>"}, {"location": "api/datajoint/diagram/", "title": "diagram.py", "text": ""}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram", "title": "<code>Diagram</code>", "text": "<p>               Bases: <code>DiGraph</code></p> <p>Schema diagram showing tables and foreign keys between in the form of a directed acyclic graph (DAG).  The diagram is derived from the connection.dependencies object.</p> <p>Usage:</p> <p>diag = Diagram(source)</p> <p>source can be a table object, a table class, a schema, or a module that has a schema.</p> <p>diag.draw()</p> <p>draws the diagram using pyplot</p> <p>diag1 + diag2  - combines the two diagrams. diag1 - diag2  - difference between diagrams diag1 * diag2  - intersection of diagrams diag + n   - expands n levels of successors diag - n   - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table</p> <p>Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed</p> Source code in <code>datajoint/diagram.py</code> <pre><code>class Diagram(nx.DiGraph):\n    \"\"\"\n    Schema diagram showing tables and foreign keys between in the form of a directed\n    acyclic graph (DAG).  The diagram is derived from the connection.dependencies object.\n\n    Usage:\n\n    &gt;&gt;&gt;  diag = Diagram(source)\n\n    source can be a table object, a table class, a schema, or a module that has a schema.\n\n    &gt;&gt;&gt; diag.draw()\n\n    draws the diagram using pyplot\n\n    diag1 + diag2  - combines the two diagrams.\n    diag1 - diag2  - difference between diagrams\n    diag1 * diag2  - intersection of diagrams\n    diag + n   - expands n levels of successors\n    diag - n   - expands n levels of predecessors\n    Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table\n\n    Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth.\n    Only those tables that are loaded in the connection object are displayed\n    \"\"\"\n\n    def __init__(self, source, context=None):\n\n        if isinstance(source, Diagram):\n            # copy constructor\n            self.nodes_to_show = set(source.nodes_to_show)\n            self.context = source.context\n            super().__init__(source)\n            return\n\n        # get the caller's context\n        if context is None:\n            frame = inspect.currentframe().f_back\n            self.context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        else:\n            self.context = context\n\n        # find connection in the source\n        try:\n            connection = source.connection\n        except AttributeError:\n            try:\n                connection = source.schema.connection\n            except AttributeError:\n                raise DataJointError(\n                    \"Could not find database connection in %s\" % repr(source[0])\n                )\n\n        # initialize graph from dependencies\n        connection.dependencies.load()\n        super().__init__(connection.dependencies)\n\n        # Enumerate nodes from all the items in the list\n        self.nodes_to_show = set()\n        try:\n            self.nodes_to_show.add(source.full_table_name)\n        except AttributeError:\n            try:\n                database = source.database\n            except AttributeError:\n                try:\n                    database = source.schema.database\n                except AttributeError:\n                    raise DataJointError(\n                        \"Cannot plot Diagram for %s\" % repr(source)\n                    )\n            for node in self:\n                if node.startswith(\"`%s`\" % database):\n                    self.nodes_to_show.add(node)\n\n    @classmethod\n    def from_sequence(cls, sequence):\n        \"\"\"\n        The join Diagram for all objects in sequence\n\n        :param sequence: a sequence (e.g. list, tuple)\n        :return: Diagram(arg1) + ... + Diagram(argn)\n        \"\"\"\n        return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n\n    def add_parts(self):\n        \"\"\"\n        Adds to the diagram the part tables of all master tables already in the diagram\n        :return:\n        \"\"\"\n\n        def is_part(part, master):\n            \"\"\"\n            :param part:  `database`.`table_name`\n            :param master:   `database`.`table_name`\n            :return: True if part is part of master.\n            \"\"\"\n            part = [s.strip(\"`\") for s in part.split(\".\")]\n            master = [s.strip(\"`\") for s in master.split(\".\")]\n            return (\n                master[0] == part[0]\n                and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n            )\n\n        self = Diagram(self)  # copy\n        self.nodes_to_show.update(\n            n\n            for n in self.nodes()\n            if any(is_part(n, m) for m in self.nodes_to_show)\n        )\n        return self\n\n    def __add__(self, arg):\n        \"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Union of the diagrams when arg is another Diagram\n                 or an expansion downstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.add(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    new = nx.algorithms.boundary.node_boundary(\n                        self, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            self, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __sub__(self, arg):\n        \"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Difference of the diagrams when arg is another Diagram or\n                 an expansion upstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.difference_update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.remove(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    graph = nx.DiGraph(self).reverse()\n                    new = nx.algorithms.boundary.node_boundary(\n                        graph, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            graph, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __mul__(self, arg):\n        \"\"\"\n        Intersection of two diagrams\n        :param arg: another Diagram\n        :return: a new Diagram comprising nodes that are present in both operands.\n        \"\"\"\n        self = Diagram(self)  # copy\n        self.nodes_to_show.intersection_update(arg.nodes_to_show)\n        return self\n\n    def topo_sort(self):\n        \"\"\"return nodes in lexicographical topological order\"\"\"\n        return topo_sort(self)\n\n    def _make_graph(self):\n        \"\"\"\n        Make the self.graph - a graph object ready for drawing\n        \"\"\"\n        # mark \"distinguished\" tables, i.e. those that introduce new primary key\n        # attributes\n        for name in self.nodes_to_show:\n            foreign_attributes = set(\n                attr\n                for p in self.in_edges(name, data=True)\n                for attr in p[2][\"attr_map\"]\n                if p[2][\"primary\"]\n            )\n            self.nodes[name][\"distinguished\"] = (\n                \"primary_key\" in self.nodes[name]\n                and foreign_attributes &lt; self.nodes[name][\"primary_key\"]\n            )\n        # include aliased nodes that are sandwiched between two displayed nodes\n        gaps = set(\n            nx.algorithms.boundary.node_boundary(self, self.nodes_to_show)\n        ).intersection(\n            nx.algorithms.boundary.node_boundary(\n                nx.DiGraph(self).reverse(), self.nodes_to_show\n            )\n        )\n        nodes = self.nodes_to_show.union(a for a in gaps if a.isdigit)\n        # construct subgraph and rename nodes to class names\n        graph = nx.DiGraph(nx.DiGraph(self).subgraph(nodes))\n        nx.set_node_attributes(\n            graph, name=\"node_type\", values={n: _get_tier(n) for n in graph}\n        )\n        # relabel nodes to class names\n        mapping = {\n            node: lookup_class_name(node, self.context) or node\n            for node in graph.nodes()\n        }\n        new_names = [mapping.values()]\n        if len(new_names) &gt; len(set(new_names)):\n            raise DataJointError(\n                \"Some classes have identical names. The Diagram cannot be plotted.\"\n            )\n        nx.relabel_nodes(graph, mapping, copy=False)\n        return graph\n\n    @staticmethod\n    def _encapsulate_edge_attributes(graph):\n        \"\"\"\n        Modifies the `nx.Graph`'s edge attribute `attr_map` to be a string representation\n        of the attribute map, and encapsulates the string in double quotes.\n        Changes the graph in place.\n\n        Implements workaround described in\n        https://github.com/pydot/pydot/issues/258#issuecomment-795798099\n        \"\"\"\n        for u, v, *_, edgedata in graph.edges(data=True):\n            if \"attr_map\" in edgedata:\n                graph.edges[u, v][\"attr_map\"] = '\"{0}\"'.format(edgedata[\"attr_map\"])\n\n    @staticmethod\n    def _encapsulate_node_names(graph):\n        \"\"\"\n        Modifies the `nx.Graph`'s node names string representations encapsulated in\n        double quotes.\n        Changes the graph in place.\n\n        Implements workaround described in\n        https://github.com/datajoint/datajoint-python/pull/1176\n        \"\"\"\n        nx.relabel_nodes(\n            graph,\n            {node: '\"{0}\"'.format(node) for node in graph.nodes()},\n            copy=False,\n        )\n\n    def make_dot(self):\n        graph = self._make_graph()\n        graph.nodes()\n\n        scale = 1.2  # scaling factor for fonts and boxes\n        label_props = {  # http://matplotlib.org/examples/color/named_colors.html\n            None: dict(\n                shape=\"circle\",\n                color=\"#FFFF0040\",\n                fontcolor=\"yellow\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            _AliasNode: dict(\n                shape=\"circle\",\n                color=\"#FF880080\",\n                fontcolor=\"#FF880080\",\n                fontsize=round(scale * 0),\n                size=0.05 * scale,\n                fixed=True,\n            ),\n            Manual: dict(\n                shape=\"box\",\n                color=\"#00FF0030\",\n                fontcolor=\"darkgreen\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Lookup: dict(\n                shape=\"plaintext\",\n                color=\"#00000020\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Computed: dict(\n                shape=\"ellipse\",\n                color=\"#FF000020\",\n                fontcolor=\"#7F0000A0\",\n                fontsize=round(scale * 10),\n                size=0.3 * scale,\n                fixed=True,\n            ),\n            Imported: dict(\n                shape=\"ellipse\",\n                color=\"#00007F40\",\n                fontcolor=\"#00007FA0\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Part: dict(\n                shape=\"plaintext\",\n                color=\"#0000000\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.1 * scale,\n                fixed=False,\n            ),\n        }\n        node_props = {\n            node: label_props[d[\"node_type\"]]\n            for node, d in dict(graph.nodes(data=True)).items()\n        }\n\n        self._encapsulate_node_names(graph)\n        self._encapsulate_edge_attributes(graph)\n        dot = nx.drawing.nx_pydot.to_pydot(graph)\n        for node in dot.get_nodes():\n            node.set_shape(\"circle\")\n            name = node.get_name().strip('\"')\n            props = node_props[name]\n            node.set_fontsize(props[\"fontsize\"])\n            node.set_fontcolor(props[\"fontcolor\"])\n            node.set_shape(props[\"shape\"])\n            node.set_fontname(\"arial\")\n            node.set_fixedsize(\"shape\" if props[\"fixed\"] else False)\n            node.set_width(props[\"size\"])\n            node.set_height(props[\"size\"])\n            if name.split(\".\")[0] in self.context:\n                cls = eval(name, self.context)\n                assert issubclass(cls, Table)\n                description = cls().describe(context=self.context).split(\"\\n\")\n                description = (\n                    (\n                        \"-\" * 30\n                        if q.startswith(\"---\")\n                        else (\n                            q.replace(\"-&gt;\", \"&amp;#8594;\")\n                            if \"-&gt;\" in q\n                            else q.split(\":\")[0]\n                        )\n                    )\n                    for q in description\n                    if not q.startswith(\"#\")\n                )\n                node.set_tooltip(\"&amp;#13;\".join(description))\n            node.set_label(\n                \"&lt;&lt;u&gt;\" + name + \"&lt;/u&gt;&gt;\"\n                if node.get(\"distinguished\") == \"True\"\n                else name\n            )\n            node.set_color(props[\"color\"])\n            node.set_style(\"filled\")\n\n        for edge in dot.get_edges():\n            # see https://graphviz.org/doc/info/attrs.html\n            src = edge.get_source()\n            dest = edge.get_destination()\n            props = graph.get_edge_data(src, dest)\n            if props is None:\n                raise DataJointError(\n                    \"Could not find edge with source \"\n                    \"'{}' and destination '{}'\".format(src, dest)\n                )\n            edge.set_color(\"#00000040\")\n            edge.set_style(\"solid\" if props[\"primary\"] else \"dashed\")\n            master_part = graph.nodes[dest][\n                \"node_type\"\n            ] is Part and dest.startswith(src + \".\")\n            edge.set_weight(3 if master_part else 1)\n            edge.set_arrowhead(\"none\")\n            edge.set_penwidth(0.75 if props[\"multi\"] else 2)\n\n        return dot\n\n    def make_svg(self):\n        from IPython.display import SVG\n\n        return SVG(self.make_dot().create_svg())\n\n    def make_png(self):\n        return io.BytesIO(self.make_dot().create_png())\n\n    def make_image(self):\n        if plot_active:\n            return plt.imread(self.make_png())\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def _repr_svg_(self):\n        return self.make_svg()._repr_svg_()\n\n    def draw(self):\n        if plot_active:\n            plt.imshow(self.make_image())\n            plt.gca().axis(\"off\")\n            plt.show()\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def save(self, filename, format=None):\n        if format is None:\n            if filename.lower().endswith(\".png\"):\n                format = \"png\"\n            elif filename.lower().endswith(\".svg\"):\n                format = \"svg\"\n        if format.lower() == \"png\":\n            with open(filename, \"wb\") as f:\n                f.write(self.make_png().getbuffer().tobytes())\n        elif format.lower() == \"svg\":\n            with open(filename, \"w\") as f:\n                f.write(self.make_svg().data)\n        else:\n            raise DataJointError(\"Unsupported file format\")\n\n    @staticmethod\n    def _layout(graph, **kwargs):\n        return pydot_layout(graph, prog=\"dot\", **kwargs)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.from_sequence", "title": "<code>from_sequence(sequence)</code>  <code>classmethod</code>", "text": "<p>The join Diagram for all objects in sequence</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <p>a sequence (e.g. list, tuple)</p> required <p>Returns:</p> Type Description <p>Diagram(arg1) + ... + Diagram(argn)</p> Source code in <code>datajoint/diagram.py</code> <pre><code>@classmethod\ndef from_sequence(cls, sequence):\n    \"\"\"\n    The join Diagram for all objects in sequence\n\n    :param sequence: a sequence (e.g. list, tuple)\n    :return: Diagram(arg1) + ... + Diagram(argn)\n    \"\"\"\n    return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.add_parts", "title": "<code>add_parts()</code>", "text": "<p>Adds to the diagram the part tables of all master tables already in the diagram</p> <p>Returns:</p> Type Description Source code in <code>datajoint/diagram.py</code> <pre><code>def add_parts(self):\n    \"\"\"\n    Adds to the diagram the part tables of all master tables already in the diagram\n    :return:\n    \"\"\"\n\n    def is_part(part, master):\n        \"\"\"\n        :param part:  `database`.`table_name`\n        :param master:   `database`.`table_name`\n        :return: True if part is part of master.\n        \"\"\"\n        part = [s.strip(\"`\") for s in part.split(\".\")]\n        master = [s.strip(\"`\") for s in master.split(\".\")]\n        return (\n            master[0] == part[0]\n            and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n        )\n\n    self = Diagram(self)  # copy\n    self.nodes_to_show.update(\n        n\n        for n in self.nodes()\n        if any(is_part(n, m) for m in self.nodes_to_show)\n    )\n    return self\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.topo_sort", "title": "<code>topo_sort()</code>", "text": "<p>return nodes in lexicographical topological order</p> Source code in <code>datajoint/diagram.py</code> <pre><code>def topo_sort(self):\n    \"\"\"return nodes in lexicographical topological order\"\"\"\n    return topo_sort(self)\n</code></pre>"}, {"location": "api/datajoint/errors/", "title": "errors.py", "text": "<p>Exception classes for the DataJoint library</p>"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>               Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n    \"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n        \"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n    \"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.LostConnectionError", "title": "<code>LostConnectionError</code>", "text": "<p>               Bases: <code>DataJointError</code></p> <p>Loss of server connection</p> Source code in <code>datajoint/errors.py</code> <pre><code>class LostConnectionError(DataJointError):\n    \"\"\"\n    Loss of server connection\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.QueryError", "title": "<code>QueryError</code>", "text": "<p>               Bases: <code>DataJointError</code></p> <p>Errors arising from queries to the database</p> Source code in <code>datajoint/errors.py</code> <pre><code>class QueryError(DataJointError):\n    \"\"\"\n    Errors arising from queries to the database\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.QuerySyntaxError", "title": "<code>QuerySyntaxError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>Errors arising from incorrect query syntax</p> Source code in <code>datajoint/errors.py</code> <pre><code>class QuerySyntaxError(QueryError):\n    \"\"\"\n    Errors arising from incorrect query syntax\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.AccessError", "title": "<code>AccessError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>User access error: insufficient privileges.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class AccessError(QueryError):\n    \"\"\"\n    User access error: insufficient privileges.\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingTableError", "title": "<code>MissingTableError</code>", "text": "<p>               Bases: <code>DataJointError</code></p> <p>Query on a table that has not been declared</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingTableError(DataJointError):\n    \"\"\"\n    Query on a table that has not been declared\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.DuplicateError", "title": "<code>DuplicateError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>An integrity error caused by a duplicate entry into a unique key</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DuplicateError(QueryError):\n    \"\"\"\n    An integrity error caused by a duplicate entry into a unique key\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.IntegrityError", "title": "<code>IntegrityError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>An integrity error triggered by foreign key constraints</p> Source code in <code>datajoint/errors.py</code> <pre><code>class IntegrityError(QueryError):\n    \"\"\"\n    An integrity error triggered by foreign key constraints\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.UnknownAttributeError", "title": "<code>UnknownAttributeError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>User requests an attribute name not found in query heading</p> Source code in <code>datajoint/errors.py</code> <pre><code>class UnknownAttributeError(QueryError):\n    \"\"\"\n    User requests an attribute name not found in query heading\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingAttributeError", "title": "<code>MissingAttributeError</code>", "text": "<p>               Bases: <code>QueryError</code></p> <p>An error arising when a required attribute value is not provided in INSERT</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingAttributeError(QueryError):\n    \"\"\"\n    An error arising when a required attribute value is not provided in INSERT\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingExternalFile", "title": "<code>MissingExternalFile</code>", "text": "<p>               Bases: <code>DataJointError</code></p> <p>Error raised when an external file managed by DataJoint is no longer accessible</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingExternalFile(DataJointError):\n    \"\"\"\n    Error raised when an external file managed by DataJoint is no longer accessible\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.BucketInaccessible", "title": "<code>BucketInaccessible</code>", "text": "<p>               Bases: <code>DataJointError</code></p> <p>Error raised when a S3 bucket is inaccessible</p> Source code in <code>datajoint/errors.py</code> <pre><code>class BucketInaccessible(DataJointError):\n    \"\"\"\n    Error raised when a S3 bucket is inaccessible\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/expression/", "title": "expression.py", "text": ""}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression", "title": "<code>QueryExpression</code>", "text": "<p>QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union.</p> <p>A QueryExpression object has a support, a restriction (an AndList), and heading. Property <code>heading</code> (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj.</p> <p>Property <code>support</code> is the list of table names or other QueryExpressions to be joined.</p> <p>The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute.</p> <p>Application of operators does not always lead to the creation of a subquery. A subquery is generated when:     1. A restriction is applied on any computed or renamed attributes     2. A projection is applied remapping remapped attributes     3. Subclasses: Join, Aggregation, and Union have additional specific rules.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class QueryExpression:\n    \"\"\"\n    QueryExpression implements query operators to derive new entity set from its input.\n    A QueryExpression object generates a SELECT statement in SQL.\n    QueryExpression operators are restrict, join, proj, aggr, and union.\n\n    A QueryExpression object has a support, a restriction (an AndList), and heading.\n    Property `heading` (type dj.Heading) contains information about the attributes.\n    It is loaded from the database and updated by proj.\n\n    Property `support` is the list of table names or other QueryExpressions to be joined.\n\n    The restriction is applied first without having access to the attributes generated by the projection.\n    Then projection is applied by selecting modifying the heading attribute.\n\n    Application of operators does not always lead to the creation of a subquery.\n    A subquery is generated when:\n        1. A restriction is applied on any computed or renamed attributes\n        2. A projection is applied remapping remapped attributes\n        3. Subclasses: Join, Aggregation, and Union have additional specific rules.\n    \"\"\"\n\n    _restriction = None\n    _restriction_attributes = None\n    _left = []  # list of booleans True for left joins, False for inner joins\n    _original_heading = None  # heading before projections\n\n    # subclasses or instantiators must provide values\n    _connection = None\n    _heading = None\n    _support = None\n    _top = None\n\n    # If the query will be using distinct\n    _distinct = False\n\n    @property\n    def connection(self):\n        \"\"\"a dj.Connection object\"\"\"\n        assert self._connection is not None\n        return self._connection\n\n    @property\n    def support(self):\n        \"\"\"A list of table names or subqueries to from the FROM clause\"\"\"\n        assert self._support is not None\n        return self._support\n\n    @property\n    def heading(self):\n        \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\"\n        return self._heading\n\n    @property\n    def original_heading(self):\n        \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\"\n        return self._original_heading or self.heading\n\n    @property\n    def restriction(self):\n        \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\"\n        if self._restriction is None:\n            self._restriction = AndList()\n        return self._restriction\n\n    @property\n    def restriction_attributes(self):\n        \"\"\"the set of attribute names invoked in the WHERE clause\"\"\"\n        if self._restriction_attributes is None:\n            self._restriction_attributes = set()\n        return self._restriction_attributes\n\n    @property\n    def primary_key(self):\n        return self.heading.primary_key\n\n    _subquery_alias_count = count()  # count for alias names used in the FROM clause\n\n    def from_clause(self):\n        support = (\n            (\n                \"(\" + src.make_sql() + \") as `$%x`\" % next(self._subquery_alias_count)\n                if isinstance(src, QueryExpression)\n                else src\n            )\n            for src in self.support\n        )\n        clause = next(support)\n        for s, left in zip(support, self._left):\n            clause += \" NATURAL{left} JOIN {clause}\".format(\n                left=\" LEFT\" if left else \"\", clause=s\n            )\n        return clause\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self.restriction\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self.restriction)\n        )\n\n    def sorting_clauses(self):\n        if not self._top:\n            return \"\"\n        clause = \", \".join(\n            _wrap_attributes(\n                _flatten_attribute_list(self.primary_key, self._top.order_by)\n            )\n        )\n        if clause:\n            clause = f\" ORDER BY {clause}\"\n        if self._top.limit is not None:\n            clause += f\" LIMIT {self._top.limit}{f' OFFSET {self._top.offset}' if self._top.offset else ''}\"\n\n        return clause\n\n    def make_sql(self, fields=None):\n        \"\"\"\n        Make the SQL SELECT statement.\n\n        :param fields: used to explicitly set the select attributes\n        \"\"\"\n        return \"SELECT {distinct}{fields} FROM {from_}{where}{sorting}\".format(\n            distinct=\"DISTINCT \" if self._distinct else \"\",\n            fields=self.heading.as_sql(fields or self.heading.names),\n            from_=self.from_clause(),\n            where=self.where_clause(),\n            sorting=self.sorting_clauses(),\n        )\n\n    # --------- query operators -----------\n    def make_subquery(self):\n        \"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = [self]\n        result._heading = self.heading.make_subquery_heading()\n        return result\n\n    def restrict(self, restriction):\n        \"\"\"\n        Produces a new expression with the new restriction applied.\n        rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n        rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n        The primary key of the result is unaffected.\n        Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n        Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n        (logical disjunction of conditions)\n        Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n        The expressions in each row equivalent:\n\n        rel &amp; True                          rel\n        rel &amp; False                         the empty entity set\n        rel &amp; 'TRUE'                        rel\n        rel &amp; 'FALSE'                       the empty entity set\n        rel - cond                          rel &amp; Not(cond)\n        rel - 'TRUE'                        rel &amp; False\n        rel - 'FALSE'                       rel\n        rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n        rel &amp; AndList()                     rel\n        rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n        rel &amp; []                            rel &amp; False\n        rel &amp; None                          rel &amp; False\n        rel &amp; any_empty_entity_set          rel &amp; False\n        rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n        rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n        rel - AndList()                     rel &amp; False\n        rel - []                            rel\n        rel - None                          rel\n        rel - any_empty_entity_set          rel\n\n        When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n        one element in arg (hence arg is treated as an OrList).\n        Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n        Two elements match when their common attributes have equal values or when they have no common attributes.\n        All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n        QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n        ultimately call restrict()\n\n        :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n        string, or an AndList.\n        \"\"\"\n        attributes = set()\n        if isinstance(restriction, Top):\n            result = (\n                self.make_subquery()\n                if self._top and not self._top.__eq__(restriction)\n                else copy.copy(self)\n            )  # make subquery to avoid overwriting existing Top\n            result._top = restriction\n            return result\n        new_condition = make_condition(self, restriction, attributes)\n        if new_condition is True:\n            return self  # restriction has no effect, return the same object\n        # check that all attributes in condition are present in the query\n        try:\n            raise DataJointError(\n                \"Attribute `%s` is not found in query.\"\n                % next(attr for attr in attributes if attr not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        # If the new condition uses any new attributes, a subquery is required.\n        # However, Aggregation's HAVING statement works fine with aliased attributes.\n        need_subquery = (\n            isinstance(self, Union)\n            or (not isinstance(self, Aggregation) and self.heading.new_attributes)\n            or self._top\n        )\n        if need_subquery:\n            result = self.make_subquery()\n        else:\n            result = copy.copy(self)\n            result._restriction = AndList(\n                self.restriction\n            )  # copy to preserve the original\n        result.restriction.append(new_condition)\n        result.restriction_attributes.update(attributes)\n        return result\n\n    def restrict_in_place(self, restriction):\n        self.__dict__.update(self.restrict(restriction).__dict__)\n\n    def __and__(self, restriction):\n        \"\"\"\n        Restriction operator e.g. ``q1 &amp; q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(restriction)\n\n    def __xor__(self, restriction):\n        \"\"\"\n        Permissive restriction operator ignoring compatibility check  e.g. ``q1 ^ q2``.\n        \"\"\"\n        if inspect.isclass(restriction) and issubclass(restriction, QueryExpression):\n            restriction = restriction()\n        if isinstance(restriction, Not):\n            return self.restrict(Not(PromiscuousOperand(restriction.restriction)))\n        return self.restrict(PromiscuousOperand(restriction))\n\n    def __sub__(self, restriction):\n        \"\"\"\n        Inverted restriction e.g. ``q1 - q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(Not(restriction))\n\n    def __neg__(self):\n        \"\"\"\n        Convert between restriction and inverted restriction e.g. ``-q1``.\n        :return: target restriction\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        if isinstance(self, Not):\n            return self.restriction\n        return Not(self)\n\n    def __mul__(self, other):\n        \"\"\"\n        join of query expressions `self` and `other` e.g. ``q1 * q2``.\n        \"\"\"\n        return self.join(other)\n\n    def __matmul__(self, other):\n        \"\"\"\n        Permissive join of query expressions `self` and `other` ignoring compatibility check\n            e.g. ``q1 @ q2``.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        return self.join(other, semantic_check=False)\n\n    def join(self, other, semantic_check=True, left=False):\n        \"\"\"\n        create the joined QueryExpression.\n        a * b  is short for A.join(B)\n        a @ b  is short for A.join(B, semantic_check=False)\n        Additionally, left=True will retain the rows of self, effectively performing a left join.\n        \"\"\"\n        # trigger subqueries if joining on renamed attributes\n        if isinstance(other, U):\n            return other * self\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"The argument of join must be a QueryExpression\")\n        if semantic_check:\n            assert_join_compatibility(self, other)\n        join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n        # needs subquery if self's FROM clause has common attributes with other's FROM clause\n        need_subquery1 = need_subquery2 = bool(\n            (set(self.original_heading.names) &amp; set(other.original_heading.names))\n            - join_attributes\n        )\n        # need subquery if any of the join attributes are derived\n        need_subquery1 = (\n            need_subquery1\n            or isinstance(self, Aggregation)\n            or any(n in self.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        need_subquery2 = (\n            need_subquery2\n            or isinstance(other, Aggregation)\n            or any(n in other.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        if need_subquery1:\n            self = self.make_subquery()\n        if need_subquery2:\n            other = other.make_subquery()\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = self.support + other.support\n        result._left = self._left + [left] + other._left\n        result._heading = self.heading.join(other.heading)\n        result._restriction = AndList(self.restriction)\n        result._restriction.append(other.restriction)\n        result._original_heading = self.original_heading.join(other.original_heading)\n        assert len(result.support) == len(result._left) + 1\n        return result\n\n    def __add__(self, other):\n        \"\"\"union e.g. ``q1 + q2``.\"\"\"\n        return Union.create(self, other)\n\n    def proj(self, *attributes, **named_attributes):\n        \"\"\"\n        Projection operator.\n\n        :param attributes:  attributes to be included in the result. (The primary key is already included).\n        :param named_attributes: new attributes computed or renamed from existing attributes.\n        :return: the projected expression.\n        Primary key attributes cannot be excluded but may be renamed.\n        If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n        Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n        Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n        self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n        self.proj() -- include only primary key\n        self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n        self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n        self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n        self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n        self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n        from other attributes available before the projection.\n        Each attribute name can only be used once.\n        \"\"\"\n        named_attributes = {\n            k: translate_attribute(v)[1] for k, v in named_attributes.items()\n        }\n        # new attributes in parentheses are included again with the new name without removing original\n        duplication_pattern = re.compile(\n            rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n        )\n        # attributes without parentheses renamed\n        rename_pattern = re.compile(\n            rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n        )\n        replicate_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        rename_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        compute_map = {\n            k: v\n            for k, v in named_attributes.items()\n            if not duplication_pattern.match(v) and not rename_pattern.match(v)\n        }\n        attributes = set(attributes)\n        # include primary key\n        attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n        # include all secondary attributes with Ellipsis\n        if Ellipsis in attributes:\n            attributes.discard(Ellipsis)\n            attributes.update(\n                (\n                    a\n                    for a in self.heading.secondary_attributes\n                    if a not in attributes and a not in rename_map.values()\n                )\n            )\n        try:\n            raise DataJointError(\n                \"%s is not a valid data type for an attribute in .proj\"\n                % next(a for a in attributes if not isinstance(a, str))\n            )\n        except StopIteration:\n            pass  # normal case\n        # remove excluded attributes, specified as `-attr'\n        excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n        attributes.difference_update(excluded)\n        excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n        attributes.difference_update(excluded)\n        try:\n            raise DataJointError(\n                \"Cannot exclude primary key attribute %s\",\n                next(a for a in excluded if a in self.primary_key),\n            )\n        except StopIteration:\n            pass  # all ok\n        # check that all attributes exist in heading\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(a for a in attributes if a not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that all mentioned names are present in heading\n        mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n        try:\n            raise DataJointError(\n                \"Attribute '%s' not found.\"\n                % next(a for a in mentions if not self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that newly created attributes do not clash with any other selected attributes\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in rename_map\n                    if a in attributes.union(compute_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in compute_map\n                    if a in attributes.union(rename_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in replicate_map\n                    if a in attributes.union(rename_map).union(compute_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # need a subquery if the projection remaps any remapped attributes\n        used = set(q for v in compute_map.values() for q in extract_column_names(v))\n        used.update(rename_map.values())\n        used.update(replicate_map.values())\n        used.intersection_update(self.heading.names)\n        need_subquery = isinstance(self, Union) or any(\n            self.heading[name].attribute_expression is not None for name in used\n        )\n        if not need_subquery and self.restriction:\n            # need a subquery if the restriction applies to attributes that have been renamed\n            need_subquery = any(\n                name in self.restriction_attributes\n                for name in self.heading.new_attributes\n            )\n\n        result = self.make_subquery() if need_subquery else copy.copy(self)\n        result._original_heading = result.original_heading\n        result._heading = result.heading.select(\n            attributes,\n            rename_map=dict(**rename_map, **replicate_map),\n            compute_map=compute_map,\n        )\n        return result\n\n    def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n        \"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if Ellipsis in attributes:\n            # expand ellipsis to include only attributes from the left table\n            attributes = set(attributes)\n            attributes.discard(Ellipsis)\n            attributes.update(self.heading.secondary_attributes)\n        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n            *attributes, **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n\n    # ---------- Fetch operators --------------------\n    @property\n    def fetch1(self):\n        return Fetch1(self)\n\n    @property\n    def fetch(self):\n        return Fetch(self)\n\n    def head(self, limit=25, **fetch_kwargs):\n        \"\"\"\n        shortcut to fetch the first few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n\n    def tail(self, limit=25, **fetch_kwargs):\n        \"\"\"\n        shortcut to fetch the last few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n\n    def __len__(self):\n        \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\"\n        result = self.make_subquery() if self._top else copy.copy(self)\n        return result.connection.query(\n            \"SELECT {select_} FROM {from_}{where}\".format(\n                select_=(\n                    \"count(*)\"\n                    if any(result._left)\n                    else \"count(DISTINCT {fields})\".format(\n                        fields=result.heading.as_sql(\n                            result.primary_key, include_aliases=False\n                        )\n                    )\n                ),\n                from_=result.from_clause(),\n                where=result.where_clause(),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n        \"\"\"\n        :return: True if the result is not empty. Equivalent to len(self) &gt; 0 but often\n            faster e.g. ``bool(q1)``.\n        \"\"\"\n        return bool(\n            self.connection.query(\n                \"SELECT EXISTS(SELECT 1 FROM {from_}{where})\".format(\n                    from_=self.from_clause(), where=self.where_clause()\n                )\n            ).fetchone()[0]\n        )\n\n    def __contains__(self, item):\n        \"\"\"\n        returns True if the restriction in item matches any entries in self\n            e.g. ``restriction in q1``.\n\n        :param item: any restriction\n        (item in query_expression) is equivalent to bool(query_expression &amp; item) but may be\n        executed more efficiently.\n        \"\"\"\n        return bool(self &amp; item)  # May be optimized e.g. using an EXISTS query\n\n    def __iter__(self):\n        \"\"\"\n        returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``.\n\n        :param self: iterator-compatible QueryExpression object\n        \"\"\"\n        self._iter_only_key = all(v.in_key for v in self.heading.attributes.values())\n        self._iter_keys = self.fetch(\"KEY\")\n        return self\n\n    def __next__(self):\n        \"\"\"\n        returns the next record on an iterator-compatible QueryExpression object\n            e.g. ``next(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: dict\n        \"\"\"\n        try:\n            key = self._iter_keys.pop(0)\n        except AttributeError:\n            # self._iter_keys is missing because __iter__ has not been called.\n            raise TypeError(\n                \"A QueryExpression object is not an iterator. \"\n                \"Use iter(obj) to create an iterator.\"\n            )\n        except IndexError:\n            raise StopIteration\n        else:\n            if self._iter_only_key:\n                return key\n            else:\n                try:\n                    return (self &amp; key).fetch1()\n                except DataJointError:\n                    # The data may have been deleted since the moment the keys were fetched\n                    # -- move on to next entry.\n                    return next(self)\n\n    def cursor(self, as_dict=False):\n        \"\"\"\n        See expression.fetch() for input description.\n        :return: query cursor\n        \"\"\"\n        sql = self.make_sql()\n        logger.debug(sql)\n        return self.connection.query(sql, as_dict=as_dict)\n\n    def __repr__(self):\n        \"\"\"\n        returns the string representation of a QueryExpression object e.g. ``str(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: str\n        \"\"\"\n        return (\n            super().__repr__()\n            if config[\"loglevel\"].lower() == \"debug\"\n            else self.preview()\n        )\n\n    def preview(self, limit=None, width=None):\n        \"\"\":return: a string of preview of the contents of the query.\"\"\"\n        return preview(self, limit, width)\n\n    def _repr_html_(self):\n        \"\"\":return: HTML to display table in Jupyter notebook.\"\"\"\n        return repr_html(self)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.connection", "title": "<code>connection</code>  <code>property</code>", "text": "<p>a dj.Connection object</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.support", "title": "<code>support</code>  <code>property</code>", "text": "<p>A list of table names or subqueries to from the FROM clause</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.heading", "title": "<code>heading</code>  <code>property</code>", "text": "<p>a dj.Heading object, reflects the effects of the projection operator .proj</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.original_heading", "title": "<code>original_heading</code>  <code>property</code>", "text": "<p>a dj.Heading object reflecting the attributes before projection</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction", "title": "<code>restriction</code>  <code>property</code>", "text": "<p>a AndList object of restrictions applied to input to produce the result</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction_attributes", "title": "<code>restriction_attributes</code>  <code>property</code>", "text": "<p>the set of attribute names invoked in the WHERE clause</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_sql", "title": "<code>make_sql(fields=None)</code>", "text": "<p>Make the SQL SELECT statement.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>used to explicitly set the select attributes</p> <code>None</code> Source code in <code>datajoint/expression.py</code> <pre><code>def make_sql(self, fields=None):\n    \"\"\"\n    Make the SQL SELECT statement.\n\n    :param fields: used to explicitly set the select attributes\n    \"\"\"\n    return \"SELECT {distinct}{fields} FROM {from_}{where}{sorting}\".format(\n        distinct=\"DISTINCT \" if self._distinct else \"\",\n        fields=self.heading.as_sql(fields or self.heading.names),\n        from_=self.from_clause(),\n        where=self.where_clause(),\n        sorting=self.sorting_clauses(),\n    )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_subquery", "title": "<code>make_subquery()</code>", "text": "<p>create a new SELECT statement where self is the FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def make_subquery(self):\n    \"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = [self]\n    result._heading = self.heading.make_subquery_heading()\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restrict", "title": "<code>restrict(restriction)</code>", "text": "<p>Produces a new expression with the new restriction applied. rel.restrict(restriction)  is equivalent to  rel &amp; restriction. rel.restrict(Not(restriction))  is equivalent to  rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class.</p> <p>The expressions in each row equivalent:</p> <p>rel &amp; True                          rel rel &amp; False                         the empty entity set rel &amp; 'TRUE'                        rel rel &amp; 'FALSE'                       the empty entity set rel - cond                          rel &amp; Not(cond) rel - 'TRUE'                        rel &amp; False rel - 'FALSE'                       rel rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2 rel &amp; AndList()                     rel rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2)) rel &amp; []                            rel &amp; False rel &amp; None                          rel &amp; False rel &amp; any_empty_entity_set          rel &amp; False rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)] rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2) rel - AndList()                     rel &amp; False rel - []                            rel rel - None                          rel rel - any_empty_entity_set          rel</p> <p>When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.</p> <p>QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict()</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList.</p> required Source code in <code>datajoint/expression.py</code> <pre><code>def restrict(self, restriction):\n    \"\"\"\n    Produces a new expression with the new restriction applied.\n    rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n    rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n    The primary key of the result is unaffected.\n    Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n    Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n    (logical disjunction of conditions)\n    Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n    The expressions in each row equivalent:\n\n    rel &amp; True                          rel\n    rel &amp; False                         the empty entity set\n    rel &amp; 'TRUE'                        rel\n    rel &amp; 'FALSE'                       the empty entity set\n    rel - cond                          rel &amp; Not(cond)\n    rel - 'TRUE'                        rel &amp; False\n    rel - 'FALSE'                       rel\n    rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n    rel &amp; AndList()                     rel\n    rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n    rel &amp; []                            rel &amp; False\n    rel &amp; None                          rel &amp; False\n    rel &amp; any_empty_entity_set          rel &amp; False\n    rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n    rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n    rel - AndList()                     rel &amp; False\n    rel - []                            rel\n    rel - None                          rel\n    rel - any_empty_entity_set          rel\n\n    When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n    one element in arg (hence arg is treated as an OrList).\n    Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n    Two elements match when their common attributes have equal values or when they have no common attributes.\n    All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n    QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n    ultimately call restrict()\n\n    :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n    string, or an AndList.\n    \"\"\"\n    attributes = set()\n    if isinstance(restriction, Top):\n        result = (\n            self.make_subquery()\n            if self._top and not self._top.__eq__(restriction)\n            else copy.copy(self)\n        )  # make subquery to avoid overwriting existing Top\n        result._top = restriction\n        return result\n    new_condition = make_condition(self, restriction, attributes)\n    if new_condition is True:\n        return self  # restriction has no effect, return the same object\n    # check that all attributes in condition are present in the query\n    try:\n        raise DataJointError(\n            \"Attribute `%s` is not found in query.\"\n            % next(attr for attr in attributes if attr not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    # If the new condition uses any new attributes, a subquery is required.\n    # However, Aggregation's HAVING statement works fine with aliased attributes.\n    need_subquery = (\n        isinstance(self, Union)\n        or (not isinstance(self, Aggregation) and self.heading.new_attributes)\n        or self._top\n    )\n    if need_subquery:\n        result = self.make_subquery()\n    else:\n        result = copy.copy(self)\n        result._restriction = AndList(\n            self.restriction\n        )  # copy to preserve the original\n    result.restriction.append(new_condition)\n    result.restriction_attributes.update(attributes)\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.join", "title": "<code>join(other, semantic_check=True, left=False)</code>", "text": "<p>create the joined QueryExpression. a * b  is short for A.join(B) a @ b  is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, semantic_check=True, left=False):\n    \"\"\"\n    create the joined QueryExpression.\n    a * b  is short for A.join(B)\n    a @ b  is short for A.join(B, semantic_check=False)\n    Additionally, left=True will retain the rows of self, effectively performing a left join.\n    \"\"\"\n    # trigger subqueries if joining on renamed attributes\n    if isinstance(other, U):\n        return other * self\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"The argument of join must be a QueryExpression\")\n    if semantic_check:\n        assert_join_compatibility(self, other)\n    join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n    # needs subquery if self's FROM clause has common attributes with other's FROM clause\n    need_subquery1 = need_subquery2 = bool(\n        (set(self.original_heading.names) &amp; set(other.original_heading.names))\n        - join_attributes\n    )\n    # need subquery if any of the join attributes are derived\n    need_subquery1 = (\n        need_subquery1\n        or isinstance(self, Aggregation)\n        or any(n in self.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    need_subquery2 = (\n        need_subquery2\n        or isinstance(other, Aggregation)\n        or any(n in other.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    if need_subquery1:\n        self = self.make_subquery()\n    if need_subquery2:\n        other = other.make_subquery()\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = self.support + other.support\n    result._left = self._left + [left] + other._left\n    result._heading = self.heading.join(other.heading)\n    result._restriction = AndList(self.restriction)\n    result._restriction.append(other.restriction)\n    result._original_heading = self.original_heading.join(other.original_heading)\n    assert len(result.support) == len(result._left) + 1\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.proj", "title": "<code>proj(*attributes, **named_attributes)</code>", "text": "<p>Projection operator.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <p>attributes to be included in the result. (The primary key is already included).</p> <code>()</code> <code>named_attributes</code> <p>new attributes computed or renamed from existing attributes.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def proj(self, *attributes, **named_attributes):\n    \"\"\"\n    Projection operator.\n\n    :param attributes:  attributes to be included in the result. (The primary key is already included).\n    :param named_attributes: new attributes computed or renamed from existing attributes.\n    :return: the projected expression.\n    Primary key attributes cannot be excluded but may be renamed.\n    If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n    Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n    Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n    self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n    self.proj() -- include only primary key\n    self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n    self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n    self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n    self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n    self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n    from other attributes available before the projection.\n    Each attribute name can only be used once.\n    \"\"\"\n    named_attributes = {\n        k: translate_attribute(v)[1] for k, v in named_attributes.items()\n    }\n    # new attributes in parentheses are included again with the new name without removing original\n    duplication_pattern = re.compile(\n        rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n    )\n    # attributes without parentheses renamed\n    rename_pattern = re.compile(\n        rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n    )\n    replicate_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    rename_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    compute_map = {\n        k: v\n        for k, v in named_attributes.items()\n        if not duplication_pattern.match(v) and not rename_pattern.match(v)\n    }\n    attributes = set(attributes)\n    # include primary key\n    attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n    # include all secondary attributes with Ellipsis\n    if Ellipsis in attributes:\n        attributes.discard(Ellipsis)\n        attributes.update(\n            (\n                a\n                for a in self.heading.secondary_attributes\n                if a not in attributes and a not in rename_map.values()\n            )\n        )\n    try:\n        raise DataJointError(\n            \"%s is not a valid data type for an attribute in .proj\"\n            % next(a for a in attributes if not isinstance(a, str))\n        )\n    except StopIteration:\n        pass  # normal case\n    # remove excluded attributes, specified as `-attr'\n    excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n    attributes.difference_update(excluded)\n    excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n    attributes.difference_update(excluded)\n    try:\n        raise DataJointError(\n            \"Cannot exclude primary key attribute %s\",\n            next(a for a in excluded if a in self.primary_key),\n        )\n    except StopIteration:\n        pass  # all ok\n    # check that all attributes exist in heading\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(a for a in attributes if a not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that all mentioned names are present in heading\n    mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n    try:\n        raise DataJointError(\n            \"Attribute '%s' not found.\"\n            % next(a for a in mentions if not self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that newly created attributes do not clash with any other selected attributes\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in rename_map\n                if a in attributes.union(compute_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in compute_map\n                if a in attributes.union(rename_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in replicate_map\n                if a in attributes.union(rename_map).union(compute_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # need a subquery if the projection remaps any remapped attributes\n    used = set(q for v in compute_map.values() for q in extract_column_names(v))\n    used.update(rename_map.values())\n    used.update(replicate_map.values())\n    used.intersection_update(self.heading.names)\n    need_subquery = isinstance(self, Union) or any(\n        self.heading[name].attribute_expression is not None for name in used\n    )\n    if not need_subquery and self.restriction:\n        # need a subquery if the restriction applies to attributes that have been renamed\n        need_subquery = any(\n            name in self.restriction_attributes\n            for name in self.heading.new_attributes\n        )\n\n    result = self.make_subquery() if need_subquery else copy.copy(self)\n    result._original_heading = result.original_heading\n    result._heading = result.heading.select(\n        attributes,\n        rename_map=dict(**rename_map, **replicate_map),\n        compute_map=compute_map,\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.aggr", "title": "<code>aggr(group, *attributes, keep_all_rows=False, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>keep_all_rows</code> <p>True=keep all the rows from self. False=keep only rows that match entries in group.</p> <code>False</code> <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n    \"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if Ellipsis in attributes:\n        # expand ellipsis to include only attributes from the left table\n        attributes = set(attributes)\n        attributes.discard(Ellipsis)\n        attributes.update(self.heading.secondary_attributes)\n    return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n        *attributes, **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.head", "title": "<code>head(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25)</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def head(self, limit=25, **fetch_kwargs):\n    \"\"\"\n    shortcut to fetch the first few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.tail", "title": "<code>tail(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def tail(self, limit=25, **fetch_kwargs):\n    \"\"\"\n    shortcut to fetch the last few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.cursor", "title": "<code>cursor(as_dict=False)</code>", "text": "<p>See expression.fetch() for input description.</p> <p>Returns:</p> Type Description <p>query cursor</p> Source code in <code>datajoint/expression.py</code> <pre><code>def cursor(self, as_dict=False):\n    \"\"\"\n    See expression.fetch() for input description.\n    :return: query cursor\n    \"\"\"\n    sql = self.make_sql()\n    logger.debug(sql)\n    return self.connection.query(sql, as_dict=as_dict)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.preview", "title": "<code>preview(limit=None, width=None)</code>", "text": "<p>Returns:</p> Type Description <p>a string of preview of the contents of the query.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def preview(self, limit=None, width=None):\n    \"\"\":return: a string of preview of the contents of the query.\"\"\"\n    return preview(self, limit, width)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Aggregation", "title": "<code>Aggregation</code>", "text": "<p>               Bases: <code>QueryExpression</code></p> <p>Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class Aggregation(QueryExpression):\n    \"\"\"\n    Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set\n    with primary key from arg.\n    The computed arguments comp1, ..., compn use aggregation calculations on the attributes of\n    group or simple projections and calculations on the attributes of arg.\n    Aggregation is used QueryExpression.aggr and U.aggr.\n    Aggregation is a private class in DataJoint, not exposed to users.\n    \"\"\"\n\n    _left_restrict = None  # the pre-GROUP BY conditions for the WHERE clause\n    _subquery_alias_count = count()\n\n    @classmethod\n    def create(cls, arg, group, keep_all_rows=False):\n        if inspect.isclass(group) and issubclass(group, QueryExpression):\n            group = group()  # instantiate if a class\n        assert isinstance(group, QueryExpression)\n        if keep_all_rows and len(group.support) &gt; 1 or group.heading.new_attributes:\n            group = group.make_subquery()  # subquery if left joining a join\n        join = arg.join(group, left=keep_all_rows)  # reuse the join logic\n        result = cls()\n        result._connection = join.connection\n        result._heading = join.heading.set_primary_key(\n            arg.primary_key\n        )  # use left operand's primary key\n        result._support = join.support\n        result._left = join._left\n        result._left_restrict = join.restriction  # WHERE clause applied before GROUP BY\n        result._grouping_attributes = result.primary_key\n\n        return result\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self._left_restrict\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self._left_restrict)\n        )\n\n    def make_sql(self, fields=None):\n        fields = self.heading.as_sql(fields or self.heading.names)\n        assert self._grouping_attributes or not self.restriction\n        distinct = set(self.heading.names) == set(self.primary_key)\n        return (\n            \"SELECT {distinct}{fields} FROM {from_}{where}{group_by}{sorting}\".format(\n                distinct=\"DISTINCT \" if distinct else \"\",\n                fields=fields,\n                from_=self.from_clause(),\n                where=self.where_clause(),\n                group_by=(\n                    \"\"\n                    if not self.primary_key\n                    else (\n                        \" GROUP BY `%s`\" % \"`,`\".join(self._grouping_attributes)\n                        + (\n                            \"\"\n                            if not self.restriction\n                            else \" HAVING (%s)\" % \")AND(\".join(self.restriction)\n                        )\n                    )\n                ),\n                sorting=self.sorting_clauses(),\n            )\n        )\n\n    def __len__(self):\n        return self.connection.query(\n            \"SELECT count(1) FROM ({subquery}) `${alias:x}`\".format(\n                subquery=self.make_sql(), alias=next(self._subquery_alias_count)\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n        return bool(\n            self.connection.query(\"SELECT EXISTS({sql})\".format(sql=self.make_sql()))\n        )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union", "title": "<code>Union</code>", "text": "<p>               Bases: <code>QueryExpression</code></p> <p>Union is the private DataJoint class that implements the union operator.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class Union(QueryExpression):\n    \"\"\"\n    Union is the private DataJoint class that implements the union operator.\n    \"\"\"\n\n    __count = count()\n\n    @classmethod\n    def create(cls, arg1, arg2):\n        if inspect.isclass(arg2) and issubclass(arg2, QueryExpression):\n            arg2 = arg2()  # instantiate if a class\n        if not isinstance(arg2, QueryExpression):\n            raise DataJointError(\n                \"A QueryExpression can only be unioned with another QueryExpression\"\n            )\n        if arg1.connection != arg2.connection:\n            raise DataJointError(\n                \"Cannot operate on QueryExpressions originating from different connections.\"\n            )\n        if set(arg1.primary_key) != set(arg2.primary_key):\n            raise DataJointError(\n                \"The operands of a union must share the same primary key.\"\n            )\n        if set(arg1.heading.secondary_attributes) &amp; set(\n            arg2.heading.secondary_attributes\n        ):\n            raise DataJointError(\n                \"The operands of a union must not share any secondary attributes.\"\n            )\n        result = cls()\n        result._connection = arg1.connection\n        result._heading = arg1.heading.join(arg2.heading)\n        result._support = [arg1, arg2]\n        return result\n\n    def make_sql(self):\n        arg1, arg2 = self._support\n        if (\n            not arg1.heading.secondary_attributes\n            and not arg2.heading.secondary_attributes\n        ):\n            # no secondary attributes: use UNION DISTINCT\n            fields = arg1.primary_key\n            return \"SELECT * FROM (({sql1}) UNION ({sql2})) as `_u{alias}{sorting}`\".format(\n                sql1=(\n                    arg1.make_sql()\n                    if isinstance(arg1, Union)\n                    else arg1.make_sql(fields)\n                ),\n                sql2=(\n                    arg2.make_sql()\n                    if isinstance(arg2, Union)\n                    else arg2.make_sql(fields)\n                ),\n                alias=next(self.__count),\n                sorting=self.sorting_clauses(),\n            )\n        # with secondary attributes, use union of left join with antijoin\n        fields = self.heading.names\n        sql1 = arg1.join(arg2, left=True).make_sql(fields)\n        sql2 = (\n            (arg2 - arg1)\n            .proj(..., **{k: \"NULL\" for k in arg1.heading.secondary_attributes})\n            .make_sql(fields)\n        )\n        return \"({sql1})  UNION ({sql2})\".format(sql1=sql1, sql2=sql2)\n\n    def from_clause(self):\n        \"\"\"The union does not use a FROM clause\"\"\"\n        assert False\n\n    def where_clause(self):\n        \"\"\"The union does not use a WHERE clause\"\"\"\n        assert False\n\n    def __len__(self):\n        return self.connection.query(\n            \"SELECT count(1) FROM ({subquery}) `${alias:x}`\".format(\n                subquery=self.make_sql(),\n                alias=next(QueryExpression._subquery_alias_count),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n        return bool(\n            self.connection.query(\"SELECT EXISTS({sql})\".format(sql=self.make_sql()))\n        )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.from_clause", "title": "<code>from_clause()</code>", "text": "<p>The union does not use a FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def from_clause(self):\n    \"\"\"The union does not use a FROM clause\"\"\"\n    assert False\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.where_clause", "title": "<code>where_clause()</code>", "text": "<p>The union does not use a WHERE clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def where_clause(self):\n    \"\"\"The union does not use a WHERE clause\"\"\"\n    assert False\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U", "title": "<code>U</code>", "text": "<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expression <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class U:\n    \"\"\"\n    dj.U objects are the universal sets representing all possible values of their attributes.\n    dj.U objects cannot be queried on their own but are useful for forming some queries.\n    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.\n    The universal set is the set of all possible combinations of values of the attributes.\n    Without any attributes, dj.U() represents the set with one element that has no attributes.\n\n    Restriction:\n\n    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.\n\n    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:\n\n    &gt;&gt;&gt; dj.U('contrast', 'brightness') &amp; stimulus\n\n    Aggregation:\n\n    In aggregation, dj.U is used for summary calculation over an entire set:\n\n    The following expression yields one element with one attribute `s` containing the total number of elements in\n    query expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(*)')\n\n    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in\n    query expression `expr`.\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(distinct attr)')\n    &gt;&gt;&gt; dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')\n\n    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`\n    over entire result set of expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, s='sum(attr)')\n\n    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of\n    their occurrences in the result set of query expression `expr`.\n\n    &gt;&gt;&gt; dj.U(attr1,attr2).aggr(expr, n='count(*)')\n\n    Joins:\n\n    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result\n    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on\n    non-primary key attributes.\n    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw\n    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first\n    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.\n    \"\"\"\n\n    def __init__(self, *primary_key):\n        self._primary_key = primary_key\n\n    @property\n    def primary_key(self):\n        return self._primary_key\n\n    def __and__(self, other):\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be restricted with a QueryExpression.\")\n        result = copy.copy(other)\n        result._distinct = True\n        result._heading = result.heading.set_primary_key(self.primary_key)\n        result = result.proj()\n        return result\n\n    def join(self, other, left=False):\n        \"\"\"\n        Joining U with a query expression has the effect of promoting the attributes of U to\n        the primary key of the other query expression.\n\n        :param other: the other query expression to join with.\n        :param left: ignored. dj.U always acts as if left=False\n        :return: a copy of the other query expression with the primary key extended.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found\"\n                % next(k for k in self.primary_key if k not in other.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        result = copy.copy(other)\n        result._heading = result.heading.set_primary_key(\n            other.primary_key\n            + [k for k in self.primary_key if k not in other.primary_key]\n        )\n        return result\n\n    def __mul__(self, other):\n        \"\"\"shorthand for join\"\"\"\n        return self.join(other)\n\n    def aggr(self, group, **named_attributes):\n        \"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if named_attributes.get(\"keep_all_rows\", False):\n            raise DataJointError(\n                \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n            )\n        return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n            **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U.join", "title": "<code>join(other, left=False)</code>", "text": "<p>Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>the other query expression to join with.</p> required <code>left</code> <p>ignored. dj.U always acts as if left=False</p> <code>False</code> <p>Returns:</p> Type Description <p>a copy of the other query expression with the primary key extended.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, left=False):\n    \"\"\"\n    Joining U with a query expression has the effect of promoting the attributes of U to\n    the primary key of the other query expression.\n\n    :param other: the other query expression to join with.\n    :param left: ignored. dj.U always acts as if left=False\n    :return: a copy of the other query expression with the primary key extended.\n    \"\"\"\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate if a class\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found\"\n            % next(k for k in self.primary_key if k not in other.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    result = copy.copy(other)\n    result._heading = result.heading.set_primary_key(\n        other.primary_key\n        + [k for k in self.primary_key if k not in other.primary_key]\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U.aggr", "title": "<code>aggr(group, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, **named_attributes):\n    \"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if named_attributes.get(\"keep_all_rows\", False):\n        raise DataJointError(\n            \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n        )\n    return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n        **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/external/", "title": "external.py", "text": ""}, {"location": "api/datajoint/external/#datajoint.external.subfold", "title": "<code>subfold(name, folds)</code>", "text": "<p>subfolding for external storage: e.g.  subfold('aBCdefg', (2, 3))  --&gt;  ['ab','cde']</p> Source code in <code>datajoint/external.py</code> <pre><code>def subfold(name, folds):\n    \"\"\"\n    subfolding for external storage: e.g.  subfold('aBCdefg', (2, 3))  --&gt;  ['ab','cde']\n    \"\"\"\n    return (\n        (name[: folds[0]].lower(),) + subfold(name[folds[0] :], folds[1:])\n        if folds\n        else ()\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable", "title": "<code>ExternalTable</code>", "text": "<p>               Bases: <code>Table</code></p> <p>The table tracking externally stored objects. Declare as ExternalTable(connection, database)</p> Source code in <code>datajoint/external.py</code> <pre><code>class ExternalTable(Table):\n    \"\"\"\n    The table tracking externally stored objects.\n    Declare as ExternalTable(connection, database)\n    \"\"\"\n\n    def __init__(self, connection, store, database):\n        self.store = store\n        self.spec = config.get_store_spec(store)\n        self._s3 = None\n        self.database = database\n        self._connection = connection\n        self._heading = Heading(\n            table_info=dict(\n                conn=connection,\n                database=database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n        self._support = [self.full_table_name]\n        if not self.is_declared:\n            self.declare()\n        self._s3 = None\n        if self.spec[\"protocol\"] == \"file\" and not Path(self.spec[\"location\"]).is_dir():\n            raise FileNotFoundError(\n                \"Inaccessible local directory %s\" % self.spec[\"location\"]\n            ) from None\n\n    @property\n    def definition(self):\n        return \"\"\"\n        # external storage tracking\n        hash  : uuid    #  hash of contents (blob), of filename + contents (attach), or relative filepath (filepath)\n        ---\n        size      :bigint unsigned     # size of object in bytes\n        attachment_name=null : varchar(255)  # the filename of an attachment\n        filepath=null : varchar(1000)  # relative filepath or attachment filename\n        contents_hash=null : uuid      # used for the filepath datatype\n        timestamp=CURRENT_TIMESTAMP  :timestamp   # automatic timestamp\n        \"\"\"\n\n    @property\n    def table_name(self):\n        return f\"{EXTERNAL_TABLE_ROOT}_{self.store}\"\n\n    @property\n    def s3(self):\n        if self._s3 is None:\n            self._s3 = s3.Folder(**self.spec)\n        return self._s3\n\n    # - low-level operations - private\n\n    def _make_external_filepath(self, relative_filepath):\n        \"\"\"resolve the complete external path based on the relative path\"\"\"\n        # Strip root\n        if self.spec[\"protocol\"] == \"s3\":\n            posix_path = PurePosixPath(PureWindowsPath(self.spec[\"location\"]))\n            location_path = (\n                Path(*posix_path.parts[1:])\n                if len(self.spec[\"location\"]) &gt; 0\n                and any(case in posix_path.parts[0] for case in (\"\\\\\", \":\"))\n                else Path(posix_path)\n            )\n            return PurePosixPath(location_path, relative_filepath)\n        # Preserve root\n        elif self.spec[\"protocol\"] == \"file\":\n            return PurePosixPath(Path(self.spec[\"location\"]), relative_filepath)\n        else:\n            assert False\n\n    def _make_uuid_path(self, uuid, suffix=\"\"):\n        \"\"\"create external path based on the uuid hash\"\"\"\n        return self._make_external_filepath(\n            PurePosixPath(\n                self.database,\n                \"/\".join(subfold(uuid.hex, self.spec[\"subfolding\"])),\n                uuid.hex,\n            ).with_suffix(suffix)\n        )\n\n    def _upload_file(self, local_path, external_path, metadata=None):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.fput(local_path, external_path, metadata)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_copy(local_path, external_path, overwrite=True)\n        else:\n            assert False\n\n    def _download_file(self, external_path, download_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.fget(external_path, download_path)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_copy(external_path, download_path)\n        else:\n            assert False\n\n    def _upload_buffer(self, buffer, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.put(external_path, buffer)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_write(external_path, buffer)\n        else:\n            assert False\n\n    def _download_buffer(self, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            return self.s3.get(external_path)\n        if self.spec[\"protocol\"] == \"file\":\n            try:\n                return Path(external_path).read_bytes()\n            except FileNotFoundError:\n                raise errors.MissingExternalFile(\n                    f\"Missing external file {external_path}\"\n                ) from None\n        assert False\n\n    def _remove_external_file(self, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.remove_object(external_path)\n        elif self.spec[\"protocol\"] == \"file\":\n            try:\n                Path(external_path).unlink()\n            except FileNotFoundError:\n                pass\n\n    def exists(self, external_filepath):\n        \"\"\"\n        :return: True if the external file is accessible\n        \"\"\"\n        if self.spec[\"protocol\"] == \"s3\":\n            return self.s3.exists(external_filepath)\n        if self.spec[\"protocol\"] == \"file\":\n            return Path(external_filepath).is_file()\n        assert False\n\n    # --- BLOBS ----\n\n    def put(self, blob):\n        \"\"\"\n        put a binary string (blob) in external store\n        \"\"\"\n        uuid = uuid_from_buffer(blob)\n        self._upload_buffer(blob, self._make_uuid_path(uuid))\n        # insert tracking info\n        self.connection.query(\n            \"INSERT INTO {tab} (hash, size) VALUES (%s, {size}) ON DUPLICATE KEY \"\n            \"UPDATE timestamp=CURRENT_TIMESTAMP\".format(\n                tab=self.full_table_name, size=len(blob)\n            ),\n            args=(uuid.bytes,),\n        )\n        return uuid\n\n    def get(self, uuid):\n        \"\"\"\n        get an object from external store.\n        \"\"\"\n        if uuid is None:\n            return None\n        # attempt to get object from cache\n        blob = None\n        cache_folder = config.get(\"cache\", None)\n        if cache_folder:\n            try:\n                cache_path = Path(cache_folder, *subfold(uuid.hex, CACHE_SUBFOLDING))\n                cache_file = Path(cache_path, uuid.hex)\n                blob = cache_file.read_bytes()\n            except FileNotFoundError:\n                pass  # not cached\n        # download blob from external store\n        if blob is None:\n            try:\n                blob = self._download_buffer(self._make_uuid_path(uuid))\n            except MissingExternalFile:\n                if not SUPPORT_MIGRATED_BLOBS:\n                    raise\n                # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths\n                relative_filepath, contents_hash = (self &amp; {\"hash\": uuid}).fetch1(\n                    \"filepath\", \"contents_hash\"\n                )\n                if relative_filepath is None:\n                    raise\n                blob = self._download_buffer(\n                    self._make_external_filepath(relative_filepath)\n                )\n            if cache_folder:\n                cache_path.mkdir(parents=True, exist_ok=True)\n                safe_write(cache_path / uuid.hex, blob)\n        return blob\n\n    # --- ATTACHMENTS ---\n\n    def upload_attachment(self, local_path):\n        attachment_name = Path(local_path).name\n        uuid = uuid_from_file(local_path, init_string=attachment_name + \"\\0\")\n        external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n        self._upload_file(local_path, external_path)\n        # insert tracking info\n        self.connection.query(\n            \"\"\"\n        INSERT INTO {tab} (hash, size, attachment_name)\n        VALUES (%s, {size}, \"{attachment_name}\")\n        ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\".format(\n                tab=self.full_table_name,\n                size=Path(local_path).stat().st_size,\n                attachment_name=attachment_name,\n            ),\n            args=[uuid.bytes],\n        )\n        return uuid\n\n    def get_attachment_name(self, uuid):\n        return (self &amp; {\"hash\": uuid}).fetch1(\"attachment_name\")\n\n    def download_attachment(self, uuid, attachment_name, download_path):\n        \"\"\"save attachment from memory buffer into the save_path\"\"\"\n        external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n        self._download_file(external_path, download_path)\n\n    # --- FILEPATH ---\n\n    def upload_filepath(self, local_filepath):\n        \"\"\"\n        Raise exception if an external entry already exists with a different contents checksum.\n        Otherwise, copy (with overwrite) file to remote and\n        If an external entry exists with the same checksum, then no copying should occur\n        \"\"\"\n        local_filepath = Path(local_filepath)\n        try:\n            relative_filepath = str(\n                local_filepath.relative_to(self.spec[\"stage\"]).as_posix()\n            )\n        except ValueError:\n            raise DataJointError(\n                \"The path {path} is not in stage {stage}\".format(\n                    path=local_filepath.parent, **self.spec\n                )\n            )\n        uuid = uuid_from_buffer(\n            init_string=relative_filepath\n        )  # hash relative path, not contents\n\n        # Check if checksum should be skipped based on file size limit\n        file_size = Path(local_filepath).stat().st_size\n        size_limit = config.get(\"filepath_checksum_size_limit_insert\")\n        skip_checksum = size_limit is not None and file_size &gt; size_limit\n\n        if skip_checksum:\n            contents_hash = None\n            logger.warning(\n                f\"Skipping checksum for '{relative_filepath}' ({file_size} bytes &gt; {size_limit} byte limit)\"\n            )\n        else:\n            contents_hash = uuid_from_file(local_filepath)\n\n        # check if the remote file already exists and verify that it matches\n        check_hash = (self &amp; {\"hash\": uuid}).fetch(\"contents_hash\")\n        if check_hash.size:\n            # the tracking entry exists, check that it's the same file as before\n            if not skip_checksum and contents_hash != check_hash[0]:\n                raise DataJointError(\n                    f\"A different version of '{relative_filepath}' has already been placed.\"\n                )\n        else:\n            # upload the file and create its tracking entry\n            self._upload_file(\n                local_filepath,\n                self._make_external_filepath(relative_filepath),\n                metadata={\"contents_hash\": str(contents_hash) if contents_hash else \"\"},\n            )\n            self.connection.query(\n                \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES (%s, {size}, '{filepath}', %s)\".format(\n                    tab=self.full_table_name,\n                    size=file_size,\n                    filepath=relative_filepath,\n                ),\n                args=(uuid.bytes, contents_hash.bytes if contents_hash else None),\n            )\n        return uuid\n\n    def download_filepath(self, filepath_hash):\n        \"\"\"\n        sync a file from external store to the local stage\n\n        :param filepath_hash: The hash (UUID) of the relative_path\n        :return: hash (UUID) of the contents of the downloaded file or Nones\n        \"\"\"\n\n        def _need_checksum(local_filepath, expected_size):\n            limit = config.get(\"filepath_checksum_size_limit\")\n            actual_size = Path(local_filepath).stat().st_size\n            if expected_size != actual_size:\n                # this should never happen without outside interference\n                raise DataJointError(\n                    f\"'{local_filepath}' downloaded but size did not match.\"\n                )\n            return limit is None or actual_size &lt; limit\n\n        if filepath_hash is not None:\n            relative_filepath, contents_hash, size = (\n                self &amp; {\"hash\": filepath_hash}\n            ).fetch1(\"filepath\", \"contents_hash\", \"size\")\n            external_path = self._make_external_filepath(relative_filepath)\n            local_filepath = Path(self.spec[\"stage\"]).absolute() / relative_filepath\n\n            file_exists = Path(local_filepath).is_file() and (\n                not _need_checksum(local_filepath, size)\n                or uuid_from_file(local_filepath) == contents_hash\n            )\n\n            if not file_exists:\n                self._download_file(external_path, local_filepath)\n                if (\n                    _need_checksum(local_filepath, size)\n                    and uuid_from_file(local_filepath) != contents_hash\n                ):\n                    # this should never happen without outside interference\n                    raise DataJointError(\n                        f\"'{local_filepath}' downloaded but did not pass checksum.\"\n                    )\n            if not _need_checksum(local_filepath, size):\n                logger.warning(\n                    f\"Skipped checksum for file with hash: {contents_hash}, and path: {local_filepath}\"\n                )\n            return str(local_filepath), contents_hash\n\n    # --- UTILITIES ---\n\n    @property\n    def references(self):\n        \"\"\"\n        :return: generator of referencing table names and their referencing columns\n        \"\"\"\n        return (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in self.connection.query(\n                \"\"\"\n        SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\"\n        \"\"\".format(\n                    tab=self.table_name, db=self.database\n                ),\n                as_dict=True,\n            )\n        )\n\n    def fetch_external_paths(self, **fetch_kwargs):\n        \"\"\"\n        generate complete external filepaths from the query.\n        Each element is a tuple: (uuid, path)\n\n        :param fetch_kwargs: keyword arguments to pass to fetch\n        \"\"\"\n        fetch_kwargs.update(as_dict=True)\n        paths = []\n        for item in self.fetch(\"hash\", \"attachment_name\", \"filepath\", **fetch_kwargs):\n            if item[\"attachment_name\"]:\n                # attachments\n                path = self._make_uuid_path(item[\"hash\"], \".\" + item[\"attachment_name\"])\n            elif item[\"filepath\"]:\n                # external filepaths\n                path = self._make_external_filepath(item[\"filepath\"])\n            else:\n                # blobs\n                path = self._make_uuid_path(item[\"hash\"])\n            paths.append((item[\"hash\"], path))\n        return paths\n\n    def unused(self):\n        \"\"\"\n        query expression for unused hashes\n\n        :return: self restricted to elements that are not in use by any tables in the schema\n        \"\"\"\n        return self - [\n            FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n                hash=ref[\"column_name\"]\n            )\n            for ref in self.references\n        ]\n\n    def used(self):\n        \"\"\"\n        query expression for used hashes\n\n        :return: self restricted to elements that in use by tables in the schema\n        \"\"\"\n        return self &amp; [\n            FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n                hash=ref[\"column_name\"]\n            )\n            for ref in self.references\n        ]\n\n    def delete(\n        self,\n        *,\n        delete_external_files=None,\n        limit=None,\n        display_progress=True,\n        errors_as_string=True,\n    ):\n        \"\"\"\n\n        :param delete_external_files: True or False. If False, only the tracking info is removed from the external\n                store table but the external files remain intact. If True, then the external files themselves are deleted too.\n        :param errors_as_string: If True any errors returned when deleting from external files will be strings\n        :param limit: (integer) limit the number of items to delete\n        :param display_progress: if True, display progress as files are cleaned up\n        :return: if deleting external files, returns errors\n        \"\"\"\n        if delete_external_files not in (True, False):\n            raise DataJointError(\n                \"The delete_external_files argument must be set to either \"\n                \"True or False in delete()\"\n            )\n\n        if not delete_external_files:\n            self.unused().delete_quick()\n        else:\n            items = self.unused().fetch_external_paths(limit=limit)\n            if display_progress:\n                items = tqdm(items)\n            # delete items one by one, close to transaction-safe\n            error_list = []\n            for uuid, external_path in items:\n                row = (self &amp; {\"hash\": uuid}).fetch()\n                if row.size:\n                    try:\n                        (self &amp; {\"hash\": uuid}).delete_quick()\n                    except Exception:\n                        pass  # if delete failed, do not remove the external file\n                    else:\n                        try:\n                            self._remove_external_file(external_path)\n                        except Exception as error:\n                            # adding row back into table after failed delete\n                            self.insert1(row[0], skip_duplicates=True)\n                            error_list.append(\n                                (\n                                    uuid,\n                                    external_path,\n                                    str(error) if errors_as_string else error,\n                                )\n                            )\n            return error_list\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.exists", "title": "<code>exists(external_filepath)</code>", "text": "<p>Returns:</p> Type Description <p>True if the external file is accessible</p> Source code in <code>datajoint/external.py</code> <pre><code>def exists(self, external_filepath):\n    \"\"\"\n    :return: True if the external file is accessible\n    \"\"\"\n    if self.spec[\"protocol\"] == \"s3\":\n        return self.s3.exists(external_filepath)\n    if self.spec[\"protocol\"] == \"file\":\n        return Path(external_filepath).is_file()\n    assert False\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.put", "title": "<code>put(blob)</code>", "text": "<p>put a binary string (blob) in external store</p> Source code in <code>datajoint/external.py</code> <pre><code>def put(self, blob):\n    \"\"\"\n    put a binary string (blob) in external store\n    \"\"\"\n    uuid = uuid_from_buffer(blob)\n    self._upload_buffer(blob, self._make_uuid_path(uuid))\n    # insert tracking info\n    self.connection.query(\n        \"INSERT INTO {tab} (hash, size) VALUES (%s, {size}) ON DUPLICATE KEY \"\n        \"UPDATE timestamp=CURRENT_TIMESTAMP\".format(\n            tab=self.full_table_name, size=len(blob)\n        ),\n        args=(uuid.bytes,),\n    )\n    return uuid\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.get", "title": "<code>get(uuid)</code>", "text": "<p>get an object from external store.</p> Source code in <code>datajoint/external.py</code> <pre><code>def get(self, uuid):\n    \"\"\"\n    get an object from external store.\n    \"\"\"\n    if uuid is None:\n        return None\n    # attempt to get object from cache\n    blob = None\n    cache_folder = config.get(\"cache\", None)\n    if cache_folder:\n        try:\n            cache_path = Path(cache_folder, *subfold(uuid.hex, CACHE_SUBFOLDING))\n            cache_file = Path(cache_path, uuid.hex)\n            blob = cache_file.read_bytes()\n        except FileNotFoundError:\n            pass  # not cached\n    # download blob from external store\n    if blob is None:\n        try:\n            blob = self._download_buffer(self._make_uuid_path(uuid))\n        except MissingExternalFile:\n            if not SUPPORT_MIGRATED_BLOBS:\n                raise\n            # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths\n            relative_filepath, contents_hash = (self &amp; {\"hash\": uuid}).fetch1(\n                \"filepath\", \"contents_hash\"\n            )\n            if relative_filepath is None:\n                raise\n            blob = self._download_buffer(\n                self._make_external_filepath(relative_filepath)\n            )\n        if cache_folder:\n            cache_path.mkdir(parents=True, exist_ok=True)\n            safe_write(cache_path / uuid.hex, blob)\n    return blob\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_attachment", "title": "<code>download_attachment(uuid, attachment_name, download_path)</code>", "text": "<p>save attachment from memory buffer into the save_path</p> Source code in <code>datajoint/external.py</code> <pre><code>def download_attachment(self, uuid, attachment_name, download_path):\n    \"\"\"save attachment from memory buffer into the save_path\"\"\"\n    external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n    self._download_file(external_path, download_path)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.upload_filepath", "title": "<code>upload_filepath(local_filepath)</code>", "text": "<p>Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur</p> Source code in <code>datajoint/external.py</code> <pre><code>def upload_filepath(self, local_filepath):\n    \"\"\"\n    Raise exception if an external entry already exists with a different contents checksum.\n    Otherwise, copy (with overwrite) file to remote and\n    If an external entry exists with the same checksum, then no copying should occur\n    \"\"\"\n    local_filepath = Path(local_filepath)\n    try:\n        relative_filepath = str(\n            local_filepath.relative_to(self.spec[\"stage\"]).as_posix()\n        )\n    except ValueError:\n        raise DataJointError(\n            \"The path {path} is not in stage {stage}\".format(\n                path=local_filepath.parent, **self.spec\n            )\n        )\n    uuid = uuid_from_buffer(\n        init_string=relative_filepath\n    )  # hash relative path, not contents\n\n    # Check if checksum should be skipped based on file size limit\n    file_size = Path(local_filepath).stat().st_size\n    size_limit = config.get(\"filepath_checksum_size_limit_insert\")\n    skip_checksum = size_limit is not None and file_size &gt; size_limit\n\n    if skip_checksum:\n        contents_hash = None\n        logger.warning(\n            f\"Skipping checksum for '{relative_filepath}' ({file_size} bytes &gt; {size_limit} byte limit)\"\n        )\n    else:\n        contents_hash = uuid_from_file(local_filepath)\n\n    # check if the remote file already exists and verify that it matches\n    check_hash = (self &amp; {\"hash\": uuid}).fetch(\"contents_hash\")\n    if check_hash.size:\n        # the tracking entry exists, check that it's the same file as before\n        if not skip_checksum and contents_hash != check_hash[0]:\n            raise DataJointError(\n                f\"A different version of '{relative_filepath}' has already been placed.\"\n            )\n    else:\n        # upload the file and create its tracking entry\n        self._upload_file(\n            local_filepath,\n            self._make_external_filepath(relative_filepath),\n            metadata={\"contents_hash\": str(contents_hash) if contents_hash else \"\"},\n        )\n        self.connection.query(\n            \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES (%s, {size}, '{filepath}', %s)\".format(\n                tab=self.full_table_name,\n                size=file_size,\n                filepath=relative_filepath,\n            ),\n            args=(uuid.bytes, contents_hash.bytes if contents_hash else None),\n        )\n    return uuid\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_filepath", "title": "<code>download_filepath(filepath_hash)</code>", "text": "<p>sync a file from external store to the local stage</p> <p>Parameters:</p> Name Type Description Default <code>filepath_hash</code> <p>The hash (UUID) of the relative_path</p> required <p>Returns:</p> Type Description <p>hash (UUID) of the contents of the downloaded file or Nones</p> Source code in <code>datajoint/external.py</code> <pre><code>def download_filepath(self, filepath_hash):\n    \"\"\"\n    sync a file from external store to the local stage\n\n    :param filepath_hash: The hash (UUID) of the relative_path\n    :return: hash (UUID) of the contents of the downloaded file or Nones\n    \"\"\"\n\n    def _need_checksum(local_filepath, expected_size):\n        limit = config.get(\"filepath_checksum_size_limit\")\n        actual_size = Path(local_filepath).stat().st_size\n        if expected_size != actual_size:\n            # this should never happen without outside interference\n            raise DataJointError(\n                f\"'{local_filepath}' downloaded but size did not match.\"\n            )\n        return limit is None or actual_size &lt; limit\n\n    if filepath_hash is not None:\n        relative_filepath, contents_hash, size = (\n            self &amp; {\"hash\": filepath_hash}\n        ).fetch1(\"filepath\", \"contents_hash\", \"size\")\n        external_path = self._make_external_filepath(relative_filepath)\n        local_filepath = Path(self.spec[\"stage\"]).absolute() / relative_filepath\n\n        file_exists = Path(local_filepath).is_file() and (\n            not _need_checksum(local_filepath, size)\n            or uuid_from_file(local_filepath) == contents_hash\n        )\n\n        if not file_exists:\n            self._download_file(external_path, local_filepath)\n            if (\n                _need_checksum(local_filepath, size)\n                and uuid_from_file(local_filepath) != contents_hash\n            ):\n                # this should never happen without outside interference\n                raise DataJointError(\n                    f\"'{local_filepath}' downloaded but did not pass checksum.\"\n                )\n        if not _need_checksum(local_filepath, size):\n            logger.warning(\n                f\"Skipped checksum for file with hash: {contents_hash}, and path: {local_filepath}\"\n            )\n        return str(local_filepath), contents_hash\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.references", "title": "<code>references</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>generator of referencing table names and their referencing columns</p>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.fetch_external_paths", "title": "<code>fetch_external_paths(**fetch_kwargs)</code>", "text": "<p>generate complete external filepaths from the query. Each element is a tuple: (uuid, path)</p> <p>Parameters:</p> Name Type Description Default <code>fetch_kwargs</code> <p>keyword arguments to pass to fetch</p> <code>{}</code> Source code in <code>datajoint/external.py</code> <pre><code>def fetch_external_paths(self, **fetch_kwargs):\n    \"\"\"\n    generate complete external filepaths from the query.\n    Each element is a tuple: (uuid, path)\n\n    :param fetch_kwargs: keyword arguments to pass to fetch\n    \"\"\"\n    fetch_kwargs.update(as_dict=True)\n    paths = []\n    for item in self.fetch(\"hash\", \"attachment_name\", \"filepath\", **fetch_kwargs):\n        if item[\"attachment_name\"]:\n            # attachments\n            path = self._make_uuid_path(item[\"hash\"], \".\" + item[\"attachment_name\"])\n        elif item[\"filepath\"]:\n            # external filepaths\n            path = self._make_external_filepath(item[\"filepath\"])\n        else:\n            # blobs\n            path = self._make_uuid_path(item[\"hash\"])\n        paths.append((item[\"hash\"], path))\n    return paths\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.unused", "title": "<code>unused()</code>", "text": "<p>query expression for unused hashes</p> <p>Returns:</p> Type Description <p>self restricted to elements that are not in use by any tables in the schema</p> Source code in <code>datajoint/external.py</code> <pre><code>def unused(self):\n    \"\"\"\n    query expression for unused hashes\n\n    :return: self restricted to elements that are not in use by any tables in the schema\n    \"\"\"\n    return self - [\n        FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n            hash=ref[\"column_name\"]\n        )\n        for ref in self.references\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.used", "title": "<code>used()</code>", "text": "<p>query expression for used hashes</p> <p>Returns:</p> Type Description <p>self restricted to elements that in use by tables in the schema</p> Source code in <code>datajoint/external.py</code> <pre><code>def used(self):\n    \"\"\"\n    query expression for used hashes\n\n    :return: self restricted to elements that in use by tables in the schema\n    \"\"\"\n    return self &amp; [\n        FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n            hash=ref[\"column_name\"]\n        )\n        for ref in self.references\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.delete", "title": "<code>delete(*, delete_external_files=None, limit=None, display_progress=True, errors_as_string=True)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>delete_external_files</code> <p>True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too.</p> <code>None</code> <code>errors_as_string</code> <p>If True any errors returned when deleting from external files will be strings</p> <code>True</code> <code>limit</code> <p>(integer) limit the number of items to delete</p> <code>None</code> <code>display_progress</code> <p>if True, display progress as files are cleaned up</p> <code>True</code> <p>Returns:</p> Type Description <p>if deleting external files, returns errors</p> Source code in <code>datajoint/external.py</code> <pre><code>def delete(\n    self,\n    *,\n    delete_external_files=None,\n    limit=None,\n    display_progress=True,\n    errors_as_string=True,\n):\n    \"\"\"\n\n    :param delete_external_files: True or False. If False, only the tracking info is removed from the external\n            store table but the external files remain intact. If True, then the external files themselves are deleted too.\n    :param errors_as_string: If True any errors returned when deleting from external files will be strings\n    :param limit: (integer) limit the number of items to delete\n    :param display_progress: if True, display progress as files are cleaned up\n    :return: if deleting external files, returns errors\n    \"\"\"\n    if delete_external_files not in (True, False):\n        raise DataJointError(\n            \"The delete_external_files argument must be set to either \"\n            \"True or False in delete()\"\n        )\n\n    if not delete_external_files:\n        self.unused().delete_quick()\n    else:\n        items = self.unused().fetch_external_paths(limit=limit)\n        if display_progress:\n            items = tqdm(items)\n        # delete items one by one, close to transaction-safe\n        error_list = []\n        for uuid, external_path in items:\n            row = (self &amp; {\"hash\": uuid}).fetch()\n            if row.size:\n                try:\n                    (self &amp; {\"hash\": uuid}).delete_quick()\n                except Exception:\n                    pass  # if delete failed, do not remove the external file\n                else:\n                    try:\n                        self._remove_external_file(external_path)\n                    except Exception as error:\n                        # adding row back into table after failed delete\n                        self.insert1(row[0], skip_duplicates=True)\n                        error_list.append(\n                            (\n                                uuid,\n                                external_path,\n                                str(error) if errors_as_string else error,\n                            )\n                        )\n        return error_list\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalMapping", "title": "<code>ExternalMapping</code>", "text": "<p>               Bases: <code>Mapping</code></p> <p>The external manager contains all the tables for all external stores for a given schema :Example:     e = ExternalMapping(schema)     external_table = e[store]</p> Source code in <code>datajoint/external.py</code> <pre><code>class ExternalMapping(Mapping):\n    \"\"\"\n    The external manager contains all the tables for all external stores for a given schema\n    :Example:\n        e = ExternalMapping(schema)\n        external_table = e[store]\n    \"\"\"\n\n    def __init__(self, schema):\n        self.schema = schema\n        self._tables = {}\n\n    def __repr__(self):\n        return \"External file tables for schema `{schema}`:\\n    \".format(\n            schema=self.schema.database\n        ) + \"\\n    \".join(\n            '\"{store}\" {protocol}:{location}'.format(store=k, **v.spec)\n            for k, v in self.items()\n        )\n\n    def __getitem__(self, store):\n        \"\"\"\n        Triggers the creation of an external table.\n        Should only be used when ready to save or read from external storage.\n\n        :param store: the name of the store\n        :return: the ExternalTable object for the store\n        \"\"\"\n        if store not in self._tables:\n            self._tables[store] = ExternalTable(\n                connection=self.schema.connection,\n                store=store,\n                database=self.schema.database,\n            )\n        return self._tables[store]\n\n    def __len__(self):\n        return len(self._tables)\n\n    def __iter__(self):\n        return iter(self._tables)\n</code></pre>"}, {"location": "api/datajoint/fetch/", "title": "fetch.py", "text": ""}, {"location": "api/datajoint/fetch/#datajoint.fetch.key", "title": "<code>key</code>", "text": "<p>object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key</p> Source code in <code>datajoint/fetch.py</code> <pre><code>class key:\n    \"\"\"\n    object that allows requesting the primary key as an argument in expression.fetch()\n    The string \"KEY\" can be used instead of the class key\n    \"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.to_dicts", "title": "<code>to_dicts(recarray)</code>", "text": "<p>convert record array to a dictionaries</p> Source code in <code>datajoint/fetch.py</code> <pre><code>def to_dicts(recarray):\n    \"\"\"convert record array to a dictionaries\"\"\"\n    for rec in recarray:\n        yield dict(zip(recarray.dtype.names, rec.tolist()))\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch", "title": "<code>Fetch</code>", "text": "<p>A fetch object that handles retrieving elements from the table expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>the QueryExpression object to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch:\n    \"\"\"\n    A fetch object that handles retrieving elements from the table expression.\n\n    :param expression: the QueryExpression object to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(\n        self,\n        *attrs,\n        offset=None,\n        limit=None,\n        order_by=None,\n        format=None,\n        as_dict=None,\n        squeeze=False,\n        download_path=\".\",\n    ):\n        \"\"\"\n        Fetches the expression results from the database into an np.array or list of dictionaries and\n        unpacks blob attributes.\n\n        :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this\n                        table. If provided, returns tuples with an entry for each attribute.\n        :param offset: the number of tuples to skip in the returned result\n        :param limit: the maximum number of tuples to return\n        :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed\n                        if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\",\n                        \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\"\n        :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or\n                        'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. .\n        :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to\n                        True for .fetch('KEY')\n        :param squeeze:  if True, remove extra dimensions from arrays\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the contents of the table in the form of a structured numpy.array or a dict list\n        \"\"\"\n        if offset or order_by or limit:\n            self._expression = self._expression.restrict(\n                Top(\n                    limit,\n                    order_by,\n                    offset,\n                )\n            )\n\n        attrs_as_dict = as_dict and attrs\n        if attrs_as_dict:\n            # absorb KEY into attrs and prepare to return attributes as dict (issue #595)\n            if any(is_key(k) for k in attrs):\n                attrs = list(self._expression.primary_key) + [\n                    a for a in attrs if a not in self._expression.primary_key\n                ]\n        if as_dict is None:\n            as_dict = bool(attrs)  # default to True for \"KEY\" and False otherwise\n        # format should not be specified with attrs or is_dict=True\n        if format is not None and (as_dict or attrs):\n            raise DataJointError(\n                \"Cannot specify output format when as_dict=True or \"\n                \"when attributes are selected to be fetched separately.\"\n            )\n        if format not in {None, \"array\", \"frame\"}:\n            raise DataJointError(\n                \"Fetch output format must be in \"\n                '{{\"array\", \"frame\"}} but \"{}\" was given'.format(format)\n            )\n\n        if not (attrs or as_dict) and format is None:\n            format = config[\"fetch_format\"]  # default to array\n            if format not in {\"array\", \"frame\"}:\n                raise DataJointError(\n                    'Invalid entry \"{}\" in datajoint.config[\"fetch_format\"]: '\n                    'use \"array\" or \"frame\"'.format(format)\n                )\n\n        get = partial(\n            _get,\n            self._expression.connection,\n            squeeze=squeeze,\n            download_path=download_path,\n        )\n        if attrs:  # a list of attributes provided\n            attributes = [a for a in attrs if not is_key(a)]\n            ret = self._expression.proj(*attributes)\n            ret = ret.fetch(\n                offset=offset,\n                limit=limit,\n                order_by=order_by,\n                as_dict=False,\n                squeeze=squeeze,\n                download_path=download_path,\n                format=\"array\",\n            )\n            if attrs_as_dict:\n                ret = [\n                    {k: v for k, v in zip(ret.dtype.names, x) if k in attrs}\n                    for x in ret\n                ]\n            else:\n                return_values = [\n                    (\n                        list(\n                            (to_dicts if as_dict else lambda x: x)(\n                                ret[self._expression.primary_key]\n                            )\n                        )\n                        if is_key(attribute)\n                        else ret[attribute]\n                    )\n                    for attribute in attrs\n                ]\n                ret = return_values[0] if len(attrs) == 1 else return_values\n        else:  # fetch all attributes as a numpy.record_array or pandas.DataFrame\n            cur = self._expression.cursor(as_dict=as_dict)\n            heading = self._expression.heading\n            if as_dict:\n                ret = [\n                    dict((name, get(heading[name], d[name])) for name in heading.names)\n                    for d in cur\n                ]\n            else:\n                ret = list(cur.fetchall())\n                record_type = (\n                    heading.as_dtype\n                    if not ret\n                    else np.dtype(\n                        [\n                            (\n                                (\n                                    name,\n                                    type(value),\n                                )  # use the first element to determine blob type\n                                if heading[name].is_blob\n                                and isinstance(value, numbers.Number)\n                                else (name, heading.as_dtype[name])\n                            )\n                            for value, name in zip(ret[0], heading.as_dtype.names)\n                        ]\n                    )\n                )\n                try:\n                    ret = np.array(ret, dtype=record_type)\n                except Exception as e:\n                    raise e\n                for name in heading:\n                    # unpack blobs and externals\n                    ret[name] = list(map(partial(get, heading[name]), ret[name]))\n                if format == \"frame\":\n                    ret = pandas.DataFrame(ret).set_index(heading.primary_key)\n        return ret\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch1", "title": "<code>Fetch1</code>", "text": "<p>Fetch object for fetching the result of a query yielding one row.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>a query expression to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch1:\n    \"\"\"\n    Fetch object for fetching the result of a query yielding one row.\n\n    :param expression: a query expression to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(self, *attrs, squeeze=False, download_path=\".\"):\n        \"\"\"\n        Fetches the result of a query expression that yields one entry.\n\n        If no attributes are specified, returns the result as a dict.\n        If attributes are specified returns the corresponding results as a tuple.\n\n        Examples:\n        d = rel.fetch1()   # as a dictionary\n        a, b = rel.fetch1('a', 'b')   # as a tuple\n\n        :params *attrs: attributes to return when expanding into a tuple.\n                 If attrs is empty, the return result is a dict\n        :param squeeze:  When true, remove extra dimensions from arrays in attributes\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the one tuple in the table in the form of a dict\n        \"\"\"\n        heading = self._expression.heading\n\n        if not attrs:  # fetch all attributes, return as ordered dict\n            cur = self._expression.cursor(as_dict=True)\n            ret = cur.fetchone()\n            if not ret or cur.fetchone():\n                raise DataJointError(\n                    \"fetch1 requires exactly one tuple in the input set.\"\n                )\n            ret = dict(\n                (\n                    name,\n                    _get(\n                        self._expression.connection,\n                        heading[name],\n                        ret[name],\n                        squeeze=squeeze,\n                        download_path=download_path,\n                    ),\n                )\n                for name in heading.names\n            )\n        else:  # fetch some attributes, return as tuple\n            attributes = [a for a in attrs if not is_key(a)]\n            result = self._expression.proj(*attributes).fetch(\n                squeeze=squeeze, download_path=download_path, format=\"array\"\n            )\n            if len(result) != 1:\n                raise DataJointError(\n                    \"fetch1 should only return one tuple. %d tuples found\" % len(result)\n                )\n            return_values = tuple(\n                (\n                    next(to_dicts(result[self._expression.primary_key]))\n                    if is_key(attribute)\n                    else result[attribute][0]\n                )\n                for attribute in attrs\n            )\n            ret = return_values[0] if len(attrs) == 1 else return_values\n        return ret\n</code></pre>"}, {"location": "api/datajoint/hash/", "title": "hash.py", "text": ""}, {"location": "api/datajoint/hash/#datajoint.hash.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n    \"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/hash/#datajoint.hash.uuid_from_stream", "title": "<code>uuid_from_stream(stream, *, init_string='')</code>", "text": "<p>:stream: stream object or open file handle :init_string: string to initialize the checksum</p> <p>Returns:</p> Type Description <p>16-byte digest of stream data</p> Source code in <code>datajoint/hash.py</code> <pre><code>def uuid_from_stream(stream, *, init_string=\"\"):\n    \"\"\"\n    :return: 16-byte digest of stream data\n    :stream: stream object or open file handle\n    :init_string: string to initialize the checksum\n    \"\"\"\n    hashed = hashlib.md5(init_string.encode())\n    chunk = True\n    chunk_size = 1 &lt;&lt; 14\n    while chunk:\n        chunk = stream.read(chunk_size)\n        hashed.update(chunk)\n    return uuid.UUID(bytes=hashed.digest())\n</code></pre>"}, {"location": "api/datajoint/heading/", "title": "heading.py", "text": ""}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute", "title": "<code>Attribute</code>", "text": "<p>               Bases: <code>namedtuple('_Attribute', default_attribute_properties)</code></p> <p>Properties of a table column (attribute)</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Attribute(namedtuple(\"_Attribute\", default_attribute_properties)):\n    \"\"\"\n    Properties of a table column (attribute)\n    \"\"\"\n\n    def todict(self):\n        \"\"\"Convert namedtuple to dict.\"\"\"\n        return dict((name, self[i]) for i, name in enumerate(self._fields))\n\n    @property\n    def sql_type(self):\n        \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\"\n        return UUID_DATA_TYPE if self.uuid else self.type\n\n    @property\n    def sql_comment(self):\n        \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\"\n        return (\":uuid:\" if self.uuid else \"\") + self.comment\n\n    @property\n    def sql(self):\n        \"\"\"\n        Convert primary key attribute tuple into its SQL CREATE TABLE clause.\n        Default values are not reflected.\n        This is used for declaring foreign keys in referencing tables\n\n        :return: SQL code for attribute declaration\n        \"\"\"\n        return '`{name}` {type} NOT NULL COMMENT \"{comment}\"'.format(\n            name=self.name, type=self.sql_type, comment=self.sql_comment\n        )\n\n    @property\n    def original_name(self):\n        if self.attribute_expression is None:\n            return self.name\n        assert self.attribute_expression.startswith(\"`\")\n        return self.attribute_expression.strip(\"`\")\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.todict", "title": "<code>todict()</code>", "text": "<p>Convert namedtuple to dict.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def todict(self):\n    \"\"\"Convert namedtuple to dict.\"\"\"\n    return dict((name, self[i]) for i, name in enumerate(self._fields))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_type", "title": "<code>sql_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>datatype (as string) in database. In most cases, it is the same as self.type</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_comment", "title": "<code>sql_comment</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full comment for the SQL declaration. Includes custom type specification</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql", "title": "<code>sql</code>  <code>property</code>", "text": "<p>Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables</p> <p>Returns:</p> Type Description <p>SQL code for attribute declaration</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n    \"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n        \"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return {k: v for k, v in self._attributes.items() if not v.is_hidden}\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n        \"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n        \"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n        \"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n        \"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            (\n                \"`%s`\" % name\n                if self.attributes[name].attribute_expression is None\n                else self.attributes[name].attribute_expression\n                + (\" as `%s`\" % name if include_aliases else \"\")\n            )\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n        \"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n                is_hidden=attr[\"name\"].startswith(\"_\"),\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n                        \"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=(\n                        attr[\"type\"].split(\"@\")[1]\n                        if category in EXTERNAL_TYPES\n                        else None\n                    ),\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n        \"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n        \"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n        \"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n        \"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n    \"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        (\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n        )\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n    \"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n    \"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n    \"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n    \"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/", "title": "jobs.py", "text": ""}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable", "title": "<code>JobTable</code>", "text": "<p>               Bases: <code>Table</code></p> <p>A base table with no definition. Allows reserving jobs</p> Source code in <code>datajoint/jobs.py</code> <pre><code>class JobTable(Table):\n    \"\"\"\n    A base table with no definition. Allows reserving jobs\n    \"\"\"\n\n    def __init__(self, conn, database):\n        self.database = database\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # job reservation table for `{database}`\n        table_name  :varchar(255)  # className of the table\n        key_hash  :char(32)  # key hash\n        ---\n        status  :enum('reserved','error','ignore')  # if tuple is missing, the job is available\n        key=null  :blob  # structure containing the key\n        error_message=\"\"  :varchar({error_message_length})  # error message returned if failed\n        error_stack=null  :mediumblob  # error stack if failed\n        user=\"\" :varchar(255) # database user\n        host=\"\"  :varchar(255)  # system hostname\n        pid=0  :int unsigned  # system process id\n        connection_id = 0  : bigint unsigned          # connection_id()\n        timestamp=CURRENT_TIMESTAMP  :timestamp   # automatic timestamp\n        \"\"\".format(\n            database=database, error_message_length=ERROR_MESSAGE_LENGTH\n        )\n        if not self.is_declared:\n            self.declare()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    @property\n    def table_name(self):\n        return \"~jobs\"\n\n    def delete(self):\n        \"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.delete_quick()\n\n    def drop(self):\n        \"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.drop_quick()\n\n    def reserve(self, table_name, key):\n        \"\"\"\n        Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n        job key, identified by its hash. When jobs are completed, the entry is removed.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :return: True if reserved job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"reserved\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def ignore(self, table_name, key):\n        \"\"\"\n        Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n        job key, identified by its hash, with status \"ignore\".\n\n        Args:\n        table_name:\n            Table name (str) - `database`.`table_name`\n        key:\n            The dict of the job's primary key\n\n        Returns:\n            True if ignore job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"ignore\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def complete(self, table_name, key):\n        \"\"\"\n        Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        \"\"\"\n        job_key = dict(table_name=table_name, key_hash=key_hash(key))\n        (self &amp; job_key).delete_quick()\n\n    def error(self, table_name, key, error_message, error_stack=None):\n        \"\"\"\n        Log an error message.  The job reservation is replaced with an error entry.\n        if an error occurs, leave an entry describing the problem\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :param error_message: string error message\n        :param error_stack: stack trace\n        \"\"\"\n        if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n            error_message = (\n                error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n                + TRUNCATION_APPENDIX\n            )\n        with config(enable_python_native_blobs=True):\n            self.insert1(\n                dict(\n                    table_name=table_name,\n                    key_hash=key_hash(key),\n                    status=\"error\",\n                    host=platform.node(),\n                    pid=os.getpid(),\n                    connection_id=self.connection.connection_id,\n                    user=self._user,\n                    key=key,\n                    error_message=error_message,\n                    error_stack=error_stack,\n                ),\n                replace=True,\n                ignore_extra_fields=True,\n            )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def delete(self):\n    \"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.delete_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def drop(self):\n    \"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.reserve", "title": "<code>reserve(table_name, key)</code>", "text": "<p>Reserve a job for computation.  When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <p>Returns:</p> Type Description <p>True if reserved job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def reserve(self, table_name, key):\n    \"\"\"\n    Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n    job key, identified by its hash. When jobs are completed, the entry is removed.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :return: True if reserved job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"reserved\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.ignore", "title": "<code>ignore(table_name, key)</code>", "text": "<p>Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\".</p> <p>Args: table_name:     Table name (str) - <code>database</code>.<code>table_name</code> key:     The dict of the job's primary key</p> <p>Returns:     True if ignore job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def ignore(self, table_name, key):\n    \"\"\"\n    Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n    job key, identified by its hash, with status \"ignore\".\n\n    Args:\n    table_name:\n        Table name (str) - `database`.`table_name`\n    key:\n        The dict of the job's primary key\n\n    Returns:\n        True if ignore job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"ignore\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.complete", "title": "<code>complete(table_name, key)</code>", "text": "<p>Log a completed job.  When a job is completed, its reservation entry is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required Source code in <code>datajoint/jobs.py</code> <pre><code>def complete(self, table_name, key):\n    \"\"\"\n    Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    \"\"\"\n    job_key = dict(table_name=table_name, key_hash=key_hash(key))\n    (self &amp; job_key).delete_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.error", "title": "<code>error(table_name, key, error_message, error_stack=None)</code>", "text": "<p>Log an error message.  The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <code>error_message</code> <p>string error message</p> required <code>error_stack</code> <p>stack trace</p> <code>None</code> Source code in <code>datajoint/jobs.py</code> <pre><code>def error(self, table_name, key, error_message, error_stack=None):\n    \"\"\"\n    Log an error message.  The job reservation is replaced with an error entry.\n    if an error occurs, leave an entry describing the problem\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :param error_message: string error message\n    :param error_stack: stack trace\n    \"\"\"\n    if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n        error_message = (\n            error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n            + TRUNCATION_APPENDIX\n        )\n    with config(enable_python_native_blobs=True):\n        self.insert1(\n            dict(\n                table_name=table_name,\n                key_hash=key_hash(key),\n                status=\"error\",\n                host=platform.node(),\n                pid=os.getpid(),\n                connection_id=self.connection.connection_id,\n                user=self._user,\n                key=key,\n                error_message=error_message,\n                error_stack=error_stack,\n            ),\n            replace=True,\n            ignore_extra_fields=True,\n        )\n</code></pre>"}, {"location": "api/datajoint/logging/", "title": "logging.py", "text": ""}, {"location": "api/datajoint/plugin/", "title": "plugin.py", "text": ""}, {"location": "api/datajoint/preview/", "title": "preview.py", "text": "<p>methods for generating previews of query expression results in python command line and Jupyter</p>"}, {"location": "api/datajoint/s3/", "title": "s3.py", "text": "<p>AWS S3 operations</p>"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder", "title": "<code>Folder</code>", "text": "<p>A Folder instance manipulates a flat folder of objects within an S3-compatible object store</p> Source code in <code>datajoint/s3.py</code> <pre><code>class Folder:\n    \"\"\"\n    A Folder instance manipulates a flat folder of objects within an S3-compatible object store\n    \"\"\"\n\n    def __init__(\n        self,\n        endpoint,\n        bucket,\n        access_key,\n        secret_key,\n        *,\n        secure=False,\n        proxy_server=None,\n        **_,\n    ):\n        # from https://docs.min.io/docs/python-client-api-reference\n        self.client = minio.Minio(\n            endpoint,\n            access_key=access_key,\n            secret_key=secret_key,\n            secure=secure,\n            http_client=(\n                urllib3.ProxyManager(\n                    proxy_server,\n                    timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n                    cert_reqs=\"CERT_REQUIRED\",\n                    retries=urllib3.Retry(\n                        total=5,\n                        backoff_factor=0.2,\n                        status_forcelist=[500, 502, 503, 504],\n                    ),\n                )\n                if proxy_server\n                else None\n            ),\n        )\n        self.bucket = bucket\n        if not self.client.bucket_exists(bucket):\n            raise errors.BucketInaccessible(\"Inaccessible s3 bucket %s\" % bucket)\n\n    def put(self, name, buffer):\n        logger.debug(\"put: {}:{}\".format(self.bucket, name))\n        return self.client.put_object(\n            self.bucket, str(name), BytesIO(buffer), length=len(buffer)\n        )\n\n    def fput(self, local_file, name, metadata=None):\n        logger.debug(\"fput: {} -&gt; {}:{}\".format(self.bucket, local_file, name))\n        return self.client.fput_object(\n            self.bucket, str(name), str(local_file), metadata=metadata\n        )\n\n    def get(self, name):\n        logger.debug(\"get: {}:{}\".format(self.bucket, name))\n        try:\n            with self.client.get_object(self.bucket, str(name)) as result:\n                data = [d for d in result.stream()]\n            return b\"\".join(data)\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                raise errors.MissingExternalFile(\"Missing s3 key %s\" % name)\n            else:\n                raise e\n\n    def fget(self, name, local_filepath):\n        \"\"\"get file from object name to local filepath\"\"\"\n        logger.debug(\"fget: {}:{}\".format(self.bucket, name))\n        name = str(name)\n        stat = self.client.stat_object(self.bucket, name)\n        meta = {k.lower().lstrip(\"x-amz-meta\"): v for k, v in stat.metadata.items()}\n        data = self.client.get_object(self.bucket, name)\n        local_filepath = Path(local_filepath)\n        local_filepath.parent.mkdir(parents=True, exist_ok=True)\n        with local_filepath.open(\"wb\") as f:\n            for d in data.stream(1 &lt;&lt; 16):\n                f.write(d)\n        if \"contents_hash\" in meta:\n            return uuid.UUID(meta[\"contents_hash\"])\n\n    def exists(self, name):\n        logger.debug(\"exists: {}:{}\".format(self.bucket, name))\n        try:\n            self.client.stat_object(self.bucket, str(name))\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                return False\n            else:\n                raise e\n        return True\n\n    def get_size(self, name):\n        logger.debug(\"get_size: {}:{}\".format(self.bucket, name))\n        try:\n            return self.client.stat_object(self.bucket, str(name)).size\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                raise errors.MissingExternalFile\n            raise e\n\n    def remove_object(self, name):\n        logger.debug(\"remove_object: {}:{}\".format(self.bucket, name))\n        try:\n            self.client.remove_object(self.bucket, str(name))\n        except minio.error.MinioException:\n            raise errors.DataJointError(\"Failed to delete %s from s3 storage\" % name)\n</code></pre>"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder.fget", "title": "<code>fget(name, local_filepath)</code>", "text": "<p>get file from object name to local filepath</p> Source code in <code>datajoint/s3.py</code> <pre><code>def fget(self, name, local_filepath):\n    \"\"\"get file from object name to local filepath\"\"\"\n    logger.debug(\"fget: {}:{}\".format(self.bucket, name))\n    name = str(name)\n    stat = self.client.stat_object(self.bucket, name)\n    meta = {k.lower().lstrip(\"x-amz-meta\"): v for k, v in stat.metadata.items()}\n    data = self.client.get_object(self.bucket, name)\n    local_filepath = Path(local_filepath)\n    local_filepath.parent.mkdir(parents=True, exist_ok=True)\n    with local_filepath.open(\"wb\") as f:\n        for d in data.stream(1 &lt;&lt; 16):\n            f.write(d)\n    if \"contents_hash\" in meta:\n        return uuid.UUID(meta[\"contents_hash\"])\n</code></pre>"}, {"location": "api/datajoint/schemas/", "title": "schemas.py", "text": ""}, {"location": "api/datajoint/schemas/#datajoint.schemas.ordered_dir", "title": "<code>ordered_dir(class_)</code>", "text": "<p>List (most) attributes of the class including inherited ones, similar to <code>dir</code> built-in function, but respects order of attribute declaration as much as possible.</p> <p>Parameters:</p> Name Type Description Default <code>class_</code> <p>class to list members for</p> required <p>Returns:</p> Type Description <p>a list of attributes declared in class_ and its superclasses</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def ordered_dir(class_):\n    \"\"\"\n    List (most) attributes of the class including inherited ones, similar to `dir` built-in function,\n    but respects order of attribute declaration as much as possible.\n\n    :param class_: class to list members for\n    :return: a list of attributes declared in class_ and its superclasses\n    \"\"\"\n    attr_list = list()\n    for c in reversed(class_.mro()):\n        attr_list.extend(e for e in c.__dict__ if e not in attr_list)\n    return attr_list\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema", "title": "<code>Schema</code>", "text": "<p>A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace <code>context</code> in which other UserTable classes are defined.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class Schema:\n    \"\"\"\n    A schema object is a decorator for UserTable classes that binds them to their database.\n    It also specifies the namespace `context` in which other UserTable classes are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_name=None,\n        context=None,\n        *,\n        connection=None,\n        create_schema=True,\n        create_tables=True,\n        add_objects=None,\n    ):\n        \"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        If the schema_name is omitted, then schema.activate(..) must be called later\n        to associate with the database.\n\n        :param schema_name: the database schema to associate.\n        :param context: dictionary for looking up foreign key references, leave None to use local context.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: When False, do not create the schema and raise an error if missing.\n        :param create_tables: When False, do not create tables and raise errors when accessing missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context in which table classes\n        are declared.\n        \"\"\"\n        self._log = None\n        self.connection = connection\n        self.database = None\n        self.context = context\n        self.create_schema = create_schema\n        self.create_tables = create_tables\n        self._jobs = None\n        self.external = ExternalMapping(self)\n        self.add_objects = add_objects\n        self.declare_list = []\n        if schema_name:\n            self.activate(schema_name)\n\n    def is_activated(self):\n        return self.database is not None\n\n    def activate(\n        self,\n        schema_name=None,\n        *,\n        connection=None,\n        create_schema=None,\n        create_tables=None,\n        add_objects=None,\n    ):\n        \"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        :param schema_name: the database schema to associate.\n            schema_name=None is used to assert that the schema has already been activated.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: If False, do not create the schema and raise an error if missing.\n        :param create_tables: If False, do not create tables and raise errors when attempting\n            to access missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context\n            in which table classes are declared.\n        \"\"\"\n        if schema_name is None:\n            if self.exists:\n                return\n            raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n        if self.database is not None and self.exists:\n            if self.database == schema_name:  # already activated\n                return\n            raise DataJointError(\n                \"The schema is already activated for schema {db}.\".format(\n                    db=self.database\n                )\n            )\n        if connection is not None:\n            self.connection = connection\n        if self.connection is None:\n            self.connection = conn()\n        self.database = schema_name\n        if create_schema is not None:\n            self.create_schema = create_schema\n        if create_tables is not None:\n            self.create_tables = create_tables\n        if add_objects:\n            self.add_objects = add_objects\n        if not self.exists:\n            if not self.create_schema or not self.database:\n                raise DataJointError(\n                    \"Database `{name}` has not yet been declared. \"\n                    \"Set argument create_schema=True to create it.\".format(\n                        name=schema_name\n                    )\n                )\n            # create database\n            logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n            try:\n                self.connection.query(\n                    \"CREATE DATABASE `{name}`\".format(name=schema_name)\n                )\n            except AccessError:\n                raise DataJointError(\n                    \"Schema `{name}` does not exist and could not be created. \"\n                    \"Check permissions.\".format(name=schema_name)\n                )\n            else:\n                self.log(\"created\")\n        self.connection.register(self)\n\n        # decorate all tables already decorated\n        for cls, context in self.declare_list:\n            if self.add_objects:\n                context = dict(context, **self.add_objects)\n            self._decorate_master(cls, context)\n\n    def _assert_exists(self, message=None):\n        if not self.exists:\n            raise DataJointError(\n                message\n                or \"Schema `{db}` has not been created.\".format(db=self.database)\n            )\n\n    def __call__(self, cls, *, context=None):\n        \"\"\"\n        Binds the supplied class to a schema. This is intended to be used as a decorator.\n\n        :param cls: class to decorate.\n        :param context: supplied when called from spawn_missing_classes\n        \"\"\"\n        context = context or self.context or inspect.currentframe().f_back.f_locals\n        if issubclass(cls, Part):\n            raise DataJointError(\n                \"The schema decorator should not be applied to Part tables.\"\n            )\n        if self.is_activated():\n            self._decorate_master(cls, context)\n        else:\n            self.declare_list.append((cls, context))\n        return cls\n\n    def _decorate_master(self, cls, context):\n        \"\"\"\n\n        :param cls: the master class to process\n        :param context: the class' declaration context\n        \"\"\"\n        self._decorate_table(\n            cls, context=dict(context, self=cls, **{cls.__name__: cls})\n        )\n        # Process part tables\n        for part in ordered_dir(cls):\n            if part[0].isupper():\n                part = getattr(cls, part)\n                if inspect.isclass(part) and issubclass(part, Part):\n                    part._master = cls\n                    # allow addressing master by name or keyword 'master'\n                    self._decorate_table(\n                        part,\n                        context=dict(\n                            context, master=cls, self=part, **{cls.__name__: cls}\n                        ),\n                    )\n\n    def _decorate_table(self, table_class, context, assert_declared=False):\n        \"\"\"\n        assign schema properties to the table class and declare the table\n        \"\"\"\n        table_class.database = self.database\n        table_class._connection = self.connection\n        table_class._heading = Heading(\n            table_info=dict(\n                conn=self.connection,\n                database=self.database,\n                table_name=table_class.table_name,\n                context=context,\n            )\n        )\n        table_class._support = [table_class.full_table_name]\n        table_class.declaration_context = context\n\n        # instantiate the class, declare the table if not already\n        instance = table_class()\n        is_declared = instance.is_declared\n        if not is_declared and not assert_declared and self.create_tables:\n            instance.declare(context)\n            self.connection.dependencies.clear()\n        is_declared = is_declared or instance.is_declared\n\n        # add table definition to the doc string\n        if isinstance(table_class.definition, str):\n            table_class.__doc__ = (\n                (table_class.__doc__ or \"\")\n                + \"\\nTable definition:\\n\\n\"\n                + table_class.definition\n            )\n\n        # fill values in Lookup tables from their contents property\n        if (\n            isinstance(instance, Lookup)\n            and hasattr(instance, \"contents\")\n            and is_declared\n        ):\n            contents = list(instance.contents)\n            if len(contents) &gt; len(instance):\n                if instance.heading.has_autoincrement:\n                    warnings.warn(\n                        (\n                            \"Contents has changed but cannot be inserted because \"\n                            \"{table} has autoincrement.\"\n                        ).format(table=instance.__class__.__name__)\n                    )\n                else:\n                    instance.insert(contents, skip_duplicates=True)\n\n    @property\n    def log(self):\n        self._assert_exists()\n        if self._log is None:\n            self._log = Log(self.connection, self.database)\n        return self._log\n\n    def __repr__(self):\n        return \"Schema `{name}`\\n\".format(name=self.database)\n\n    @property\n    def size_on_disk(self):\n        \"\"\"\n        :return: size of the entire schema in bytes\n        \"\"\"\n        self._assert_exists()\n        return int(\n            self.connection.query(\n                \"\"\"\n            SELECT SUM(data_length + index_length)\n            FROM information_schema.tables WHERE table_schema='{db}'\n            \"\"\".format(\n                    db=self.database\n                )\n            ).fetchone()[0]\n        )\n\n    def spawn_missing_classes(self, context=None):\n        \"\"\"\n        Creates the appropriate python user table classes from tables in the schema and places them\n        in the context.\n\n        :param context: alternative context to place the missing classes into, e.g. locals()\n        \"\"\"\n        self._assert_exists()\n        if context is None:\n            if self.context is not None:\n                context = self.context\n            else:\n                # if context is missing, use the calling namespace\n                frame = inspect.currentframe().f_back\n                context = frame.f_locals\n                del frame\n        tables = [\n            row[0]\n            for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n            if lookup_class_name(\n                \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n            )\n            is None\n        ]\n        master_classes = (Lookup, Manual, Imported, Computed)\n        part_tables = []\n        for table_name in tables:\n            class_name = to_camel_case(table_name)\n            if class_name not in context:\n                try:\n                    cls = next(\n                        cls\n                        for cls in master_classes\n                        if re.fullmatch(cls.tier_regexp, table_name)\n                    )\n                except StopIteration:\n                    if re.fullmatch(Part.tier_regexp, table_name):\n                        part_tables.append(table_name)\n                else:\n                    # declare and decorate master table classes\n                    context[class_name] = self(\n                        type(class_name, (cls,), dict()), context=context\n                    )\n\n        # attach parts to masters\n        for table_name in part_tables:\n            groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n            class_name = to_camel_case(groups[\"part\"])\n            try:\n                master_class = context[to_camel_case(groups[\"master\"])]\n            except KeyError:\n                raise DataJointError(\n                    \"The table %s does not follow DataJoint naming conventions\"\n                    % table_name\n                )\n            part_class = type(class_name, (Part,), dict(definition=...))\n            part_class._master = master_class\n            self._decorate_table(part_class, context=context, assert_declared=True)\n            setattr(master_class, class_name, part_class)\n\n    def drop(self, force=False):\n        \"\"\"\n        Drop the associated schema if it exists\n        \"\"\"\n        if not self.exists:\n            logger.info(\n                \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                    database=self.database\n                )\n            )\n        elif (\n            not config[\"safemode\"]\n            or force\n            or user_choice(\n                \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n            )\n            == \"yes\"\n        ):\n            logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n            try:\n                self.connection.query(\n                    \"DROP DATABASE `{database}`\".format(database=self.database)\n                )\n                logger.debug(\n                    \"Schema `{database}` was dropped successfully.\".format(\n                        database=self.database\n                    )\n                )\n            except AccessError:\n                raise AccessError(\n                    \"An attempt to drop schema `{database}` \"\n                    \"has failed. Check permissions.\".format(database=self.database)\n                )\n\n    @property\n    def exists(self):\n        \"\"\"\n        :return: true if the associated schema exists on the server\n        \"\"\"\n        if self.database is None:\n            raise DataJointError(\"Schema must be activated first.\")\n        return bool(\n            self.connection.query(\n                \"SELECT schema_name \"\n                \"FROM information_schema.schemata \"\n                \"WHERE schema_name = '{database}'\".format(database=self.database)\n            ).rowcount\n        )\n\n    @property\n    def jobs(self):\n        \"\"\"\n        schema.jobs provides a view of the job reservation table for the schema\n\n        :return: jobs table\n        \"\"\"\n        self._assert_exists()\n        if self._jobs is None:\n            self._jobs = JobTable(self.connection, self.database)\n        return self._jobs\n\n    @property\n    def code(self):\n        self._assert_exists()\n        return self.save()\n\n    def save(self, python_filename=None):\n        \"\"\"\n        Generate the code for a module that recreates the schema.\n        This method is in preparation for a future release and is not officially supported.\n\n        :return: a string containing the body of a complete Python module defining this schema.\n        \"\"\"\n        self.connection.dependencies.load()\n        self._assert_exists()\n        module_count = itertools.count()\n        # add virtual modules for referenced modules with names vmod0, vmod1, ...\n        module_lookup = collections.defaultdict(\n            lambda: \"vmod\" + str(next(module_count))\n        )\n        db = self.database\n\n        def make_class_definition(table):\n            tier = _get_tier(table).__name__\n            class_name = table.split(\".\")[1].strip(\"`\")\n            indent = \"\"\n            if tier == \"Part\":\n                class_name = class_name.split(\"__\")[-1]\n                indent += \"    \"\n            class_name = to_camel_case(class_name)\n\n            def replace(s):\n                d, tabs = s.group(1), s.group(2)\n                return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                    to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n                )\n\n            return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n                \"{indent}class {class_name}(dj.{tier}):\\n\"\n                '{indent}    definition = \"\"\"\\n'\n                '{indent}    {defi}\"\"\"'\n            ).format(\n                class_name=class_name,\n                indent=indent,\n                tier=tier,\n                defi=re.sub(\n                    r\"`([^`]+)`.`([^`]+)`\",\n                    replace,\n                    FreeTable(self.connection, table).describe(),\n                ).replace(\"\\n\", \"\\n    \" + indent),\n            )\n\n        tables = self.connection.dependencies.topo_sort()\n        body = \"\\n\\n\".join(make_class_definition(table) for table in tables)\n        python_code = \"\\n\\n\".join(\n            (\n                '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n                \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n                \"\\n\".join(\n                    \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                        module=v, schema_name=k\n                    )\n                    for k, v in module_lookup.items()\n                ),\n                body,\n            )\n        )\n        if python_filename is None:\n            return python_code\n        with open(python_filename, \"wt\") as f:\n            f.write(python_code)\n\n    def list_tables(self):\n        \"\"\"\n        Return a list of all tables in the schema except tables with ~ in first character such\n        as ~logs and ~job\n\n        :return: A list of table names from the database schema.\n        \"\"\"\n        self.connection.dependencies.load()\n        return [\n            t\n            for d, t in (\n                table_name.replace(\"`\", \"\").split(\".\")\n                for table_name in self.connection.dependencies.topo_sort()\n            )\n            if d == self.database\n        ]\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.activate", "title": "<code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)</code>", "text": "<p>Associate database schema <code>schema_name</code>. If the schema does not exist, attempt to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <p>the database schema to associate. schema_name=None is used to assert that the schema has already been activated.</p> <code>None</code> <code>connection</code> <p>Connection object. Defaults to datajoint.conn().</p> <code>None</code> <code>create_schema</code> <p>If False, do not create the schema and raise an error if missing.</p> <code>None</code> <code>create_tables</code> <p>If False, do not create tables and raise errors when attempting to access missing tables.</p> <code>None</code> <code>add_objects</code> <p>a mapping with additional objects to make available to the context in which table classes are declared.</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def activate(\n    self,\n    schema_name=None,\n    *,\n    connection=None,\n    create_schema=None,\n    create_tables=None,\n    add_objects=None,\n):\n    \"\"\"\n    Associate database schema `schema_name`. If the schema does not exist, attempt to\n    create it on the server.\n\n    :param schema_name: the database schema to associate.\n        schema_name=None is used to assert that the schema has already been activated.\n    :param connection: Connection object. Defaults to datajoint.conn().\n    :param create_schema: If False, do not create the schema and raise an error if missing.\n    :param create_tables: If False, do not create tables and raise errors when attempting\n        to access missing tables.\n    :param add_objects: a mapping with additional objects to make available to the context\n        in which table classes are declared.\n    \"\"\"\n    if schema_name is None:\n        if self.exists:\n            return\n        raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n    if self.database is not None and self.exists:\n        if self.database == schema_name:  # already activated\n            return\n        raise DataJointError(\n            \"The schema is already activated for schema {db}.\".format(\n                db=self.database\n            )\n        )\n    if connection is not None:\n        self.connection = connection\n    if self.connection is None:\n        self.connection = conn()\n    self.database = schema_name\n    if create_schema is not None:\n        self.create_schema = create_schema\n    if create_tables is not None:\n        self.create_tables = create_tables\n    if add_objects:\n        self.add_objects = add_objects\n    if not self.exists:\n        if not self.create_schema or not self.database:\n            raise DataJointError(\n                \"Database `{name}` has not yet been declared. \"\n                \"Set argument create_schema=True to create it.\".format(\n                    name=schema_name\n                )\n            )\n        # create database\n        logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n        try:\n            self.connection.query(\n                \"CREATE DATABASE `{name}`\".format(name=schema_name)\n            )\n        except AccessError:\n            raise DataJointError(\n                \"Schema `{name}` does not exist and could not be created. \"\n                \"Check permissions.\".format(name=schema_name)\n            )\n        else:\n            self.log(\"created\")\n    self.connection.register(self)\n\n    # decorate all tables already decorated\n    for cls, context in self.declare_list:\n        if self.add_objects:\n            context = dict(context, **self.add_objects)\n        self._decorate_master(cls, context)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of the entire schema in bytes</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.spawn_missing_classes", "title": "<code>spawn_missing_classes(context=None)</code>", "text": "<p>Creates the appropriate python user table classes from tables in the schema and places them in the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>alternative context to place the missing classes into, e.g. locals()</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def spawn_missing_classes(self, context=None):\n    \"\"\"\n    Creates the appropriate python user table classes from tables in the schema and places them\n    in the context.\n\n    :param context: alternative context to place the missing classes into, e.g. locals()\n    \"\"\"\n    self._assert_exists()\n    if context is None:\n        if self.context is not None:\n            context = self.context\n        else:\n            # if context is missing, use the calling namespace\n            frame = inspect.currentframe().f_back\n            context = frame.f_locals\n            del frame\n    tables = [\n        row[0]\n        for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n        if lookup_class_name(\n            \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n        )\n        is None\n    ]\n    master_classes = (Lookup, Manual, Imported, Computed)\n    part_tables = []\n    for table_name in tables:\n        class_name = to_camel_case(table_name)\n        if class_name not in context:\n            try:\n                cls = next(\n                    cls\n                    for cls in master_classes\n                    if re.fullmatch(cls.tier_regexp, table_name)\n                )\n            except StopIteration:\n                if re.fullmatch(Part.tier_regexp, table_name):\n                    part_tables.append(table_name)\n            else:\n                # declare and decorate master table classes\n                context[class_name] = self(\n                    type(class_name, (cls,), dict()), context=context\n                )\n\n    # attach parts to masters\n    for table_name in part_tables:\n        groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n        class_name = to_camel_case(groups[\"part\"])\n        try:\n            master_class = context[to_camel_case(groups[\"master\"])]\n        except KeyError:\n            raise DataJointError(\n                \"The table %s does not follow DataJoint naming conventions\"\n                % table_name\n            )\n        part_class = type(class_name, (Part,), dict(definition=...))\n        part_class._master = master_class\n        self._decorate_table(part_class, context=context, assert_declared=True)\n        setattr(master_class, class_name, part_class)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.drop", "title": "<code>drop(force=False)</code>", "text": "<p>Drop the associated schema if it exists</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def drop(self, force=False):\n    \"\"\"\n    Drop the associated schema if it exists\n    \"\"\"\n    if not self.exists:\n        logger.info(\n            \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                database=self.database\n            )\n        )\n    elif (\n        not config[\"safemode\"]\n        or force\n        or user_choice(\n            \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n        )\n        == \"yes\"\n    ):\n        logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n        try:\n            self.connection.query(\n                \"DROP DATABASE `{database}`\".format(database=self.database)\n            )\n            logger.debug(\n                \"Schema `{database}` was dropped successfully.\".format(\n                    database=self.database\n                )\n            )\n        except AccessError:\n            raise AccessError(\n                \"An attempt to drop schema `{database}` \"\n                \"has failed. Check permissions.\".format(database=self.database)\n            )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.exists", "title": "<code>exists</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>true if the associated schema exists on the server</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.jobs", "title": "<code>jobs</code>  <code>property</code>", "text": "<p>schema.jobs provides a view of the job reservation table for the schema</p> <p>Returns:</p> Type Description <p>jobs table</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.save", "title": "<code>save(python_filename=None)</code>", "text": "<p>Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported.</p> <p>Returns:</p> Type Description <p>a string containing the body of a complete Python module defining this schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def save(self, python_filename=None):\n    \"\"\"\n    Generate the code for a module that recreates the schema.\n    This method is in preparation for a future release and is not officially supported.\n\n    :return: a string containing the body of a complete Python module defining this schema.\n    \"\"\"\n    self.connection.dependencies.load()\n    self._assert_exists()\n    module_count = itertools.count()\n    # add virtual modules for referenced modules with names vmod0, vmod1, ...\n    module_lookup = collections.defaultdict(\n        lambda: \"vmod\" + str(next(module_count))\n    )\n    db = self.database\n\n    def make_class_definition(table):\n        tier = _get_tier(table).__name__\n        class_name = table.split(\".\")[1].strip(\"`\")\n        indent = \"\"\n        if tier == \"Part\":\n            class_name = class_name.split(\"__\")[-1]\n            indent += \"    \"\n        class_name = to_camel_case(class_name)\n\n        def replace(s):\n            d, tabs = s.group(1), s.group(2)\n            return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n            )\n\n        return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n            \"{indent}class {class_name}(dj.{tier}):\\n\"\n            '{indent}    definition = \"\"\"\\n'\n            '{indent}    {defi}\"\"\"'\n        ).format(\n            class_name=class_name,\n            indent=indent,\n            tier=tier,\n            defi=re.sub(\n                r\"`([^`]+)`.`([^`]+)`\",\n                replace,\n                FreeTable(self.connection, table).describe(),\n            ).replace(\"\\n\", \"\\n    \" + indent),\n        )\n\n    tables = self.connection.dependencies.topo_sort()\n    body = \"\\n\\n\".join(make_class_definition(table) for table in tables)\n    python_code = \"\\n\\n\".join(\n        (\n            '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n            \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n            \"\\n\".join(\n                \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                    module=v, schema_name=k\n                )\n                for k, v in module_lookup.items()\n            ),\n            body,\n        )\n    )\n    if python_filename is None:\n        return python_code\n    with open(python_filename, \"wt\") as f:\n        f.write(python_code)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.list_tables", "title": "<code>list_tables()</code>", "text": "<p>Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job</p> <p>Returns:</p> Type Description <p>A list of table names from the database schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_tables(self):\n    \"\"\"\n    Return a list of all tables in the schema except tables with ~ in first character such\n    as ~logs and ~job\n\n    :return: A list of table names from the database schema.\n    \"\"\"\n    self.connection.dependencies.load()\n    return [\n        t\n        for d, t in (\n            table_name.replace(\"`\", \"\").split(\".\")\n            for table_name in self.connection.dependencies.topo_sort()\n        )\n        if d == self.database\n    ]\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.VirtualModule", "title": "<code>VirtualModule</code>", "text": "<p>               Bases: <code>ModuleType</code></p> <p>A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class VirtualModule(types.ModuleType):\n    \"\"\"\n    A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database.\n    It declares the schema objects and a class for each table.\n    \"\"\"\n\n    def __init__(\n        self,\n        module_name,\n        schema_name,\n        *,\n        create_schema=False,\n        create_tables=False,\n        connection=None,\n        add_objects=None,\n    ):\n        \"\"\"\n        Creates a python module with the given name from the name of a schema on the server and\n        automatically adds classes to it corresponding to the tables in the schema.\n\n        :param module_name: displayed module name\n        :param schema_name: name of the database in mysql\n        :param create_schema: if True, create the schema on the database server\n        :param create_tables: if True, module.schema can be used as the decorator for declaring new\n        :param connection: a dj.Connection object to pass into the schema\n        :param add_objects: additional objects to add to the module\n        :return: the python module containing classes from the schema object and the table classes\n        \"\"\"\n        super(VirtualModule, self).__init__(name=module_name)\n        _schema = Schema(\n            schema_name,\n            create_schema=create_schema,\n            create_tables=create_tables,\n            connection=connection,\n        )\n        if add_objects:\n            self.__dict__.update(add_objects)\n        self.__dict__[\"schema\"] = _schema\n        _schema.spawn_missing_classes(context=self.__dict__)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.list_schemas", "title": "<code>list_schemas(connection=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>connection</code> <p>a dj.Connection object</p> <code>None</code> <p>Returns:</p> Type Description <p>list of all accessible schemas on the server</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_schemas(connection=None):\n    \"\"\"\n    :param connection: a dj.Connection object\n    :return: list of all accessible schemas on the server\n    \"\"\"\n    return [\n        r[0]\n        for r in (connection or conn()).query(\n            \"SELECT schema_name \"\n            \"FROM information_schema.schemata \"\n            'WHERE schema_name &lt;&gt; \"information_schema\"'\n        )\n    ]\n</code></pre>"}, {"location": "api/datajoint/settings/", "title": "settings.py", "text": "<p>Settings for DataJoint</p>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config", "title": "<code>Config</code>", "text": "<p>               Bases: <code>MutableMapping</code></p> Source code in <code>datajoint/settings.py</code> <pre><code>class Config(collections.abc.MutableMapping):\n    instance = None\n\n    def __init__(self, *args, **kwargs):\n        if not Config.instance:\n            Config.instance = Config.__Config(*args, **kwargs)\n        else:\n            Config.instance._conf.update(dict(*args, **kwargs))\n\n    def __getattr__(self, name):\n        return getattr(self.instance, name)\n\n    def __getitem__(self, item):\n        return self.instance.__getitem__(item)\n\n    def __setitem__(self, item, value):\n        self.instance.__setitem__(item, value)\n\n    def __str__(self):\n        return pprint.pformat(self.instance._conf, indent=4)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __delitem__(self, key):\n        del self.instance._conf[key]\n\n    def __iter__(self):\n        return iter(self.instance._conf)\n\n    def __len__(self):\n        return len(self.instance._conf)\n\n    def save(self, filename, verbose=False):\n        \"\"\"\n        Saves the settings in JSON format to the given file path.\n\n        :param filename: filename of the local JSON settings file.\n        :param verbose: report having saved the settings file\n        \"\"\"\n        with open(filename, \"w\") as fid:\n            json.dump(self._conf, fid, indent=4)\n        if verbose:\n            logger.info(\"Saved settings in \" + filename)\n\n    def load(self, filename):\n        \"\"\"\n        Updates the setting from config file in JSON format.\n\n        :param filename: filename of the local JSON settings file. If None, the local config file is used.\n        \"\"\"\n        if filename is None:\n            filename = LOCALCONFIG\n        with open(filename, \"r\") as fid:\n            logger.info(f\"DataJoint is configured from {os.path.abspath(filename)}\")\n            self._conf.update(json.load(fid))\n\n    def save_local(self, verbose=False):\n        \"\"\"\n        saves the settings in the local config file\n        \"\"\"\n        self.save(LOCALCONFIG, verbose)\n\n    def save_global(self, verbose=False):\n        \"\"\"\n        saves the settings in the global config file\n        \"\"\"\n        self.save(os.path.expanduser(os.path.join(\"~\", GLOBALCONFIG)), verbose)\n\n    def get_store_spec(self, store):\n        \"\"\"\n        find configuration of external stores for blobs and attachments\n        \"\"\"\n        try:\n            spec = self[\"stores\"][store]\n        except KeyError:\n            raise DataJointError(\n                \"Storage {store} is requested but not configured\".format(store=store)\n            )\n\n        spec[\"subfolding\"] = spec.get(\"subfolding\", DEFAULT_SUBFOLDING)\n        spec_keys = {  # REQUIRED in uppercase and allowed in lowercase\n            \"file\": (\"PROTOCOL\", \"LOCATION\", \"subfolding\", \"stage\"),\n            \"s3\": (\n                \"PROTOCOL\",\n                \"ENDPOINT\",\n                \"BUCKET\",\n                \"ACCESS_KEY\",\n                \"SECRET_KEY\",\n                \"LOCATION\",\n                \"secure\",\n                \"subfolding\",\n                \"stage\",\n                \"proxy_server\",\n            ),\n        }\n\n        try:\n            spec_keys = spec_keys[spec.get(\"protocol\", \"\").lower()]\n        except KeyError:\n            raise DataJointError(\n                'Missing or invalid protocol in dj.config[\"stores\"][\"{store}\"]'.format(\n                    store=store\n                )\n            )\n\n        # check that all required keys are present in spec\n        try:\n            raise DataJointError(\n                'dj.config[\"stores\"][\"{store}\"] is missing \"{k}\"'.format(\n                    store=store,\n                    k=next(\n                        k.lower()\n                        for k in spec_keys\n                        if k.isupper() and k.lower() not in spec\n                    ),\n                )\n            )\n        except StopIteration:\n            pass\n\n        # check that only allowed keys are present in spec\n        try:\n            raise DataJointError(\n                'Invalid key \"{k}\" in dj.config[\"stores\"][\"{store}\"]'.format(\n                    store=store,\n                    k=next(\n                        k\n                        for k in spec\n                        if k.upper() not in spec_keys and k.lower() not in spec_keys\n                    ),\n                )\n            )\n        except StopIteration:\n            pass  # no invalid keys\n\n        return spec\n\n    @contextmanager\n    def __call__(self, **kwargs):\n        \"\"\"\n        The config object can also be used in a with statement to change the state of the configuration\n        temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a\n        double underscore '__'. The context manager yields the changed config object.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.config(safemode=False, database__host=\"localhost\") as cfg:\n        &gt;&gt;&gt;     # do dangerous stuff here\n        \"\"\"\n\n        try:\n            backup = self.instance\n            self.instance = Config.__Config(self.instance._conf)\n            new = {k.replace(\"__\", \".\"): v for k, v in kwargs.items()}\n            self.instance._conf.update(new)\n            yield self\n        except:\n            self.instance = backup\n            raise\n        else:\n            self.instance = backup\n\n    class __Config:\n        \"\"\"\n        Stores datajoint settings. Behaves like a dictionary, but applies validator functions\n        when certain keys are set.\n\n        The default parameters are stored in datajoint.settings.default . If a local config file\n        exists, the settings specified in this file override the default settings.\n        \"\"\"\n\n        def __init__(self, *args, **kwargs):\n            self._conf = dict(default)\n            # use the free update to set keys\n            self._conf.update(dict(*args, **kwargs))\n\n        def __getitem__(self, key):\n            return self._conf[key]\n\n        def __setitem__(self, key, value):\n            logger.debug(\"Setting {0:s} to {1:s}\".format(str(key), str(value)))\n            if validators[key](value):\n                self._conf[key] = value\n            else:\n                raise DataJointError(\"Validator for {0:s} did not pass\".format(key))\n            valid_logging_levels = {\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"}\n            if key == \"loglevel\":\n                if value not in valid_logging_levels:\n                    raise ValueError(\n                        f\"'{value}' is not a valid logging value {tuple(valid_logging_levels)}\"\n                    )\n                logger.setLevel(value)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save", "title": "<code>save(filename, verbose=False)</code>", "text": "<p>Saves the settings in JSON format to the given file path.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>filename of the local JSON settings file.</p> required <code>verbose</code> <p>report having saved the settings file</p> <code>False</code> Source code in <code>datajoint/settings.py</code> <pre><code>def save(self, filename, verbose=False):\n    \"\"\"\n    Saves the settings in JSON format to the given file path.\n\n    :param filename: filename of the local JSON settings file.\n    :param verbose: report having saved the settings file\n    \"\"\"\n    with open(filename, \"w\") as fid:\n        json.dump(self._conf, fid, indent=4)\n    if verbose:\n        logger.info(\"Saved settings in \" + filename)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.load", "title": "<code>load(filename)</code>", "text": "<p>Updates the setting from config file in JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>filename of the local JSON settings file. If None, the local config file is used.</p> required Source code in <code>datajoint/settings.py</code> <pre><code>def load(self, filename):\n    \"\"\"\n    Updates the setting from config file in JSON format.\n\n    :param filename: filename of the local JSON settings file. If None, the local config file is used.\n    \"\"\"\n    if filename is None:\n        filename = LOCALCONFIG\n    with open(filename, \"r\") as fid:\n        logger.info(f\"DataJoint is configured from {os.path.abspath(filename)}\")\n        self._conf.update(json.load(fid))\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_local", "title": "<code>save_local(verbose=False)</code>", "text": "<p>saves the settings in the local config file</p> Source code in <code>datajoint/settings.py</code> <pre><code>def save_local(self, verbose=False):\n    \"\"\"\n    saves the settings in the local config file\n    \"\"\"\n    self.save(LOCALCONFIG, verbose)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_global", "title": "<code>save_global(verbose=False)</code>", "text": "<p>saves the settings in the global config file</p> Source code in <code>datajoint/settings.py</code> <pre><code>def save_global(self, verbose=False):\n    \"\"\"\n    saves the settings in the global config file\n    \"\"\"\n    self.save(os.path.expanduser(os.path.join(\"~\", GLOBALCONFIG)), verbose)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.get_store_spec", "title": "<code>get_store_spec(store)</code>", "text": "<p>find configuration of external stores for blobs and attachments</p> Source code in <code>datajoint/settings.py</code> <pre><code>def get_store_spec(self, store):\n    \"\"\"\n    find configuration of external stores for blobs and attachments\n    \"\"\"\n    try:\n        spec = self[\"stores\"][store]\n    except KeyError:\n        raise DataJointError(\n            \"Storage {store} is requested but not configured\".format(store=store)\n        )\n\n    spec[\"subfolding\"] = spec.get(\"subfolding\", DEFAULT_SUBFOLDING)\n    spec_keys = {  # REQUIRED in uppercase and allowed in lowercase\n        \"file\": (\"PROTOCOL\", \"LOCATION\", \"subfolding\", \"stage\"),\n        \"s3\": (\n            \"PROTOCOL\",\n            \"ENDPOINT\",\n            \"BUCKET\",\n            \"ACCESS_KEY\",\n            \"SECRET_KEY\",\n            \"LOCATION\",\n            \"secure\",\n            \"subfolding\",\n            \"stage\",\n            \"proxy_server\",\n        ),\n    }\n\n    try:\n        spec_keys = spec_keys[spec.get(\"protocol\", \"\").lower()]\n    except KeyError:\n        raise DataJointError(\n            'Missing or invalid protocol in dj.config[\"stores\"][\"{store}\"]'.format(\n                store=store\n            )\n        )\n\n    # check that all required keys are present in spec\n    try:\n        raise DataJointError(\n            'dj.config[\"stores\"][\"{store}\"] is missing \"{k}\"'.format(\n                store=store,\n                k=next(\n                    k.lower()\n                    for k in spec_keys\n                    if k.isupper() and k.lower() not in spec\n                ),\n            )\n        )\n    except StopIteration:\n        pass\n\n    # check that only allowed keys are present in spec\n    try:\n        raise DataJointError(\n            'Invalid key \"{k}\" in dj.config[\"stores\"][\"{store}\"]'.format(\n                store=store,\n                k=next(\n                    k\n                    for k in spec\n                    if k.upper() not in spec_keys and k.lower() not in spec_keys\n                ),\n            )\n        )\n    except StopIteration:\n        pass  # no invalid keys\n\n    return spec\n</code></pre>"}, {"location": "api/datajoint/table/", "title": "table.py", "text": ""}, {"location": "api/datajoint/table/#datajoint.table.Table", "title": "<code>Table</code>", "text": "<p>               Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n    \"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def class_name(self):\n        return self.__class__.__name__\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n        \"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        # Enforce strict CamelCase #1150\n        if not is_camel_case(self.class_name):\n            raise DataJointError(\n                \"Table class name `{name}` is invalid. Please use CamelCase. \".format(\n                    name=self.class_name\n                )\n                + \"Classes defining tables should be formatted in strict CamelCase.\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n        \"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warning(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n        \"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n        \"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n        \"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n        \"\"\"\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n        \"\"\"\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n        \"\"\"\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n        \"\"\"\n        return part tables either as entries in a dict with foreign key information or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        self.connection.dependencies.load(force=False)\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n        \"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n        \"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n        \"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n        \"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n        \"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        # collects the field list from first row (passed by reference)\n        field_list = []\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n        \"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n        force_masters: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n            force_masters: If `True`, include part/master pairs in the cascade.\n                Default is `False`.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n        visited_masters = set()\n\n        def cascade(table):\n            \"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0])\n                    if match is None:\n                        raise DataJointError(\n                            \"Cascading deletes failed because the error message is missing foreign key information.\"\n                            \"Make sure you have REFERENCES privilege to all dependent tables.\"\n                        ) from None\n                    match = match.groupdict()\n                    # if schema name missing, use table\n                    if \"`.`\" not in match[\"child\"]:\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                        child._restriction_attributes = table.restriction_attributes\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n\n                    master_name = get_master(child.full_table_name)\n                    if (\n                        force_masters\n                        and master_name\n                        and master_name != table.full_table_name\n                        and master_name not in visited_masters\n                    ):\n                        master = FreeTable(table.connection, master_name)\n                        master._restriction_attributes = set()\n                        master._restriction = [\n                            make_condition(  # &amp;= may cause in target tables in subquery\n                                master,\n                                (master.proj() &amp; child.proj()).fetch(),\n                                master._restriction_attributes,\n                            )\n                        ]\n                        visited_masters.add(master_name)\n                        cascade(master)\n                    else:\n                        cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warning(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        elif not transaction:\n            logger.info(\"Delete completed\")\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Delete committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warning(\"Delete cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n        \"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n        \"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n        \"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def describe(self, context=None, printout=False):\n        \"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    (\n                        attr.name\n                        if attr.default is None\n                        else \"%s=%s\" % (attr.name, attr.default)\n                    ),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n        \"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n        \"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n            \"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n    \"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    # Enforce strict CamelCase #1150\n    if not is_camel_case(self.class_name):\n        raise DataJointError(\n            \"Table class name `{name}` is invalid. Please use CamelCase. \".format(\n                name=self.class_name\n            )\n            + \"Classes defining tables should be formatted in strict CamelCase.\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n    \"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warning(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n    \"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n    \"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n    \"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n    \"\"\"\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n    \"\"\"\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n    \"\"\"\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key information or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n    \"\"\"\n    return part tables either as entries in a dict with foreign key information or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    self.connection.dependencies.load(force=False)\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n    \"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n    \"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n    \"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    # collects the field list from first row (passed by reference)\n    field_list = []\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n    \"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False, force_masters=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.     force_masters: If <code>True</code>, include part/master pairs in the cascade.         Default is <code>False</code>.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n    force_masters: bool = False,\n) -&gt; int:\n    \"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n        force_masters: If `True`, include part/master pairs in the cascade.\n            Default is `False`.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n    visited_masters = set()\n\n    def cascade(table):\n        \"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0])\n                if match is None:\n                    raise DataJointError(\n                        \"Cascading deletes failed because the error message is missing foreign key information.\"\n                        \"Make sure you have REFERENCES privilege to all dependent tables.\"\n                    ) from None\n                match = match.groupdict()\n                # if schema name missing, use table\n                if \"`.`\" not in match[\"child\"]:\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                    child._restriction_attributes = table.restriction_attributes\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n\n                master_name = get_master(child.full_table_name)\n                if (\n                    force_masters\n                    and master_name\n                    and master_name != table.full_table_name\n                    and master_name not in visited_masters\n                ):\n                    master = FreeTable(table.connection, master_name)\n                    master._restriction_attributes = set()\n                    master._restriction = [\n                        make_condition(  # &amp;= may cause in target tables in subquery\n                            master,\n                            (master.proj() &amp; child.proj()).fetch(),\n                            master._restriction_attributes,\n                        )\n                    ]\n                    visited_masters.add(master_name)\n                    cascade(master)\n                else:\n                    cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warning(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    elif not transaction:\n        logger.info(\"Delete completed\")\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Delete committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warning(\"Delete cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n    \"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n    \"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n    \"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default)\n                ),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.lookup_class_name", "title": "<code>lookup_class_name(name, context, depth=3)</code>", "text": "<p>given a table name in the form <code>schema_name</code>.<code>table_name</code>, find its class in the context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p><code>schema_name</code>.<code>table_name</code></p> required <code>context</code> <p>dictionary representing the namespace</p> required <code>depth</code> <p>search depth into imported modules, helps avoid infinite recursion.</p> <code>3</code> <p>Returns:</p> Type Description <p>class name found in the context or None if not found</p> Source code in <code>datajoint/table.py</code> <pre><code>def lookup_class_name(name, context, depth=3):\n    \"\"\"\n    given a table name in the form `schema_name`.`table_name`, find its class in the context.\n\n    :param name: `schema_name`.`table_name`\n    :param context: dictionary representing the namespace\n    :param depth: search depth into imported modules, helps avoid infinite recursion.\n    :return: class name found in the context or None if not found\n    \"\"\"\n    # breadth-first search\n    nodes = [dict(context=context, context_name=\"\", depth=depth)]\n    while nodes:\n        node = nodes.pop(0)\n        for member_name, member in node[\"context\"].items():\n            # skip IPython's implicit variables\n            if not member_name.startswith(\"_\"):\n                if inspect.isclass(member) and issubclass(member, Table):\n                    if member.full_table_name == name:  # found it!\n                        return \".\".join([node[\"context_name\"], member_name]).lstrip(\".\")\n                    try:  # look for part tables\n                        parts = member.__dict__\n                    except AttributeError:\n                        pass  # not a UserTable -- cannot have part tables.\n                    else:\n                        for part in (\n                            getattr(member, p)\n                            for p in parts\n                            if p[0].isupper() and hasattr(member, p)\n                        ):\n                            if (\n                                inspect.isclass(part)\n                                and issubclass(part, Table)\n                                and part.full_table_name == name\n                            ):\n                                return \".\".join(\n                                    [node[\"context_name\"], member_name, part.__name__]\n                                ).lstrip(\".\")\n                elif (\n                    node[\"depth\"] &gt; 0\n                    and inspect.ismodule(member)\n                    and member.__name__ != \"datajoint\"\n                ):\n                    try:\n                        nodes.append(\n                            dict(\n                                context=dict(inspect.getmembers(member)),\n                                context_name=node[\"context_name\"] + \".\" + member_name,\n                                depth=node[\"depth\"] - 1,\n                            )\n                        )\n                    except ImportError:\n                        pass  # could not import, so do not attempt\n    return None\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>               Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n    \"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log", "title": "<code>Log</code>", "text": "<p>               Bases: <code>Table</code></p> <p>The log table for each schema. Instances are callable.  Calls log the time and identifying information along with the event.</p> <p>Parameters:</p> Name Type Description Default <code>skip_logging</code> <p>if True, then log entry is skipped by default. See call</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>class Log(Table):\n    \"\"\"\n    The log table for each schema.\n    Instances are callable.  Calls log the time and identifying information along with the event.\n\n    :param skip_logging: if True, then log entry is skipped by default. See __call__\n    \"\"\"\n\n    _table_name = \"~log\"\n\n    def __init__(self, conn, database, skip_logging=False):\n        self.database = database\n        self.skip_logging = skip_logging\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # event logging table for `{database}`\n        id       :int unsigned auto_increment     # event order id\n        ---\n        timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp\n        version  :varchar(12)                     # datajoint version\n        user     :varchar(255)                    # user@host\n        host=\"\"  :varchar(255)                    # system hostname\n        event=\"\" :varchar(255)                    # event message\n        \"\"\".format(\n            database=database\n        )\n\n        super().__init__()\n\n        if not self.is_declared:\n            self.declare()\n            self.connection.dependencies.clear()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    def __call__(self, event, skip_logging=None):\n        \"\"\"\n\n        :param event: string to write into the log table\n        :param skip_logging: If True then do not log. If None, then use self.skip_logging\n        \"\"\"\n        skip_logging = self.skip_logging if skip_logging is None else skip_logging\n        if not skip_logging:\n            try:\n                self.insert1(\n                    dict(\n                        user=self._user,\n                        version=version + \"py\",\n                        host=platform.uname().node,\n                        event=event,\n                    ),\n                    skip_duplicates=True,\n                    ignore_extra_fields=True,\n                )\n            except DataJointError:\n                logger.info(\"could not log event in table ~log\")\n\n    def delete(self):\n        \"\"\"\n        bypass interactive prompts and cascading dependencies\n\n        :return: number of deleted items\n        \"\"\"\n        return self.delete_quick(get_count=True)\n\n    def drop(self):\n        \"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n        self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> <p>Returns:</p> Type Description <p>number of deleted items</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(self):\n    \"\"\"\n    bypass interactive prompts and cascading dependencies\n\n    :return: number of deleted items\n    \"\"\"\n    return self.delete_quick(get_count=True)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n    \"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/user_tables/", "title": "user_tables.py", "text": "<p>Hosts the table tiers, user tables should be derived from.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.TableMeta", "title": "<code>TableMeta</code>", "text": "<p>               Bases: <code>type</code></p> <p>TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch().</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class TableMeta(type):\n    \"\"\"\n    TableMeta subclasses allow applying some instance methods and properties directly\n    at class level. For example, this allows Table.fetch() instead of Table().fetch().\n    \"\"\"\n\n    def __getattribute__(cls, name):\n        # trigger instantiation for supported class attrs\n        return (\n            cls().__getattribute__(name)\n            if name in supported_class_attrs\n            else super().__getattribute__(name)\n        )\n\n    def __and__(cls, arg):\n        return cls() &amp; arg\n\n    def __xor__(cls, arg):\n        return cls() ^ arg\n\n    def __sub__(cls, arg):\n        return cls() - arg\n\n    def __neg__(cls):\n        return -cls()\n\n    def __mul__(cls, arg):\n        return cls() * arg\n\n    def __matmul__(cls, arg):\n        return cls() @ arg\n\n    def __add__(cls, arg):\n        return cls() + arg\n\n    def __iter__(cls):\n        return iter(cls())\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable", "title": "<code>UserTable</code>", "text": "<p>               Bases: <code>Table</code></p> <p>A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema().</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class UserTable(Table, metaclass=TableMeta):\n    \"\"\"\n    A subclass of UserTable is a dedicated class interfacing a base table.\n    UserTable is initialized by the decorator generated by schema().\n    \"\"\"\n\n    # set by @schema\n    _connection = None\n    _heading = None\n    _support = None\n\n    # set by subclass\n    tier_regexp = None\n    _prefix = None\n\n    @property\n    def definition(self):\n        \"\"\"\n        :return: a string containing the table definition using the DataJoint DDL.\n        \"\"\"\n        raise NotImplementedError(\n            'Subclasses of Table must implement the property \"definition\"'\n        )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def table_name(cls):\n        \"\"\"\n        :return: the table name of the table formatted for mysql.\n        \"\"\"\n        if cls._prefix is None:\n            raise AttributeError(\"Class prefix is not defined!\")\n        return cls._prefix + from_camel_case(cls.__name__)\n\n    @ClassProperty\n    def full_table_name(cls):\n        if cls not in {Manual, Imported, Lookup, Computed, Part, UserTable}:\n            # for derived classes only\n            if cls.database is None:\n                raise DataJointError(\n                    \"Class %s is not properly declared (schema decorator not applied?)\"\n                    % cls.__name__\n                )\n            return r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.definition", "title": "<code>definition</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a string containing the table definition using the DataJoint DDL.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.table_name", "title": "<code>table_name()</code>", "text": "<p>Returns:</p> Type Description <p>the table name of the table formatted for mysql.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>@ClassProperty\ndef table_name(cls):\n    \"\"\"\n    :return: the table name of the table formatted for mysql.\n    \"\"\"\n    if cls._prefix is None:\n        raise AttributeError(\"Class prefix is not defined!\")\n    return cls._prefix + from_camel_case(cls.__name__)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Manual", "title": "<code>Manual</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Lookup", "title": "<code>Lookup</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Imported", "title": "<code>Imported</code>", "text": "<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n    \"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Computed", "title": "<code>Computed</code>", "text": "<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n    \"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part", "title": "<code>Part</code>", "text": "<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n    \"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n        \"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n        \"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n    \"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n    \"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/utils/", "title": "utils.py", "text": "<p>General-purpose utilities</p>"}, {"location": "api/datajoint/utils/#datajoint.utils.user_choice", "title": "<code>user_choice(prompt, choices=('yes', 'no'), default=None)</code>", "text": "<p>Prompts the user for confirmation.  The default value, if any, is capitalized.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>Information to display to the user.</p> required <code>choices</code> <p>an iterable of possible choices.</p> <code>('yes', 'no')</code> <code>default</code> <p>default choice</p> <code>None</code> <p>Returns:</p> Type Description <p>the user's choice</p> Source code in <code>datajoint/utils.py</code> <pre><code>def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n    \"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = \", \".join(\n        (choice.title() if choice == default else choice for choice in choices)\n    )\n    response = None\n    while response not in choices:\n        response = input(prompt + \" [\" + choice_list + \"]: \")\n        response = response.lower() if response else default\n    return response\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.get_master", "title": "<code>get_master(full_table_name)</code>", "text": "<p>If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + <code>__</code>.</p> <p>Example:    <code>ephys</code>.<code>session</code>    -- master    <code>ephys</code>.<code>session__recording</code>  -- part</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <code>str</code> <p>Full table name including part.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Supposed master full table name or empty string if not a part table name.</p> Source code in <code>datajoint/utils.py</code> <pre><code>def get_master(full_table_name: str) -&gt; str:\n    \"\"\"\n    If the table name is that of a part table, then return what the master table name would be.\n    This follows DataJoint's table naming convention where a master and a part must be in the\n    same schema and the part table is prefixed with the master table name + ``__``.\n\n    Example:\n       `ephys`.`session`    -- master\n       `ephys`.`session__recording`  -- part\n\n    :param full_table_name: Full table name including part.\n    :type full_table_name: str\n    :return: Supposed master full table name or empty string if not a part table name.\n    :rtype: str\n    \"\"\"\n    match = re.match(r\"(?P&lt;master&gt;`\\w+`.`\\w+)__(?P&lt;part&gt;\\w+)`\", full_table_name)\n    return match[\"master\"] + \"`\" if match else \"\"\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.is_camel_case", "title": "<code>is_camel_case(s)</code>", "text": "<p>Check if a string is in CamelCase notation.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string to check</p> required <p>Returns:</p> Type Description <p>True if the string is in CamelCase notation, False otherwise Example: &gt;&gt;&gt; is_camel_case(\"TableName\")  # returns True &gt;&gt;&gt; is_camel_case(\"table_name\")  # returns False</p> Source code in <code>datajoint/utils.py</code> <pre><code>def is_camel_case(s):\n    \"\"\"\n    Check if a string is in CamelCase notation.\n\n    :param s: string to check\n    :returns: True if the string is in CamelCase notation, False otherwise\n    Example:\n    &gt;&gt;&gt; is_camel_case(\"TableName\")  # returns True\n    &gt;&gt;&gt; is_camel_case(\"table_name\")  # returns False\n    \"\"\"\n    return bool(re.match(r\"^[A-Z][A-Za-z0-9]*$\", s))\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.to_camel_case", "title": "<code>to_camel_case(s)</code>", "text": "<p>Convert names with under score (_) separation into camel case names.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in under_score notation</p> required <p>Returns:</p> Type Description <p>string in CamelCase notation Example: &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def to_camel_case(s):\n    \"\"\"\n    Convert names with under score (_) separation into camel case names.\n\n    :param s: string in under_score notation\n    :returns: string in CamelCase notation\n    Example:\n    &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"\n    \"\"\"\n\n    def to_upper(match):\n        return match.group(0)[-1].upper()\n\n    return re.sub(r\"(^|[_\\W])+[a-zA-Z]\", to_upper, s)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.from_camel_case", "title": "<code>from_camel_case(s)</code>", "text": "<p>Convert names in camel case into underscore (_) separated names</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in CamelCase notation</p> required <p>Returns:</p> Type Description <p>string in under_score notation Example: &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def from_camel_case(s):\n    \"\"\"\n    Convert names in camel case into underscore (_) separated names\n\n    :param s: string in CamelCase notation\n    :returns: string in under_score notation\n    Example:\n    &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"\n    \"\"\"\n\n    def convert(match):\n        return (\"_\" if match.groups()[0] else \"\") + match.group(0).lower()\n\n    if not is_camel_case(s):\n        raise DataJointError(\n            \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\"\n        )\n    return re.sub(r\"(\\B[A-Z])|(\\b[A-Z])\", convert, s)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_write", "title": "<code>safe_write(filepath, blob)</code>", "text": "<p>A two-step write.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>full path</p> required <code>blob</code> <p>binary data</p> required Source code in <code>datajoint/utils.py</code> <pre><code>def safe_write(filepath, blob):\n    \"\"\"\n    A two-step write.\n\n    :param filename: full path\n    :param blob: binary data\n    \"\"\"\n    filepath = Path(filepath)\n    if not filepath.is_file():\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = filepath.with_suffix(filepath.suffix + \".saving\")\n        temp_file.write_bytes(blob)\n        temp_file.rename(filepath)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_copy", "title": "<code>safe_copy(src, dest, overwrite=False)</code>", "text": "<p>Copy the contents of src file into dest file as a two-step process. Skip if dest exists already</p> Source code in <code>datajoint/utils.py</code> <pre><code>def safe_copy(src, dest, overwrite=False):\n    \"\"\"\n    Copy the contents of src file into dest file as a two-step process. Skip if dest exists already\n    \"\"\"\n    src, dest = Path(src), Path(dest)\n    if not (dest.exists() and src.samefile(dest)) and (overwrite or not dest.is_file()):\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = dest.with_suffix(dest.suffix + \".copying\")\n        shutil.copyfile(str(src), str(temp_file))\n        temp_file.rename(dest)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.parse_sql", "title": "<code>parse_sql(filepath)</code>", "text": "<p>yield SQL statements from an SQL file</p> Source code in <code>datajoint/utils.py</code> <pre><code>def parse_sql(filepath):\n    \"\"\"\n    yield SQL statements from an SQL file\n    \"\"\"\n    delimiter = \";\"\n    statement = []\n    with Path(filepath).open(\"rt\") as f:\n        for line in f:\n            line = line.strip()\n            if not line.startswith(\"--\") and len(line) &gt; 1:\n                if line.startswith(\"delimiter\"):\n                    delimiter = line.split()[1]\n                else:\n                    statement.append(line)\n                    if line.endswith(delimiter):\n                        yield \" \".join(statement)\n                        statement = []\n        if statement:\n            yield \" \".join(statement)\n</code></pre>"}, {"location": "api/datajoint/version/", "title": "version.py", "text": ""}, {"location": "client/credentials/", "title": "Credentials", "text": "<p>Configure the connection through DataJoint's <code>config</code> object:</p> <pre><code>&gt; import datajoint as dj\nDataJoint 0.4.9 (February 1, 2017)\nNo configuration found. Use `dj.config` to configure and save the configuration.\n</code></pre> <p>You may now set the database credentials:</p> <pre><code>dj.config['database.host'] = \"alicelab.datajoint.io\"\ndj.config['database.user'] = \"alice\"\ndj.config['database.password'] = \"haha not my real password\"\n</code></pre> <p>Skip setting the password to make DataJoint prompt to enter the password every time.</p> <p>You may save the configuration in the local work directory with <code>dj.config.save_local()</code> or for all your projects in <code>dj.config.save_global()</code>. Configuration changes should be made through the <code>dj.config</code> interface; the config file should not be modified directly by the user.</p> <p>You may leave the user or the password as <code>None</code>, in which case you will be prompted to enter them manually for every session. Setting the password as an empty string allows access without a password.</p> <p>Note that the system environment variables <code>DJ_HOST</code>, <code>DJ_USER</code>, and <code>DJ_PASS</code> will overwrite the settings in the config file. You can use them to set the connection credentials instead of config files.</p> <p>To change the password, the <code>dj.set_password</code> function will walk you through the process:</p> <pre><code>dj.set_password()\n</code></pre> <p>After that, update the password in the configuration and save it as described above:</p> <pre><code>dj.config['database.password'] = 'my#cool!new*psswrd'\ndj.config.save_local()   # or dj.config.save_global()\n</code></pre>"}, {"location": "client/install/", "title": "Install and Connect", "text": "<p>DataJoint is implemented for Python 3.4+. You may install it from PyPI:</p> <pre><code>pip3 install datajoint\n</code></pre> <p>or upgrade</p> <pre><code>pip3 install --upgrade datajoint\n</code></pre>"}, {"location": "client/install/#datajoint-python-windows-install-guide", "title": "DataJoint Python Windows Install Guide", "text": "<p>This document outlines the steps necessary to install DataJoint on Windows for use in connecting to a remote server hosting a DataJoint database. Some limited discussion of installing MySQL is discussed in <code>MySQL for Windows</code>, but is not covered in-depth since this is an uncommon usage scenario and not strictly required to connect to DataJoint pipelines.</p>"}, {"location": "client/install/#quick-steps", "title": "Quick steps", "text": "<p>Quick install steps for advanced users are as follows:</p> <ul> <li>Install latest Python 3.x and ensure it is in <code>PATH</code> (3.6.3 current at time of writing)   <pre><code>pip install datajoint\n</code></pre></li> </ul> <p>For ERD drawing support:</p> <ul> <li>Install Graphviz for Windows and ensure it is in <code>PATH</code> (64 bit builds currently tested; URL below.)   <pre><code>pip install pydotplus matplotlib\n</code></pre></li> </ul> <p>Detailed instructions follow.</p>"}, {"location": "client/install/#step-1-install-python", "title": "Step 1: install Python", "text": "<p>Python for Windows is available from:</p> <p>https://www.python.org/downloads/windows</p> <p>The latest 64 bit 3.x version, currently 3.6.3, is available from the Python site.</p> <p>From here run the installer to install Python.</p> <p>For a single-user machine, the regular installation process is sufficient - be sure to select the <code>Add Python to PATH</code> option:</p> <p></p> <p>For a shared machine, run the installer as administrator (right-click, run as administrator) and select the advanced installation. Be sure to select options as follows:</p> <p> </p>"}, {"location": "client/install/#step-2-verify-installation", "title": "Step 2: verify installation", "text": "<p>To verify the Python installation and make sure that your system is ready to install DataJoint, open a command window by entering <code>cmd</code> into the Windows search bar:</p> <p></p> <p>From here <code>python</code> and the Python package manager <code>pip</code> can be verified by running <code>python -V</code> and <code>pip -V</code>, respectively:</p> <p></p> <p>If you receive the error message that either <code>pip</code> or <code>python</code> is not a recognized command, please uninstall Python and ensure that the option to add Python to the <code>PATH</code> variable was properly configured.</p>"}, {"location": "client/install/#step-3-install-datajoint", "title": "Step 3: install DataJoint", "text": "<p>DataJoint (and other Python modules) can be easily installed using the <code>pip</code> Python package manager which is installed as a part of Python and was verified in the previous step.</p> <p>To install DataJoint simply run <code>pip install datajoint</code>:</p> <p></p> <p>This will proceed to install DataJoint, along with several other required packages from the PIP repository. When finished, a summary of the activity should be presented:</p> <p></p> <p>Note: You can find out more about the packages installed here and many other freely available open source packages via pypi, the Python package index site.</p>"}, {"location": "client/install/#optional-step-4-install-packages-for-erd-support", "title": "(Optional) step 4: install packages for ERD support", "text": "<p>To draw diagrams of your DataJoint schema, the following additional steps should be followed.</p>"}, {"location": "client/install/#install-graphviz", "title": "Install Graphviz", "text": "<p>DataJoint currently utilizes Graphviz to generate the ERD visualizations. Although a Windows version of Graphviz is available from the main site, it is an older and out of date 32-bit version. The recommended pre-release builds of the 64 bit version are available here:</p> <p>https://ci.appveyor.com/project/ellson/graphviz-pl238</p> <p>More specifically, the build artifacts from the <code>Win64; Configuration: Release</code> are recommended, available here.</p> <p>This is a regular Windows installer executable, and will present a dialog when starting:</p> <p></p> <p>It is important that an option to place Graphviz in the <code>PATH</code> be selected.</p> <p>For a personal installation:</p> <p></p> <p>To install system wide:</p> <p></p> <p>Once installed, Graphviz can be verified from a fresh command window as follows:</p> <p></p> <p>If you receive the error message that the <code>dot</code> program is not a recognized command, please uninstall Graphviz and ensure that the option to add Python to the PATH variable was properly configured.</p> <p>Important: in some cases, running the <code>dot -c</code> command in a command prompt is required to properly initialize the Graphviz installation.</p>"}, {"location": "client/install/#install-pydotplus", "title": "Install PyDotPlus", "text": "<p>The PyDotPlus library links the Graphviz installation to DataJoint and is easily installed via <code>pip</code>:</p> <p></p>"}, {"location": "client/install/#install-matplotlib", "title": "Install Matplotlib", "text": "<p>The Matplotlib library provides useful plotting utilities which are also used by DataJoint's <code>Diagram</code> drawing facility. The package is easily installed via <code>pip</code>:</p> <p></p>"}, {"location": "client/install/#optional-step-5-install-jupyter-notebook", "title": "(Optional) step 5: install Jupyter Notebook", "text": "<p>As described on the www.jupyter.org website:</p> <p>''' The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. '''</p> <p>Although not a part of DataJoint, Jupyter Notebook can be a very useful tool for building and interacting with DataJoint pipelines. It is easily installed from <code>pip</code> as well:</p> <p> </p> <p>Once installed, Jupyter Notebook can be started via the <code>jupyter notebook</code> command, which should now be on your path:</p> <p></p> <p>By default Jupyter Notebook will start a local private web server session from the directory where it was started and start a web browser session connected to the session.</p> <p> </p> <p>You now should be able to use the notebook viewer to navigate the filesystem and to create new project folders and interactive Jupyter/Python/DataJoint notebooks.</p>"}, {"location": "client/install/#git-for-windows", "title": "Git for Windows", "text": "<p>The Git version control system is not a part of DataJoint but is recommended for interacting with the broader Python/Git/GitHub sharing ecosystem.</p> <p>The Git for Windows installer is available from https://git-scm.com/download/win.</p> <p></p> <p>The default settings should be sufficient and correct in most cases.</p>"}, {"location": "client/install/#mysql-for-windows", "title": "MySQL for Windows", "text": "<p>For hosting pipelines locally, the MySQL server package is required.</p> <p>MySQL for windows can be installed via the installers available from the MySQL website. Please note that although DataJoint should be fully compatible with a Windows MySQL server installation, this mode of operation is not tested by the DataJoint team.</p>"}, {"location": "client/settings/", "title": "Configuration Settings", "text": "<p>If you are not using DataJoint on your own, or are setting up a DataJoint system for other users, some additional configuration options may be required to support TLS or external storage.</p>"}, {"location": "client/settings/#tls-configuration", "title": "TLS Configuration", "text": "<p>Starting with v0.12, DataJoint will by default use TLS if it is available. TLS can be forced on or off with the boolean <code>dj.config['database.use_tls']</code>.</p>"}, {"location": "compute/distributed/", "title": "Distributed Computing", "text": ""}, {"location": "compute/distributed/#job-reservations", "title": "Job reservations", "text": "<p>Running <code>populate</code> on the same table on multiple computers will causes them to attempt to compute the same data all at once. This will not corrupt the data since DataJoint will reject any duplication. One solution could be to cause the different computing nodes to populate the tables in random order. This would reduce some collisions but not completely prevent them.</p> <p>To allow efficient distributed computing, DataJoint provides a built-in job reservation process. When <code>dj.Computed</code> tables are auto-populated using job reservation, a record of each ongoing computation is kept in a schema-wide <code>jobs</code> table, which is used internally by DataJoint to coordinate the auto-population effort among multiple computing processes.</p> <p>Job reservations are activated by setting the keyword argument <code>reserve_jobs=True</code> in <code>populate</code> calls.</p> <p>With job management enabled, the <code>make</code> method of each table class will also consult the <code>jobs</code> table for reserved jobs as part of determining the next record to compute and will create an entry in the <code>jobs</code> table as part of the attempt to compute the resulting record for that key. If the operation is a success, the record is removed. In the event of failure, the job reservation entry is updated to indicate the details of failure. Using this simple mechanism, multiple processes can participate in the auto-population effort without duplicating computational effort, and any errors encountered during the course of the computation can be individually inspected to determine the cause of the issue.</p> <p>As part of DataJoint, the jobs table can be queried using native DataJoint syntax. For example, to list the jobs currently being run:</p> <pre><code>In [1]: schema.jobs\nOut[1]:\n*table_name    *key_hash      status       error_message  user           host           pid       connection_id  timestamp      key        error_stack\n+------------+ +------------+ +----------+ +------------+ +------------+ +------------+ +-------+ +------------+ +------------+ +--------+ +------------+\n__job_results  e4da3b7fbbce23 reserved                    datajoint@localhos localhost     15571     59             2017-09-04 14: &lt;BLOB&gt;     &lt;BLOB&gt;\n(2 tuples)\n</code></pre> <p>The above output shows that a record for the <code>JobResults</code> table is currently reserved for computation, along with various related details of the reservation, such as the MySQL connection ID, client user and host, process ID on the remote system, timestamp, and the key for the record that the job is using for its computation. Since DataJoint table keys can be of varying types, the key is stored in a binary format to allow the table to store arbitrary types of record key data. The subsequent sections will discuss querying the jobs table for key data.</p> <p>As mentioned above, jobs encountering errors during computation will leave their record reservations in place, and update the reservation record with details of the error.</p> <p>For example, if a Python process is interrupted via the keyboard, a KeyboardError will be logged to the database as follows:</p> <pre><code>In [2]: schema.jobs\nOut[2]:\n*table_name    *key_hash      status     error_message  user           host           pid       connection_id  timestamp      key        error_stack\n+------------+ +------------+ +--------+ +------------+ +------------+ +------------+ +-------+ +------------+ +------------+ +--------+ +------------+\n__job_results  3416a75f4cea91 error      KeyboardInterr datajoint@localhos localhost     15571     59             2017-09-04 14: &lt;BLOB&gt;     &lt;BLOB&gt;\n(1 tuples)\n</code></pre> <p>By leaving the job reservation record in place, the error can be inspected, and if necessary the corresponding <code>dj.Computed</code> update logic can be corrected. From there the jobs entry can be cleared, and the computation can then be resumed. In the meantime, the presence of the job reservation will prevent this particular record from being processed during subsequent auto-population calls. Inspecting the job record for failure details can proceed much like any other DataJoint query.</p> <p>For example, given the above table, errors can be inspected as follows:</p> <pre><code>In [3]: (schema.jobs &amp; 'status=\"error\"' ).fetch(as_dict=True)\nOut[3]:\n[OrderedDict([('table_name', '__job_results'),\n               ('key_hash', 'c81e728d9d4c2f636f067f89cc14862c'),\n               ('status', 'error'),\n               ('key', rec.array([(2,)],\n                         dtype=[('id', 'O')])),\n               ('error_message', 'KeyboardInterrupt'),\n               ('error_stack', None),\n               ('user', 'datajoint@localhost'),\n               ('host', 'localhost'),\n               ('pid', 15571),\n               ('connection_id', 59),\n               ('timestamp', datetime.datetime(2017, 9, 4, 15, 3, 53))])]\n</code></pre> <p>This particular error occurred when processing the record with ID <code>2</code>, resulted from a <code>KeyboardInterrupt</code>, and has no additional error trace.</p> <p>After any system or code errors have been resolved, the table can simply be cleaned of errors and the computation rerun.</p> <p>For example:</p> <pre><code>In [4]: (schema.jobs &amp; 'status=\"error\"' ).delete()\n</code></pre> <p>In some cases, it may be preferable to inspect the jobs table records using populate keys. Since job keys are hashed and stored as a blob in the jobs table to support the varying types of keys, we need to query using the key hash instead of simply using the raw key data.</p> <p>This can be done by using <code>dj.key_hash</code> to convert the key as follows:</p> <pre><code>In [4]: jk = {'table_name': JobResults.table_name, 'key_hash' : dj.key_hash({'id': 2})}\n\nIn [5]: schema.jobs &amp; jk\nOut[5]:\n*table_name    *key_hash      status     key        error_message  error_stac user           host      pid        connection_id  timestamp\n+------------+ +------------+ +--------+ +--------+ +------------+ +--------+ +------------+ +-------+ +--------+ +------------+ +------------+\n__job_results  c81e728d9d4c2f error      =BLOB=     KeyboardInterr =BLOB=     datajoint@localhost  localhost     15571     59             2017-09-04 14:\n(Total: 1)\n\nIn [6]: (schema.jobs &amp; jk).delete()\n\nIn [7]: schema.jobs &amp; jk\nOut[7]:\n*table_name    *key_hash    status     key        error_message  error_stac user     host     pid     connection_id  timestamp\n+------------+ +----------+ +--------+ +--------+ +------------+ +--------+ +------+ +------+ +-----+ +------------+ +-----------+\n\n(Total: 0)\n</code></pre>"}, {"location": "compute/distributed/#managing-connections", "title": "Managing connections", "text": "<p>The DataJoint method <code>dj.kill</code> allows for viewing and termination of database connections. Restrictive conditions can be used to identify specific connections. Restrictions are specified as strings and can involve any of the attributes of <code>information_schema.processlist</code>: <code>ID</code>, <code>USER</code>, <code>HOST</code>, <code>DB</code>, <code>COMMAND</code>, <code>TIME</code>, <code>STATE</code>, and <code>INFO</code>.</p> <p>Examples:</p> <p><code>dj.kill('HOST LIKE \"%compute%\"')</code> lists only connections from hosts containing \"compute\".   <code>dj.kill('TIME &gt; 600')</code> lists only connections older than 10 minutes.</p> <p>A list of connections meeting the restriction conditions (if present) are presented to the user, along with the option to kill processes. By default, output is ordered by ascending connection ID. To change the output order of dj.kill(), an additional order_by argument can be provided.</p> <p>For example, to sort the output by hostname in descending order:</p> <pre><code>In [3]: dj.kill(None, None, 'host desc')\nOut[3]:\n     ID USER         HOST          STATE         TIME    INFO\n+--+ +----------+ +-----------+ +-----------+ +-----+\n     33 chris        localhost:54840                 1261  None\n     17 chris        localhost:54587                 3246  None\n     4 event_scheduler localhost    Waiting on empty queue  187180  None\nprocess to kill or \"q\" to quit &gt; q\n</code></pre>"}, {"location": "compute/key-source/", "title": "Key Source", "text": ""}, {"location": "compute/key-source/#default-key-source", "title": "Default key source", "text": "<p>Key source refers to the set of primary key values over which autopopulate iterates, calling the <code>make</code> method at each iteration. Each <code>key</code> from the key source is passed to the table's <code>make</code> call. By default, the key source for a table is the join of its primary dependencies.</p> <p>For example, consider a schema with three tables. The <code>Stimulus</code> table contains one attribute <code>stimulus_type</code> with one of two values, \"Visual\" or \"Auditory\". The <code>Modality</code> table contains one attribute <code>modality</code> with one of three values, \"EEG\", \"fMRI\", and \"PET\". The <code>Protocol</code> table has primary dependencies on both the <code>Stimulus</code> and <code>Modality</code> tables.</p> <p>The key source for <code>Protocol</code> will then be all six combinations of <code>stimulus_type</code> and <code>modality</code> as shown in the figure below.</p> <p></p>"}, {"location": "compute/key-source/#custom-key-source", "title": "Custom key source", "text": "<p>A custom key source can be configured by setting the <code>key_source</code> property within a table class, after the <code>definition</code> string.</p> <p>Any query object can be used as the key source. In most cases the new key source will be some alteration of the default key source. Custom key sources often involve restriction to limit the key source to only relevant entities. Other designs may involve using only one of a table's primary dependencies.</p> <p>In the example below, the <code>EEG</code> table depends on the <code>Recording</code> table that lists all recording sessions. However, the <code>populate</code> method of <code>EEG</code> should only ingest recordings where the <code>recording_type</code> is <code>EEG</code>. Setting a custom key source prevents the <code>populate</code> call from iterating over recordings of the wrong type.</p> <pre><code>@schema\nclass EEG(dj.Imported):\ndefinition = \"\"\"\n-&gt; Recording\n---\nsample_rate : float\neeg_data : longblob\n\"\"\"\nkey_source = Recording &amp; 'recording_type = \"EEG\"'\n</code></pre>"}, {"location": "compute/make/", "title": "Transactions in Make", "text": "<p>Each call of the make method is enclosed in a transaction. DataJoint users do not need to explicitly manage transactions but must be aware of their use.</p> <p>Transactions produce two effects:</p> <p>First, the state of the database appears stable within the <code>make</code> call  throughout the transaction: two executions of the same query  will yield identical results within the same <code>make</code> call.</p> <p>Second, any changes to the database (inserts) produced by the <code>make</code> method will not become visible to other processes until the <code>make</code> call completes execution. If the <code>make</code> method raises an exception, all changes made so far will be discarded and will never become visible to other processes.</p> <p>Transactions are particularly important in maintaining group integrity with master-part relationships. The <code>make</code> call of a master table first inserts the master entity and then inserts all the matching part entities in the part tables. None of the entities become visible to other processes until the entire <code>make</code> call completes, at which point they all become visible.</p>"}, {"location": "compute/make/#three-part-make-pattern-for-long-computations", "title": "Three-Part Make Pattern for Long Computations", "text": "<p>For long-running computations, DataJoint provides an advanced pattern called the three-part make that separates the <code>make</code> method into three distinct phases. This pattern is essential for maintaining database performance and data integrity during expensive computations.</p>"}, {"location": "compute/make/#the-problem-long-transactions", "title": "The Problem: Long Transactions", "text": "<p>Traditional <code>make</code> methods perform all operations within a single database transaction:</p> <pre><code>def make(self, key):\n    # All within one transaction\n    data = (ParentTable &amp; key).fetch1()  # Fetch\n    result = expensive_computation(data)  # Compute (could take hours)\n    self.insert1(dict(key, result=result))  # Insert\n</code></pre> <p>This approach has significant limitations: - Database locks: Long transactions hold locks on tables, blocking other operations - Connection timeouts: Database connections may timeout during long computations - Memory pressure: All fetched data must remain in memory throughout the computation - Failure recovery: If computation fails, the entire transaction is rolled back</p>"}, {"location": "compute/make/#the-solution-three-part-make-pattern", "title": "The Solution: Three-Part Make Pattern", "text": "<p>The three-part make pattern splits the <code>make</code> method into three distinct phases, allowing the expensive computation to occur outside of database transactions:</p> <pre><code>def make_fetch(self, key):\n    \"\"\"Phase 1: Fetch all required data from parent tables\"\"\"\n    fetched_data = ((ParentTable1 &amp; key).fetch1(), (ParentTable2 &amp; key).fetch1()) \n    return fetched_data # must be a sequence, eg tuple or list\n\ndef make_compute(self, key, *fetched_data):\n    \"\"\"Phase 2: Perform expensive computation (outside transaction)\"\"\"\n    computed_result = expensive_computation(*fetched_data)\n    return computed_result # must be a sequence, eg tuple or list\n\ndef make_insert(self, key, *computed_result):\n    \"\"\"Phase 3: Insert results into the current table\"\"\"\n    self.insert1(dict(key, result=computed_result))\n</code></pre>"}, {"location": "compute/make/#execution-flow", "title": "Execution Flow", "text": "<p>To achieve data intensity without long transactions, the three-part make pattern follows this sophisticated execution sequence:</p> <pre><code># Step 1: Fetch data outside transaction\nfetched_data1 = self.make_fetch(key)\ncomputed_result = self.make_compute(key, *fetched_data1)\n\n# Step 2: Begin transaction and verify data consistency\nbegin transaction:\n    fetched_data2 = self.make_fetch(key)\n    if fetched_data1 != fetched_data2:  # deep comparison\n        cancel transaction  # Data changed during computation\n    else:\n        self.make_insert(key, *computed_result)\n        commit_transaction\n</code></pre>"}, {"location": "compute/make/#key-benefits", "title": "Key Benefits", "text": "<ol> <li>Reduced Database Lock Time: Only the fetch and insert operations occur within transactions, minimizing lock duration</li> <li>Connection Efficiency: Database connections are only used briefly for data transfer</li> <li>Memory Management: Fetched data can be processed and released during computation</li> <li>Fault Tolerance: Computation failures don't affect database state</li> <li>Scalability: Multiple computations can run concurrently without database contention</li> </ol>"}, {"location": "compute/make/#referential-integrity-protection", "title": "Referential Integrity Protection", "text": "<p>The pattern includes a critical safety mechanism: referential integrity verification. Before inserting results, the system:</p> <ol> <li>Re-fetches the source data within the transaction</li> <li>Compares it with the originally fetched data using deep hashing</li> <li>Only proceeds with insertion if the data hasn't changed</li> </ol> <p>This prevents the \"phantom read\" problem where source data changes during long computations, ensuring that results remain consistent with their inputs.</p>"}, {"location": "compute/make/#implementation-details", "title": "Implementation Details", "text": "<p>The pattern is implemented using Python generators in the <code>AutoPopulate</code> class:</p> <p><pre><code>def make(self, key):\n    # Step 1: Fetch data from parent tables\n    fetched_data = self.make_fetch(key)\n    computed_result = yield fetched_data\n\n    # Step 2: Compute if not provided\n    if computed_result is None:\n        computed_result = self.make_compute(key, *fetched_data)\n        yield computed_result\n\n    # Step 3: Insert the computed result\n    self.make_insert(key, *computed_result)\n    yield\n</code></pre> Therefore, it is possible to override the <code>make</code> method to implement the three-part make pattern by using the <code>yield</code> statement to return the fetched data and computed result as above.</p>"}, {"location": "compute/make/#use-cases", "title": "Use Cases", "text": "<p>This pattern is particularly valuable for:</p> <ul> <li>Machine learning model training: Hours-long training sessions</li> <li>Image processing pipelines: Large-scale image analysis</li> <li>Statistical computations: Complex statistical analyses</li> <li>Data transformations: ETL processes with heavy computation</li> <li>Simulation runs: Time-consuming simulations</li> </ul>"}, {"location": "compute/make/#example-long-running-image-analysis", "title": "Example: Long-Running Image Analysis", "text": "<p>Here's an example of how to implement the three-part make pattern for a long-running image analysis task:</p> <pre><code>@schema\nclass ImageAnalysis(dj.Computed):\n    definition = \"\"\"\n    # Complex image analysis results\n    -&gt; Image\n    ---\n    analysis_result : longblob\n    processing_time : float\n    \"\"\"\n\n    def make_fetch(self, key):\n        \"\"\"Fetch the image data needed for analysis\"\"\"\n        image_data = (Image &amp; key).fetch1('image')\n        params = (Params &amp; key).fetch1('params')\n        return (image_data, params) # pack fetched_data\n\n    def make_compute(self, key, image_data, params):\n        \"\"\"Perform expensive image analysis outside transaction\"\"\"\n        import time\n        start_time = time.time()\n\n        # Expensive computation that could take hours\n        result = complex_image_analysis(image_data, params)\n        processing_time = time.time() - start_time\n        return result, processing_time\n\n    def make_insert(self, key, analysis_result, processing_time):\n        \"\"\"Insert the analysis results\"\"\"\n        self.insert1(dict(key, \n                         analysis_result=analysis_result,\n                         processing_time=processing_time))\n</code></pre> <p>The exact same effect may be achieved by overriding the <code>make</code> method as a generator function using the <code>yield</code> statement to return the fetched data and computed result as above:</p> <p><pre><code>@schema\nclass ImageAnalysis(dj.Computed):\n    definition = \"\"\"\n    # Complex image analysis results\n    -&gt; Image\n    ---\n    analysis_result : longblob\n    processing_time : float\n    \"\"\"\n\n    def make(self, key):\n        image_data = (Image &amp; key).fetch1('image')\n        params = (Params &amp; key).fetch1('params')\n        computed_result = yield (image, params) # pack fetched_data\n\n        if computed_result is None:\n            # Expensive computation that could take hours\n            import time\n            start_time = time.time()\n            result = complex_image_analysis(image_data, params)\n            processing_time = time.time() - start_time\n            computed_result = result, processing_time  #pack\n            yield computed_result\n\n        result, processing_time = computed_result # unpack\n        self.insert1(dict(key,\n                         analysis_result=result,\n                         processing_time=processing_time))\n        yield  # yield control back to the caller\n</code></pre> We expect that most users will prefer to use the three-part implementation over the generator function implementation due to its conceptual complexity.</p>"}, {"location": "compute/populate/", "title": "Auto-populate", "text": "<p>Auto-populated tables are used to define, execute, and coordinate computations in a DataJoint pipeline.</p> <p>Tables in the initial portions of the pipeline are populated from outside the pipeline. In subsequent steps, computations are performed automatically by the DataJoint pipeline in auto-populated tables.</p> <p>Computed tables belong to one of the two auto-populated data tiers: <code>dj.Imported</code> and <code>dj.Computed</code>. DataJoint does not enforce the distinction between imported and computed tables: the difference is purely semantic, a convention for developers to follow. If populating a table requires access to external files such as raw storage that is not part of the database, the table is designated as imported. Otherwise it is computed.</p> <p>Auto-populated tables are defined and queried exactly as other tables. (See Manual Tables.) Their data definition follows the same definition syntax.</p>"}, {"location": "compute/populate/#make", "title": "Make", "text": "<p>For auto-populated tables, data should never be entered using insert directly. Instead these tables must define the callback method <code>make(self, key)</code>. The <code>insert</code> method then can only be called on <code>self</code> inside this callback method.</p> <p>Imagine that there is a table <code>test.Image</code> that contains 2D grayscale images in its <code>image</code> attribute. Let us define the computed table, <code>test.FilteredImage</code> that filters the image in some way and saves the result in its <code>filtered_image</code> attribute.</p> <p>The class will be defined as follows.</p> <pre><code>@schema\nclass FilteredImage(dj.Computed):\n     definition = \"\"\"\n     # Filtered image\n     -&gt; Image\n     ---\n     filtered_image : longblob\n     \"\"\"\n\n     def make(self, key):\n          img = (test.Image &amp; key).fetch1('image')\n          key['filtered_image'] = myfilter(img)\n          self.insert1(key)\n</code></pre> <p>The <code>make</code> method receives one argument: the dict <code>key</code> containing the primary key value of an element of key source to be worked on.</p> <p>The key represents the partially filled entity, usually already containing the primary key attributes of the key source.</p> <p>The <code>make</code> callback does three things:</p> <ol> <li>Fetches data from tables upstream in the pipeline using the <code>key</code> for restriction.</li> <li>Computes and adds any missing attributes to the fields already in <code>key</code>.</li> <li>Inserts the entire entity into <code>self</code>.</li> </ol> <p><code>make</code> may populate multiple entities in one call when <code>key</code> does not specify the entire primary key of the populated table.</p>"}, {"location": "compute/populate/#populate", "title": "Populate", "text": "<p>The inherited <code>populate</code> method of <code>dj.Imported</code> and <code>dj.Computed</code> automatically calls <code>make</code> for every key for which the auto-populated table is missing data.</p> <p>The <code>FilteredImage</code> table can be populated as</p> <pre><code>FilteredImage.populate()\n</code></pre> <p>The progress of long-running calls to <code>populate()</code> in datajoint-python can be visualized by adding the <code>display_progress=True</code> argument to the populate call.</p> <p>Note that it is not necessary to specify which data needs to be computed. DataJoint will call <code>make</code>, one-by-one, for every key in <code>Image</code> for which <code>FilteredImage</code> has not yet been computed.</p> <p>Chains of auto-populated tables form computational pipelines in DataJoint.</p>"}, {"location": "compute/populate/#populate-options", "title": "Populate options", "text": "<p>The <code>populate</code> method accepts a number of optional arguments that provide more features and allow greater control over the method's behavior.</p> <ul> <li><code>restrictions</code> - A list of restrictions, restricting as <code>(tab.key_source &amp; AndList(restrictions)) - tab.proj()</code>.   Here <code>target</code> is the table to be populated, usually <code>tab</code> itself.</li> <li><code>suppress_errors</code> - If <code>True</code>, encountering an error will cancel the current <code>make</code> call, log the error, and continue to the next <code>make</code> call.   Error messages will be logged in the job reservation table (if <code>reserve_jobs</code> is   <code>True</code>) and returned as a list.   See also <code>return_exception_objects</code> and <code>reserve_jobs</code>.   Defaults to <code>False</code>.</li> <li><code>return_exception_objects</code> - If <code>True</code>, error objects are returned instead of error   messages.   This applies only when <code>suppress_errors</code> is <code>True</code>.   Defaults to <code>False</code>.</li> <li><code>reserve_jobs</code> - If <code>True</code>, reserves job to indicate to other distributed processes.   The job reservation table may be access as <code>schema.jobs</code>.   Errors are logged in the jobs table.   Defaults to <code>False</code>.</li> <li><code>order</code> - The order of execution, either <code>\"original\"</code>, <code>\"reverse\"</code>, or <code>\"random\"</code>.   Defaults to <code>\"original\"</code>.</li> <li><code>display_progress</code> - If <code>True</code>, displays a progress bar.   Defaults to <code>False</code>.</li> <li><code>limit</code> - If not <code>None</code>, checks at most this number of keys.   Defaults to <code>None</code>.</li> <li><code>max_calls</code> - If not <code>None</code>, populates at most this many keys.   Defaults to <code>None</code>, which means no limit.</li> </ul>"}, {"location": "compute/populate/#progress", "title": "Progress", "text": "<p>The method <code>table.progress</code> reports how many <code>key_source</code> entries have been populated and how many remain. Two optional parameters allow more advanced use of the method. A parameter of restriction conditions can be provided, specifying which entities to consider. A Boolean parameter <code>display</code> (default is <code>True</code>) allows disabling the output, such that the numbers of remaining and total entities are returned but not printed.</p>"}, {"location": "concepts/data-model/", "title": "Data Model", "text": ""}, {"location": "concepts/data-model/#what-is-a-data-model", "title": "What is a data model?", "text": "<p>A data model is a conceptual framework that defines how data is organized, represented, and transformed. It gives us the components for creating blueprints for the structure and operations of data management systems, ensuring consistency and efficiency in data handling.</p> <p>Data management systems are built to accommodate these models, allowing us to manage data according to the principles laid out by the model. If you\u2019re studying data science or engineering, you\u2019ve likely encountered different data models, each providing a unique approach to organizing and manipulating data.</p> <p>A data model is defined by considering the following key aspects:</p> <ul> <li>What are the fundamental elements used to structure the data?</li> <li>What operations are available for defining, creating, and manipulating the data?</li> <li>What mechanisms exist to enforce the structure and rules governing valid data interactions?</li> </ul>"}, {"location": "concepts/data-model/#types-of-data-models", "title": "Types of data models", "text": "<p>Among the most familiar data models are those based on files and folders: data of any kind are lumped together into binary strings called files, files are collected into folders, and folders can be nested within other folders to create a folder hierarchy.</p> <p>Another family of data models are various tabular models. For example, items in CSV files are listed in rows, and the attributes of each item are stored in columns. Various spreadsheet models allow forming dependencies between cells and groups of cells, including complex calculations.</p> <p>The object data model is common in programming, where data are represented as objects in memory with properties and methods for transformations of such data.</p>"}, {"location": "concepts/data-model/#relational-data-model", "title": "Relational data model", "text": "<p>The relational model is a way of thinking about data as sets and operations on sets. Formalized almost a half-century ago (Codd, 1969). The relational data model is one of the most powerful and precise ways to store and manage structured data. At its core, this model organizes all data into tables--representing mathematical relations---where each table consists of rows (representing mathematical tuples) and columns (often called attributes).</p>"}, {"location": "concepts/data-model/#core-principles-of-the-relational-data-model", "title": "Core principles of the relational data model", "text": "<p>Data representation:   Data are represented and manipulated in the form of relations.   A relation is a set (i.e. an unordered collection) of entities of values for each of   the respective named attributes of the relation.   Base relations represent stored data while derived relations are formed from base   relations through query expressions.   A collection of base relations with their attributes, domain constraints, uniqueness   constraints, and referential constraints is called a schema.</p> <p>Domain constraints:   Each attribute (column) in a table is associated with a specific attribute domain (or   datatype, a set of possible values), ensuring that the data entered is valid.   Attribute domains may not include relations, which keeps the data model   flat, i.e. free of nested structures.</p> <p>Uniqueness constraints:   Entities within relations are addressed by values of their attributes.   To identify and relate data elements, uniqueness constraints are imposed on subsets   of attributes.   Such subsets are then referred to as keys.   One key in a relation is designated as the primary key used for referencing its elements.</p> <p>Referential constraints:   Associations among data are established by means of referential constraints with the   help of foreign keys.   A referential constraint on relation A referencing relation B allows only those   entities in A whose foreign key attributes match the key attributes of an entity in B.</p> <p>Declarative queries:   Data queries are formulated through declarative, as opposed to imperative,   specifications of sought results.   This means that query expressions convey the logic for the result rather than the   procedure for obtaining it.   Formal languages for query expressions include relational algebra, relational   calculus, and SQL.</p> <p>The relational model has many advantages over both hierarchical file systems and tabular models for maintaining data integrity and providing flexible access to interesting subsets of the data.</p> <p>Popular implementations of the relational data model rely on the Structured Query Language (SQL). SQL comprises distinct sublanguages for schema definition, data manipulation, and data queries. SQL thoroughly dominates in the space of relational databases and is often conflated with the relational data model in casual discourse. Various terminologies are used to describe related concepts from the relational data model. Similar to spreadsheets, relations are often visualized as tables with attributes corresponding to columns and entities corresponding to rows. In particular, SQL uses the terms table, column, and row.</p>"}, {"location": "concepts/data-model/#the-datajoint-model", "title": "The DataJoint Model", "text": "<p>DataJoint is a conceptual refinement of the relational data model offering a more expressive and rigorous framework for database programming (Yatsenko et al., 2018). The DataJoint model facilitates conceptual clarity, efficiency, workflow management, and precise and flexible data queries. By enforcing entity normalization, simplifying dependency declarations, offering a rich query algebra, and visualizing relationships through schema diagrams, DataJoint makes relational database programming more intuitive and robust for complex data pipelines.</p> <p>The model has emerged over a decade of continuous development of complex data pipelines for neuroscience experiments (Yatsenko et al., 2015). DataJoint has allowed researchers with no prior knowledge of databases to collaborate effectively on common data pipelines sustaining data integrity and supporting flexible access. DataJoint is currently implemented as client libraries in MATLAB and Python. These libraries work by transpiling DataJoint queries into SQL before passing them on to conventional relational database systems that serve as the backend, in combination with bulk storage systems for storing large contiguous data objects.</p> <p>DataJoint comprises:</p> <ul> <li>a schema definition language</li> <li>a data manipulation language</li> <li>a data query language</li> <li>a diagramming notation for visualizing relationships between modeled entities</li> </ul> <p>The key refinement of DataJoint over other relational data models and their implementations is DataJoint's support of entity normalization.</p>"}, {"location": "concepts/data-model/#core-principles-of-the-datajoint-model", "title": "Core principles of the DataJoint model", "text": "<p>Entity Normalization   DataJoint enforces entity normalization, ensuring that every entity set (table) is   well-defined, with each element belonging to the same type, sharing the same   attributes, and distinguished by the same primary key. This principle reduces   redundancy and avoids data anomalies, similar to Boyce-Codd Normal Form, but with a   more intuitive structure than traditional SQL.</p> <p>Simplified Schema Definition and Dependency Management   DataJoint introduces a schema definition language that is more expressive and less   error-prone than SQL. Dependencies are explicitly declared using arrow notation   (-&gt;), making referential constraints easier to understand and visualize. The   dependency structure is enforced as an acyclic directed graph, which simplifies   workflows by preventing circular dependencies.</p> <p>Integrated Query Operators producing a Relational Algebra   DataJoint introduces five query operators (restrict, join, project, aggregate, and   union) with algebraic closure, allowing them to be combined seamlessly. These   operators are designed to maintain operational entity normalization, ensuring query   outputs remain valid entity sets.</p> <p>Diagramming Notation for Conceptual Clarity   DataJoint\u2019s schema diagrams simplify the representation of relationships between   entity sets compared to ERM diagrams. Relationships are expressed as dependencies   between entity sets, which are visualized using solid or dashed lines for primary   and secondary dependencies, respectively.</p> <p>Unified Logic for Binary Operators   DataJoint simplifies binary operations by requiring attributes involved in joins or   comparisons to be homologous (i.e., sharing the same origin). This avoids the   ambiguity and pitfalls of natural joins in SQL, ensuring more predictable query   results.</p> <p>Optimized Data Pipelines for Scientific Workflows   DataJoint treats the database as a data pipeline where each entity set defines a   step in the workflow. This makes it ideal for scientific experiments and complex   data processing, such as in neuroscience. Its MATLAB and Python libraries transpile   DataJoint queries into SQL, bridging the gap between scientific programming and   relational databases.</p>"}, {"location": "concepts/data-pipelines/", "title": "Data Pipelines", "text": ""}, {"location": "concepts/data-pipelines/#what-is-a-data-pipeline", "title": "What is a data pipeline?", "text": "<p>A scientific data pipeline is a collection of processes and systems for organizing the data, computations, and workflows used by a research group as they jointly perform complex sequences of data acquisition, processing, and analysis.</p> <p>A variety of tools can be used for supporting shared data pipelines:</p> <p>Data repositories   Research teams set up a shared data repository.   This minimal data management tool allows depositing and retrieving data and managing   user access.   For example, this may include a collection of files with standard naming conventions   organized into folders and sub-folders.   Or a data repository might reside on the cloud, for example in a collection of S3   buckets.   This image of data management -- where files are warehoused and retrieved from a   hierarchically-organized system of folders -- is an approach that is likely familiar   to most scientists.</p> <p>Database systems   Databases are a form of data repository providing additional capabilities:</p> <pre><code>1. Defining, communicating, and enforcing structure in the stored data.\n2. Maintaining data integrity: correct identification of data and consistent cross-references, dependencies, and groupings among the data.\n3. Supporting queries that retrieve various cross-sections and transformation of the deposited data.\n\nMost scientists have some familiarity with these concepts, for example the notion of maintaining consistency between data and the metadata that describes it, or applying a filter to an Excel spreadsheet to retrieve specific subsets of information.\nHowever, usually the more advanced concepts involved in building and using relational databases fall under the specific expertise of data scientists.\n</code></pre> <p>Data pipelines   Data pipeline frameworks may include all the features of a database system along   with additional functionality:</p> <pre><code>1. Integrating computations to perform analyses and manage intermediate results in a principled way.\n2. Supporting distributed computations without conflict.\n3. Defining, communicating, and enforcing **workflow**, making clear the sequence of steps that must be performed for data entry, acquisition, and processing.\n\nAgain, the informal notion of an analysis \"workflow\" will be familiar to most scientists, along with the logistical difficulties associated with managing a workflow that is shared by multiple scientists within or across labs.\n</code></pre> <p>Therefore, a full-featured data pipeline framework may also be described as a scientific workflow system.</p> <p>Major features of data management frameworks: data repositories, databases, and data pipelines.</p> <p></p>"}, {"location": "concepts/data-pipelines/#what-is-datajoint", "title": "What is DataJoint?", "text": "<p>DataJoint is a free open-source framework for creating scientific data pipelines directly from MATLAB or Python (or any mixture of the two). The data are stored in a language-independent way that allows interoperability between MATLAB and Python, with additional languages in the works. DataJoint pipelines become the central tool in the operations of data-intensive labs or consortia as they organize participants with different roles and skills around a common framework.</p> <p>In DataJoint, a data pipeline is a sequence of steps (more generally, a directed acyclic graph) with integrated data storage at each step. The pipeline may have some nodes requiring manual data entry or import from external sources, some that read from raw data files, and some that perform computations on data stored in other database nodes. In a typical scenario, experimenters and acquisition instruments feed data into nodes at the head of the pipeline, while downstream nodes perform automated computations for data processing and analysis.</p> <p>For example, this is the pipeline for a simple mouse experiment involving calcium imaging in mice.</p> <p></p> <p>In this example, the experimenter first enters information about a mouse, then enters information about each imaging session in that mouse, and then each scan performed in each imaging session. Next the automated portion of the pipeline takes over to import the raw imaging data, perform image alignment to compensate for motion, image segmentation to identify cells in the images, and extraction of calcium traces. Finally, the receptive field (RF) computation is performed by relating the calcium signals to the visual stimulus information.</p>"}, {"location": "concepts/data-pipelines/#how-datajoint-works", "title": "How DataJoint works", "text": "<p>DataJoint enables data scientists to build and operate scientific data pipelines.</p> <p>Conceptual overview of DataJoint operation.</p> <p></p> <p>DataJoint provides a simple and powerful data model, which is detailed more formally in Yatsenko D, Walker EY, Tolias AS (2018). DataJoint: A Simpler Relational Data Model.. Put most generally, a \"data model\" defines how to think about data and the operations that can be performed on them. DataJoint's model is a refinement of the relational data model: all nodes in the pipeline are simple tables storing data, tables are related by their shared attributes, and query operations can combine the contents of multiple tables. DataJoint enforces specific constraints on the relationships between tables that help maintain data integrity and enable flexible access. DataJoint uses a succinct data definition language, a powerful data query language, and expressive visualizations of the pipeline. A well-defined and principled approach to data organization and computation enables teams of scientists to work together efficiently. The data become immediately available to all participants with appropriate access privileges. Some of the \"participants\" may be computational agents that perform processing and analysis, and so DataJoint features a built-in distributed job management process to allow distributing analysis between any number of computers.</p> <p>From a practical point of view, the back-end data architecture may vary depending on project requirements. Typically, the data architecture includes a relational database server (e.g. MySQL) and a bulk data storage system (e.g. AWS S3 or a filesystem). However, users need not interact with the database directly, but via MATLAB or Python objects that are each associated with an individual table in the database. One of the main advantages of this approach is that DataJoint clearly separates the data model facing the user from the data architecture implementing data management and computing. DataJoint works well in combination with good code sharing (e.g. with git) and environment sharing (e.g. with Docker).</p> <p>DataJoint is designed for quick prototyping and continuous exploration as experimental designs change or evolve. New analysis methods can be added or removed at any time, and the structure of the workflow itself can change over time, for example as new data acquisition methods are developed.</p> <p>With DataJoint, data sharing and publishing is no longer a separate step at the end of the project. Instead data sharing is an inherent feature of the process: to share data with other collaborators or to publish the data to the world, one only needs to set the access privileges.</p>"}, {"location": "concepts/data-pipelines/#real-life-example", "title": "Real-life example", "text": "<p>The Mesoscale Activity Project (MAP) is a collaborative project between four neuroscience labs. MAP uses DataJoint for data acquisition, processing, analysis, interfaces, and external sharing.</p> <p>The DataJoint pipeline for the MAP project.</p> <p></p> <p>The pipeline is hosted in the cloud through Amazon Web Services (AWS). MAP data scientists at the Janelia Research Campus and Baylor College of Medicine defined the data pipeline. Experimental scientists enter manual data directly into the pipeline using the Helium web interface. The raw data are preprocessed using the DataJoint client libraries in MATLAB and Python; the preprocessed data are ingested into the pipeline while the bulky and raw data are shared using Globus transfer through the PETREL storage servers provided by the Argonne National Lab. Data are made immediately available for exploration and analysis to collaborating labs, and the analysis results are also immediately shared. Analysis data may be visualized through web interfaces. Intermediate results may be exported into the NWB format for sharing with external groups.</p>"}, {"location": "concepts/data-pipelines/#summary-of-datajoint-features", "title": "Summary of DataJoint features", "text": "<ol> <li>A free, open-source framework for scientific data pipelines and workflow management</li> <li>Data hosting in cloud or in-house</li> <li>MySQL, filesystems, S3, and Globus for data management</li> <li>Define, visualize, and query data pipelines from MATLAB or Python</li> <li>Enter and view data through GUIs</li> <li>Concurrent access by multiple users and computational agents</li> <li>Data integrity: identification, dependencies, groupings</li> <li>Automated distributed computation</li> </ol>"}, {"location": "concepts/principles/", "title": "Principles", "text": ""}, {"location": "concepts/principles/#theoretical-foundations", "title": "Theoretical Foundations", "text": "<p>DataJoint Core implements a systematic framework for the joint management of structured scientific data and its associated computations. The framework builds on the theoretical foundations of the Relational Model and the Entity-Relationship Model, introducing a number of critical clarifications for the effective use of databases as scientific data pipelines. Notably, DataJoint introduces the concept of computational dependencies as a native first-class citizen of the data model. This integration of data structure and computation into a single model, defines a new class of computational scientific databases.</p> <p>This page defines the key principles of this model without attachment to a specific implementation while a more complete description of the model can be found in Yatsenko et al, 2018.</p> <p>DataJoint developers are developing these principles into an open standard to allow multiple alternative implementations.</p>"}, {"location": "concepts/principles/#data-representation", "title": "Data Representation", "text": ""}, {"location": "concepts/principles/#tables-entity-sets", "title": "Tables = Entity Sets", "text": "<p>DataJoint uses only one data structure in all its operations\u2014the entity set.</p> <ol> <li>All data are represented in the form of entity sets, i.e. an ordered collection of entities.</li> <li>All entities of an entity set belong to the same well-defined entity class and have the same set of named attributes.</li> <li>Attributes in an entity set has a data type (or domain), representing the set of its valid values.</li> <li>Each entity in an entity set provides the attribute values for all of the attributes of its entity class.</li> <li>Each entity set has a primary key, i.e. a subset of attributes that, jointly, uniquely identify any entity in the set.</li> </ol> <p>These formal terms have more common (even if less precise) variants:</p> formal common entity set table attribute column attribute value field <p>A collection of stored tables make up a database. Derived tables are formed through query expressions.</p>"}, {"location": "concepts/principles/#table-definition", "title": "Table Definition", "text": "<p>DataJoint introduces a streamlined syntax for defining a stored table.</p> <p>Each line in the definition defines an attribute with its name, data type, an optional default value, and an optional comment in the format:</p> <pre><code>name [=default] : type [# comment]\n</code></pre> <p>Primary attributes come first and are separated from the rest of the attributes with the divider <code>---</code>.</p> <p>For example, the following code defines the entity set for entities of class <code>Employee</code>:</p> <pre><code>employee_id : int\n---\nssn = null : int     # optional social security number\ndate_of_birth : date\ngender : enum('male', 'female', 'other')\nhome_address=\"\" : varchar(1000)\nprimary_phone=\"\" : varchar(12)\n</code></pre>"}, {"location": "concepts/principles/#data-tiers", "title": "Data Tiers", "text": "<p>Stored tables are designated into one of four tiers indicating how their data originates.</p> table tier data origin lookup contents are part of the table definition, defined a priori rather than entered externally. Typical stores general facts, parameters, options, etc. manual contents are populated by external mechanisms such as manual entry through web apps or by data ingest scripts imported contents are populated automatically by pipeline computations accessing data from upstream in the pipeline and from external data sources such as raw data stores. computed contents are populated automatically by pipeline computations accessing data from upstream in the pipeline."}, {"location": "concepts/principles/#object-serialization", "title": "Object Serialization", "text": ""}, {"location": "concepts/principles/#data-normalization", "title": "Data Normalization", "text": "<p>A collection of data is considered normalized when organized into a collection of entity sets, where each entity set represents a well-defined entity class with all its attributes applicable to each entity in the set and the same primary key identifying</p> <p>The normalization procedure often includes splitting data from one table into several tables, one for each proper entity set.</p>"}, {"location": "concepts/principles/#databases-and-schemas", "title": "Databases and Schemas", "text": "<p>Stored tables are named and grouped into namespaces called schemas. A collection of schemas make up a database. A database has a globally unique address or name. A schema has a unique name within its database. Within a connection to a particular database, a stored table is identified as <code>schema.Table</code>. A schema typically groups tables that are logically related.</p>"}, {"location": "concepts/principles/#dependencies", "title": "Dependencies", "text": "<p>Entity sets can form referential dependencies that express and</p>"}, {"location": "concepts/principles/#diagramming", "title": "Diagramming", "text": ""}, {"location": "concepts/principles/#data-integrity", "title": "Data integrity", "text": ""}, {"location": "concepts/principles/#entity-integrity", "title": "Entity integrity", "text": "<p>Entity integrity is the guarantee made by the data management process of the 1:1 mapping between real-world entities and their digital representations. In practice, entity integrity is ensured when it is made clear</p>"}, {"location": "concepts/principles/#referential-integrity", "title": "Referential integrity", "text": ""}, {"location": "concepts/principles/#group-integrity", "title": "Group integrity", "text": ""}, {"location": "concepts/principles/#data-manipulations", "title": "Data manipulations", "text": ""}, {"location": "concepts/principles/#data-queries", "title": "Data queries", "text": ""}, {"location": "concepts/principles/#query-operators", "title": "Query Operators", "text": ""}, {"location": "concepts/principles/#pipeline-computations", "title": "Pipeline computations", "text": ""}, {"location": "concepts/teamwork/", "title": "Teamwork", "text": ""}, {"location": "concepts/teamwork/#data-management-in-a-science-project", "title": "Data management in a science project", "text": "<p>Science labs organize their projects as a sequence of activities of experiment design, data acquisition, and processing and analysis.</p> <p></p> Workflow and dataflow in a common findings-centered approach to data science in a science lab. <p>Many labs lack a uniform data management strategy that would span longitudinally across the entire project lifecycle as well as laterally across different projects.</p> <p>Prior to publishing their findings, the research team may need to publish the data to support their findings. Without a data management system, this requires custom repackaging of the data to conform to the FAIR principles for scientific data management.</p>"}, {"location": "concepts/teamwork/#data-centric-project-organization", "title": "Data-centric project organization", "text": "<p>DataJoint is designed to support a data-centric approach to large science projects in which data are viewed as a principal output of the research project and are managed systematically throughout in a single framework through the entire process.</p> <p>This approach requires formulating a general data science plan and upfront investment for setting up resources and processes and training the teams. The team uses DataJoint to build data pipelines to support multiple projects.</p> <p></p> Workflow and dataflow in a data pipeline-centered approach. <p>Data pipelines support project data across their entire lifecycle, including the following functions</p> <ul> <li>experiment design</li> <li>animal colony management</li> <li>electronic lab book: manual data entry during experiments through graphical user interfaces.</li> <li>acquisition from instrumentation in the course of experiments</li> <li>ingest from raw acquired data</li> <li>computations for data analysis</li> <li>visualization of analysis results</li> <li>export for sharing and publishing</li> </ul> <p>Through all these activities, all these data are made accessible to all authorized participants and distributed computations can be done in parallel without compromising data integrity.</p>"}, {"location": "concepts/teamwork/#team-roles", "title": "Team roles", "text": "<p>The adoption of a uniform data management framework allows separation of roles and division of labor among team members, leading to greater efficiency and better scaling.</p> <p></p> Distinct responsibilities of data science and data engineering."}, {"location": "concepts/teamwork/#scientists", "title": "Scientists", "text": "<p>Design and conduct experiments, collecting data. They interact with the data pipeline through graphical user interfaces designed by others. They understand what analysis is used to test their hypotheses.</p>"}, {"location": "concepts/teamwork/#data-scientists", "title": "Data scientists", "text": "<p>Have the domain expertise and select and implement the processing and analysis methods for experimental data. Data scientists are in charge of defining and managing the data pipeline using DataJoint's data model, but they may not know the details of the underlying architecture. They interact with the pipeline using client programming interfaces directly from languages such as MATLAB and Python.</p> <p>The bulk of this manual is written for working data scientists, except for System Administration.</p>"}, {"location": "concepts/teamwork/#data-engineers", "title": "Data engineers", "text": "<p>Work with the data scientists to support the data pipeline. They rely on their understanding of the DataJoint data model to configure and administer the required IT resources such as database servers, data storage servers, networks, cloud instances, Globus endpoints, etc. Data engineers can provide general solutions such as web hosting, data publishing, interfaces, exports and imports.</p> <p>The System Administration section of this tutorial contains materials helpful in accomplishing these tasks.</p> <p>DataJoint is designed to delineate a clean boundary between data science and data engineering. This allows data scientists to use the same uniform data model for data pipelines backed by a variety of information technologies. This delineation also enables economies of scale as a single data engineering team can support a wide spectrum of science projects.</p>"}, {"location": "concepts/terminology/", "title": "Terminology", "text": ""}, {"location": "concepts/terminology/#terminology", "title": "Terminology", "text": "<p>DataJoint introduces a principled data model, which is described in detail in Yatsenko et al., 2018. This data model is a conceptual refinement of the Relational Data Model and also draws on the Entity-Relationship Model (ERM).</p> <p>The Relational Data Model was inspired by the concepts of relations in Set Theory. When the formal relational data model was formulated, it introduced additional terminology (e.g. relation, attribute, tuple, domain). Practical programming languages such as SQL do not precisely follow the relational data model and introduce other terms to approximate relational concepts (e.g. table, column, row, datatype). Subsequent data models (e.g. ERM) refined the relational data model and introduced their own terminology to describe analogous concepts (e.g. entity set, relationship set, attribute set). As a result, similar concepts may be described using different sets of terminologies, depending on the context and the speaker's background.</p> <p>For example, what is known as a relation in the formal relational model is called a table in SQL; the analogous concept in ERM and DataJoint is called an entity set.</p> <p>The DataJoint documentation follows the terminology defined in Yatsenko et al, 2018, except entity set is replaced with the more colloquial table or query result in most cases.</p> <p>The table below summarizes the terms used for similar concepts across the related data models.</p> <p>Data model terminology | Relational | ERM | SQL | DataJoint (formal) | This manual | | -- | -- | -- | -- | -- | | relation | entity set | table | entity set | table | | tuple | entity | row | entity | entity | | domain | value set | datatype | datatype | datatype | | attribute | attribute | column | attribute | attribute | | attribute value | attribute value | field value | attribute value | attribute value | | primary key | primary key | primary key | primary key | primary key | | foreign key | foreign key | foreign key | foreign key | foreign key | | schema | schema | schema or database | schema | schema | | relational expression | data query | <code>SELECT</code> statement | query expression | query expression |</p>"}, {"location": "concepts/terminology/#datajoint-databases-schemas-packages-and-modules", "title": "DataJoint: databases, schemas, packages, and modules", "text": "<p>A database is collection of tables on the database server. DataJoint users do not interact with it directly.</p> <p>A DataJoint schema is</p> <p>- a database on the database server containing tables with data and   - a collection of classes (in MATLAB or Python) associated with the database, one   class for each table.</p> <p>In MATLAB, the collection of classes is organized as a package, i.e. a file folder starting with a <code>+</code>.</p> <p>In Python, the collection of classes is any set of classes decorated with the appropriate <code>schema</code> object. Very commonly classes for tables in one database are organized as a distinct Python module. Thus, typical DataJoint projects have one module per database. However, this organization is up to the user's discretion.</p>"}, {"location": "concepts/terminology/#base-tables", "title": "Base tables", "text": "<p>Base tables are tables stored in the database, and are often referred to simply as tables in DataJoint. Base tables are distinguished from derived tables, which result from relational operators.</p>"}, {"location": "concepts/terminology/#relvars-and-relation-values", "title": "Relvars and relation values", "text": "<p>Early versions of the DataJoint documentation referred to the relation objects as relvars. This term emphasizes the fact that relational variables and expressions do not contain actual data but are rather symbolic representations of data to be retrieved from the database. The specific value of a relvar would then be referred to as the relation value. The value of a relvar can change with changes in the state of the database.</p> <p>The more recent iteration of the documentation has grown less pedantic and more often uses the term table instead.</p>"}, {"location": "concepts/terminology/#metadata", "title": "Metadata", "text": "<p>The vocabulary of DataJoint does not include this term.</p> <p>In data science, the term metadata commonly means \"data about the data\" rather than the data themselves. For example, metadata could include data sizes, timestamps, data types, indexes, keywords.</p> <p>In contrast, neuroscientists often use the term to refer to conditions and annotations about experiments. This distinction arose when such information was stored separately from experimental recordings, such as in physical notebooks. Such \"metadata\" are used to search and to classify the data and are in fact an integral part of the actual data.</p> <p>In DataJoint, all data other than blobs can be used in searches and categorization. These fields may originate from manual annotations, preprocessing, or analyses just as easily as from recordings or behavioral performance. Since \"metadata\" in the neuroscience sense are not distinguished from any other data in a pipeline, DataJoint avoids the term entirely. Instead, DataJoint differentiates data into data tiers.</p>"}, {"location": "concepts/terminology/#glossary", "title": "Glossary", "text": "<p>We've taken careful consideration to use consistent terminology.</p> Term Definition DAG directed acyclic graph (DAG) is a set of nodes and connected with a set of directed edges that form no cycles. This means that there is never a path back to a node after passing through it by following the directed edges. Formal workflow management systems represent workflows in the form of DAGs. data pipeline A sequence of data transformation steps from data sources through multiple intermediate structures. More generally, a data pipeline is a directed acyclic graph.  In DataJoint, each step is represented by a table in a relational database. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. DataJoint pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. foreign key a field that is linked to another table's primary key. primary key the subset of table attributes that uniquely identify each entity in the table. secondray attribute any field in a table not in the primary key. workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables."}, {"location": "design/alter/", "title": "Altering Populated Pipelines", "text": "<p>Tables can be altered after they have been declared and populated. This is useful when you want to add new secondary attributes or change the data type of existing attributes. Users can use the <code>definition</code> property to update a table's attributes and then use <code>alter</code> to apply the changes in the database. Currently, <code>alter</code> does not support changes to primary key attributes.</p> <p>Let's say we have a table <code>Student</code> with the following attributes:</p> <pre><code>@schema\nclass Student(dj.Manual):\n    definition = \"\"\"\n    student_id: int\n    ---\n    first_name: varchar(40)\n    last_name: varchar(40)\n    home_address: varchar(100)\n    \"\"\"\n</code></pre> <p>We can modify the table to include a new attribute <code>email</code>:</p> <pre><code>Student.definition = \"\"\"\nstudent_id: int\n---\nfirst_name: varchar(40)\nlast_name: varchar(40)\nhome_address: varchar(100)\nemail: varchar(100)\n\"\"\"\nStudent.alter()\n</code></pre> <p>The <code>alter</code> method will update the table in the database to include the new attribute <code>email</code> added by the user in the table's <code>definition</code> property.</p> <p>Similarly, you can modify the data type or length of an existing attribute. For example, to alter the <code>home_address</code> attribute to have a length of 200 characters:</p> <pre><code>Student.definition = \"\"\"\nstudent_id: int\n---\nfirst_name: varchar(40)\nlast_name: varchar(40)\nhome_address: varchar(200)\nemail: varchar(100)\n\"\"\"\nStudent.alter()\n</code></pre>"}, {"location": "design/diagrams/", "title": "Diagrams", "text": "<p>Diagrams are a great way to visualize the pipeline and understand the flow of data. DataJoint diagrams are based on entity relationship diagram (ERD). Objects of type <code>dj.Diagram</code> allow visualizing portions of the data pipeline in graphical form. Tables are depicted as nodes and dependencies as directed edges between them. The <code>draw</code> method plots the graph.</p>"}, {"location": "design/diagrams/#diagram-notation", "title": "Diagram notation", "text": "<p>Consider the following diagram</p> <p></p> <p>DataJoint uses the following conventions:</p> <ul> <li>Tables are indicated as nodes in the graph.   The corresponding class name is indicated by each node.</li> <li>Data tiers are indicated as colors and symbols:<ul> <li>Lookup=gray rectangle</li> <li>Manual=green rectangle</li> <li>Imported=blue oval</li> <li>Computed=red circle</li> <li>Part=black text The names of part tables are indicated in a smaller font.</li> </ul> </li> <li>Dependencies are indicated as edges in the graph and always directed downward, forming a directed acyclic graph.</li> <li>Foreign keys contained within the primary key are indicated as solid lines.   This means that the referenced table becomes part of the primary key of the dependent table.</li> <li>Foreign keys that are outside the primary key are indicated by dashed lines.</li> <li>If the primary key of the dependent table has no other attributes besides the foreign key, the foreign key is a thick solid line, indicating a 1:{0,1} relationship.</li> <li>Foreign keys made without renaming the foreign key attributes are in black whereas foreign keys that rename the attributes are indicated in red.</li> </ul>"}, {"location": "design/diagrams/#diagramming-an-entire-schema", "title": "Diagramming an entire schema", "text": "<p>To plot the Diagram for an entire schema, an Diagram object can be initialized with the schema object (which is normally used to decorate table objects)</p> <pre><code>import datajoint as dj\nschema = dj.Schema('my_database')\ndj.Diagram(schema).draw()\n</code></pre> <p>or alternatively an object that has the schema object as an attribute, such as the module defining a schema:</p> <pre><code>import datajoint as dj\nimport seq    # import the sequence module defining the seq database\ndj.Diagram(seq).draw()   # draw the Diagram\n</code></pre> <p>Note that calling the <code>.draw()</code> method is not necessary when working in a Jupyter notebook. You can simply let the object display itself, for example by entering <code>dj.Diagram(seq)</code> in a notebook cell. The Diagram will automatically render in the notebook by calling its <code>_repr_html_</code> method. A Diagram displayed without <code>.draw()</code> will be rendered as an SVG, and hovering the mouse over a table will reveal a compact version of the output of the <code>.describe()</code> method.</p>"}, {"location": "design/diagrams/#initializing-with-a-single-table", "title": "Initializing with a single table", "text": "<p>A <code>dj.Diagram</code> object can be initialized with a single table.</p> <pre><code>dj.Diagram(seq.Genome).draw()\n</code></pre> <p>A single node makes a rather boring graph but ERDs can be added together or subtracted from each other using graph algebra.</p>"}, {"location": "design/diagrams/#adding-diagrams-together", "title": "Adding diagrams together", "text": "<p>However two graphs can be added, resulting in new graph containing the union of the sets of nodes from the two original graphs. The corresponding foreign keys will be automatically</p> <pre><code># plot the Diagram with tables Genome and Species from module seq.\n(dj.Diagram(seq.Genome) + dj.Diagram(seq.Species)).draw()\n</code></pre>"}, {"location": "design/diagrams/#expanding-diagrams-upstream-and-downstream", "title": "Expanding diagrams upstream and downstream", "text": "<p>Adding a number to an Diagram object adds nodes downstream in the pipeline while subtracting a number from Diagram object adds nodes upstream in the pipeline.</p> <p>Examples:</p> <pre><code># Plot all the tables directly downstream from `seq.Genome`\n(dj.Diagram(seq.Genome)+1).draw()\n</code></pre> <pre><code># Plot all the tables directly upstream from `seq.Genome`\n(dj.Diagram(seq.Genome)-1).draw()\n</code></pre> <pre><code># Plot the local neighborhood of `seq.Genome`\n(dj.Diagram(seq.Genome)+1-1+1-1).draw()\n</code></pre>"}, {"location": "design/drop/", "title": "Drop", "text": "<p>The <code>drop</code> method completely removes a table from the database, including its definition. It also removes all dependent tables, recursively. DataJoint will first display the tables being dropped and the number of entities in each before prompting the user for confirmation to proceed.</p> <p>The <code>drop</code> method is often used during initial design to allow altered table definitions to take effect.</p> <pre><code># drop the Person table from its schema\nPerson.drop()\n</code></pre>"}, {"location": "design/drop/#dropping-part-tables", "title": "Dropping part tables", "text": "<p>A part table is usually removed as a consequence of calling <code>drop</code> on its master table. To enforce this workflow, calling <code>drop</code> directly on a part table produces an error. In some cases, it may be necessary to override this behavior. To remove a part table without removing its master, use the argument <code>force=True</code>.</p>"}, {"location": "design/integrity/", "title": "Data Integrity", "text": "<p>The term data integrity describes guarantees made by the data management process that prevent errors and corruption in data due to technical failures and human errors arising in the course of continuous use by multiple agents. DataJoint pipelines respect the following forms of data integrity: entity integrity, referential integrity, and group integrity as described in more detail below.</p>"}, {"location": "design/integrity/#entity-integrity", "title": "Entity integrity", "text": "<p>In a proper relational design, each table represents a collection of discrete real-world entities of some kind. Entity integrity is the guarantee made by the data management process that entities from the real world are reliably and uniquely represented in the database system. Entity integrity states that the data management process must prevent duplicate representations or misidentification of entities. DataJoint enforces entity integrity through the use of primary keys.</p> <p>Entity integrity breaks down when a process allows data pertaining to the same real-world entity to be entered into the database system multiple times. For example, a school database system may use unique ID numbers to distinguish students. Suppose the system automatically generates an ID number each time a student record is entered into the database without checking whether a record already exists for that student. Such a system violates entity integrity, because the same student may be assigned multiple ID numbers. The ID numbers succeed in uniquely identifying each student record but fail to do so for the actual students.</p> <p>Note that a database cannot guarantee or enforce entity integrity by itself. Entity integrity is a property of the entire data management process as a whole, including institutional practices and user actions in addition to database configurations.</p>"}, {"location": "design/integrity/#referential-integrity", "title": "Referential integrity", "text": "<p>Referential integrity is the guarantee made by the data management process that related data across the database remain present, correctly associated, and mutually consistent. Guaranteeing referential integrity means enforcing the constraint that no entity can exist in the database without all the other entities on which it depends. Referential integrity cannot exist without entity integrity: references to entity cannot be validated if the identity of the entity itself is not guaranteed.</p> <p>Referential integrity fails when a data management process allows new data to be entered that refers to other data missing from the database. For example, assume that each electrophysiology recording must refer to the mouse subject used during data collection. Perhaps an experimenter attempts to insert ephys data into the database that refers to a nonexistent mouse, due to a misspelling. A system guaranteeing referential integrity, such as DataJoint, will refuse the erroneous data.</p> <p>Enforcement of referential integrity does not stop with data ingest. Deleting data in DataJoint also deletes any dependent downstream data. Such cascading deletions are necessary to maintain referential integrity. Consider the deletion of a mouse subject without the deletion of the experimental sessions involving that mouse. A database that allows such deletion will break referential integrity, as the experimental sessions for the removed mouse depend on missing data. Any data management process that allows data to be deleted with no consideration of dependent data cannot maintain referential integrity.</p> <p>Updating data already present in a database system also jeopardizes referential integrity. For this reason, the DataJoint workflow does not include updates to entities once they have been ingested into a pipeline. Allowing updates to upstream entities would break the referential integrity of any dependent data downstream. For example, permitting a user to change the name of a mouse subject would invalidate any experimental sessions that used that mouse, presuming the mouse name was part of the primary key. The proper way to change data in DataJoint is to delete the existing entities and to insert corrected ones, preserving referential integrity.</p>"}, {"location": "design/integrity/#group-integrity", "title": "Group integrity", "text": "<p>Group integrity denotes the guarantee made by the data management process that entities composed of multiple parts always appear in their complete form. Group integrity in DataJoint is formalized through master-part relationships. The master-part relationship has important implications for dependencies, because a downstream entity depending on a master entity set may be considered to depend on the parts as well.</p>"}, {"location": "design/integrity/#relationships", "title": "Relationships", "text": "<p>In DataJoint, the term relationship is used rather generally to describe the effects of particular configurations of dependencies between multiple entity sets. It is often useful to classify relationships as one-to-one, many-to-one, one-to-many, and many-to-many.</p> <p>In a one-to-one relationship, each entity in a downstream table has exactly one corresponding entity in the upstream table. A dependency of an entity set containing the death dates of mice on an entity set describing the mice themselves would obviously be a one-to-one relationship, as in the example below.</p> <pre><code>@schema\nclass Mouse(dj.Manual):\ndefinition = \"\"\"\nmouse_name : varchar(64)\n---\nmouse_dob : datetime\n\"\"\"\n\n@schema\nclass MouseDeath(dj.Manual):\ndefinition = \"\"\"\n-&gt; Mouse\n---\ndeath_date : datetime\n\"\"\"\n</code></pre> <p></p> <p>In a one-to-many relationship, multiple entities in a downstream table may depend on the same entity in the upstream table. The example below shows a table containing individual channel data from multi-channel recordings, representing a one-to-many relationship.</p> <p><pre><code>@schema\nclass EEGRecording(dj.Manual):\ndefinition = \"\"\"\n-&gt; Session\neeg_recording_id : int\n---\neeg_system : varchar(64)\nnum_channels : int\n\"\"\"\n\n@schema\nclass ChannelData(dj.Imported):\ndefinition = \"\"\"\n-&gt; EEGRecording\nchannel_idx : int\n---\nchannel_data : longblob\n\"\"\"\n</code></pre> </p> <p>In a many-to-one relationship, each entity in a table is associated with multiple entities from another table. Many-to-one relationships between two tables are usually established using a separate membership table. The example below includes a table of mouse subjects, a table of subject groups, and a membership part table listing the subjects in each group. A many-to-one relationship exists between the <code>Mouse</code> table and the <code>SubjectGroup</code> table, with is expressed through entities in <code>GroupMember</code>.</p> <pre><code>@schema\nclass Mouse(dj.Manual):\ndefinition = \"\"\"\nmouse_name : varchar(64)\n---\nmouse_dob : datetime\n\"\"\"\n\n@schema\nclass SubjectGroup(dj.Manual):\ndefinition = \"\"\"\ngroup_number : int\n---\ngroup_name : varchar(64)\n\"\"\"\n\nclass GroupMember(dj.Part):\n     definition = \"\"\"\n     -&gt; master\n     -&gt; Mouse\n     \"\"\"\n</code></pre> <p></p> <p>In a many-to-many relationship, multiple entities in one table may each relate to multiple entities in another upstream table. Many-to-many relationships between two tables are usually established using a separate association table. Each entity in the association table links one entity from each of the two upstream tables it depends on. The below example of a many-to-many relationship contains a table of recording modalities and a table of multimodal recording sessions. Entities in a third table represent the modes used for each session.</p> <pre><code>@schema\nclass RecordingModality(dj.Lookup):\ndefinition = \"\"\"\nmodality : varchar(64)\n\"\"\"\n\n@schema\nclass MultimodalSession(dj.Manual):\ndefinition = \"\"\"\n-&gt; Session\nmodes : int\n\"\"\"\nclass SessionMode(dj.Part):\n     definition = \"\"\"\n     -&gt; master\n     -&gt; RecordingModality\n     \"\"\"\n</code></pre> <p></p> <p>The types of relationships between entity sets are expressed in the Diagram of a schema.</p>"}, {"location": "design/normalization/", "title": "Entity Normalization", "text": "<p>DataJoint uses a uniform way of representing any data. It does so in the form of entity sets, unordered collections of entities of the same type. The term entity normalization describes the commitment to represent all data as well-formed entity sets. Entity normalization is a conceptual refinement of the relational data model and is the central principle of the DataJoint model (Yatsenko et al., 2018). Entity normalization leads to clear and logical database designs and to easily comprehensible data queries.</p> <p>Entity sets are a type of relation (from the relational data model) and are often visualized as tables. Hence the terms relation, entity set, and table can be used interchangeably when entity normalization is assumed.</p>"}, {"location": "design/normalization/#criteria-of-a-well-formed-entity-set", "title": "Criteria of a well-formed entity set", "text": "<ol> <li>All elements of an entity set belong to the same well-defined and readily identified entity type from the model world.</li> <li>All attributes of an entity set are applicable directly to each of its elements, although some attribute values may be missing (set to null).</li> <li>All elements of an entity set must be distinguishable form each other by the same primary key.</li> <li>Primary key attribute values cannot be missing, i.e. set to null.</li> <li>All elements of an entity set participate in the same types of relationships with other entity sets.</li> </ol>"}, {"location": "design/normalization/#entity-normalization-in-schema-design", "title": "Entity normalization in schema design", "text": "<p>Entity normalization applies to schema design in that the designer is responsible for the identification of the essential entity types in their model world and of the dependencies among the entity types.</p> <p>The term entity normalization may also apply to a procedure for refactoring a schema design that does not meet the above criteria into one that does. In some cases, this may require breaking up some entity sets into multiple entity sets, which may cause some entities to be represented across multiple entity sets. In other cases, this may require converting attributes into their own entity sets. Technically speaking, entity normalization entails compliance with the Boyce-Codd normal form while lacking the representational power for the applicability of more complex normal forms (Kent, 1983). Adherence to entity normalization prevents redundancies in storage and data manipulation anomalies. The same criteria originally motivated the formulation of the classical relational normal forms.</p>"}, {"location": "design/normalization/#entity-normalization-in-data-queries", "title": "Entity normalization in data queries", "text": "<p>Entity normalization applies to data queries as well. DataJoint's query operators are designed to preserve the entity normalization of their inputs. For example, the outputs of operators restriction, proj, and aggr retain the same entity type as the (first) input. The join operator produces a new entity type comprising the pairing of the entity types of its inputs. Universal sets explicitly introduce virtual entity sets when necessary to accomplish a query.</p>"}, {"location": "design/normalization/#examples-of-poor-normalization", "title": "Examples of poor normalization", "text": "<p>Design choices lacking entity normalization may lead to data inconsistencies or anomalies. Below are several examples of poorly normalized designs and their normalized alternatives.</p>"}, {"location": "design/normalization/#indirect-attributes", "title": "Indirect attributes", "text": "<p>All attributes should apply to the entity itself. Avoid attributes that actually apply to one of the entity's other attributes. For example, consider the table <code>Author</code> with attributes <code>author_name</code>, <code>institution</code>, and <code>institution_address</code>. The attribute <code>institution_address</code> should really be held in a separate <code>Institution</code> table that <code>Author</code> depends on.</p>"}, {"location": "design/normalization/#repeated-attributes", "title": "Repeated attributes", "text": "<p>Avoid tables with repeated attributes of the same category. A better solution is to create a separate table that depends on the first (often a part table), with multiple individual entities rather than repeated attributes. For example, consider the table <code>Protocol</code> that includes the attributes <code>equipment1</code>, <code>equipment2</code>, and <code>equipment3</code>. A better design would be to create a <code>ProtocolEquipment</code> table that links each entity in <code>Protocol</code> with multiple entities in <code>Equipment</code> through dependencies.</p>"}, {"location": "design/normalization/#attributes-that-do-not-apply-to-all-entities", "title": "Attributes that do not apply to all entities", "text": "<p>All attributes should be relevant to every entity in a table. Attributes that apply only to a subset of entities in a table likely belong in a separate table containing only that subset of entities. For example, a table <code>Protocol</code> should include the attribute <code>stimulus</code> only if all experiment protocols include stimulation. If the not all entities in <code>Protocol</code> involve stimulation, then the <code>stimulus</code> attribute should be moved to a part table that has <code>Protocol</code> as its master. Only protocols using stimulation will have an entry in this part table.</p>"}, {"location": "design/normalization/#transient-attributes", "title": "Transient attributes", "text": "<p>Attributes should be relevant to all entities in a table at all times. Attributes that do not apply to all entities should be moved to another dependent table containing only the appropriate entities. This principle also applies to attributes that have not yet become meaningful for some entities or that will not remain meaningful indefinitely. For example, consider the table <code>Mouse</code> with attributes <code>birth_date</code> and <code>death_date</code>, where <code>death_date</code> is set to <code>NULL</code> for living mice. Since the <code>death_date</code> attribute is not meaningful for mice that are still living, the proper design would include a separate table <code>DeceasedMouse</code> that depends on <code>Mouse</code>. <code>DeceasedMouse</code> would only contain entities for dead mice, which improves integrity and averts the need for updates.</p>"}, {"location": "design/recall/", "title": "Work with Existing Pipelines", "text": ""}, {"location": "design/recall/#loading-classes", "title": "Loading Classes", "text": "<p>This section describes how to work with database schemas without access to the original code that generated the schema. These situations often arise when the database is created by another user who has not shared the generating code yet or when the database schema is created from a programming language other than Python.</p> <pre><code>import datajoint as dj\n</code></pre>"}, {"location": "design/recall/#working-with-schemas-and-their-modules", "title": "Working with schemas and their modules", "text": "<p>Typically a DataJoint schema is created as a dedicated Python module. This module defines a schema object that is used to link classes declared in the module to tables in the database schema. As an example, examine the university module: university.py.</p> <p>You may then import the module to interact with its tables:</p> <pre><code>import university as uni\ndj.Diagram(uni)\n</code></pre> <p></p> <p>Note that dj.Diagram can extract the diagram from a schema object or from a Python module containing its schema object, lending further support to the convention of one-to-one correspondence between database schemas and Python modules in a DataJoint project:</p> <p><code>dj.Diagram(uni)</code></p> <p>is equivalent to</p> <p><code>dj.Diagram(uni.schema)</code></p> <pre><code># students without majors\nuni.Student - uni.StudentMajor\n</code></pre> <p></p>"}, {"location": "design/recall/#spawning-missing-classes", "title": "Spawning missing classes", "text": "<p>Now imagine that you do not have access to <code>university.py</code> or you do not have its latest version. You can still connect to the database schema but you will not have classes declared to interact with it.</p> <p>So let's start over in this scenario.</p> <p>You may use the <code>dj.list_schemas</code> function (new in DataJoint 0.12.0) to list the names of database schemas available to you.</p> <pre><code>import datajoint as dj\ndj.list_schemas()\n</code></pre> <pre><code>*['dimitri_alter','dimitri_attach','dimitri_blob','dimitri_blobs',\n'dimitri_nphoton','dimitri_schema','dimitri_university','dimitri_uuid',\n'university']*\n</code></pre> <p>Just as with a new schema, we start by creating a schema object to connect to the chosen database schema:</p> <pre><code>schema = dj.Schema('dimitri_university')\n</code></pre> <p>If the schema already exists, <code>dj.Schema</code> is initialized as usual and you may plot the schema diagram. But instead of seeing class names, you will see the raw table names as they appear in the database.</p> <pre><code># let's plot its diagram\ndj.Diagram(schema)\n</code></pre> <p></p> <p>You may view the diagram but, at this point, there is no way to interact with these tables. A similar situation arises when another developer has added new tables to the schema but has not yet shared the updated module code with you. Then the diagram will show a mixture of class names and database table names.</p> <p>Now you may use the <code>spawn_missing_classes</code> method to spawn classes into the local namespace for any tables missing their classes:</p> <pre><code>schema.spawn_missing_classes()\ndj.Diagram(schema)\n</code></pre> <p></p> <p>Now you may interact with these tables as if they were declared right here in this namespace:</p> <pre><code># students without majors\nStudent - StudentMajor\n</code></pre> <p></p>"}, {"location": "design/recall/#creating-a-virtual-module", "title": "Creating a virtual module", "text": "<p>Virtual modules provide a way to access the classes corresponding to tables in a DataJoint schema without having to create local files.</p> <p><code>spawn_missing_classes</code> creates the new classes in the local namespace. However, it is often more convenient to import a schema with its Python module, equivalent to the Python command:</p> <pre><code>import university as uni\n</code></pre> <p>We can mimic this import without having access to <code>university.py</code> using the <code>VirtualModule</code> class object:</p> <pre><code>import datajoint as dj\n\nuni = dj.VirtualModule(module_name='university.py', schema_name='dimitri_university')\n</code></pre> <p>Now <code>uni</code> behaves as an imported module complete with the schema object and all the table classes.</p> <pre><code>dj.Diagram(uni)\n</code></pre> <p></p> <pre><code>uni.Student - uni.StudentMajor\n</code></pre> <p></p> <p><code>dj.VirtualModule</code> takes required arguments</p> <ul> <li><code>module_name</code>: displayed module name.</li> </ul> <ul> <li><code>schema_name</code>: name of the database in MySQL.</li> </ul> <p>And <code>dj.VirtualModule</code> takes optional arguments.</p> <p>First, <code>create_schema=False</code> assures that an error is raised when the schema does not already exist. Set it to <code>True</code> if you want to create an empty schema.</p> <pre><code>dj.VirtualModule('what', 'nonexistent')\n</code></pre> <p>Returns</p> <pre><code>---------------------------------------------------------------------------\nDataJointError                            Traceback (most recent call last)\n.\n.\n.\nDataJointError: Database named `nonexistent` was not defined. Set argument create_schema=True to create it.\n</code></pre> <p>The other optional argument, <code>create_tables=False</code> is passed to the schema object. It prevents the use of the schema object of the virtual module for creating new tables in the existing schema. This is a precautionary measure since virtual modules are often used for completed schemas. You may set this argument to <code>True</code> if you wish to add new tables to the existing schema. A more common approach in this scenario would be to create a new schema object and to use the <code>spawn_missing_classes</code> function to make the classes available.</p> <p>However, you if do decide to create new tables in an existing tables using the virtual module, you may do so by using the schema object from the module as the decorator for declaring new tables:</p> <pre><code>uni = dj.VirtualModule('university.py', 'dimitri_university', create_tables=True)\n</code></pre> <pre><code>@uni.schema\nclass Example(dj.Manual):\n    definition = \"\"\"\n    -&gt; uni.Student\n    ---\n    example : varchar(255)\n    \"\"\"\n</code></pre> <pre><code>dj.Diagram(uni)\n</code></pre> <p></p>"}, {"location": "design/schema/", "title": "Schema Creation", "text": ""}, {"location": "design/schema/#schemas", "title": "Schemas", "text": "<p>On the database server, related tables are grouped into a named collection called a schema. This grouping organizes the data and allows control of user access. A database server may contain multiple schemas each containing a subset of the tables. A single pipeline may comprise multiple schemas. Tables are defined within a schema, so a schema must be created before the creation of any tables.</p> <p>By convention, the <code>datajoint</code> package is imported as <code>dj</code>.   The documentation refers to the package as <code>dj</code> throughout.</p> <p>Create a new schema using the <code>dj.Schema</code> class object:</p> <pre><code>import datajoint as dj\nschema = dj.Schema('alice_experiment')\n</code></pre> <p>This statement creates the database schema <code>alice_experiment</code> on the server.</p> <p>The returned object <code>schema</code> will then serve as a decorator for DataJoint classes, as described in table declaration syntax.</p> <p>It is a common practice to have a separate Python module for each schema. Therefore, each such module has only one <code>dj.Schema</code> object defined and is usually named <code>schema</code>.</p> <p>The <code>dj.Schema</code> constructor can take a number of optional parameters after the schema name.</p> <ul> <li><code>context</code> - Dictionary for looking up foreign key references.   Defaults to <code>None</code> to use local context.</li> <li><code>connection</code> - Specifies the DataJoint connection object.   Defaults to <code>dj.conn()</code>.</li> <li><code>create_schema</code> - When <code>False</code>, the schema object will not create a schema on the database and will raise an error if one does not already exist.   Defaults to <code>True</code>.</li> <li><code>create_tables</code> - When <code>False</code>, the schema object will not create tables on the database and will raise errors when accessing missing tables.   Defaults to <code>True</code>.</li> </ul>"}, {"location": "design/schema/#working-with-existing-data", "title": "Working with existing data", "text": "<p>See the chapter recall for how to work with data in existing pipelines, including accessing a pipeline from one language when the pipeline was developed using another.</p>"}, {"location": "design/tables/attach/", "title": "External Data", "text": ""}, {"location": "design/tables/attach/#file-attachment-datatype", "title": "File Attachment Datatype", "text": ""}, {"location": "design/tables/attach/#configuration-usage", "title": "Configuration &amp; Usage", "text": "<p>Corresponding to issue #480, the <code>attach</code> attribute type allows users to <code>attach</code> files into DataJoint schemas as DataJoint-managed files. This is in contrast to traditional <code>blobs</code> which are encodings of programming language data structures such as arrays.</p> <p>The functionality is modeled after email attachments, where users <code>attach</code> a file along with a message and message recipients have access to a copy of that file upon retrieval of the message.</p> <p>For DataJoint <code>attach</code> attributes, DataJoint will copy the input file into a DataJoint store, hash the file contents, and track the input file name. Subsequent <code>fetch</code> operations will transfer a copy of the file to the local directory of the Python process and return a pointer to it's location for subsequent client usage. This allows arbitrary files to be <code>uploaded</code> or <code>attached</code> to a DataJoint schema for later use in processing. File integrity is preserved by checksum comparison against the attachment data and verifying the contents during retrieval.</p> <p>For example, given a <code>localattach</code> store:</p> <pre><code>dj.config['stores'] = {\n  'localattach': {\n    'protocol': 'file',\n    'location': '/data/attach'\n  }\n}\n</code></pre> <p>A <code>ScanAttachment</code> table can be created:</p> <pre><code>@schema\nclass ScanAttachment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    scan_image:    attach@localattach  # attached image scans\n    \"\"\"\n</code></pre> <p>Files can be added using an insert pointing to the source file:</p> <pre><code>&gt;&gt;&gt; ScanAttachment.insert1((0, '/input/image0.tif'))\n</code></pre> <p>And then retrieved to the current directory using <code>fetch</code>:</p> <pre><code>&gt;&gt;&gt; s0 = (ScanAttachment &amp; {'session_id': 0}).fetch1()\n&gt;&gt;&gt; s0\n{'session_id': 0, 'scan_image': './image0.tif'}\n&gt;&gt;&gt; fh = open(s0['scan_image'], 'rb')\n&gt;&gt;&gt; fh\n&lt;_io.BufferedReader name='./image0.tif')\n</code></pre>"}, {"location": "design/tables/attributes/", "title": "Datatypes", "text": "<p>DataJoint supports the following datatypes. To conserve database resources, use the smallest and most restrictive datatype sufficient for your data. This also ensures that only valid data are entered into the pipeline.</p>"}, {"location": "design/tables/attributes/#most-common-datatypes", "title": "Most common datatypes", "text": "<ul> <li><code>tinyint</code>: an 8-bit integer number, ranging from -128 to 127.</li> <li><code>tinyint unsigned</code>: an 8-bit positive integer number, ranging from 0 to 255.</li> <li><code>smallint</code>: a 16-bit integer number, ranging from -32,768 to 32,767.</li> <li><code>smallint unsigned</code>: a 16-bit positive integer, ranging from 0 to 65,535.</li> <li><code>int</code>: a 32-bit integer number, ranging from -2,147,483,648 to 2,147,483,647.</li> <li><code>int unsigned</code>: a 32-bit positive integer, ranging from 0 to 4,294,967,295.</li> <li><code>enum</code>: one of several explicitly enumerated values specified as strings.    Use this datatype instead of text strings to avoid spelling variations and to save    storage space.    For example, the datatype for an anesthesia attribute could be    <code>enum(\"urethane\", \"isoflurane\", \"fentanyl\")</code>.    Do not use enums in primary keys due to the difficulty of changing their definitions    consistently in multiple tables.</li> </ul> <ul> <li><code>date</code>: date as <code>'YYYY-MM-DD'</code>.</li> <li><code>time</code>: time as <code>'HH:MM:SS'</code>.</li> <li><code>datetime</code>: Date and time to the second as <code>'YYYY-MM-DD HH:MM:SS'</code></li> <li><code>timestamp</code>: Date and time to the second as <code>'YYYY-MM-DD HH:MM:SS'</code>.    The default value may be set to <code>CURRENT_TIMESTAMP</code>.    Unlike <code>datetime</code>, a <code>timestamp</code> value will be adjusted to the local time zone.</li> </ul> <ul> <li><code>char(N)</code>: a character string up to N characters (but always takes the entire N bytes to store).</li> <li><code>varchar(N)</code>: a text string of arbitrary length up to N characters that takes M+1 or M+2 bytes of storage, where M is the actual length of each stored string.</li> <li><code>float</code>: a single-precision floating-point number.    Takes 4 bytes.    Single precision is sufficient for many measurements.</li> </ul> <ul> <li><code>double</code>: a double-precision floating-point number.    Takes 8 bytes.    Because equality comparisons are error-prone, neither <code>float</code> nor <code>double</code> should be    used in primary keys.</li> <li><code>decimal(N,F)</code>: a fixed-point number with N total decimal digits and F fractional digits.    This datatype is well suited to represent numbers whose magnitude is well defined    and does not warrant the use of floating-point representation or requires precise    decimal representations (e.g. dollars and cents).    Because of its well-defined precision, <code>decimal</code> values can be used in equality    comparison and be included in primary keys.</li> </ul> <ul> <li><code>longblob</code>: arbitrary numeric array (e.g. matrix, image, structure), up to 4 GiB in size.    Numeric arrays are compatible between MATLAB and Python (NumPy).    The <code>longblob</code> and other <code>blob</code> datatypes can be configured to store data    externally by using the <code>blob@store</code> syntax.</li> </ul>"}, {"location": "design/tables/attributes/#less-common-but-supported-datatypes", "title": "Less common (but supported) datatypes", "text": "<ul> <li><code>decimal(N,F) unsigned</code>: same as <code>decimal</code>, but limited to nonnegative values.</li> <li><code>mediumint</code> a 24-bit integer number, ranging from -8,388,608 to 8,388,607.</li> <li><code>mediumint unsigned</code>: a 24-bit positive integer, ranging from 0 to 16,777,216.</li> <li><code>mediumblob</code>: arbitrary numeric array, up to 16 MiB</li> <li><code>blob</code>: arbitrary numeric array, up to 64 KiB</li> <li><code>tinyblob</code>: arbitrary numeric array, up to 256 bytes (actually smaller due to header info).</li> </ul>"}, {"location": "design/tables/attributes/#special-datajoint-only-datatypes", "title": "Special DataJoint-only datatypes", "text": "<p>These types abstract certain kinds of non-database data to facilitate use together with DataJoint.</p> <ul> <li><code>attach</code>: a file attachment similar to email attachments facillitating sending/receiving an opaque data file to/from a DataJoint pipeline.</li> </ul> <ul> <li><code>filepath@store</code>: a filepath used to link non-DataJoint managed files into a DataJoint pipeline.</li> </ul>"}, {"location": "design/tables/attributes/#datatypes-not-yet-supported", "title": "Datatypes not (yet) supported", "text": "<ul> <li><code>binary</code></li> <li><code>text</code></li> <li><code>longtext</code></li> <li><code>bit</code></li> </ul> <p>For additional information about these datatypes, see http://dev.mysql.com/doc/refman/5.6/en/data-types.html</p>"}, {"location": "design/tables/blobs/", "title": "Blobs", "text": "<p>DataJoint provides functionality for serializing and deserializing complex data types into binary blobs for efficient storage and compatibility with MATLAB's mYm serialization. This includes support for:</p> <ul> <li>Basic Python data types (e.g., integers, floats, strings, dictionaries).</li> <li>NumPy arrays and scalars.</li> <li>Specialized data types like UUIDs, decimals, and datetime objects.</li> </ul>"}, {"location": "design/tables/blobs/#serialization-and-deserialization-process", "title": "Serialization and Deserialization Process", "text": "<p>Serialization converts Python objects into a binary representation for efficient storage within the database. Deserialization converts the binary representation back into the original Python object.</p> <p>Blobs over 1 KiB are compressed using the zlib library to reduce storage requirements.</p>"}, {"location": "design/tables/blobs/#supported-data-types", "title": "Supported Data Types", "text": "<p>DataJoint supports the following data types for serialization:</p> <ul> <li>Scalars: Integers, floats, booleans, strings.</li> <li>Collections: Lists, tuples, sets, dictionaries.</li> <li>NumPy: Arrays, structured arrays, and scalars.</li> <li>Custom Types: UUIDs, decimals, datetime objects, MATLAB cell and struct arrays.</li> </ul>"}, {"location": "design/tables/customtype/", "title": "Custom Types", "text": "<p>In modern scientific research, data pipelines often involve complex workflows that generate diverse data types. From high-dimensional imaging data to machine learning models, these data types frequently exceed the basic representations supported by traditional relational databases. For example:</p> <ul> <li>A lab working on neural connectivity might use graph objects to represent brain   networks.</li> <li>Researchers processing raw imaging data might store custom objects for pre-processing   configurations.</li> <li>Computational biologists might store fitted machine learning models or parameter   objects for downstream predictions.</li> </ul> <p>To handle these diverse needs, DataJoint provides the <code>dj.AttributeAdapter</code> method. It enables researchers to store and retrieve complex, non-standard data types\u2014like Python objects or data structures\u2014in a relational database while maintaining the reproducibility, modularity, and query capabilities required for scientific workflows.</p>"}, {"location": "design/tables/customtype/#uses-in-scientific-research", "title": "Uses in Scientific Research", "text": "<p>Imagine a neuroscience lab studying neural connectivity. Researchers might generate graphs (e.g., networkx.Graph) to represent connections between brain regions, where:</p> <ul> <li>Nodes are brain regions.</li> <li>Edges represent connections weighted by signal strength or another metric.</li> </ul> <p>Storing these graph objects in a database alongside other experimental data (e.g., subject metadata, imaging parameters) ensures:</p> <ol> <li>Centralized Data Management: All experimental data and analysis results are stored    together for easy access and querying.</li> <li>Reproducibility: The exact graph objects used in analysis can be retrieved later for    validation or further exploration.</li> <li>Scalability: Graph data can be integrated into workflows for larger datasets or    across experiments.</li> </ol> <p>However, since graphs are not natively supported by relational databases, here\u2019s where <code>dj.AttributeAdapter</code> becomes essential. It allows researchers to define custom logic for serializing graphs (e.g., as edge lists) and deserializing them back into Python objects, bridging the gap between advanced data types and the database.</p>"}, {"location": "design/tables/customtype/#example-storing-graphs-in-datajoint", "title": "Example: Storing Graphs in DataJoint", "text": "<p>To store a networkx.Graph object in a DataJoint table, researchers can define a custom attribute type in a datajoint table class:</p> <pre><code>import datajoint as dj\n\nclass GraphAdapter(dj.AttributeAdapter):\n\n    attribute_type = 'longblob'   # this is how the attribute will be declared\n\n    def put(self, obj):\n        # convert the nx.Graph object  into an edge list\n        assert isinstance(obj, nx.Graph)\n        return list(obj.edges)\n\n    def get(self, value):\n        # convert edge list back into an nx.Graph\n        return nx.Graph(value)\n\n\n# instantiate for use as a datajoint type\ngraph = GraphAdapter()\n\n\n# define a table with a graph attribute\nschema = dj.schema('test_graphs')\n\n\n@schema\nclass Connectivity(dj.Manual):\n    definition = \"\"\"\n    conn_id : int\n    ---\n    conn_graph = null : &lt;graph&gt;  # a networkx.Graph object\n    \"\"\"\n</code></pre>"}, {"location": "design/tables/declare/", "title": "Declaration Syntax", "text": ""}, {"location": "design/tables/declare/#creating-tables", "title": "Creating Tables", "text": ""}, {"location": "design/tables/declare/#classes-represent-tables", "title": "Classes represent tables", "text": "<p>To make it easy to work with tables in MATLAB and Python, DataJoint programs create a separate class for each table. Computer programmers refer to this concept as object-relational mapping. For example, the class <code>experiment.Subject</code> in the DataJoint client language may correspond to the table called <code>subject</code> on the database server. Users never need to see the database directly; they only interact with data in the database by creating and interacting with DataJoint classes.</p>"}, {"location": "design/tables/declare/#data-tiers", "title": "Data tiers", "text": "<p>The table class must inherit from one of the following superclasses to indicate its data tier: <code>dj.Lookup</code>, <code>dj.Manual</code>, <code>dj.Imported</code>, <code>dj.Computed</code>, or <code>dj.Part</code>. See tiers and master-part.</p>"}, {"location": "design/tables/declare/#defining-a-table", "title": "Defining a table", "text": "<p>To define a DataJoint table in Python:</p> <ol> <li> <p>Define a class inheriting from the appropriate DataJoint class: <code>dj.Lookup</code>, <code>dj.Manual</code>, <code>dj.Imported</code> or <code>dj.Computed</code>.</p> </li> <li> <p>Decorate the class with the schema object (see schema)</p> </li> <li> <p>Define the class property <code>definition</code> to define the table heading.</p> </li> </ol> <p>For example, the following code defines the table <code>Person</code>:</p> <pre><code>import datajoint as dj\nschema = dj.Schema('alice_experiment')\n\n@schema\nclass Person(dj.Manual):\n     definition = '''\n          username : varchar(20)   # unique user name\n     ---\n     first_name : varchar(30)\n     last_name  : varchar(30)\n     '''\n</code></pre> <p>The <code>@schema</code> decorator uses the class name and the data tier to check whether an appropriate table exists on the database. If a table does not already exist, the decorator creates one on the database using the definition property. The decorator attaches the information about the table to the class, and then returns the class.</p> <p>The class will become usable after you define the <code>definition</code> property as described in Table definition.</p>"}, {"location": "design/tables/declare/#datajoint-classes-in-python", "title": "DataJoint classes in Python", "text": "<p>DataJoint for Python is implemented through the use of classes providing access to the actual tables stored on the database. Since only a single table exists on the database for any class, interactions with all instances of the class are equivalent. As such, most methods can be called on the classes themselves rather than on an object, for convenience. Whether calling a DataJoint method on a class or on an instance, the result will only depend on or apply to the corresponding table. All of the basic functionality of DataJoint is built to operate on the classes themselves, even when called on an instance. For example, calling <code>Person.insert(...)</code> (on the class) and <code>Person.insert(...)</code> (on an instance) both have the identical effect of inserting data into the table on the database server. DataJoint does not prevent a user from working with instances, but the workflow is complete without the need for instantiation. It is up to the user whether to implement additional functionality as class methods or methods called on instances.</p>"}, {"location": "design/tables/declare/#valid-class-names", "title": "Valid class names", "text": "<p>Note that in both MATLAB and Python, the class names must follow the CamelCase compound word notation:</p> <ul> <li>start with a capital letter and</li> <li>contain only alphanumerical characters (no underscores).</li> </ul> <p>Examples of valid class names:</p> <p><code>TwoPhotonScan</code>, <code>Scan2P</code>, <code>Ephys</code>, <code>MembraneVoltage</code></p> <p>Invalid class names:</p> <p><code>Two_photon_Scan</code>, <code>twoPhotonScan</code>, <code>2PhotonScan</code>, <code>membranePotential</code>, <code>membrane_potential</code></p>"}, {"location": "design/tables/declare/#table-definition", "title": "Table Definition", "text": "<p>DataJoint models data as sets of entities with shared attributes, often visualized as tables with rows and columns. Each row represents a single entity and the values of all of its attributes. Each column represents a single attribute with a name and a datatype, applicable to entity in the table. Unlike rows in a spreadsheet, entities in DataJoint don't have names or numbers: they can only be identified by the values of their attributes. Defining a table means defining the names and datatypes of the attributes as well as the constraints to be applied to those attributes. Both MATLAB and Python use the same syntax define tables.</p> <p>For example, the following code in defines the table <code>User</code>, that contains users of the database:</p> <p>The table definition is contained in the <code>definition</code> property of the class.</p> <pre><code>@schema\nclass User(dj.Manual):\n     definition = \"\"\"\n     # database users\n     username : varchar(20)   # unique user name\n     ---\n     first_name : varchar(30)\n     last_name  : varchar(30)\n     role : enum('admin', 'contributor', 'viewer')\n     \"\"\"\n</code></pre> <p>This defines the class <code>User</code> that creates the table in the database and provides all its data manipulation functionality.</p>"}, {"location": "design/tables/declare/#table-creation-on-the-database-server", "title": "Table creation on the database server", "text": "<p>Users do not need to do anything special to have a table created in the database. Tables are created at the time of class definition. In fact, table creation on the database is one of the jobs performed by the decorator <code>@schema</code> of the class.</p>"}, {"location": "design/tables/declare/#changing-the-definition-of-an-existing-table", "title": "Changing the definition of an existing table", "text": "<p>Once the table is created in the database, the definition string has no further effect. In other words, changing the definition string in the class of an existing table will not actually update the table definition. To change the table definition, one must first drop the existing table. This means that all the data will be lost, and the new definition will be applied to create the new empty table.</p> <p>Therefore, in the initial phases of designing a DataJoint pipeline, it is common to experiment with variations of the design before populating it with substantial amounts of data.</p> <p>It is possible to modify a table without dropping it. This topic is covered separately.</p>"}, {"location": "design/tables/declare/#reverse-engineering-the-table-definition", "title": "Reverse-engineering the table definition", "text": "<p>DataJoint objects provide the <code>describe</code> method, which displays the table definition used to define the table when it was created in the database. This definition may differ from the definition string of the class if the definition string has been edited after creation of the table.</p> <p>Examples</p> <pre><code>s = lab.User.describe()\n</code></pre>"}, {"location": "design/tables/declare/#definition-syntax", "title": "Definition Syntax", "text": "<p>The table definition consists of one or more lines. Each line can be one of the following:</p> <ul> <li>The optional first line starting with a <code>#</code> provides a description of the table's purpose.   It may also be thought of as the table's long title.</li> <li>A new attribute definition in any of the following forms (see Attributes for valid datatypes):   <code>name : datatype</code> <code>name : datatype # comment</code> <code>name = default : datatype</code> <code>name = default : datatype  # comment</code></li> <li>The divider <code>---</code> (at least three hyphens) separating primary key attributes above from secondary attributes below.</li> <li>A foreign key in the format <code>-&gt; ReferencedTable</code>.   (See Dependencies.)</li> </ul> <p>For example, the table for Persons may have the following definition:</p> <pre><code># Persons in the lab\nusername :  varchar(16)   #  username in the database\n---\nfull_name  : varchar(255)\nstart_date :  date   # date when joined the lab\n</code></pre> <p>This will define the table with attributes <code>username</code>, <code>full_name</code>, and <code>start_date</code>, in which <code>username</code> is the primary key.</p>"}, {"location": "design/tables/declare/#attribute-names", "title": "Attribute names", "text": "<p>Attribute names must be in lowercase and must start with a letter. They can only contain alphanumerical characters and underscores. The attribute name cannot exceed 64 characters.</p> <p>Valid attribute names    <code>first_name</code>, <code>two_photon_scan</code>, <code>scan_2p</code>, <code>two_photon_scan</code></p> <p>Invalid attribute names    <code>firstName</code>, <code>first name</code>, <code>2photon_scan</code>, <code>two-photon_scan</code>, <code>TwoPhotonScan</code></p> <p>Ideally, attribute names should be unique across all tables that are likely to be used in queries together. For example, tables often have attributes representing the start times of sessions, recordings, etc. Such attributes must be uniquely named in each table, such as <code>session_start_time</code> or <code>recording_start_time</code>.</p>"}, {"location": "design/tables/declare/#default-values", "title": "Default values", "text": "<p>Secondary attributes can be given default values. A default value will be used for an attribute if no other value is given at the time the entity is inserted into the table. Generally, default values are numerical values or character strings. Default values for dates must be given as strings as well, contained within quotes (with the exception of <code>CURRENT_TIMESTAMP</code>). Note that default values can only be used when inserting as a mapping. Primary key attributes cannot have default values (with the exceptions of <code>auto_increment</code> and <code>CURRENT_TIMESTAMP</code> attributes; see primary-key).</p> <p>An attribute with a default value of <code>NULL</code> is called a nullable attribute. A nullable attribute can be thought of as applying to all entities in a table but having an optional value that may be absent in some entities. Nullable attributes should not be used to indicate that an attribute is inapplicable to some entities in a table (see normalization). Nullable attributes should be used sparingly to indicate optional rather than inapplicable attributes that still apply to all entities in the table. <code>NULL</code> is a special literal value and does not need to be enclosed in quotes.</p> <p>Here are some examples of attributes with default values:</p> <pre><code>failures = 0 : int\ndue_date = \"2020-05-31\" : date\nadditional_comments = NULL : varchar(256)\n</code></pre>"}, {"location": "design/tables/dependencies/", "title": "Dependencies", "text": ""}, {"location": "design/tables/dependencies/#understanding-dependencies", "title": "Understanding dependencies", "text": "<p>A schema contains collections of tables of related data. Accordingly, entities in one table often derive some of their meaning or context from entities in other tables. A foreign key defines a dependency of entities in one table on entities in another within a schema. In more complex designs, dependencies can even exist between entities in tables from different schemas. Dependencies play a functional role in DataJoint and do not simply label the structure of a pipeline. Dependencies provide entities in one table with access to data in another table and establish certain constraints on entities containing a foreign key.</p> <p>A DataJoint pipeline, including the dependency relationships established by foreign keys, can be visualized as a graph with nodes and edges. The diagram of such a graph is called the entity relationship diagram or Diagram. The nodes of the graph are tables and the edges connecting them are foreign keys. The edges are directed and the overall graph is a directed acyclic graph, a graph with no loops.</p> <p>For example, the Diagram below is the pipeline for multipatching experiments</p> <p></p> <p>The graph defines the direction of the workflow. The tables at the top of the flow need to be populated first, followed by those tables one step below and so forth until the last table is populated at the bottom of the pipeline. The top of the pipeline tends to be dominated by lookup tables (gray stars) and manual tables (green squares). The middle has many imported tables (blue triangles), and the bottom has computed tables (red stars).</p>"}, {"location": "design/tables/dependencies/#defining-a-dependency", "title": "Defining a dependency", "text": "<p>Foreign keys are defined with arrows <code>-&gt;</code> in the table definition, pointing to another table.</p> <p>A foreign key may be defined as part of the primary-key.</p> <p>In the Diagram, foreign keys from the primary key are shown as solid lines. This means that the primary key of the referenced table becomes part of the primary key of the new table. A foreign key outside the primary key is indicated by dashed line in the ERD.</p> <p>For example, the following definition for the table <code>mp.Slice</code> has three foreign keys, including one within the primary key.</p> <pre><code># brain slice\n-&gt; mp.Subject\nslice_id        : smallint       # slice number within subject\n---\n-&gt; mp.BrainRegion\n-&gt; mp.Plane\nslice_date        : date                 # date of the slicing (not patching)\nthickness         : smallint unsigned    # slice thickness in microns\nexperimenter      : varchar(20)          # person who performed this experiment\n</code></pre> <p>You can examine the resulting table heading with</p> <pre><code>mp.BrainSlice.heading\n</code></pre> <p>The heading of <code>mp.Slice</code> may look something like</p> <pre><code>subject_id      : char(8)        # experiment subject id\nslice_id        : smallint       # slice number within subject\n---\nbrain_region        : varchar(12)        # abbreviated name for brain region\nplane               : varchar(12)        # plane of section\nslice_date          : date               # date of the slicing (not patching)\nthickness           : smallint unsigned  # slice thickness in microns\nexperimenter        : varchar(20)        # person who performed this experiment\n</code></pre> <p>This displayed heading reflects the actual attributes in the table. The foreign keys have been replaced by the primary key attributes of the referenced tables, including their data types and comments.</p>"}, {"location": "design/tables/dependencies/#how-dependencies-work", "title": "How dependencies work", "text": "<p>The foreign key <code>-&gt; A</code> in the definition of table <code>B</code> has the following effects:</p> <ol> <li>The primary key attributes of <code>A</code> are made part of <code>B</code>'s definition.</li> <li>A referential constraint is created in <code>B</code> with reference to <code>A</code>.</li> <li>If one does not already exist, an index is created to speed up searches in <code>B</code> for matches to <code>A</code>.    (The reverse search is already fast because it uses the primary key of <code>A</code>.)</li> </ol> <p>A referential constraint means that an entity in <code>B</code> cannot exist without a matching entity in <code>A</code>. Matching means attributes in <code>B</code> that correspond to the primary key of <code>A</code> must have the same values. An attempt to insert an entity into <code>B</code> that does not have a matching counterpart in <code>A</code> will fail. Conversely, deleting an entity from <code>A</code> that has matching entities in <code>B</code> will result in the deletion of those matching entities and so forth, recursively, downstream in the pipeline.</p> <p>When <code>B</code> references <code>A</code> with a foreign key, one can say that <code>B</code> depends on <code>A</code>. In DataJoint terms, <code>B</code> is the dependent table and <code>A</code> is the referenced table with respect to the foreign key from <code>B</code> to <code>A</code>.</p> <p>Note to those already familiar with the theory of relational databases: The usage of the words \"depends\" and \"dependency\" here should not be confused with the unrelated concept of functional dependencies that is used to define normal forms.</p>"}, {"location": "design/tables/dependencies/#referential-integrity", "title": "Referential integrity", "text": "<p>Dependencies enforce the desired property of databases known as referential integrity. Referential integrity is the guarantee made by the data management process that related data across the database remain present, correctly associated, and mutually consistent. Guaranteeing referential integrity means enforcing the constraint that no entity can exist in the database without all the other entities on which it depends. An entity in table <code>B</code> depends on an entity in table <code>A</code> when they belong to them or are computed from them.</p>"}, {"location": "design/tables/dependencies/#dependencies-with-renamed-attributes", "title": "Dependencies with renamed attributes", "text": "<p>In most cases, a dependency includes the primary key attributes of the referenced table as they appear in its table definition. Sometimes it can be helpful to choose a new name for a foreign key attribute that better fits the context of the dependent table. DataJoint provides the following projection syntax to rename the primary key attributes when they are included in the new table.</p> <p>The dependency</p> <pre><code>-&gt;  Table.project(new_attr='old_attr')\n</code></pre> <p>renames the primary key attribute <code>old_attr</code> of <code>Table</code> as <code>new_attr</code> before integrating it into the table definition. Any additional primary key attributes will retain their original names. For example, the table <code>Experiment</code> may depend on table <code>User</code> but rename the <code>user</code> attribute into <code>operator</code> as follows:</p> <pre><code>-&gt; User.proj(operator='user')\n</code></pre> <p>In the above example, an entity in the dependent table depends on exactly one entity in the referenced table. Sometimes entities may depend on multiple entities from the same table. Such a design requires a way to distinguish between dependent attributes having the same name in the reference table. For example, a table for <code>Synapse</code> may reference the table <code>Cell</code> twice as <code>presynaptic</code> and <code>postsynaptic</code>. The table definition may appear as</p> <pre><code># synapse between two cells\n-&gt; Cell.proj(presynaptic='cell_id')\n-&gt; Cell.proj(postsynaptic='cell_id')\n---\nconnection_strength : double  # (pA) peak synaptic current\n</code></pre> <p>If the primary key of <code>Cell</code> is (<code>animal_id</code>, <code>slice_id</code>, <code>cell_id</code>), then the primary key of <code>Synapse</code> resulting from the above definition will be (<code>animal_id</code>, <code>slice_id</code>, <code>presynaptic</code>, <code>postsynaptic</code>). Projection always returns all of the primary key attributes of a table, so <code>animal_id</code> and <code>slice_id</code> are included, with their original names.</p> <p>Note that the design of the <code>Synapse</code> table above imposes the constraint that the synapse can only be found between cells in the same animal and in the same slice.</p> <p>Allowing representation of synapses between cells from different slices requires the renamimg of <code>slice_id</code> as well:</p> <pre><code># synapse between two cells\n-&gt; Cell(presynaptic_slice='slice_id', presynaptic_cell='cell_id')\n-&gt; Cell(postsynaptic_slice='slice_id', postsynaptic_cell='cell_id')\n---\nconnection_strength : double  # (pA) peak synaptic current\n</code></pre> <p>In this case, the primary key of <code>Synapse</code> will be (<code>animal_id</code>, <code>presynaptic_slice</code>, <code>presynaptic_cell</code>, <code>postsynaptic_slice</code>, <code>postsynaptic_cell</code>). This primary key still imposes the constraint that synapses can only form between cells within the same animal but now allows connecting cells across different slices.</p> <p>In the Diagram, renamed foreign keys are shown as red lines with an additional dot node in the middle to indicate that a renaming took place.</p>"}, {"location": "design/tables/dependencies/#foreign-key-options", "title": "Foreign key options", "text": "<p>Note: Foreign key options are currently in development.</p> <p>Foreign keys allow the additional options <code>nullable</code> and <code>unique</code>, which can be inserted in square brackets following the arrow.</p> <p>For example, in the following table definition</p> <pre><code>rig_id  : char(4)   # experimental rig\n---\n-&gt; Person\n</code></pre> <p>each rig belongs to a person, but the table definition does not prevent one person owning multiple rigs. With the <code>unique</code> option, a person may only appear once in the entire table, which means that no one person can own more than one rig.</p> <pre><code>rig_id  : char(4)   # experimental rig\n---\n-&gt; [unique] Person\n</code></pre> <p>With the <code>nullable</code> option, a rig may not belong to anyone, in which case the foreign key attributes for <code>Person</code> are set to <code>NULL</code>:</p> <pre><code>rig_id  : char(4)   # experimental rig\n---\n-&gt; [nullable] Person\n</code></pre> <p>Finally with both <code>unique</code> and <code>nullable</code>, a rig may or may not be owned by anyone and each person may own up to one rig.</p> <pre><code>rig_id  : char(4)   # experimental rig\n---\n-&gt; [unique, nullable] Person\n</code></pre> <p>Foreign keys made from the primary key cannot be nullable but may be unique.</p>"}, {"location": "design/tables/filepath/", "title": "Filepath Datatype", "text": "<p>Note: Filepath Datatype is available as a preview feature in DataJoint Python v0.12. This means that the feature is required to be explicitly enabled. To do so, make sure to set the environment variable <code>FILEPATH_FEATURE_SWITCH=TRUE</code> prior to use.</p>"}, {"location": "design/tables/filepath/#configuration-usage", "title": "Configuration &amp; Usage", "text": "<p>Corresponding to issue #481, the <code>filepath</code> attribute type links DataJoint records to files already managed outside of DataJoint. This can aid in sharing data with other systems such as allowing an image viewer application to directly use files from a DataJoint pipeline, or to allow downstream tables to reference data which reside outside of DataJoint pipelines.</p> <p>To define a table using the <code>filepath</code> datatype, an existing DataJoint store should be created and then referenced in the new table definition. For example, given a simple store:</p> <pre><code>  dj.config['stores'] = {\n    'data': {\n      'protocol': 'file',\n      'location': '/data',\n      'stage': '/data'\n    }\n  }\n</code></pre> <p>we can define an <code>ScanImages</code> table as follows:</p> <pre><code>@schema\nclass ScanImages(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    image_id:    int\n    ---\n    image_path:  filepath@data\n    \"\"\"\n</code></pre> <p>This table can now be used for tracking paths within the <code>/data</code> local directory. For example:</p> <pre><code>&gt;&gt;&gt; ScanImages.insert1((0, 0, '/data/images/image_0.tif'))\n&gt;&gt;&gt; (ScanImages() &amp; {'session_id': 0}).fetch1(as_dict=True)\n{'session_id': 0, 'image_id': 0, 'image_path': '/data/images/image_0.tif'}\n</code></pre> <p>As can be seen from the example, unlike blob records, file paths are managed as path locations to the underlying file.</p>"}, {"location": "design/tables/filepath/#integrity-notes", "title": "Integrity Notes", "text": "<p>Unlike other data in DataJoint, data in <code>filepath</code> records are deliberately intended for shared use outside of DataJoint.  To help ensure integrity of <code>filepath</code> records, DataJoint will record a checksum of the file data on <code>insert</code>, and will verify this checksum on <code>fetch</code>. However, since the underlying file data may be shared with other applications, special care should be taken to ensure records stored in <code>filepath</code> attributes are not modified outside of the pipeline, or, if they are, that records in the pipeline are updated accordingly. A safe method of changing <code>filepath</code> data is as follows:</p> <ol> <li>Delete the <code>filepath</code> database record.     This will ensure that any downstream records in the pipeline depending     on the <code>filepath</code> record are purged from the database.</li> <li>Modify <code>filepath</code> data.</li> <li>Re-insert corresponding the <code>filepath</code> record.     This will add the record back to DataJoint with an updated file checksum.</li> <li>Compute any downstream dependencies, if needed.     This will ensure that downstream results dependent on the <code>filepath</code>     record are updated to reflect the newer <code>filepath</code> contents.</li> </ol>"}, {"location": "design/tables/filepath/#disable-fetch-verification", "title": "Disable Fetch Verification", "text": "<p>Note: Skipping the checksum is not recommended as it ensures file integrity i.e. downloaded files are not corrupted. With S3 stores, most of the time to complete a <code>.fetch()</code> is from the file download itself as opposed to evaluating the checksum. This option will primarily benefit <code>filepath</code> usage connected to a local <code>file</code> store.</p> <p>To disable checksums you can set a threshold in bytes for when to stop evaluating checksums like in the example below:</p> <pre><code>dj.config[\"filepath_checksum_size_limit\"] = 5 * 1024**3 # Skip for all files greater than 5GiB\n</code></pre> <p>The default is <code>None</code> which means it will always verify checksums.</p>"}, {"location": "design/tables/indexes/", "title": "Indexes", "text": "<p>Table indexes are data structures that allow fast lookups by an indexed attribute or combination of attributes.</p> <p>In DataJoint, indexes are created by one of the three mechanisms:</p> <ol> <li>Primary key</li> <li>Foreign key</li> <li>Explicitly defined indexes</li> </ol> <p>The first two mechanisms are obligatory. Every table has a primary key, which serves as an unique index. Therefore, restrictions by a primary key are very fast. Foreign keys create additional indexes unless a suitable index already exists.</p>"}, {"location": "design/tables/indexes/#indexes-for-single-primary-key-tables", "title": "Indexes for single primary key tables", "text": "<p>Let\u2019s say a mouse in the lab has a lab-specific ID but it also has a separate id issued by the animal facility.</p> <pre><code>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int  # lab-specific ID\n    ---\n    tag_id : int  # animal facility ID\n    \"\"\"\n</code></pre> <p>In this case, searching for a mouse by <code>mouse_id</code> is much faster than by <code>tag_id</code> because <code>mouse_id</code> is a primary key, and is therefore indexed.</p> <p>To make searches faster on fields other than the primary key or a foreign key, you can add a secondary index explicitly.</p> <p>Regular indexes are declared as <code>index(attr1, ..., attrN)</code> on a separate line anywhere in the table declaration (below the primary key divide).</p> <p>Indexes can be declared with unique constraint as <code>unique index (attr1, ..., attrN)</code>.</p> <p>Let\u2019s redeclare the table with a unique index on <code>tag_id</code>.</p> <p><pre><code>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int  # lab-specific ID\n    ---\n    tag_id : int  # animal facility ID\n    unique index (tag_id)\n    \"\"\"\n</code></pre> Now, searches with <code>mouse_id</code> and <code>tag_id</code> are similarly fast.</p>"}, {"location": "design/tables/indexes/#indexes-for-tables-with-multiple-primary-keys", "title": "Indexes for tables with multiple primary keys", "text": "<p>Let\u2019s now imagine that rats in a lab are identified by the combination of <code>lab_name</code> and <code>rat_id</code> in a table <code>Rat</code>.</p> <p><pre><code>@schema\nclass Rat(dj.Manual):\n    definition = \"\"\"\n    lab_name : char(16)\n    rat_id : int unsigned # lab-specific ID\n    ---\n    date_of_birth = null : date\n    \"\"\"\n</code></pre> Note that despite the fact that <code>rat_id</code> is in the index, searches by <code>rat_id</code> alone are not helped by the index because it is not first in the index. This is similar to searching for a word in a dictionary that orders words alphabetically. Searching by the first letters of a word is easy but searching by the last few letters of a word requires scanning the whole dictionary.</p> <p>In this table, the primary key is a unique index on the combination <code>(lab_name, rat_id)</code>. Therefore searches on these attributes or on <code>lab_name</code> alone are fast. But this index cannot help searches on <code>rat_id</code> alone. Similarly, searing by <code>date_of_birth</code> requires a full-table scan and is inefficient.</p> <p>To speed up searches by the <code>rat_id</code> and <code>date_of_birth</code>, we can explicit indexes to <code>Rat</code>:</p> <pre><code>@schema\nclass Rat2(dj.Manual):\n    definition = \"\"\"\n    lab_name : char(16)\n    rat_id : int unsigned # lab-specific ID\n    ---\n    date_of_birth = null : date\n\n    index(rat_id)\n    index(date_of_birth)\n    \"\"\"\n</code></pre>"}, {"location": "design/tables/lookup/", "title": "Lookup Tables", "text": "<p>Lookup tables contain basic facts that are not specific to an experiment and are fairly persistent. Their contents are typically small. In GUIs, lookup tables are often used for drop-down menus or radio buttons. In computed tables, they are often used to specify alternative methods for computations. Lookup tables are commonly populated from their <code>contents</code> property. In a diagram they are shown in gray. The decision of which tables are lookup tables and which are manual can be somewhat arbitrary.</p> <p>The table below is declared as a lookup table with its contents property provided to generate entities.</p> <pre><code>@schema\nclass User(dj.Lookup):\n     definition = \"\"\"\n     # users in the lab\n     username : varchar(20)   # user in the lab\n     ---\n     first_name  : varchar(20)   # user first name\n     last_name   : varchar(20)   # user last name\n     \"\"\"\n     contents = [\n          ['cajal', 'Santiago', 'Cajal'],\n          ['hubel', 'David', 'Hubel'],\n          ['wiesel', 'Torsten', 'Wiesel']\n          ]\n</code></pre>"}, {"location": "design/tables/manual/", "title": "Manual Tables", "text": "<p>Manual tables are populated during experiments through a variety of interfaces. Not all manual information is entered by typing. Automated software can enter it directly into the database. What makes a manual table manual is that it does not perform any computations within the DataJoint pipeline.</p> <p>The following code defines three manual tables <code>Animal</code>, <code>Session</code>, and <code>Scan</code>:</p> <pre><code>@schema\nclass Animal(dj.Manual):\n     definition = \"\"\"\n     # information about animal\n     animal_id : int  # animal id assigned by the lab\n     ---\n     -&gt; Species\n     date_of_birth=null : date  # YYYY-MM-DD optional\n     sex='' : enum('M', 'F', '')   # leave empty if unspecified\n     \"\"\"\n\n@schema\nclass Session(dj.Manual):\n     definition = \"\"\"\n     # Experiment Session\n     -&gt; Animal\n     session  : smallint  # session number for the animal\n     ---\n     session_date : date  # YYYY-MM-DD\n     -&gt; User\n     -&gt; Anesthesia\n     -&gt; Rig\n     \"\"\"\n\n@schema\nclass Scan(dj.Manual):\n     definition = \"\"\"\n     # Two-photon imaging scan\n     -&gt; Session\n     scan : smallint  # scan number within the session\n     ---\n     -&gt; Lens\n     laser_wavelength : decimal(5,1)  # um\n     laser_power      : decimal(4,1)  # mW\n     \"\"\"\n</code></pre>"}, {"location": "design/tables/master-part/", "title": "Master-Part Relationship", "text": "<p>Often an entity in one table is inseparably associated with a group of entities in another, forming a master-part relationship. The master-part relationship ensures that all parts of a complex representation appear together or not at all. This has become one of the most powerful data integrity principles in DataJoint.</p> <p>As an example, imagine segmenting an image to identify regions of interest. The resulting segmentation is inseparable from the ROIs that it produces. In this case, the two tables might be called <code>Segmentation</code> and <code>Segmentation.ROI</code>.</p> <p>In Python, the master-part relationship is expressed by making the part a nested class of the master. The part is subclassed from <code>dj.Part</code> and does not need the <code>@schema</code> decorator.</p> <pre><code>@schema\nclass Segmentation(dj.Computed):\n     definition = \"\"\"  # image segmentation\n     -&gt; Image\n     \"\"\"\n\n     class ROI(dj.Part):\n          definition = \"\"\"  # Region of interest resulting from segmentation\n          -&gt; Segmentation\n          roi  : smallint   # roi number\n          ---\n          roi_pixels  : longblob   #  indices of pixels\n          roi_weights : longblob   #  weights of pixels\n          \"\"\"\n\n     def make(self, key):\n          image = (Image &amp; key).fetch1('image')\n          self.insert1(key)\n          count = itertools.count()\n          Segmentation.ROI.insert(\n               dict(key, roi=next(count), roi_pixel=roi_pixels, roi_weights=roi_weights)\n               for roi_pixels, roi_weights in mylib.segment(image))\n</code></pre>"}, {"location": "design/tables/master-part/#populating", "title": "Populating", "text": "<p>Master-part relationships can form in any data tier, but DataJoint observes them more strictly for auto-populated tables. To populate both the master <code>Segmentation</code> and the part <code>Segmentation.ROI</code>, it is sufficient to call the <code>populate</code> method of the master:</p> <pre><code>Segmentation.populate()\n</code></pre> <p>Note that the entities in the master and the matching entities in the part are inserted within a single <code>make</code> call of the master, which means that they are a processed inside a single transactions: either all are inserted and committed or the entire transaction is rolled back. This ensures that partial results never appear in the database.</p> <p>For example, imagine that a segmentation is performed, but an error occurs halfway through inserting the results. If this situation were allowed to persist, then it might appear that 20 ROIs were detected where 45 had actually been found.</p>"}, {"location": "design/tables/master-part/#deleting", "title": "Deleting", "text": "<p>To delete from a master-part pair, one should never delete from the part tables directly. The only valid method to delete from a part table is to delete the master. This has been an unenforced rule, but upcoming versions of DataJoint will prohibit direct deletes from the master table. DataJoint's delete operation is also enclosed in a transaction.</p> <p>Together, the rules of master-part relationships ensure a key aspect of data integrity: results of computations involving multiple components and steps appear in their entirety or not at all.</p>"}, {"location": "design/tables/master-part/#multiple-parts", "title": "Multiple parts", "text": "<p>The master-part relationship cannot be chained or nested. DataJoint does not allow part tables of other part tables per se. However, it is common to have a master table with multiple part tables that depend on each other. For example:</p> <pre><code>@schema\nclass ArrayResponse(dj.Computed):\ndefinition = \"\"\"\narray: int\n\"\"\"\n\nclass ElectrodeResponse(dj.Part):\ndefinition = \"\"\"\n-&gt; master\nelectrode: int    # electrode number on the probe\n\"\"\"\n\nclass ChannelResponse(dj.Part):\ndefinition = \"\"\"\n-&gt; ElectrodeResponse\nchannel: int\n---\nresponse: longblob  # response of a channel\n\"\"\"\n</code></pre> <p>Conceptually, one or more channels belongs to an electrode, and one or more electrodes belong to an array. This example assumes that information about an array's response (which consists ultimately of the responses of multiple electrodes each consisting of multiple channel responses) including it's electrodes and channels are entered together.</p>"}, {"location": "design/tables/primary/", "title": "Primary Key", "text": ""}, {"location": "design/tables/primary/#primary-keys-in-datajoint", "title": "Primary keys in DataJoint", "text": "<p>Entities in tables are neither named nor numbered. DataJoint does not answer questions of the type \"What is the 10th element of this table?\" Instead, entities are distinguished by the values of their attributes. Furthermore, the entire entity is not required for identification. In each table, a subset of its attributes are designated to be the primary key. Attributes in the primary key alone are sufficient to differentiate any entity from any other within the table.</p> <p>Each table must have exactly one primary key: a subset of its attributes that uniquely identify each entity in the table. The database uses the primary key to prevent duplicate entries, to relate data across tables, and to accelerate data queries. The choice of the primary key will determine how you identify entities. Therefore, make the primary key short, expressive, and persistent.</p> <p>For example, mice in our lab are assigned unique IDs. The mouse ID number <code>animal_id</code> of type <code>smallint</code> can serve as the primary key for the table <code>Mice</code>. An experiment performed on a mouse may be identified in the table <code>Experiments</code> by two attributes: <code>animal_id</code> and <code>experiment_number</code>.</p> <p>DataJoint takes the concept of primary keys somewhat more seriously than other models and query languages. Even table expressions, i.e. those tables produced through operations on other tables, have a well-defined primary key. All operators on tables are designed in such a way that the results always have a well-defined primary key.</p> <p>In all representations of tables in DataJoint, the primary key attributes are always listed before other attributes and highlighted for emphasis (e.g. in a bold font or marked with an asterisk *)</p>"}, {"location": "design/tables/primary/#defining-a-primary-key", "title": "Defining a primary key", "text": "<p>In table declarations, the primary key attributes always come first and are separated from the other attributes with a line containing at least three hyphens. For example, the following is the definition of a table containing database users where <code>username</code> is the primary key.</p> <pre><code># database users\nusername : varchar(20)   # unique user name\n---\nfirst_name : varchar(30)\nlast_name  : varchar(30)\nrole : enum('admin', 'contributor', 'viewer')\n</code></pre>"}, {"location": "design/tables/primary/#entity-integrity", "title": "Entity integrity", "text": "<p>The primary key defines and enforces the desired property of databases known as entity integrity. Entity integrity ensures that there is a one-to-one and unambiguous mapping between real-world entities and their representations in the database system. The data management process must prevent any duplication or misidentification of entities.</p> <p>To enforce entity integrity, DataJoint implements several rules:</p> <ul> <li>Every table must have a primary key.</li> <li>Primary key attributes cannot have default values (with the exception of <code>auto_increment</code> and <code>CURRENT_TIMESTAMP</code>; see below).</li> <li>Operators on tables are defined with respect to the primary key and preserve a primary key in their results.</li> </ul>"}, {"location": "design/tables/primary/#datatypes-in-primary-keys", "title": "Datatypes in primary keys", "text": "<p>All integer types, dates, timestamps, and short character strings make good primary key attributes. Character strings are somewhat less suitable because they can be long and because they may have invisible trailing spaces. Floating-point numbers should be avoided because rounding errors may lead to misidentification of entities. Enums are okay as long as they do not need to be modified after dependencies are already created referencing the table. Finally, DataJoint does not support blob types in primary keys.</p> <p>The primary key may be composite, i.e. comprising several attributes. In DataJoint, hierarchical designs often produce tables whose primary keys comprise many attributes.</p>"}, {"location": "design/tables/primary/#choosing-primary-key-attributes", "title": "Choosing primary key attributes", "text": "<p>A primary key comprising real-world attributes is a good choice when such real-world attributes are already properly and permanently assigned. Whatever characteristics are used to uniquely identify the actual entities can be used to identify their representations in the database.</p> <p>If there are no attributes that could readily serve as a primary key, an artificial attribute may be created solely for the purpose of distinguishing entities. In such cases, the primary key created for management in the database must also be used to uniquely identify the entities themselves. If the primary key resides only in the database while entities remain indistinguishable in the real world, then the process cannot ensure entity integrity. When a primary key is created as part of data management rather than based on real-world attributes, an institutional process must ensure the uniqueness and permanence of such an identifier.</p> <p>For example, the U.S. government assigns every worker an identifying attribute, the social security number. However, the government must go to great lengths to ensure that this primary key is assigned exactly once, by checking against other less convenient candidate keys (i.e. the combination of name, parents' names, date of birth, place of birth, etc.). Just like the SSN, well managed primary keys tend to get institutionalized and find multiple uses.</p> <p>Your lab must maintain a system for uniquely identifying important entities. For example, experiment subjects and experiment protocols must have unique IDs. Use these as the primary keys in the corresponding tables in your DataJoint databases.</p>"}, {"location": "design/tables/primary/#using-hashes-as-primary-keys", "title": "Using hashes as primary keys", "text": "<p>Some tables include too many attributes in their primary keys. For example, the stimulus condition in a psychophysics experiment may have a dozen parameters such that a change in any one of them makes a different valid stimulus condition. In such a case, all the attributes would need to be included in the primary key to ensure entity integrity. However, long primary keys make it difficult to reference individual entities. To be most useful, primary keys need to be relatively short.</p> <p>This problem is effectively solved through the use of a hash of all the identifying attributes as the primary key. For example, MD5 or SHA-1 hash algorithms can be used for this purpose. To keep their representations human-readable, they may be encoded in base-64 ASCII. For example, the 128-bit MD5 hash can be represented by 21 base-64 ASCII characters, but for many applications, taking the first 8 to 12 characters is sufficient to avoid collisions.</p>"}, {"location": "design/tables/primary/#auto_increment", "title": "<code>auto_increment</code>", "text": "<p>Some entities are created by the very action of being entered into the database. The action of entering them into the database gives them their identity. It is impossible to duplicate them since entering the same thing twice still means creating two distinct entities.</p> <p>In such cases, the use of an auto-incremented primary key is warranted. These are declared by adding the word <code>auto_increment</code> after the data type in the declaration. The datatype must be an integer. Then the database will assign incrementing numbers at each insert.</p> <p>The example definition below defines an auto-incremented primary key</p> <pre><code># log entries\nentry_id  :  smallint auto_increment\n---\nentry_text :  varchar(4000)\nentry_time = CURRENT_TIMESTAMP : timestamp(3)  # automatic timestamp with millisecond precision\n</code></pre> <p>DataJoint passes <code>auto_increment</code> behavior to the underlying MySQL and therefore it has the same limitation: it can only be used for tables with a single attribute in the primary key.</p> <p>If you need to auto-increment an attribute in a composite primary key, you will need to do so programmatically within a transaction to avoid collisions.</p> <p>For example, let\u2019s say that you want to auto-increment <code>scan_idx</code> in a table called <code>Scan</code> whose primary key is <code>(animal_id, session, scan_idx)</code>. You must already have the values for <code>animal_id</code> and <code>session</code> in the dictionary <code>key</code>. Then you can do the following:</p> <pre><code>U().aggr(Scan &amp; key, next='max(scan_idx)+1')\n\n# or\n\nSession.aggr(Scan, next='max(scan_idx)+1') &amp; key\n</code></pre> <p>Note that the first option uses a universal set.</p>"}, {"location": "design/tables/tiers/", "title": "Data Tiers", "text": "<p>DataJoint assigns all tables to one of the following data tiers that differentiate how the data originate.</p>"}, {"location": "design/tables/tiers/#table-tiers", "title": "Table tiers", "text": "Tier Superclass Description Lookup <code>dj.Lookup</code> Small tables containing general facts and settings of the data pipeline; not specific to any experiment or dataset. Manual <code>dj.Manual</code> Data entered from outside the pipeline, either by hand or with external helper scripts. Imported <code>dj.Imported</code> Data ingested automatically inside the pipeline but requiring access to data outside the pipeline. Computed <code>dj.Computed</code> Data computed automatically entirely inside the pipeline. <p>Table data tiers indicate to database administrators how valuable the data are. Manual data are the most valuable, as re-entry may be tedious or impossible. Computed data are safe to delete, as the data can always be recomputed from within DataJoint. Imported data are safer than manual data but less safe than computed data because of dependency on external data sources. With these considerations, database administrators may opt not to back up computed data, for example, or to back up imported data less frequently than manual data.</p> <p>The data tier of a table is specified by the superclass of its class. For example, the User class in definitions uses the <code>dj.Manual</code> superclass. Therefore, the corresponding User table on the database would be of the Manual tier. Furthermore, the classes for imported and computed tables have additional capabilities for automated processing as described in Auto-populate.</p>"}, {"location": "design/tables/tiers/#internal-conventions-for-naming-tables", "title": "Internal conventions for naming tables", "text": "<p>On the server side, DataJoint uses a naming scheme to generate a table name corresponding to a given class. The naming scheme includes prefixes specifying each table's data tier.</p> <p>First, the name of the class is converted from <code>CamelCase</code> to <code>snake_case</code> (separation by underscores). Then the name is prefixed according to the data tier.</p> <ul> <li><code>Manual</code> tables have no prefix.</li> <li><code>Lookup</code> tables are prefixed with <code>#</code>.</li> <li><code>Imported</code> tables are prefixed with <code>_</code>, a single underscore.</li> <li><code>Computed</code> tables are prefixed with <code>__</code>, two underscores.</li> </ul> <p>For example:</p> <p>The table for the class <code>StructuralScan</code> subclassing <code>dj.Manual</code> will be named <code>structural_scan</code>.</p> <p>The table for the class <code>SpatialFilter</code> subclassing <code>dj.Lookup</code> will be named <code>#spatial_filter</code>.</p> <p>Again, the internal table names including prefixes are used only on the server side. These are never visible to the user, and DataJoint users do not need to know these conventions However, database administrators may use these naming patterns to set backup policies or to restrict access based on data tiers.</p>"}, {"location": "design/tables/tiers/#part-tables", "title": "Part tables", "text": "<p>Part tables do not have their own tier. Instead, they share the same tier as their master table. The prefix for part tables also differs from the other tiers. They are prefixed by the name of their master table, separated by two underscores.</p> <p>For example, the table for the class <code>Channel(dj.Part)</code> with the master <code>Ephys(dj.Imported)</code> will be named <code>_ephys__channel</code>.</p>"}, {"location": "internal/transpilation/", "title": "Transpiler Design", "text": "<p>This section contains the information and reasoning that went into the design of the DataJoint-to-SQL transpiler.</p> <p>MySQL appears to differ from standard SQL by the sequence of evaluating the clauses of the SELECT statement.</p> <pre><code>Standard SQL: FROM &gt; WHERE &gt; GROUP BY &gt; HAVING &gt; SELECT\nMySQL: FROM &gt; WHERE &gt; SELECT &gt; GROUP BY &gt; HAVING\n</code></pre> <p>Moving <code>SELECT</code> to an earlier phase allows the <code>GROUP BY</code> and <code>HAVING</code> clauses to use alias column names created by the <code>SELECT</code> clause. The current implementation targets the MySQL implementation where table column aliases can be used in <code>HAVING</code>. If postgres or CockroachDB cannot be coerced to work this way, restrictions of aggregations will have to be updated accordingly.</p>"}, {"location": "internal/transpilation/#queryexpression", "title": "QueryExpression", "text": "<p><code>QueryExpression</code> is the main object representing a distinct <code>SELECT</code> statement. It implements operators <code>&amp;</code>, <code>*</code>, and <code>proj</code>  \u2014 restriction, join, and projection.</p> <p>Property <code>heading</code> describes all attributes.</p> <p>Operator <code>proj</code> creates a new heading.</p> <p>Property <code>restriction</code> contains the <code>AndList</code> of conditions. Operator <code>&amp;</code> creates a new restriction appending the new condition to the input's restriction.</p> <p>Property <code>support</code> represents the <code>FROM</code> clause and contains a list of either <code>QueryExpression</code> objects or table names in the case of base queries. The join operator <code>*</code> adds new elements to the <code>support</code> attribute.</p> <p>At least one element must be present in <code>support</code>. Multiple elements in <code>support</code> indicate a join.</p> <p>From the user's perspective <code>QueryExpression</code> objects are immutable: once created they cannot be modified. All operators derive new objects.</p>"}, {"location": "internal/transpilation/#alias-attributes", "title": "Alias attributes", "text": "<p><code>proj</code> can create an alias attribute by renaming an existing attribute or calculating a new attribute. Alias attributes are the primary reason why subqueries are sometimes required.</p>"}, {"location": "internal/transpilation/#subqueries", "title": "Subqueries", "text": "<p>Projections, restrictions, and joins do not necessarily trigger new subqueries: the resulting <code>QueryExpression</code> object simply merges the properties of its inputs into self: <code>heading</code>, <code>restriction</code>, and <code>support</code>.</p> <p>The input object is treated as a subquery in the following cases:</p> <ol> <li>A restriction is applied that uses alias attributes in the heading.</li> <li>A projection uses an alias attribute to create a new alias attribute.</li> <li>A join is performed on an alias attribute.</li> <li>An Aggregation is used a restriction.</li> </ol> <p>An error arises if</p> <ol> <li>If a restriction or a projection attempts to use attributes not in the current heading.</li> <li>If attempting to join on attributes that are not join-compatible</li> <li>If attempting to restrict by a non-join-compatible expression</li> </ol> <p>A subquery is created by creating a new <code>QueryExpression</code> object (or a subclass object) with its <code>support</code> pointing to the input object.</p>"}, {"location": "internal/transpilation/#join-compatibility", "title": "Join compatibility", "text": "<p>The join is always natural (i.e. equijoin on the namesake attributes).</p> <p>Before version 0.13: As of version <code>0.12.*</code> and earlier, two query expressions were considered join-compatible if their namesake attributes were the primary key of at least one of the input expressions. This rule was easiest to implement but does not provide best semantics.</p> <p>Version 0.13: In version <code>0.13.*</code>, two query expressions are considered join-compatible if their namesake attributes are either in the primary key or in a foreign key in both input expressions.</p> <p>Future (potentially version 0.14+): This compatibility requirement will be further restricted to require that the namesake attributes ultimately derive from the same primary key attribute by being passed down through foreign keys.</p> <p>The same join compatibility rules apply when restricting one query expression with another.</p>"}, {"location": "internal/transpilation/#join-mechanics", "title": "Join mechanics", "text": "<p>Any restriction applied to the inputs of a join can be applied to its output. Therefore, those inputs that are not turned into queries donate their supports, restrictions, and projections to the join itself.</p>"}, {"location": "internal/transpilation/#table", "title": "Table", "text": "<p><code>Table</code> is a subclass of <code>QueryExpression</code> implementing table manipulation methods such as <code>insert</code>, <code>insert1</code>, <code>delete</code>, <code>update1</code>, and <code>drop</code>.</p> <p>The restriction operator <code>&amp;</code> applied to a <code>Table</code> preserves its class identity so that the result remains of type <code>Table</code>. However, <code>proj</code> converts the result into a <code>QueryExpression</code> object. This may produce a base query that is not an instance of Table.</p>"}, {"location": "internal/transpilation/#aggregation", "title": "Aggregation", "text": "<p><code>Aggregation</code> is a subclass of <code>QueryExpression</code>. Its main input is the aggregating query expression and it takes an additional second input \u2014 the aggregated query expression.</p> <p>The SQL equivalent of aggregation is</p> <ol> <li>the NATURAL LEFT JOIN of the two inputs.</li> <li>followed by a GROUP BY on the primary key arguments of the first input</li> <li>followed by a projection.</li> </ol> <p>The projection works the same as <code>.proj</code> with respect to the first input. With respect to the second input, the projection part of aggregation allows only calculated attributes that use aggregating functions (eg <code>SUM</code>, <code>AVG</code>, <code>COUNT</code>) applied to the attributes of the aggregated (second) input and non-aggregating functions on the attribute of the aggregating (first) input.</p> <p><code>Aggregation</code> supports all the same operators as <code>QueryExpression</code> except:</p> <ol> <li><code>restriction</code> turns into a <code>HAVING</code> clause instead of a <code>WHERE</code> clause. This allows applying any valid restriction without making a subquery (at least for MySQL). Therefore, restricting an <code>Aggregation</code> object never results in a subquery.</li> <li>In joins, aggregation always turns into a subquery.</li> </ol> <p>All other rules for subqueries remain the same as for <code>QueryExpression</code></p>"}, {"location": "internal/transpilation/#union", "title": "Union", "text": "<p><code>Union</code> is a subclass of <code>QueryExpression</code>. A <code>Union</code> object results from the <code>+</code> operator on two <code>QueryExpression</code> objects. Its <code>support</code> property contains the list of expressions (at least two) to unify. Thus the <code>+</code> operator on unions simply merges their supports, making a bigger union.</p> <p>The <code>Union</code> operator performs an OUTER JOIN of its inputs provided that the inputs have the same primary key and no secondary attributes in common.</p> <p>Union treats all its inputs as subqueries except for unrestricted Union objects.</p>"}, {"location": "internal/transpilation/#universal-sets-dju", "title": "Universal Sets <code>dj.U</code>", "text": "<p><code>dj.U</code> is a special operand in query expressions that allows performing special operations.  By itself, it can never form a query and is not a subclass of <code>QueryExpression</code>. Other query expressions are modified through participation in operations with <code>dj.U</code>.</p>"}, {"location": "internal/transpilation/#aggregating-by-dju", "title": "Aggregating by <code>dj.U</code>", "text": ""}, {"location": "internal/transpilation/#restricting-a-dju-object-with-a-queryexpression-object", "title": "Restricting a <code>dj.U</code> object with a <code>QueryExpression</code> object", "text": ""}, {"location": "internal/transpilation/#joining-a-dju-object", "title": "Joining a <code>dj.U</code> object", "text": ""}, {"location": "internal/transpilation/#query-backprojection", "title": "Query \"Backprojection\"", "text": "<p>Once a QueryExpression is used in a <code>fetch</code> operation or becomes a subquery in another query, it can project out all unnecessary attributes from its own inputs, recursively. This is implemented by the <code>finalize</code> method. This simplification produces much leaner queries resulting in improved query performance in version 0.13, especially on complex queries with blob data, compensating for MySQL's deficiencies in query optimization.</p>"}, {"location": "manipulation/", "title": "Data Manipulation", "text": "<p>Data manipulation operations change the state of the data stored in the database without modifying the structure of the stored data. These operations include insert, delete, and update.</p> <p>Data manipulation operations in DataJoint respect the integrity constraints.</p>"}, {"location": "manipulation/delete/", "title": "Delete", "text": "<p>The <code>delete</code> method deletes entities from a table and all dependent entries in dependent tables.</p> <p>Delete is often used in conjunction with the restriction operator to define the subset of entities to delete. Delete is performed as an atomic transaction so that partial deletes never occur.</p>"}, {"location": "manipulation/delete/#examples", "title": "Examples", "text": "<pre><code># delete all entries from tuning.VonMises\ntuning.VonMises.delete()\n\n# delete entries from tuning.VonMises for mouse 1010\n(tuning.VonMises &amp; 'mouse=1010').delete()\n\n# delete entries from tuning.VonMises except mouse 1010\n(tuning.VonMises - 'mouse=1010').delete()\n</code></pre>"}, {"location": "manipulation/delete/#deleting-from-part-tables", "title": "Deleting from part tables", "text": "<p>Entities in a part table are usually removed as a consequence of deleting the master table.</p> <p>To enforce this workflow, calling <code>delete</code> directly on a part table produces an error. In some cases, it may be necessary to override this behavior. To remove entities from a part table without calling <code>delete</code> master, use the argument <code>force_parts=True</code>. To include the corresponding entries in the master table, use the argument <code>force_masters=True</code>.</p>"}, {"location": "manipulation/insert/", "title": "Insert", "text": "<p>The <code>insert</code> method of DataJoint table objects inserts entities into the table.</p> <p>In Python there is a separate method <code>insert1</code> to insert one entity at a time. The entity may have the form of a Python dictionary with key names matching the attribute names in the table.</p> <pre><code>lab.Person.insert1(\n          dict(username='alice',\n               first_name='Alice',\n               last_name='Cooper'))\n</code></pre> <p>The entity also may take the form of a sequence of values in the same order as the attributes in the table.</p> <pre><code>lab.Person.insert1(['alice', 'Alice', 'Cooper'])\n</code></pre> <p>Additionally, the entity may be inserted as a NumPy record array  or Pandas DataFrame.</p> <p>The <code>insert</code> method accepts a sequence or a generator of multiple entities and is used to insert multiple entities at once.</p> <pre><code>lab.Person.insert([\n          ['alice',   'Alice',   'Cooper'],\n          ['bob',     'Bob',     'Dylan'],\n          ['carol',   'Carol',   'Douglas']])\n</code></pre> <p>Several optional parameters can be used with <code>insert</code>:</p> <p><code>replace</code> If <code>True</code>, replaces the existing entity.   (Default <code>False</code>.)</p> <p><code>skip_duplicates</code> If <code>True</code>, silently skip duplicate inserts.   (Default <code>False</code>.)</p> <p><code>ignore_extra_fields</code> If <code>False</code>, fields that are not in the heading raise an error.   (Default <code>False</code>.)</p> <p><code>allow_direct_insert</code> If <code>True</code>, allows inserts outside of populate calls.   Applies only in auto-populated tables.   (Default <code>None</code>.)</p>"}, {"location": "manipulation/insert/#batched-inserts", "title": "Batched inserts", "text": "<p>Inserting a set of entities in a single <code>insert</code> differs from inserting the same set of entities one-by-one in a <code>for</code> loop in two ways:</p> <ol> <li>Network overhead is reduced.    Network overhead can be tens of milliseconds per query.    Inserting 1000 entities in a single <code>insert</code> call may save a few seconds over    inserting them individually.</li> <li>The insert is performed as an all-or-nothing transaction.    If even one insert fails because it violates any constraint, then none of the    entities in the set are inserted.</li> </ol> <p>However, inserting too many entities in a single query may run against buffer size or packet size limits of the database server. Due to these limitations, performing inserts of very large numbers of entities should be broken up into moderately sized batches, such as a few hundred at a time.</p>"}, {"location": "manipulation/insert/#server-side-inserts", "title": "Server-side inserts", "text": "<p>Data inserted into a table often come from other tables already present on the database server. In such cases, data can be fetched from the first table and then inserted into another table, but this results in transfers back and forth between the database and the local system. Instead, data can be inserted from one table into another without transfers between the database and the local system using queries.</p> <p>In the example below, a new schema has been created in preparation for phase two of a project. Experimental protocols from the first phase of the project will be reused in the second phase. Since the entities are already present on the database in the <code>Protocol</code> table of the <code>phase_one</code> schema, we can perform a server-side insert into <code>phase_two.Protocol</code> without fetching a local copy.</p> <pre><code># Server-side inserts are faster...\nphase_two.Protocol.insert(phase_one.Protocol)\n\n# ...than fetching before inserting\nprotocols = phase_one.Protocol.fetch()\nphase_two.Protocol.insert(protocols)\n</code></pre>"}, {"location": "manipulation/transactions/", "title": "Transactions", "text": "<p>In some cases, a sequence of several operations must be performed as a single operation: interrupting the sequence of such operations halfway would leave the data in an invalid state. While the sequence is in progress, other processes accessing the database will not see the partial results until the transaction is complete. The sequence may include data queries and manipulations.</p> <p>In such cases, the sequence of operations may be enclosed in a transaction.</p> <p>Transactions are formed using the <code>transaction</code> property of the connection object. The connection object may be obtained from any table object. The <code>transaction</code> property can then be used as a context manager in Python's <code>with</code> statement.</p> <p>For example, the following code inserts matching entries for the master table <code>Session</code> and its part table <code>Session.Experimenter</code>.</p> <pre><code># get the connection object\nconnection = Session.connection\n\n# insert Session and Session.Experimenter entries in a transaction\nwith connection.transaction:\n     key = {'subject_id': animal_id, 'session_time': session_time}\n     Session.insert1({**key, 'brain_region':region, 'cortical_layer':layer})\n     Session.Experimenter.insert1({**key, 'experimenter': username})\n</code></pre> <p>Here, to external observers, both inserts will take effect together upon exiting from the <code>with</code> block or will not have any effect at all. For example, if the second insert fails due to an error, the first insert will be rolled back.</p>"}, {"location": "manipulation/update/", "title": "Cautious Update", "text": "<p>In database programming, the update operation refers to modifying the values of individual attributes in an entity within a table without replacing the entire entity. Such an in-place update mechanism is not part of DataJoint's data manipulation model, because it circumvents data dependency constraints.</p> <p>This is not to say that data cannot be changed once they are part of a pipeline. In DataJoint, data is changed by replacing entire entities rather than by updating the values of their attributes. The process of deleting existing entities and inserting new entities with corrected values ensures the integrity of the data throughout the pipeline.</p> <p>This approach applies specifically to automated tables (see Auto-populated tables). However, manual tables are often edited outside DataJoint through other interfaces. It is up to the user's discretion to allow updates in manual tables, and the user must be cognizant of the fact that updates will not trigger re-computation of dependent data.</p>"}, {"location": "manipulation/update/#usage", "title": "Usage", "text": "<p>For some cases, it becomes necessary to deliberately correct existing values where a user has chosen to accept the above responsibility despite the caution.</p> <p>The <code>update1</code> method accomplishes this if the record already exists. Note that updates to primary key values are not allowed.</p> <p>The method should only be used to fix problems, and not as part of a regular workflow. When updating an entry, make sure that any information stored in dependent tables that depends on the update values is properly updated as well.</p>"}, {"location": "manipulation/update/#examples", "title": "Examples", "text": "<pre><code># with record as a dict specifying the primary and\n# secondary attribute values\ntable.update1(record)\n\n# update value in record with id as primary key\ntable.update1({'id': 1, 'value': 3})\n\n# reset value to default with id as primary key\ntable.update1({'id': 1, 'value': None})\n# or\ntable.update1({'id': 1})\n</code></pre>"}, {"location": "query/aggregation/", "title": "Aggr", "text": "<p>Aggregation, performed with the <code>aggr</code> operator, is a special form of <code>proj</code> with the additional feature of allowing aggregation calculations on another table. It has the form <code>tab.aggr(other, ...)</code> where <code>other</code> is another table. Without the argument <code>other</code>, <code>aggr</code> and <code>proj</code> are exactly equivalent. Aggregation allows adding calculated attributes to each entity in <code>tab</code> based on aggregation functions over attributes in the matching entities of <code>other</code>.</p> <p>Aggregation functions include <code>count</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>avg</code>, <code>std</code>, <code>variance</code>, and others. Aggregation functions can only be used in the definitions of new attributes within the <code>aggr</code> operator.</p> <p>As with <code>proj</code>, the output of <code>aggr</code> has the same entity class, the same primary key, and the same number of elements as <code>tab</code>. Primary key attributes are always included in the output and may be renamed, just like in <code>proj</code>.</p>"}, {"location": "query/aggregation/#examples", "title": "Examples", "text": "<pre><code># Number of students in each course section\nSection.aggr(Enroll, n=\"count(*)\")\n\n# Average grade in each course\nCourse.aggr(Grade * LetterGrade, avg_grade=\"avg(points)\")\n</code></pre>"}, {"location": "query/example-schema/", "title": "Example Schema", "text": "<p>The example schema below contains data for a university enrollment system. Information about students, departments, courses, etc. are organized in multiple tables.</p> <p>Warning:   Empty primary keys, such as in the <code>CurrentTerm</code> table, are not yet supported by   DataJoint.   This feature will become available in a future release.   See Issue #113 for more   information.</p> <pre><code>@schema\nclass Student (dj.Manual):\ndefinition = \"\"\"\nstudent_id      : int unsigned # university ID\n---\nfirst_name      : varchar(40)\nlast_name       : varchar(40)\nsex             : enum('F', 'M', 'U')\ndate_of_birth   : date\nhome_address    : varchar(200) # street address\nhome_city       : varchar(30)\nhome_state      : char(2) # two-letter abbreviation\nhome_zipcode    : char(10)\nhome_phone      : varchar(14)\n\"\"\"\n\n@schema\nclass Department (dj.Manual):\ndefinition = \"\"\"\ndept         : char(6) # abbreviated department name, e.g. BIOL\n---\ndept_name    : varchar(200) # full department name\ndept_address : varchar(200) # mailing address\ndept_phone   : varchar(14)\n\"\"\"\n\n@schema\nclass StudentMajor (dj.Manual):\ndefinition = \"\"\"\n-&gt; Student\n---\n-&gt; Department\ndeclare_date :  date # when student declared her major\n\"\"\"\n\n@schema\nclass Course (dj.Manual):\ndefinition = \"\"\"\n-&gt; Department\ncourse      : int unsigned # course number, e.g. 1010\n---\ncourse_name : varchar(200) # e.g. \"Cell Biology\"\ncredits     : decimal(3,1) # number of credits earned by completing the course\n\"\"\"\n\n@schema\nclass Term (dj.Manual):\ndefinition = \"\"\"\nterm_year : year\nterm      : enum('Spring', 'Summer', 'Fall')\n\"\"\"\n\n@schema\nclass Section (dj.Manual):\ndefinition = \"\"\"\n-&gt; Course\n-&gt; Term\nsection : char(1)\n---\nroom  :  varchar(12) # building and room code\n\"\"\"\n\n@schema\nclass CurrentTerm (dj.Manual):\ndefinition = \"\"\"\n---\n-&gt; Term\n\"\"\"\n\n@schema\nclass Enroll (dj.Manual):\ndefinition = \"\"\"\n-&gt; Section\n-&gt; Student\n\"\"\"\n\n@schema\nclass LetterGrade (dj.Manual):\ndefinition = \"\"\"\ngrade : char(2)\n---\npoints : decimal(3,2)\n\"\"\"\n\n@schema\nclass Grade (dj.Manual):\ndefinition = \"\"\"\n-&gt; Enroll\n---\n-&gt; LetterGrade\n\"\"\"\n</code></pre>"}, {"location": "query/example-schema/#example-schema-diagram", "title": "Example schema diagram", "text": "<p>Example schema for a university database. Tables contain data on students, departments, courses, etc.</p>"}, {"location": "query/fetch/", "title": "Fetch", "text": "<p>Data queries in DataJoint comprise two distinct steps:</p> <ol> <li>Construct the <code>query</code> object to represent the required data using tables and operators.</li> <li>Fetch the data from <code>query</code> into the workspace of the host language -- described in this section.</li> </ol> <p>Note that entities returned by <code>fetch</code> methods are not guaranteed to be sorted in any particular order unless specifically requested. Furthermore, the order is not guaranteed to be the same in any two queries, and the contents of two identical queries may change between two sequential invocations unless they are wrapped in a transaction. Therefore, if you wish to fetch matching pairs of attributes, do so in one <code>fetch</code> call.</p> <p>The examples below are based on the example schema for this part of the documentation.</p>"}, {"location": "query/fetch/#entire-table", "title": "Entire table", "text": "<p>The following statement retrieves the entire table as a NumPy recarray.</p> <pre><code>data = query.fetch()\n</code></pre> <p>To retrieve the data as a list of <code>dict</code>:</p> <pre><code>data = query.fetch(as_dict=True)\n</code></pre> <p>In some cases, the amount of data returned by fetch can be quite large; in these cases it can be useful to use the <code>size_on_disk</code> attribute to determine if running a bare fetch would be wise. Please note that it is only currently possible to query the size of entire tables stored directly in the database at this time.</p>"}, {"location": "query/fetch/#as-separate-variables", "title": "As separate variables", "text": "<pre><code>name, img = query.fetch1('name', 'image')  # when query has exactly one entity\nname, img = query.fetch('name', 'image')  # [name, ...] [image, ...]\n</code></pre>"}, {"location": "query/fetch/#primary-key-values", "title": "Primary key values", "text": "<pre><code>keydict = tab.fetch1(\"KEY\")  # single key dict when tab has exactly one entity\nkeylist = tab.fetch(\"KEY\")  # list of key dictionaries [{}, ...]\n</code></pre> <p><code>KEY</code> can also used when returning attribute values as separate variables, such that one of the returned variables contains the entire primary keys.</p>"}, {"location": "query/fetch/#sorting-and-limiting-the-results", "title": "Sorting and limiting the results", "text": "<p>To sort the result, use the <code>order_by</code> keyword argument.</p> <pre><code># ascending order:\ndata = query.fetch(order_by='name')\n# descending order:\ndata = query.fetch(order_by='name desc')\n# by name first, year second:\ndata = query.fetch(order_by=('name desc', 'year'))\n# sort by the primary key:\ndata = query.fetch(order_by='KEY')\n# sort by name but for same names order by primary key:\ndata = query.fetch(order_by=('name', 'KEY desc'))\n</code></pre> <p>The <code>order_by</code> argument can be a string specifying the attribute to sort by. By default the sort is in ascending order. Use <code>'attr desc'</code> to sort in descending order by attribute <code>attr</code>.  The value can also be a sequence of strings, in which case, the sort performed on all the attributes jointly in the order specified.</p> <p>The special attribute name <code>'KEY'</code> represents the primary key attributes in order that they appear in the index. Otherwise, this name can be used as any other argument.</p> <p>If an attribute happens to be a SQL reserved word, it needs to be enclosed in backquotes.  For example:</p> <pre><code>data = query.fetch(order_by='`select` desc')\n</code></pre> <p>The <code>order_by</code> value is eventually passed to the <code>ORDER BY</code> clause.</p> <p>Similarly, the <code>limit</code> and <code>offset</code> arguments can be used to limit the result to a subset of entities.</p> <p>For example, one could do the following:</p> <pre><code>data = query.fetch(order_by='name', limit=10, offset=5)\n</code></pre> <p>Note that an <code>offset</code> cannot be used without specifying a <code>limit</code> as well.</p>"}, {"location": "query/fetch/#usage-with-pandas", "title": "Usage with Pandas", "text": "<p>The pandas library is a popular library for data analysis in Python which can easily be used with DataJoint query results. Since the records returned by <code>fetch()</code> are contained within a <code>numpy.recarray</code>, they can be easily converted to <code>pandas.DataFrame</code> objects by passing them into the <code>pandas.DataFrame</code> constructor. For example:</p> <pre><code>import pandas as pd\nframe = pd.DataFrame(tab.fetch())\n</code></pre> <p>Calling <code>fetch()</code> with the argument <code>format=\"frame\"</code> returns results as <code>pandas.DataFrame</code> objects indexed by the table's primary key attributes.</p> <pre><code>frame = tab.fetch(format=\"frame\")\n</code></pre> <p>Returning results as a <code>DataFrame</code> is not possible when fetching a particular subset of attributes or when <code>as_dict</code> is set to <code>True</code>.</p>"}, {"location": "query/iteration/", "title": "Iteration", "text": "<p>The DataJoint model primarily handles data as sets, in the form of tables. However, it can sometimes be useful to access or to perform actions such as visualization upon individual entities sequentially. In DataJoint this is accomplished through iteration.</p> <p>In the simple example below, iteration is used to display the names and values of the attributes of each entity in the simple table or table expression.</p> <pre><code>for entity in table:\n    print(entity)\n</code></pre> <p>This example illustrates the function of the iterator: DataJoint iterates through the whole table expression, returning the entire entity during each step. In this case, each entity will be returned as a <code>dict</code> containing all attributes.</p> <p>At the start of the above loop, DataJoint internally fetches only the primary keys of the entities. Since only the primary keys are needed to distinguish between entities, DataJoint can then iterate over the list of primary keys to execute the loop. At each step of the loop, DataJoint uses a single primary key to fetch an entire entity for use in the iteration, such that <code>print(entity)</code> will print all attributes of each entity. By first fetching only the primary keys and then fetching each entity individually, DataJoint saves memory at the cost of network overhead. This can be particularly useful for tables containing large amounts of data in secondary attributes.</p> <p>The memory savings of the above syntax may not be worth the additional network overhead in all cases, such as for tables with little data stored as secondary attributes. In the example below, DataJoint fetches all of the attributes of each entity in a single call and then iterates over the list of entities stored in memory.</p> <pre><code>for entity in table.fetch(as_dict=True):\n    print(entity)\n</code></pre>"}, {"location": "query/join/", "title": "Join", "text": ""}, {"location": "query/join/#join-operator", "title": "Join operator <code>*</code>", "text": "<p>The Join operator <code>A * B</code> combines the matching information in <code>A</code> and <code>B</code>. The result contains all matching combinations of entities from both arguments.</p>"}, {"location": "query/join/#principles-of-joins", "title": "Principles of joins", "text": "<ol> <li>The operands <code>A</code> and <code>B</code> must be join-compatible.</li> <li>The primary key of the result is the union of the primary keys of the operands.</li> </ol>"}, {"location": "query/join/#examples-of-joins", "title": "Examples of joins", "text": "<p>Example 1 : When the operands have no common attributes, the result is the cross product -- all combinations of entities.</p> <p></p> <p>Example 2 : When the operands have common attributes, only entities with matching values are kept.</p> <p></p> <p>Example 3 : Joining on secondary attribute.</p> <p></p>"}, {"location": "query/join/#properties-of-join", "title": "Properties of join", "text": "<ol> <li> <p>When <code>A</code> and <code>B</code> have the same attributes, the join <code>A * B</code> becomes equivalent to the set intersection <code>A</code> \u2229 <code>B</code>.    Hence, DataJoint does not need a separate intersection operator.</p> </li> <li> <p>Commutativity: <code>A * B</code> is equivalent to <code>B * A</code>.</p> </li> <li> <p>Associativity: <code>(A * B) * C</code> is equivalent to <code>A * (B * C)</code>.</p> </li> </ol>"}, {"location": "query/operators/", "title": "Operators", "text": "<p>Data queries have the form of expressions using operators to derive the desired table. The expressions themselves do not contain any data. They represent the desired data symbolically.</p> <p>Once a query is formed, the fetch methods are used to bring the data into the local workspace. Since the expressions are only symbolic representations, repeated <code>fetch</code> calls may yield different results as the state of the database is modified.</p> <p>DataJoint implements a complete algebra of operators on tables:</p> operator notation meaning join A * B All matching information from A and B restriction A &amp; cond The subset of entities from A that meet the condition restriction A - cond The subset of entities from A that do not meet the condition proj A.proj(...) Selects and renames attributes from A or computes new attributes aggr A.aggr(B, ...) Same as projection with computations based on matching information in B union A + B All unique entities from both A and B universal set* dj.U() All unique entities from both A and B top* dj.Top() The top rows of A <p>*While not technically query operators, it is useful to discuss Universal Set and Top in the same context.</p>"}, {"location": "query/operators/#principles-of-relational-algebra", "title": "Principles of relational algebra", "text": "<p>DataJoint's algebra improves upon the classical relational algebra and upon other query languages to simplify and enhance the construction and interpretation of precise and efficient data queries.</p> <ol> <li>Entity integrity: Data are represented and manipulated in the form of tables representing well-formed entity sets.    This applies to the inputs and outputs of query operators.    The output of a query operator is an entity set with a well-defined entity type, a    primary key, unique attribute names, etc.</li> <li>Algebraic closure: All operators operate on entity sets and yield entity sets.    Thus query expressions may be used as operands in other expressions or may be    assigned to variables to be used in other expressions.</li> <li>Attributes are identified by names:  All attributes have explicit names.    This includes results of queries.    Operators use attribute names to determine how to perform the operation.    The order of the attributes is not significant.</li> </ol>"}, {"location": "query/operators/#matching-entities", "title": "Matching entities", "text": "<p>Binary operators in DataJoint are based on the concept of matching entities; this phrase will be used throughout the documentation.</p> <pre><code>Two entities **match** when they have no common attributes or when their common\nattributes contain the same values.\n</code></pre> <p>Here common attributes are those that have the same names in both entities. It is usually assumed that the common attributes are of compatible datatypes to allow equality comparisons.</p> <p>Another way to phrase the same definition is</p> <pre><code>Two entities match when they have no common attributes whose values differ.\n</code></pre> <p>It may be conceptually convenient to imagine that all tables always have an additional invisible attribute, <code>omega</code> whose domain comprises only one value, 1. Then the definition of matching entities is simplified:</p> <pre><code>Two entities match when their common attributes contain the same values.\n</code></pre> <p>Matching entities can be merged into a single entity without any conflicts of attribute names and values.</p>"}, {"location": "query/operators/#examples", "title": "Examples", "text": "<p>This is a matching pair of entities:</p> <p></p> <p>and so is this one:</p> <p></p> <p>but these entities do not match:</p> <p></p>"}, {"location": "query/operators/#join-compatibility", "title": "Join compatibility", "text": "<p>All binary operators with other tables as their two operands require that the operands be join-compatible, which means that:</p> <ol> <li>All common attributes in both operands (attributes with the same name) must be part of either the primary key or a foreign key.</li> <li>All common attributes in the two relations must be of a compatible datatype for equality comparisons.</li> </ol>"}, {"location": "query/operators/#restriction", "title": "Restriction", "text": "<p>The restriction operator <code>A &amp; cond</code> selects the subset of entities from <code>A</code> that meet the condition <code>cond</code>. The exclusion operator <code>A - cond</code> selects the complement of restriction, i.e. the subset of entities from <code>A</code> that do not meet the condition <code>cond</code>. This means that the restriction and exclusion operators are complementary. The same query could be constructed using either <code>A &amp; cond</code> or <code>A - Not(cond)</code>.</p> <p></p> <p>The condition <code>cond</code> may be one of the following:</p> Python <ul> <li>another table</li> <li>a mapping, e.g. <code>dict</code></li> <li>an expression in a character string</li> <li>a collection of conditions as a <code>list</code>, <code>tuple</code>, or Pandas <code>DataFrame</code></li> <li>a Boolean expression (<code>True</code> or <code>False</code>)</li> <li>an <code>AndList</code></li> <li>a <code>Not</code> object</li> <li>a query expression</li> </ul> Permissive Operators <p>To circumvent compatibility checks, DataJoint offers permissive operators for Restriction (<code>^</code>) and Join (<code>@</code>). Use with Caution.</p>"}, {"location": "query/operators/#proj", "title": "Proj", "text": "<p>The <code>proj</code> operator represents projection and is used to select attributes (columns) from a table, to rename them, or to create new calculated attributes.</p> <ol> <li> <p>A simple projection selects a subset of attributes of the original table, which may not include the primary key.</p> </li> <li> <p>A more complex projection renames an attribute in another table. This could be useful when one table should be referenced multiple times in another. A user table, could contain all personnel. A project table references one person for the lead and another the coordinator, both referencing the common personnel pool.</p> </li> <li> <p>Projection can also perform calculations (as available in MySQL) on a single attribute.</p> </li> </ol>"}, {"location": "query/operators/#aggr", "title": "Aggr", "text": "<p>Aggregation is a special form of <code>proj</code> with the added feature of allowing   aggregation calculations on another table. It has the form <code>table.aggr   (other, ...)</code> where <code>other</code> is another table. Aggregation allows adding calculated   attributes to each entity in <code>table</code> based on aggregation functions over attributes   in the matching entities of <code>other</code>.</p> <p>Aggregation functions include <code>count</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>avg</code>, <code>std</code>, <code>variance</code>, and others.</p>"}, {"location": "query/operators/#union", "title": "Union", "text": "<p>The result of the union operator <code>A + B</code> contains all the entities from both operands.</p> <p>Entity normalization requires that <code>A</code> and <code>B</code> are of the same type, with with the same primary key, using homologous attributes. Without secondary attributes, the result is the simple set union. With secondary attributes, they must have the same names and datatypes. The two operands must also be disjoint, without any duplicate primary key values across both inputs. These requirements prevent ambiguity of attribute values and preserve entity identity.</p> Principles of union <ol> <li> <p>As in all operators, the order of the attributes in the operands is not significant.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> must have the same primary key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> may not have any common non-key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>The result <code>A + B</code> will have the same primary key as <code>A</code> and <code>B</code>.</p> </li> <li> <p>The result <code>A + B</code> will have all the non-key attributes from both <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are found in both <code>A</code> and <code>B</code> (based on the primary key), the secondary attributes will be filled from the corresponding entities in <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are only found in either <code>A</code> or <code>B</code>, the other operand's secondary attributes will filled with null values.</p> </li> </ol> <p>For union, order does not matter.</p> <p></p> <p></p> Properties of union <ol> <li>Commutative: <code>A + B</code> is equivalent to <code>B + A</code>.</li> <li>Associative: <code>(A + B) + C</code> is equivalent to <code>A + (B + C)</code>.</li> </ol>"}, {"location": "query/operators/#universal-set", "title": "Universal Set", "text": "<p>All of the above operators are designed to preserve their input type. Some queries may require creating a new entity type not already represented by existing tables. This means that the new type must be defined as part of the query.</p> <p>Universal sets fulfill this role using <code>dj.U</code> notation. They denote the set of all possible entities with given attributes of any possible datatype. Attributes of universal sets are allowed to be matched to any namesake attributes, even those that do not come from the same initial source.</p> <p>Universal sets should be used sparingly when no suitable base tables already exist. In some cases, defining a new base table can make queries clearer and more semantically constrained.</p> <p>The examples below will use the table definitions in table tiers.</p>"}, {"location": "query/operators/#top", "title": "Top", "text": "<p>Similar to the universal set operator, the top operator uses <code>dj.Top</code> notation. It is used to restrict a query by the given <code>limit</code>, <code>order_by</code>, and <code>offset</code> parameters:</p> <pre><code>Session &amp; dj.Top(limit=10, order_by='session_date')\n</code></pre> <p>The result of this expression returns the first 10 rows of <code>Session</code> and sorts them by their <code>session_date</code> in ascending order.</p>"}, {"location": "query/operators/#order_by", "title": "<code>order_by</code>", "text": "Example Description <code>order_by=\"session_date DESC\"</code> Sort by <code>session_date</code> in descending order <code>order_by=\"KEY\"</code> Sort by the primary key <code>order_by=\"KEY DESC\"</code> Sort by the primary key in descending order <code>order_by=[\"subject_id\", \"session_date\"]</code> Sort by <code>subject_id</code>, then sort matching <code>subject_id</code>s by their <code>session_date</code> <p>The default values for <code>dj.Top</code> parameters are <code>limit=1</code>, <code>order_by=\"KEY\"</code>, and <code>offset=0</code>.</p>"}, {"location": "query/operators/#restriction_1", "title": "Restriction", "text": "<p><code>&amp;</code> and <code>-</code> operators permit restriction.</p>"}, {"location": "query/operators/#by-a-mapping", "title": "By a mapping", "text": "<p>For a Session table, that has the attribute <code>session_date</code>, we can restrict to sessions from January 1st, 2022:</p> <pre><code>Session &amp; {'session_date': \"2022-01-01\"}\n</code></pre> <p>If there were any typos (e.g., using <code>sess_date</code> instead of <code>session_date</code>), our query will return all of the entities of <code>Session</code>.</p>"}, {"location": "query/operators/#by-a-string", "title": "By a string", "text": "<p>Conditions may include arithmetic operations, functions, range tests, etc. Restriction of table <code>A</code> by a string containing an attribute not found in table <code>A</code> produces an error.</p> <pre><code>Session &amp; 'user = \"Alice\"' # (1)\nSession &amp; 'session_date &gt;= \"2022-01-01\"' # (2)\n</code></pre> <ol> <li>All the sessions performed by Alice</li> <li>All of the sessions on or after January 1st, 2022</li> </ol>"}, {"location": "query/operators/#by-a-collection", "title": "By a collection", "text": "<p>When <code>cond</code> is a collection of conditions, the conditions are applied by logical disjunction (logical OR). Restricting a table by a collection will return all entities that meet any of the conditions in the collection.</p> <p>For example, if we restrict the <code>Session</code> table by a collection containing two conditions, one for user and one for date, the query will return any sessions with a matching user or date.</p> <p>A collection can be a list, a tuple, or a Pandas <code>DataFrame</code>.</p> <pre><code>cond_list = ['user = \"Alice\"', 'session_date = \"2022-01-01\"'] # (1)\ncond_tuple = ('user = \"Alice\"', 'session_date = \"2022-01-01\"') # (2)\nimport pandas as pd\ncond_frame = pd.DataFrame(data={'user': ['Alice'], 'session_date': ['2022-01-01']}) # (3)\n\nSession() &amp; ['user = \"Alice\"', 'session_date = \"2022-01-01\"']\n</code></pre> <ol> <li>A list</li> <li>A tuple</li> <li>A data frame</li> </ol> <p><code>dj.AndList</code> represents logical conjunction(logical AND). Restricting a table by an <code>AndList</code> will return all entities that meet all of the conditions in the list. <code>A &amp; dj.AndList([c1, c2, c3])</code> is equivalent to <code>A &amp; c1 &amp; c2 &amp; c3</code>.</p> <pre><code>Student() &amp; dj.AndList(['user = \"Alice\"', 'session_date = \"2022-01-01\"'])\n</code></pre> <p>The above will show all the sessions that Alice conducted on the given day.</p>"}, {"location": "query/operators/#by-a-not-object", "title": "By a <code>Not</code> object", "text": "<p>The special function <code>dj.Not</code> represents logical negation, such that <code>A &amp; dj.Not (cond)</code> is equivalent to <code>A - cond</code>.</p>"}, {"location": "query/operators/#by-a-query", "title": "By a query", "text": "<p>Restriction by a query object is a generalization of restriction by a table. The example below creates a query object corresponding to all the users named Alice. The <code>Session</code> table is then restricted by the query object, returning all the sessions performed by Alice.</p> <pre><code>query = User &amp; 'user = \"Alice\"'\nSession &amp; query\n</code></pre>"}, {"location": "query/operators/#proj_1", "title": "Proj", "text": "<p>Renaming an attribute in python can be done via keyword arguments:</p> <pre><code>table.proj(new_attr='old_attr')\n</code></pre> <p>This can be done in the context of a table definition:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    # Experiment Session\n    -&gt; Animal\n    session             : smallint  # session number for the animal\n    ---\n    session_datetime    : datetime  # YYYY-MM-DD HH:MM:SS\n    session_start_time  : float     # seconds relative to session_datetime\n    session_end_time    : float     # seconds relative to session_datetime\n    -&gt; User.proj(experimenter='username')\n    -&gt; User.proj(supervisor='username')\n    \"\"\"\n</code></pre> <p>Or to rename multiple values in a table with the following syntax: <code>Table.proj(*existing_attributes,*renamed_attributes)</code></p> <pre><code>Session.proj('session','session_date',start='session_start_time',end='session_end_time')\n</code></pre> <p>Projection can also be used to to compute new attributes from existing ones.</p> <pre><code>Session.proj(duration='session_end_time-session_start_time') &amp; 'duration &gt; 10'\n</code></pre>"}, {"location": "query/operators/#aggr_1", "title": "Aggr", "text": "<p>For more complicated calculations, we can use aggregation.</p> <pre><code>Subject.aggr(Session,n=\"count(*)\") # (1)\nSubject.aggr(Session,average_start=\"avg(session_start_time)\") # (2)\n</code></pre> <ol> <li>Number of sessions per subject.</li> <li>Average <code>session_start_time</code> for each subject</li> </ol>"}, {"location": "query/operators/#universal-set_1", "title": "Universal set", "text": "<p>Universal sets offer the complete list of combinations of attributes.</p> <pre><code># All home cities of students\ndj.U('laser_wavelength', 'laser_power') &amp; Scan # (1)\ndj.U('laser_wavelength', 'laser_power').aggr(Scan, n=\"count(*)\") # (2)\ndj.U().aggr(Session, n=\"max(session)\") # (3)\n</code></pre> <ol> <li>All combinations of wavelength and power.</li> <li>Total number of scans for each combination.</li> <li>Largest session number.</li> </ol> <p><code>dj.U()</code>, as shown in the last example above, is often useful for integer IDs. For an example of this process, see the source code for Element Array Electrophysiology's <code>insert_new_params</code>.</p>"}, {"location": "query/principles/", "title": "Query Principles", "text": "<p>Data queries retrieve data from the database. A data query is performed with the help of a query object, which is a symbolic representation of the query that does not in itself contain any actual data. The simplest query object is an instance of a table class, representing the contents of an entire table.</p> <p>For example, if <code>experiment.Session</code> is a DataJoint table class, you can create a query object to retrieve its entire contents as follows:</p> <pre><code>query  = experiment.Session()\n</code></pre> <p>More generally, a query object may be formed as a query expression constructed by applying operators to other query objects.</p> <p>For example, the following query retrieves information about all experiments and scans for mouse 102 (excluding experiments with no scans):</p> <pre><code>query = experiment.Session * experiment.Scan &amp; 'animal_id = 102'\n</code></pre> <p>Note that for brevity, query operators can be applied directly to class objects rather than instance objects so that <code>experiment.Session</code> may be used in place of <code>experiment.Session()</code>.</p> <p>You can preview the contents of the query in Python, Jupyter Notebook, or MATLAB by simply displaying the object. In the image below, the object <code>query</code> is first defined as a restriction of the table <code>EEG</code> by values of the attribute <code>eeg_sample_rate</code> greater than 1000 Hz. Displaying the object gives a preview of the entities that will be returned by <code>query</code>. Note that this preview only lists a few of the entities that will be returned. Also, the preview does not contain any data for attributes of datatype <code>blob</code>.</p> <p></p> <p>Defining a query object and previewing the entities returned by the query.</p> <p>Once the desired query object is formed, the query can be executed using its fetch methods. To fetch means to transfer the data represented by the query object from the database server into the workspace of the host language.</p> <pre><code>s = query.fetch()\n</code></pre> <p>Here fetching from the <code>query</code> object produces the NumPy record array <code>s</code> of the queried data.</p>"}, {"location": "query/principles/#checking-for-returned-entities", "title": "Checking for returned entities", "text": "<p>The preview of the query object shown above displayed only a few of the entities returned by the query but also displayed the total number of entities that would be returned. It can be useful to know the number of entities returned by a query, or even whether a query will return any entities at all, without having to fetch all the data themselves.</p> <p>The <code>bool</code> function applied to a query object evaluates to <code>True</code> if the query returns any entities and to <code>False</code> if the query result is empty.</p> <p>The <code>len</code> function applied to a query object determines the number of entities returned by the query.</p> <pre><code># number of sessions since the start of 2018.\nn = len(Session &amp; 'session_date &gt;= \"2018-01-01\"')\n</code></pre>"}, {"location": "query/principles/#normalization-in-queries", "title": "Normalization in queries", "text": "<p>Query objects adhere to entity entity normalization just like the stored tables do. The result of a query is a well-defined entity set with an readily identifiable entity class and designated primary attributes that jointly distinguish any two entities from each other. The query operators are designed to keep the result normalized even in complex query expressions.</p>"}, {"location": "query/project/", "title": "Proj", "text": "<p>The <code>proj</code> operator represents projection and is used to select attributes (columns) from a table, to rename them, or to create new calculated attributes.</p>"}, {"location": "query/project/#simple-projection", "title": "Simple projection", "text": "<p>The simple projection selects a subset of attributes of the original table. However, the primary key attributes are always included.</p> <p>Using the example schema, let table <code>department</code> have attributes dept, dept_name, dept_address, and dept_phone. The primary key attribute is in bold.</p> <p>Then <code>department.proj()</code> will have attribute dept.</p> <p><code>department.proj('dept')</code> will have attribute dept.</p> <p><code>department.proj('dept_name', 'dept_phone')</code> will have attributes dept, dept_name, and dept_phone.</p>"}, {"location": "query/project/#renaming", "title": "Renaming", "text": "<p>In addition to selecting attributes, <code>proj</code> can rename them. Any attribute can be renamed, including primary key attributes.</p> <p>This is done using keyword arguments: <code>tab.proj(new_attr='old_attr')</code></p> <p>For example, let table <code>tab</code> have attributes mouse, session, session_date, stimulus, and behavior. The primary key attributes are in bold.</p> <p>Then</p> <pre><code>tab.proj(animal='mouse', 'stimulus')\n</code></pre> <p>will have attributes animal, session, and stimulus.</p> <p>Renaming is often used to control the outcome of a join. For example, let <code>tab</code> have attributes slice, and cell. Then <code>tab * tab</code> will simply yield <code>tab</code>. However,</p> <pre><code>tab * tab.proj(other='cell')\n</code></pre> <p>yields all ordered pairs of all cells in each slice.</p>"}, {"location": "query/project/#calculations", "title": "Calculations", "text": "<p>In addition to selecting or renaming attributes, <code>proj</code> can compute new attributes from existing ones.</p> <p>For example, let <code>tab</code> have attributes <code>mouse</code>, <code>scan</code>, <code>surface_z</code>, and <code>scan_z</code>. To obtain the new attribute <code>depth</code> computed as <code>scan_z - surface_z</code> and then to restrict to <code>depth &gt; 500</code>:</p> <pre><code>tab.proj(depth='scan_z-surface_z') &amp; 'depth &gt; 500'\n</code></pre> <p>Calculations are passed to SQL and are not parsed by DataJoint. For available functions, you may refer to the MySQL documentation.</p>"}, {"location": "query/query-caching/", "title": "Query Caching", "text": "<p>Query caching allows avoiding repeated queries to the database by caching the results locally for faster retrieval.</p> <p>To enable queries, set the query cache local path in <code>dj.config</code>, create the directory, and activate the query caching.</p> <pre><code># set the query cache path\ndj.config['query_cache'] = os.path.expanduser('~/dj_query_cache')\n\n# access the active connection object for the tables\nconn = dj.conn() # if queries co-located with tables\nconn = module.schema.connection # if schema co-located with tables\nconn = module.table.connection # most flexible\n\n# activate query caching for a namespace called 'main'\nconn.set_query_cache(query_cache='main')\n</code></pre> <p>The <code>query_cache</code> argument is an arbitrary string serving to differentiate cache states; setting a new value will effectively start a new cache, triggering retrieval of new values once.</p> <p>To turn off query caching, use the following:</p> <pre><code>conn.set_query_cache(query_cache=None)\n# or\nconn.set_query_cache()\n</code></pre> <p>While query caching is enabled, any insert or delete calls and any transactions are disabled and will raise an error. This ensures that stale data are not used for updating the database in violation of data integrity.</p> <p>To clear and remove the query cache, use the following:</p> <pre><code>conn.purge_query_cache()\n</code></pre>"}, {"location": "query/restrict/", "title": "Restriction", "text": ""}, {"location": "query/restrict/#restriction-operators-and-", "title": "Restriction operators <code>&amp;</code> and <code>-</code>", "text": "<p>The restriction operator <code>A &amp; cond</code> selects the subset of entities from <code>A</code> that meet the condition <code>cond</code>. The exclusion operator <code>A - cond</code> selects the complement of restriction, i.e. the subset of entities from <code>A</code> that do not meet the condition <code>cond</code>.</p> <p>Restriction and exclusion.</p> <p></p> <p>The condition <code>cond</code> may be one of the following:</p> <ul> <li>another table</li> <li>a mapping, e.g. <code>dict</code></li> <li>an expression in a character string</li> <li>a collection of conditions as a <code>list</code>, <code>tuple</code>, or Pandas <code>DataFrame</code></li> <li>a Boolean expression (<code>True</code> or <code>False</code>)</li> <li>an <code>AndList</code></li> <li>a <code>Not</code> object</li> <li>a query expression</li> </ul> <p>As the restriction and exclusion operators are complementary, queries can be constructed using both operators that will return the same results. For example, the queries <code>A &amp; cond</code> and <code>A - Not(cond)</code> will return the same entities.</p>"}, {"location": "query/restrict/#restriction-by-a-table", "title": "Restriction by a table", "text": "<p>When restricting table <code>A</code> with another table, written <code>A &amp; B</code>, the two tables must be join-compatible (see <code>join-compatible</code> in Operators). The result will contain all entities from <code>A</code> for which there exist a matching entity in <code>B</code>. Exclusion of table <code>A</code> with table <code>B</code>, or <code>A - B</code>, will contain all entities from <code>A</code> for which there are no matching entities in <code>B</code>.</p> <p>Restriction by another table.</p> <p></p> <p>Exclusion by another table.</p> <p></p>"}, {"location": "query/restrict/#restriction-by-a-table-with-no-common-attributes", "title": "Restriction by a table with no common attributes", "text": "<p>Restriction of table <code>A</code> with another table <code>B</code> having none of the same attributes as <code>A</code> will simply return all entities in <code>A</code>, unless <code>B</code> is empty as described below. Exclusion of table <code>A</code> with <code>B</code> having no common attributes will return no entities, unless <code>B</code> is empty as described below.</p> <p>Restriction by a table having no common attributes.</p> <p></p> <p>Exclusion by a table having no common attributes.</p> <p></p>"}, {"location": "query/restrict/#restriction-by-an-empty-table", "title": "Restriction by an empty table", "text": "<p>Restriction of table <code>A</code> with an empty table will return no entities regardless of whether there are any matching attributes. Exclusion of table <code>A</code> with an empty table will return all entities in <code>A</code>.</p> <p>Restriction by an empty table.</p> <p></p> <p>Exclusion by an empty table.</p> <p></p>"}, {"location": "query/restrict/#restriction-by-a-mapping", "title": "Restriction by a mapping", "text": "<p>A key-value mapping may be used as an operand in restriction. For each key that is an attribute in <code>A</code>, the paired value is treated as part of an equality condition. Any key-value pairs without corresponding attributes in <code>A</code> are ignored.</p> <p>Restriction by an empty mapping or by a mapping with no keys matching the attributes in <code>A</code> will return all the entities in <code>A</code>. Exclusion by an empty mapping or by a mapping with no matches will return no entities.</p> <p>For example, let's say that table <code>Session</code> has the attribute <code>session_date</code> of datatype <code>datetime</code>. You are interested in sessions from January 1st, 2018, so you write the following restriction query using a mapping.</p> <pre><code>Session &amp; {'session_date': \"2018-01-01\"}\n</code></pre> <p>Our mapping contains a typo omitting the final <code>e</code> from <code>session_date</code>, so no keys in our mapping will match any attribute in <code>Session</code>. As such, our query will return all of the entities of <code>Session</code>.</p>"}, {"location": "query/restrict/#restriction-by-a-string", "title": "Restriction by a string", "text": "<p>Restriction can be performed when <code>cond</code> is an explicit condition on attribute values, expressed as a string. Such conditions may include arithmetic operations, functions, range tests, etc. Restriction of table <code>A</code> by a string containing an attribute not found in table <code>A</code> produces an error.</p> <pre><code># All the sessions performed by Alice\nSession &amp; 'user = \"Alice\"'\n\n# All the experiments at least one minute long\nExperiment &amp; 'duration &gt;= 60'\n</code></pre>"}, {"location": "query/restrict/#restriction-by-a-collection", "title": "Restriction by a collection", "text": "<p>A collection can be a list, a tuple, or a Pandas <code>DataFrame</code>.</p> <pre><code># a list:\ncond_list = ['first_name = \"Aaron\"', 'last_name = \"Aaronson\"']\n\n# a tuple:\ncond_tuple = ('first_name = \"Aaron\"', 'last_name = \"Aaronson\"')\n\n# a dataframe:\nimport pandas as pd\ncond_frame = pd.DataFrame(\n               data={'first_name': ['Aaron'], 'last_name': ['Aaronson']})\n</code></pre> <p>When <code>cond</code> is a collection of conditions, the conditions are applied by logical disjunction (logical OR). Thus, restriction of table <code>A</code> by a collection will return all entities in <code>A</code> that meet any of the conditions in the collection. For example, if you restrict the <code>Student</code> table by a collection containing two conditions, one for a first and one for a last name, your query will return any students with a matching first name or a matching last name.</p> <pre><code>Student() &amp; ['first_name = \"Aaron\"', 'last_name = \"Aaronson\"']\n</code></pre> <p>Restriction by a collection, returning all entities matching any condition in the collection.</p> <p></p> <p>Restriction by an empty collection returns no entities. Exclusion of table <code>A</code> by an empty collection returns all the entities of <code>A</code>.</p>"}, {"location": "query/restrict/#restriction-by-a-boolean-expression", "title": "Restriction by a Boolean expression", "text": "<p><code>A &amp; True</code> and <code>A - False</code> are equivalent to <code>A</code>.</p> <p><code>A &amp; False</code> and <code>A - True</code> are empty.</p>"}, {"location": "query/restrict/#restriction-by-an-andlist", "title": "Restriction by an <code>AndList</code>", "text": "<p>The special function <code>dj.AndList</code> represents logical conjunction (logical AND). Restriction of table <code>A</code> by an <code>AndList</code> will return all entities in <code>A</code> that meet all of the conditions in the list. <code>A &amp; dj.AndList([c1, c2, c3])</code> is equivalent to <code>A &amp; c1 &amp; c2 &amp; c3</code>. Usually, it is more convenient to simply write out all of the conditions, as <code>A &amp; c1 &amp; c2 &amp; c3</code>. However, when a list of conditions has already been generated, the list can simply be passed as the argument to <code>dj.AndList</code>.</p> <p>Restriction of table <code>A</code> by an empty <code>AndList</code>, as in <code>A &amp; dj.AndList([])</code>, will return all of the entities in <code>A</code>. Exclusion by an empty <code>AndList</code> will return no entities.</p>"}, {"location": "query/restrict/#restriction-by-a-not-object", "title": "Restriction by a <code>Not</code> object", "text": "<p>The special function <code>dj.Not</code> represents logical negation, such that <code>A &amp; dj.Not(cond)</code> is equivalent to <code>A - cond</code>.</p>"}, {"location": "query/restrict/#restriction-by-a-query", "title": "Restriction by a query", "text": "<p>Restriction by a query object is a generalization of restriction by a table (which is also a query object), because DataJoint queries always produce well-defined entity sets, as described in  entity normalization. As such, restriction by queries follows the same behavior as restriction by tables described above.</p> <p>The example below creates a query object corresponding to all the sessions performed by the user Alice. The <code>Experiment</code> table is then restricted by the query object, returning all the experiments that are part of sessions performed by Alice.</p> <pre><code>query = Session &amp; 'user = \"Alice\"'\nExperiment &amp; query\n</code></pre>"}, {"location": "query/restrict/#restriction-by-djtop", "title": "Restriction by <code>dj.Top</code>", "text": "<p>Restriction by <code>dj.Top</code> returns the number of entities specified by the <code>limit</code> argument. These entities can be returned in the order specified by the <code>order_by</code> argument. And finally, the <code>offset</code> argument can be used to offset the returned entities which is useful for pagination in web applications.</p> <pre><code># Return the first 10 sessions in descending order of session date\nSession &amp; dj.Top(limit=10, order_by='session_date DESC')\n</code></pre>"}, {"location": "query/union/", "title": "Union", "text": "<p>The union operator is not yet implemented -- this page serves as the specification for the upcoming implementation. Union is rarely needed in practice.</p>"}, {"location": "query/union/#union-operator", "title": "Union operator <code>+</code>", "text": "<p>The result of the union operator <code>A + B</code> contains all the entities from both operands. Entity normalization requires that the operands in a union both belong to the same entity type with the same primary key using homologous attributes. In the absence of any secondary attributes, the result of a union is the simple set union.</p> <p>When secondary attributes are present, they must have the same names and datatypes in both operands. The two operands must also be disjoint, without any duplicate primary key values across both inputs. These requirements prevent ambiguity of attribute values and preserve entity identity.</p>"}, {"location": "query/union/#principles-of-union", "title": "Principles of union", "text": "<ol> <li>As in all operators, the order of the attributes in the operands is not significant.</li> <li>Operands <code>A</code> and <code>B</code> must have the same primary key attributes.    Otherwise, an error will be raised.</li> <li>Operands <code>A</code> and <code>B</code> may not have any common non-key attributes.    Otherwise, an error will be raised.</li> <li>The result <code>A + B</code> will have the same primary key as <code>A</code> and <code>B</code>.</li> <li>The result <code>A + B</code> will have all the non-key attributes from both <code>A</code> and <code>B</code>.</li> <li>For entities that are found in both <code>A</code> and <code>B</code> (based on the primary key), the secondary attributes will be filled from the corresponding entities in <code>A</code> and <code>B</code>.</li> <li>For entities that are only found in either <code>A</code> or <code>B</code>, the other operand's secondary attributes will filled with null values.</li> </ol>"}, {"location": "query/union/#examples-of-union", "title": "Examples of union", "text": "<p>Example 1 : Note that the order of the attributes does not matter.</p> <p></p> <p>Example 2 : Non-key attributes are combined from both tables and filled with NULLs when missing.</p> <p></p>"}, {"location": "query/union/#properties-of-union", "title": "Properties of union", "text": "<ol> <li>Commutative: <code>A + B</code> is equivalent to <code>B + A</code>.</li> <li>Associative: <code>(A + B) + C</code> is equivalent to <code>A + (B + C)</code>.</li> </ol>"}, {"location": "query/universals/", "title": "Universal Sets", "text": "<p>All query operators are designed to preserve the entity types of their inputs. However, some queries require creating a new entity type that is not represented by any stored tables. This means that a new entity type must be explicitly defined as part of the query. Universal sets fulfill this role.</p> <p>Universal sets are used in DataJoint to define virtual tables with arbitrary primary key structures for use in query expressions. A universal set, defined using class <code>dj.U</code>, denotes the set of all possible entities with given attributes of any possible datatype. Universal sets allow query expressions using virtual tables when no suitable base table exists. Attributes of universal sets are allowed to be matched to any namesake attributes, even those that do not come from the same initial source.</p> <p>For example, you may like to query the university database for the complete list of students' home cities, along with the number of students from each city. The schema for the university database does not have a table for cities and states. A virtual table can fill the role of the nonexistent base table, allowing queries that would not be possible otherwise.</p> <pre><code># All home cities of students\ndj.U('home_city', 'home_state') &amp; Student\n\n# Total number of students from each city\ndj.U('home_city', 'home_state').aggr(Student, n=\"count(*)\")\n\n# Total number of students from each state\nU('home_state').aggr(Student, n=\"count(*)\")\n\n# Total number of students in the database\nU().aggr(Student, n=\"count(*)\")\n</code></pre> <p>The result of aggregation on a universal set is restricted to the entities with matches in the aggregated table, such as <code>Student</code> in the example above. In other words, <code>X.aggr(A, ...)</code> is interpreted as <code>(X &amp; A).aggr(A, ...)</code> for universal set <code>X</code>. All attributes of a universal set are considered primary.</p> <p>Universal sets should be used sparingly when no suitable base tables already exist. In some cases, defining a new base table can make queries clearer and more semantically constrained.</p>"}, {"location": "sysadmin/bulk-storage/", "title": "Bulk Storage Systems", "text": ""}, {"location": "sysadmin/bulk-storage/#why-external-bulk-storage", "title": "Why External Bulk Storage?", "text": "<p>DataJoint supports the storage of large data objects associated with relational records externally from the MySQL Database itself. This is significant and useful for a number of reasons.</p>"}, {"location": "sysadmin/bulk-storage/#cost", "title": "Cost", "text": "<p>One reason is that the high-performance storage commonly used in database systems is more expensive than typical commodity storage. Therefore, storing the smaller identifying information typically used in queries on fast, relational database storage and storing the larger bulk data used for analysis or processing on lower cost commodity storage enables large savings in storage expense.</p>"}, {"location": "sysadmin/bulk-storage/#flexibility", "title": "Flexibility", "text": "<p>Storing bulk data separately also facilitates more flexibility in usage, since the bulk data can managed using separate maintenance processes than those in the relational storage.</p> <p>For example, larger relational databases may require many hours to be restored in the event of system failures. If the relational portion of the data is stored separately, with the larger bulk data stored on another storage system, this downtime can be reduced to a matter of minutes. Similarly, due to the lower cost of bulk commodity storage, more emphasis can be put into redundancy of this data and backups to help protect the non-relational data.</p>"}, {"location": "sysadmin/bulk-storage/#performance", "title": "Performance", "text": "<p>Storing the non-relational bulk data separately can have system performance impacts by removing data transfer, disk I/O, and memory load from the database server and shifting these to the bulk storage system. Additionally, DataJoint supports caching of bulk data records which can allow for faster processing of records which already have been retrieved in previous queries.</p>"}, {"location": "sysadmin/bulk-storage/#data-sharing", "title": "Data Sharing", "text": "<p>DataJoint provides pluggable support for different external bulk storage backends, allowing data sharing by publishing bulk data to S3-Protocol compatible data shares both in the cloud and on locally managed systems and other common tools for data sharing, such as Globus, etc.</p>"}, {"location": "sysadmin/bulk-storage/#bulk-storage-scenarios", "title": "Bulk Storage Scenarios", "text": "<p>Typical bulk storage considerations relate to the cost of the storage backend per unit of storage, the amount of data which will be stored, the desired focus of the shared data (system performance, data flexibility, data sharing), and data access. Some common scenarios are given in the following table:</p> Scenario Storage Solution System Requirements Notes Local Object Cache Local External Storage Local Hard Drive Used to Speed Access to other Storage LAN Object Cache Network External Storage Local Network Share Used to Speed Access to other storage, reduce Cloud/Network Costs/Overhead Local Object Store Local/Network External Storage Local/Network Storage Used to store objects externally from the database Local S3-Compatible Store Local S3-Compatible Server Network S3-Server Used to host S3-Compatible services locally (e.g. minio) for internal use or to lower cloud costs Cloud S3-Compatible Storage Cloud Provider Internet Connectivity Used to reduce/remove requirement for external storage management, data sharing Globus Storage Globus Endpoint Local/Local Network Storage, Internet Connectivity Used for institutional data transfer or publishing."}, {"location": "sysadmin/bulk-storage/#bulk-storage-considerations", "title": "Bulk Storage Considerations", "text": "<p>Although external bulk storage provides a variety of advantages for storage cost and data sharing, it also uses slightly different data input/retrieval semantics and as such has different performance characteristics.</p>"}, {"location": "sysadmin/bulk-storage/#performance-characteristics", "title": "Performance Characteristics", "text": "<p>In the direct database connection scenario, entire result sets are either added or retrieved from the database in a single stream action. In the case of external storage, individual record components are retrieved in a set of sequential actions per record, each one subject to the network round trip to the given storage medium. As such, tables using many small records may be ill suited to external storage usage in the absence of a caching mechanism. While some of these impacts may be addressed by code changes in a future release of DataJoint, to some extent, the impact is directly related from needing to coordinate the activities of the database data stream with the external storage system, and so cannot be avoided.</p>"}, {"location": "sysadmin/bulk-storage/#network-traffic", "title": "Network Traffic", "text": "<p>Some of the external storage solutions mentioned above incur cost both at a data volume and transfer bandwidth level. The number of users querying the database, data access, and use of caches should be considered in these cases to reduce this cost if applicable.</p>"}, {"location": "sysadmin/bulk-storage/#data-coherency", "title": "Data Coherency", "text": "<p>When storing all data directly in the relational data store, it is relatively easy to ensure that all data in the database is consistent in the event of system issues such as crash recoveries, since MySQL\u2019s relational storage engine manages this for you. When using external storage however, it is important to ensure that any data recoveries of the database system are paired with a matching point-in-time of the external storage system. While DataJoint does use hashing to help facilitate a guarantee that external files are uniquely named throughout their lifecycle, the pairing of a given relational dataset against a given filesystem state is loosely coupled, and so an incorrect pairing could result in processing failures or other issues.</p>"}, {"location": "sysadmin/database-admin/", "title": "Database Administration", "text": ""}, {"location": "sysadmin/database-admin/#hosting", "title": "Hosting", "text": "<p>Let\u2019s say a person, a lab, or a multi-lab consortium decide to use DataJoint as their data pipeline platform. What IT resources and support will be required?</p> <p>DataJoint uses a MySQL-compatible database server such as MySQL, MariaDB, Percona Server, or Amazon Aurora to store the structured data used for all relational operations. Large blocks of data associated with these records such as multidimensional numeric arrays (signals, images, scans, movies, etc) can be stored within the database or stored in additionally configured bulk storage.</p> <p>The first decisions you need to make are where this server will be hosted and how it will be administered. The server may be hosted on your personal computer, on a dedicated machine in your lab, or in a cloud-based database service.</p>"}, {"location": "sysadmin/database-admin/#cloud-hosting", "title": "Cloud hosting", "text": "<p>Increasingly, many teams make use of cloud-hosted database services, which allow great flexibility and easy administration of the database server. A cloud hosting option will be provided through https://works.datajoint.com. DataJoint Works simplifies the setup for labs that wish to host their data pipelines in the cloud and allows sharing pipelines between multiple groups and locations. Being an open-source solution, other cloud services such as Amazon RDS can also be used in this role, albeit with less DataJoint-centric customization.</p>"}, {"location": "sysadmin/database-admin/#self-hosting", "title": "Self hosting", "text": "<p>In the most basic configuration, the relational database management system (database server) is installed on an individual user's personal computer. To support a group of users, a specialized machine can be configured as a dedicated database server. This server can be accessed by multiple DataJoint clients to query the data and perform computations.</p> <p>For larger groups and multi-site collaborations with heavy workloads, the database server cluster may be configured in the cloud or on premises. The following section provides some basic guidelines for these configurations here and in the subsequent sections of the documentation.</p>"}, {"location": "sysadmin/database-admin/#general-server-hardware-support-requirements", "title": "General server / hardware support requirements", "text": "<p>The following table lists some likely scenarios for DataJoint database server deployments and some reasonable estimates of the required computer hardware. The required IT/systems support needed to ensure smooth operations in the absence of local database expertise is also listed.</p>"}, {"location": "sysadmin/database-admin/#it-infrastructures", "title": "IT infrastructures", "text": "Usage Scenario DataJoint Database Computer Required IT Support Single User Personal Laptop or Workstation Self-Supported or Ad-Hoc General IT Support Small Group (e.g. 2-10 Users) Workstation or Small Server Ad-Hoc General or Experienced IT Support Medium Group (e.g. 10-30 Users) Small to Medium Server Ad-Hoc/Part Time Experienced or Specialized IT Support Large Group/Department (e.g. 30-50+ Users) Medium/Large Server or Multi-Server Replication Part Time/Dedicated Experienced or Specialized IT Support Multi-Location Collaboration (30+ users, Geographically Distributed) Large Server, Advanced Replication Dedicated Specialized IT Support"}, {"location": "sysadmin/database-admin/#configuration", "title": "Configuration", "text": ""}, {"location": "sysadmin/database-admin/#hardware-considerations", "title": "Hardware considerations", "text": "<p>As in any computer system, CPU, RAM memory, disk storage, and network speed are important components of performance. The relational database component of DataJoint is no exception to this rule. This section discusses the various factors relating to selecting a server for your DataJoint pipelines.</p>"}, {"location": "sysadmin/database-admin/#cpu", "title": "CPU", "text": "<p>CPU speed and parallelism (number of cores/threads) will impact the speed of queries and the number of simultaneous queries which can be efficiently supported by the system. It is a good rule of thumb to have enough cores to support the number of active users and background tasks you expect to have running during a typical 'busy' day of usage. For example, a team of 10 people might want to have 8 cores to support a few active queries and background tasks.</p>"}, {"location": "sysadmin/database-admin/#ram", "title": "RAM", "text": "<p>The amount of RAM will impact the amount of DataJoint data kept in memory, allowing for faster querying of data since the data can be searched and returned to the user without needing to access the slower disk drives. It is a good idea to get enough memory to fully store the more important and frequently accessed portions of your dataset with room to spare, especially if in-database blob storage is used instead of external bulk storage.</p>"}, {"location": "sysadmin/database-admin/#disk", "title": "Disk", "text": "<p>The disk storage for a DataJoint database server should have fast random access, ideally with flash-based storage to eliminate the rotational delay of mechanical hard drives.</p>"}, {"location": "sysadmin/database-admin/#networking", "title": "Networking", "text": "<p>When network connections are used, network speed and latency are important to ensure that large query results can be quickly transferred across the network and that delays due to data entry/query round-trip have minimal impact on the runtime of the program.</p>"}, {"location": "sysadmin/database-admin/#general-recommendations", "title": "General recommendations", "text": "<p>DataJoint datasets can consist of many thousands or even millions of records. Generally speaking one would want to make sure that the relational database system has sufficient CPU speed and parallelism to support a typical number of concurrent users and to execute searches quickly. The system should have enough RAM to store the primary key values of commonly used tables and operating system caches. Disk storage should be fast enough to support quick loading of and searching through the data. Lastly, network bandwidth must be sufficient to support transferring user records quickly.</p>"}, {"location": "sysadmin/database-admin/#large-scale-installations", "title": "Large-scale installations", "text": "<p>Database replication may be beneficial if system downtime or precise database responsiveness is a concern Replication can allow for easier coordination of maintenance activities, faster recovery in the event of system problems, and distribution of the database workload across server machines to increase throughput and responsiveness.</p>"}, {"location": "sysadmin/database-admin/#multi-master-replication", "title": "Multi-master replication", "text": "<p>Multi-master replication configurations allow for all replicas to be used in a read/ write fashion, with the workload being distributed among all machines. However, multi-master replication is also more complicated, requiring front-end machines to distribute the workload, similar performance characteristics on all replicas to prevent bottlenecks, and redundant network connections to ensure the replicated machines are always in sync.</p>"}, {"location": "sysadmin/database-admin/#recommendations", "title": "Recommendations", "text": "<p>It is usually best to go with the simplest solution which can suit the requirements of the installation, adjusting workloads where possible and adding complexity only as needs dictate.</p> <p>Resource requirements of course depend on the data collection and processing needs of the given pipeline, but there are general size guidelines that can inform any system configuration decisions. A reasonably powerful workstation or small server should support the needs of a small group (2-10 users). A medium or large server should support the needs of a larger user community (10-30 users). A replicated or distributed setup of 2 or more medium or large servers may be required in larger cases. These requirements can be reduced through the use of external or cloud storage, which is discussed in the subsequent section.</p> Usage Scenario DataJoint Database Computer Hardware Recommendation Single User Personal Laptop or Workstation 4 Cores, 8-16GB or more of RAM, SSD or better storage Small Group (e.g. 2-10 Users) Workstation or Small Server 8 or more Cores, 16GB or more of RAM, SSD or better storage Medium Group (e.g. 10-30 Users) Small to Medium Server 8-16 or more Cores, 32GB or more of RAM, SSD/RAID or better storage Large Group/Department (e.g. 30-50+ Users) Medium/Large Server or Multi-Server Replication 16-32 or more Cores, 64GB or more of RAM, SSD Raid storage, multiple machines Multi-Location Collaboration (30+ users, Geographically Distributed) Large Server, Advanced Replication 16-32 or more Cores, 64GB or more of RAM, SSD Raid storage, multiple machines; potentially multiple machines in multiple locations"}, {"location": "sysadmin/database-admin/#docker", "title": "Docker", "text": "<p>A Docker image is available for a MySQL server configured to work with DataJoint: https://github.com/datajoint/mysql-docker.</p>"}, {"location": "sysadmin/database-admin/#user-management", "title": "User Management", "text": "<p>Create user accounts on the MySQL server. For example, if your username is alice, the SQL code for this step is:</p> <pre><code>CREATE USER 'alice'@'%' IDENTIFIED BY 'alices-secret-password';\n</code></pre> <p>Existing users can be listed using the following SQL:</p> <pre><code>SELECT user, host from mysql.user;\n</code></pre> <p>Teams that use DataJoint typically divide their data into schemas grouped together by common prefixes. For example, a lab may have a collection of schemas that begin with <code>common_</code>. Some common processing may be organized into several schemas that begin with <code>pipeline_</code>. Typically each user has all privileges to schemas that begin with their username.</p> <p>For example, alice may have privileges to select and insert data from the common schemas (but not create new tables), and have all privileges to the pipeline schemas.</p> <p>Then the SQL code to grant her privileges might look like:</p> <pre><code>GRANT SELECT, INSERT ON `common\\_%`.* TO 'alice'@'%';\nGRANT ALL PRIVILEGES ON `pipeline\\_%`.* TO 'alice'@'%';\nGRANT ALL PRIVILEGES ON `alice\\_%`.* TO 'alice'@'%';\n</code></pre> <p>To note, the <code>ALL PRIVILEGES</code> option allows the user to create and remove databases without administrator intervention.</p> <p>Once created, a user's privileges can be listed using the <code>SHOW GRANTS</code> statement.</p> <pre><code>SHOW GRANTS FOR 'alice'@'%';\n</code></pre>"}, {"location": "sysadmin/database-admin/#grouping-with-wildcards", "title": "Grouping with Wildcards", "text": "<p>Depending on the complexity of your installation, using additional wildcards to group access rules together might make managing user access rules simpler. For example, the following equivalent convention:</p> <pre><code>GRANT ALL PRIVILEGES ON `user_alice\\_%`.* TO 'alice'@'%';\n</code></pre> <p>Could then facilitate using a rule like:</p> <pre><code>GRANT SELECT ON `user\\_%\\_%`.* TO 'bob'@'%';\n</code></pre> <p>to enable <code>bob</code> to query all other users tables using the <code>user_username_database</code> convention without needing to explicitly give him access to <code>alice\\_%</code>, <code>charlie\\_%</code>, and so on.</p> <p>This convention can be further expanded to create notions of groups and protected schemas for background processing, etc. For example:</p> <pre><code>GRANT ALL PRIVILEGES ON `group\\_shared\\_%`.* TO 'alice'@'%';\nGRANT ALL PRIVILEGES ON `group\\_shared\\_%`.* TO 'bob'@'%';\n\nGRANT ALL PRIVILEGES ON `group\\_wonderland\\_%`.* TO 'alice'@'%';\nGRANT SELECT ON `group\\_wonderland\\_%`.* TO 'alice'@'%';\n</code></pre> <p>could allow both bob an alice to read/write into the <code>group\\_shared</code> databases, but in the case of the <code>group\\_wonderland</code> databases, read write access is restricted to alice.</p>"}, {"location": "sysadmin/database-admin/#backups-and-recovery", "title": "Backups and Recovery", "text": "<p>Backing up your DataJoint installation is critical to ensuring that your work is safe and can be continued in the event of system failures, and several mechanisms are available to use.</p> <p>Much like your live installation, your backup will consist of two portions:</p> <ul> <li>Backup of the Relational Data</li> <li>Backup of optional external bulk storage</li> </ul> <p>This section primarily deals with backup of the relational data since most of the optional bulk storage options use \"regular\" flat-files for storage and can be backed up via any \"normal\" disk backup regime.</p> <p>There are many options to backup MySQL; subsequent sections discuss a few options.</p>"}, {"location": "sysadmin/database-admin/#cloud-hosted-backups", "title": "Cloud hosted backups", "text": "<p>In the case of cloud-hosted options, many cloud vendors provide automated backup of your data, and some facility for downloading such backups externally. Due to the wide variety of cloud-specific options, discussion of these options falls outside of the scope of this documentation. However, since the cloud server is also a MySQL server, other options listed here may work for your situation.</p>"}, {"location": "sysadmin/database-admin/#disk-based-backup", "title": "Disk-based backup", "text": "<p>The simplest option for many cases is to perform a disk-level backup of your MySQL installation using standard disk backup tools. It should be noted that all database activity should be stopped for the duration of the backup to prevent errors with the backed up data. This can be done in one of two ways:</p> <ul> <li>Stopping the MySQL server program</li> <li>Using database locks</li> </ul> <p>These methods are required since MySQL data operations can be ongoing in the background even when no user activity is ongoing. To use a database lock to perform a backup, the following commands can be used as the MySQL administrator:</p> <pre><code>FLUSH TABLES WITH READ LOCK;\nUNLOCK TABLES;\n</code></pre> <p>The backup should be performed between the issuing of these two commands, ensuring the database data is consistent on disk when it is backed up.</p>"}, {"location": "sysadmin/database-admin/#mysqldump", "title": "MySQLDump", "text": "<p>Disk based backups may not be feasible for every installation, or a database may require constant activity such that stopping it for backups is not feasible. In such cases, the simplest option is MySQLDump,  a command line tool that prints the contents of your database contents in SQL form.</p> <p>This tool is generally acceptable for most cases and is especially well suited for smaller installations due to its simplicity and ease of use.</p> <p>For larger installations, the lower speed of MySQLDump can be a limitation, since it has to convert the database contents to and from SQL rather than dealing with the database files directly. Additionally, since backups are performed within a transaction, the backup will be valid up to the time the backup began rather than to its completion, which can make ensuring that the latest data are fully backed up more difficult as the time it takes to run a backup grows.</p>"}, {"location": "sysadmin/database-admin/#percona-xtrabackup", "title": "Percona XTraBackup", "text": "<p>The Percona <code>xtrabackup</code> tool provides near-realtime backup capability of a MySQL installation, with extended support for replicated databases, and is a good tool for backing up larger databases.</p> <p>However, this tool requires local disk access as well as reasonably fast backup media, since it builds an ongoing transaction log in real time to ensure that backups are valid up to the point of their completion. This strategy fails if it cannot keep up with the write speed of the database. Further, the backups it generates are in binary format and include incomplete database transactions, which require careful attention to detail when restoring.</p> <p>As such, this solution is recommended only for advanced use cases or larger databases where limitations of the other solutions may apply.</p>"}, {"location": "sysadmin/database-admin/#locking-and-ddl-issues", "title": "Locking and DDL issues", "text": "<p>One important thing to note is that at the time of writing, MySQL's transactional system is not <code>data definition language</code> aware, meaning that changes to table structures occurring during some backup schemes can result in corrupted backup copies. If schema changes will be occurring during your backup window, it is a good idea to ensure that appropriate locking mechanisms are used to prevent these changes during critical steps of the backup process.</p> <p>However, on busy installations which cannot be stopped, the use of locks in many backup utilities may cause issues if your programs expect to write data to the database during the backup window.</p> <p>In such cases it might make sense to review the given backup tools for locking related options or to use other mechanisms such as replicas or alternate backup tools to prevent interaction of the database.</p>"}, {"location": "sysadmin/database-admin/#replication-and-snapshots-for-backup", "title": "Replication and snapshots for backup", "text": "<p>Larger databases consisting of many Terabytes of data may take many hours or even days to backup and restore, and so downtime resulting from system failure can create major impacts to ongoing work.</p> <p>While not backup tools per-se, use of MySQL replication and disk snapshots can be useful to assist in reducing the downtime resulting from a full database outage.</p> <p>Replicas can be configured so that one copy of the data is immediately online in the event of server crash. When a server fails in this case, users and programs simply restart and point to the new server before resuming work.</p> <p>Replicas can also reduce the system load generated by regular backup procedures, since they can be backed up instead of the main server. Additionally they can allow more flexibility in a given backup scheme, such as allowing for disk snapshots on a busy system that would not otherwise be able to be stopped. A replica copy can be stopped temporarily and then resumed while a disk snapshot or other backup operation occurs.</p>"}, {"location": "sysadmin/external-store/", "title": "External Store", "text": "<p>DataJoint organizes most of its data in a relational database. Relational databases excel at representing relationships between entities and storing structured data. However, relational databases are not particularly well-suited for storing large continuous chunks of data such as images, signals, and movies. An attribute of type <code>longblob</code> can contain an object up to 4 GiB in size (after compression) but storing many such large objects may hamper the performance of queries on the entire table. A good rule of thumb is that objects over 10 MiB in size should not be put in the relational database. In addition, storing data in cloud-hosted relational databases (e.g. AWS RDS) may be more expensive than in cloud-hosted simple storage systems (e.g. AWS S3).</p> <p>DataJoint allows the use of <code>external</code> storage to store large data objects within its relational framework but outside of the main database.</p> <p>Defining an externally-stored attribute is used using the notation <code>blob@storename</code> (see also: definition syntax) and works the same way as a <code>longblob</code> attribute from the users perspective. However, its data are stored in an external storage system rather than in the relational database.</p> <p>Various systems can play the role of external storage, including a shared file system accessible to all team members with access to these objects or a cloud storage solutions such as AWS S3.</p> <p>For example, the following table stores motion-aligned two-photon movies.</p> <pre><code># Motion aligned movies\n-&gt; twophoton.Scan\n---\naligned_movie :  blob@external  # motion-aligned movie in 'external' store\n</code></pre> <p>All insert and fetch operations work identically for <code>external</code> attributes as they do for <code>blob</code> attributes, with the same serialization protocol. Similar to <code>blobs</code>, <code>external</code> attributes cannot be used in restriction conditions.</p> <p>Multiple external storage configurations may be used simultaneously with the <code>@storename</code> portion of the attribute definition determining the storage location.</p> <pre><code># Motion aligned movies\n-&gt; twophoton.Scan\n---\naligned_movie :  blob@external-raw  # motion-aligned movie in 'external-raw' store\n</code></pre>"}, {"location": "sysadmin/external-store/#principles-of-operation", "title": "Principles of operation", "text": "<p>External storage is organized to emulate individual attribute values in the relational database. DataJoint organizes external storage to preserve the same data integrity principles as in relational storage.</p> <ol> <li> <p>The external storage locations are specified in the DataJoint connection configuration with one specification for each store.</p> <pre><code>dj.config['stores'] = {\n'external': dict(  # 'regular' external storage for this pipeline\n               protocol='s3',\n               endpoint='s3.amazonaws.com:9000',\n               bucket = 'testbucket',\n               location = 'datajoint-projects/lab1',\n               access_key='1234567',\n               secret_key='foaf1234'),\n'external-raw': dict( # 'raw' storage for this pipeline\n               protocol='file',\n               location='/net/djblobs/myschema')\n}\n# external object cache - see fetch operation below for details.\ndj.config['cache'] = '/net/djcache'\n</code></pre> </li> <li> <p>Each schema corresponds to a dedicated folder at the storage location with the same name as the database schema.</p> </li> <li> <p>Stored objects are identified by the SHA-256 hashes (in web-safe base-64 ASCII) of their serialized contents.    This scheme allows for the same object\u2014used multiple times in the same schema\u2014to be    stored only once.</p> </li> <li> <p>In the <code>external-raw</code> storage, the objects are saved as files with the hash as the filename.</p> </li> <li> <p>In the <code>external</code> storage, external files are stored in a directory layout corresponding to the hash of the filename. By default, this corresponds to the first 2 characters of the hash, followed by the second 2 characters of the hash, followed by the actual file.</p> </li> <li> <p>Each database schema has an auxiliary table named <code>~external_&lt;storename&gt;</code> for each configured external store.</p> <p>It is automatically created the first time external storage is used.  The primary key of <code>~external_&lt;storename&gt;</code> is the hash of the data (for blobs and  attachments) or of the relative paths to the files for filepath-based storage.  Other attributes are the <code>count</code> of references by tables in the schema, the <code>size</code>  of the object in bytes, and the <code>timestamp</code> of the last event (creation, update, or  deletion).</p> <p>Below are sample entries in <code>~external_&lt;storename&gt;</code>.</p> HASH size filepath contents_hash timestamp 1GEqtEU6JYEOLS4sZHeHDxWQ3JJfLlH VZio1ga25vd2 1039536788 NULL NULL 2017-06-07 23:14:01 <p>The fields <code>filepath</code> and <code>contents_hash</code> relate to the  filepath datatype, which will be discussed  separately.</p> </li> <li> <p>Attributes of type <code>@&lt;storename&gt;</code> are declared as renamed foreign keys referencing the <code>~external_&lt;storename&gt;</code> table (but are not shown as such to the user).</p> </li> <li> <p>The insert operation encodes and hashes the blob data. If an external object is not present in storage for the same hash, the object is saved and if the save operation is successful, corresponding entities in table <code>~external_&lt;storename&gt;</code> for that store are created.</p> </li> <li> <p>The delete  operation first deletes the foreign key reference in the target table. The external table entry and actual external object is not actually deleted at this time (<code>soft-delete</code>).</p> </li> <li> <p>The fetch operation uses the hash values to find the data.    In order to prevent excessive network overhead, a special external store named    <code>cache</code> can be configured.    If the <code>cache</code> is enabled, the <code>fetch</code> operation need not access    <code>~external_&lt;storename&gt;</code> directly.    Instead <code>fetch</code> will retrieve the cached object without downloading directly from    the <code>real</code> external store.</p> </li> <li> <p>Cleanup is performed regularly when the database is in light use or off-line.</p> </li> <li> <p>DataJoint never removes objects from the local <code>cache</code> folder.     The <code>cache</code> folder may just be periodically emptied entirely or based on file     access date.     If dedicated <code>cache</code> folders are maintained for each schema, then a special     procedure will be provided to remove all objects that are no longer listed in     <code>~external_&lt;storename&gt;</code>.</p> </li> </ol> <p>Data removal from external storage is separated from the delete operations to ensure that data are not lost in race conditions between inserts and deletes of the same objects, especially in cases of transactional processing or in processes that are likely to get terminated. The cleanup steps are performed in a separate process when the risks of race conditions are minimal. The process performing the cleanups must be isolated to prevent interruptions resulting in loss of data integrity.</p>"}, {"location": "sysadmin/external-store/#configuration", "title": "Configuration", "text": "<p>The following steps must be performed to enable external storage:</p> <ol> <li> <p>Assign external location settings for each storage as shown in the Step 1 example above. Use <code>dj.config</code> for configuration.</p> <ul> <li><code>protocol</code> [<code>s3</code>, <code>file</code>] Specifies whether <code>s3</code> or <code>file</code> external storage is   desired.</li> <li><code>endpoint</code> [<code>s3</code>] Specifies the remote endpoint to the external data for all schemas as well as the target port.</li> <li><code>bucket</code> [<code>s3</code>] Specifies the appropriate <code>s3</code> bucket organization.</li> <li><code>location</code> [<code>s3</code>, <code>file</code>] Specifies the subdirectory within the root or bucket of store to preserve data. External objects are thus stored remotely with the following path structure: <code>&lt;bucket (if applicable)&gt;/&lt;location&gt;/&lt;schema_name&gt;/&lt;subfolding_strategy&gt;/&lt;object&gt;</code>.</li> <li><code>access_key</code> [<code>s3</code>] Specifies the access key credentials for accessing the external location.</li> <li><code>secret_key</code> [<code>s3</code>] Specifies the secret key credentials for accessing the external location.</li> <li><code>secure</code> [<code>s3</code>] Optional specification to establish secure external storage connection with TLS (aka SSL, HTTPS). Defaults to <code>False</code>.</li> </ul> </li> <li> <p>Optionally, for each schema specify the <code>cache</code> folder for local fetch cache.</p> <p>This is done by saving the path in the <code>cache</code> key of the DataJoint configuration dictionary:</p> <pre><code>  dj.config['cache'] = '/temp/dj-cache'\n</code></pre> </li> </ol>"}, {"location": "sysadmin/external-store/#cleanup", "title": "Cleanup", "text": "<p>Deletion of records containing externally stored blobs is a <code>soft-delete</code> which only removes the database-side records from the database. To cleanup the external tracking table or the actual external files, a separate process is provided as follows.</p> <p>To remove only the tracking entries in the external table, call <code>delete</code> on the <code>~external_&lt;storename&gt;</code> table for the external configuration with the argument <code>delete_external_files=False</code>.</p> <p>Note: Currently, cleanup operations on a schema's external table are not 100%   transaction safe and so must be run when there is no write activity occurring   in tables which use a given schema / external store pairing.</p> <pre><code>schema.external['external_raw'].delete(delete_external_files=False)\n</code></pre> <p>To remove the tracking entries as well as the underlying files, call <code>delete</code> on the external table for the external configuration with the argument <code>delete_external_files=True</code>.</p> <pre><code>schema.external['external_raw'].delete(delete_external_files=True)\n</code></pre> <p>Note: Setting <code>delete_external_files=True</code> will always attempt to delete   the underlying data file, and so should not typically be used with   the <code>filepath</code> datatype.</p>"}, {"location": "sysadmin/external-store/#migration-between-datajoint-v011-and-v012", "title": "Migration between DataJoint v0.11 and v0.12", "text": "<p>Note: Please read carefully if you have used external storage in DataJoint v0.11!</p> <p>The initial implementation of external storage was reworked for DataJoint v0.12. These changes are backward-incompatible with DataJoint v0.11 so care should be taken when upgrading. This section outlines some details of the change and a general process for upgrading to a format compatible with DataJoint v0.12 when a schema rebuild is not desired.</p> <p>The primary changes to the external data implementation are:</p> <ul> <li>The external object tracking mechanism was modified. Tracking tables were extended for additional external datatypes and split into per-store tables to improve database performance in schemas with many external objects.</li> </ul> <ul> <li>The external storage format was modified to use a nested subfolder structure (<code>folding</code>) to improve performance and interoperability with some filesystems that have limitations or performance problems when storing large numbers of files in single directories.</li> </ul> <p>Depending on the circumstances, the simplest way to migrate data to v0.12 may be to drop and repopulate the affected schemas. This will construct the schema and storage structure in the v0.12 format and save the need for database migration. When recreation is not possible or is not preferred to upgrade to DataJoint v0.12, the following process should be followed:</p> <p>1. Stop write activity to all schemas using external storage.</p> <p>2. Perform a full backup of your database(s).</p> <p>3. Upgrade your DataJoint installation to v0.12</p> <p>4. Adjust your external storage configuration (in <code>datajoint.config</code>)      to the new v0.12 configuration format (see above).</p> <p>5. Migrate external tracking tables for each schema to use the new format. For   instance in Python:</p> <pre><code>```python\nimport datajoint.migrate as migrate\ndb_schema_name='schema_1'\nexternal_store='raw'\nmigrate.migrate_dj011_external_blob_storage_to_dj012(db_schema_name, external_store)\n```\n</code></pre> <p>6. Verify pipeline functionality after this process has completed. For instance in   Python:</p> <pre><code>```python\nx = myschema.TableWithExternal.fetch('external_field', limit=1)[0]\n```\n</code></pre> <p>Note: This migration function is provided on a best-effort basis, and will   convert the external tracking tables into a format which is compatible   with DataJoint v0.12. While we have attempted to ensure correctness   of the process, all use-cases have not been heavily tested. Please be sure to fully   back-up your data and be prepared to investigate problems with the   migration, should they occur.</p> <p>Please note:</p> <ul> <li>The migration only migrates the tracking table format and does not modify the backing file structure to support <code>folding</code>. The DataJoint v0.12 logic is able to work with this format, but to take advantage of the new backend storage, manual adjustment of the tracking table and files, or a full rebuild of the schema should be performed.</li> </ul> <ul> <li>Additional care to ensure all clients are using v0.12 should be taken after the upgrade. Legacy clients may incorrectly create data in the old format which would then need to be combined or otherwise reconciled with the data in v0.12 format. You might wish to take the opportunity to version-pin your installations so that future changes requiring controlled upgrades can be coordinated on a system wide basis.</li> </ul>"}, {"location": "tutorials/dj-top/", "title": "Using the dj.Top restriction", "text": "<p>First you will need to install and connect to a DataJoint data pipeline.</p> <p>Now let's start by importing the <code>datajoint</code> client.</p> In\u00a0[1]: Copied! <pre>import datajoint as dj\n\ndj.config[\"database.host\"] = \"127.0.0.1\"\nschema = dj.Schema(\"university\")\n</pre> import datajoint as dj  dj.config[\"database.host\"] = \"127.0.0.1\" schema = dj.Schema(\"university\") <pre>[2024-12-20 11:10:20,120][INFO]: Connecting root@127.0.0.1:3306\n[2024-12-20 11:10:20,259][INFO]: Connected root@127.0.0.1:3306\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int unsigned   # university-wide ID number\n    ---\n    first_name      : varchar(40)\n    last_name       : varchar(40)\n    sex             : enum('F', 'M', 'U')\n    date_of_birth   : date\n    home_address    : varchar(120) # mailing street address\n    home_city       : varchar(60)  # mailing address\n    home_state      : char(2)      # US state acronym: e.g. OH\n    home_zip        : char(10)     # zipcode e.g. 93979-4979\n    home_phone      : varchar(20)  # e.g. 414.657.6883x0881\n    \"\"\"\n</pre> @schema class Student(dj.Manual):     definition = \"\"\"     student_id : int unsigned   # university-wide ID number     ---     first_name      : varchar(40)     last_name       : varchar(40)     sex             : enum('F', 'M', 'U')     date_of_birth   : date     home_address    : varchar(120) # mailing street address     home_city       : varchar(60)  # mailing address     home_state      : char(2)      # US state acronym: e.g. OH     home_zip        : char(10)     # zipcode e.g. 93979-4979     home_phone      : varchar(20)  # e.g. 414.657.6883x0881     \"\"\" In\u00a0[3]: Copied! <pre>@schema\nclass Department(dj.Manual):\n    definition = \"\"\"\n    dept : varchar(6)   # abbreviated department name, e.g. BIOL\n    ---\n    dept_name    : varchar(200)  # full department name\n    dept_address : varchar(200)  # mailing address\n    dept_phone   : varchar(20)\n    \"\"\"\n</pre> @schema class Department(dj.Manual):     definition = \"\"\"     dept : varchar(6)   # abbreviated department name, e.g. BIOL     ---     dept_name    : varchar(200)  # full department name     dept_address : varchar(200)  # mailing address     dept_phone   : varchar(20)     \"\"\" In\u00a0[4]: Copied! <pre>@schema\nclass StudentMajor(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    ---\n    -&gt; Department\n    declare_date :  date  # when student declared her major\n    \"\"\"\n</pre> @schema class StudentMajor(dj.Manual):     definition = \"\"\"     -&gt; Student     ---     -&gt; Department     declare_date :  date  # when student declared her major     \"\"\" In\u00a0[5]: Copied! <pre>@schema\nclass Course(dj.Manual):\n    definition = \"\"\"\n    -&gt; Department\n    course  : int unsigned   # course number, e.g. 1010\n    ---\n    course_name :  varchar(200)  # e.g. \"Neurobiology of Sensation and Movement.\"\n    credits     :  decimal(3,1)  # number of credits earned by completing the course\n    \"\"\"\n\n\n@schema\nclass Term(dj.Manual):\n    definition = \"\"\"\n    term_year : year\n    term      : enum('Spring', 'Summer', 'Fall')\n    \"\"\"\n\n\n@schema\nclass Section(dj.Manual):\n    definition = \"\"\"\n    -&gt; Course\n    -&gt; Term\n    section : char(1)\n    ---\n    auditorium   :  varchar(12)\n    \"\"\"\n\n\n@schema\nclass CurrentTerm(dj.Manual):\n    definition = \"\"\"\n    -&gt; Term\n    \"\"\"\n\n\n@schema\nclass Enroll(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    -&gt; Section\n    \"\"\"\n\n\n@schema\nclass LetterGrade(dj.Lookup):\n    definition = \"\"\"\n    grade : char(2)\n    ---\n    points : decimal(3,2)\n    \"\"\"\n    contents = [\n        [\"A\", 4.00],\n        [\"A-\", 3.67],\n        [\"B+\", 3.33],\n        [\"B\", 3.00],\n        [\"B-\", 2.67],\n        [\"C+\", 2.33],\n        [\"C\", 2.00],\n        [\"C-\", 1.67],\n        [\"D+\", 1.33],\n        [\"D\", 1.00],\n        [\"F\", 0.00],\n    ]\n\n\n@schema\nclass Grade(dj.Manual):\n    definition = \"\"\"\n    -&gt; Enroll\n    ---\n    -&gt; LetterGrade\n    \"\"\"\n</pre> @schema class Course(dj.Manual):     definition = \"\"\"     -&gt; Department     course  : int unsigned   # course number, e.g. 1010     ---     course_name :  varchar(200)  # e.g. \"Neurobiology of Sensation and Movement.\"     credits     :  decimal(3,1)  # number of credits earned by completing the course     \"\"\"   @schema class Term(dj.Manual):     definition = \"\"\"     term_year : year     term      : enum('Spring', 'Summer', 'Fall')     \"\"\"   @schema class Section(dj.Manual):     definition = \"\"\"     -&gt; Course     -&gt; Term     section : char(1)     ---     auditorium   :  varchar(12)     \"\"\"   @schema class CurrentTerm(dj.Manual):     definition = \"\"\"     -&gt; Term     \"\"\"   @schema class Enroll(dj.Manual):     definition = \"\"\"     -&gt; Student     -&gt; Section     \"\"\"   @schema class LetterGrade(dj.Lookup):     definition = \"\"\"     grade : char(2)     ---     points : decimal(3,2)     \"\"\"     contents = [         [\"A\", 4.00],         [\"A-\", 3.67],         [\"B+\", 3.33],         [\"B\", 3.00],         [\"B-\", 2.67],         [\"C+\", 2.33],         [\"C\", 2.00],         [\"C-\", 1.67],         [\"D+\", 1.33],         [\"D\", 1.00],         [\"F\", 0.00],     ]   @schema class Grade(dj.Manual):     definition = \"\"\"     -&gt; Enroll     ---     -&gt; LetterGrade     \"\"\" In\u00a0[6]: Copied! <pre>from tqdm import tqdm\nimport faker\nimport random\nimport datetime\n\nfake = faker.Faker()\n</pre> from tqdm import tqdm import faker import random import datetime  fake = faker.Faker() In\u00a0[7]: Copied! <pre>def yield_students():\n    fake_name = {\"F\": fake.name_female, \"M\": fake.name_male}\n    while True:  # ignore invalid values\n        try:\n            sex = random.choice((\"F\", \"M\"))\n            first_name, last_name = fake_name[sex]().split(\" \")[:2]\n            street_address, city = fake.address().split(\"\\n\")\n            city, state = city.split(\", \")\n            state, zipcode = state.split(\" \")\n        except ValueError:\n            continue\n        else:\n            yield dict(\n                first_name=first_name,\n                last_name=last_name,\n                sex=sex,\n                home_address=street_address,\n                home_city=city,\n                home_state=state,\n                home_zip=zipcode,\n                date_of_birth=str(\n                    fake.date_time_between(start_date=\"-35y\", end_date=\"-15y\").date()\n                ),\n                home_phone=fake.phone_number()[:20],\n            )\n</pre> def yield_students():     fake_name = {\"F\": fake.name_female, \"M\": fake.name_male}     while True:  # ignore invalid values         try:             sex = random.choice((\"F\", \"M\"))             first_name, last_name = fake_name[sex]().split(\" \")[:2]             street_address, city = fake.address().split(\"\\n\")             city, state = city.split(\", \")             state, zipcode = state.split(\" \")         except ValueError:             continue         else:             yield dict(                 first_name=first_name,                 last_name=last_name,                 sex=sex,                 home_address=street_address,                 home_city=city,                 home_state=state,                 home_zip=zipcode,                 date_of_birth=str(                     fake.date_time_between(start_date=\"-35y\", end_date=\"-15y\").date()                 ),                 home_phone=fake.phone_number()[:20],             ) In\u00a0[8]: Copied! <pre>Student.insert(dict(k, student_id=i) for i, k in zip(range(100, 300), yield_students()))\n\nDepartment.insert(\n    dict(\n        dept=dept,\n        dept_name=name,\n        dept_address=fake.address(),\n        dept_phone=fake.phone_number()[:20],\n    )\n    for dept, name in [\n        [\"CS\", \"Computer Science\"],\n        [\"BIOL\", \"Life Sciences\"],\n        [\"PHYS\", \"Physics\"],\n        [\"MATH\", \"Mathematics\"],\n    ]\n)\n\nStudentMajor.insert(\n    {**s, **d, \"declare_date\": fake.date_between(start_date=datetime.date(1999, 1, 1))}\n    for s, d in zip(\n        Student.fetch(\"KEY\"), random.choices(Department.fetch(\"KEY\"), k=len(Student()))\n    )\n    if random.random() &lt; 0.75\n)\n\n# from https://www.utah.edu/\nCourse.insert(\n    [\n        [\"BIOL\", 1006, \"World of Dinosaurs\", 3],\n        [\"BIOL\", 1010, \"Biology in the 21st Century\", 3],\n        [\"BIOL\", 1030, \"Human Biology\", 3],\n        [\"BIOL\", 1210, \"Principles of Biology\", 4],\n        [\"BIOL\", 2010, \"Evolution &amp; Diversity of Life\", 3],\n        [\"BIOL\", 2020, \"Principles of Cell Biology\", 3],\n        [\"BIOL\", 2021, \"Principles of Cell Science\", 4],\n        [\"BIOL\", 2030, \"Principles of Genetics\", 3],\n        [\"BIOL\", 2210, \"Human Genetics\", 3],\n        [\"BIOL\", 2325, \"Human Anatomy\", 4],\n        [\"BIOL\", 2330, \"Plants &amp; Society\", 3],\n        [\"BIOL\", 2355, \"Field Botany\", 2],\n        [\"BIOL\", 2420, \"Human Physiology\", 4],\n        [\"PHYS\", 2040, \"Classcal Theoretical Physics II\", 4],\n        [\"PHYS\", 2060, \"Quantum Mechanics\", 3],\n        [\"PHYS\", 2100, \"General Relativity and Cosmology\", 3],\n        [\"PHYS\", 2140, \"Statistical Mechanics\", 4],\n        [\"PHYS\", 2210, \"Physics for Scientists and Engineers I\", 4],\n        [\"PHYS\", 2220, \"Physics for Scientists and Engineers II\", 4],\n        [\"PHYS\", 3210, \"Physics for Scientists I (Honors)\", 4],\n        [\"PHYS\", 3220, \"Physics for Scientists II (Honors)\", 4],\n        [\"MATH\", 1250, \"Calculus for AP Students I\", 4],\n        [\"MATH\", 1260, \"Calculus for AP Students II\", 4],\n        [\"MATH\", 1210, \"Calculus I\", 4],\n        [\"MATH\", 1220, \"Calculus II\", 4],\n        [\"MATH\", 2210, \"Calculus III\", 3],\n        [\"MATH\", 2270, \"Linear Algebra\", 4],\n        [\"MATH\", 2280, \"Introduction to Differential Equations\", 4],\n        [\"MATH\", 3210, \"Foundations of Analysis I\", 4],\n        [\"MATH\", 3220, \"Foundations of Analysis II\", 4],\n        [\"CS\", 1030, \"Foundations of Computer Science\", 3],\n        [\"CS\", 1410, \"Introduction to Object-Oriented Programming\", 4],\n        [\"CS\", 2420, \"Introduction to Algorithms &amp; Data Structures\", 4],\n        [\"CS\", 2100, \"Discrete Structures\", 3],\n        [\"CS\", 3500, \"Software Practice\", 4],\n        [\"CS\", 3505, \"Software Practice II\", 3],\n        [\"CS\", 3810, \"Computer Organization\", 4],\n        [\"CS\", 4400, \"Computer Systems\", 4],\n        [\"CS\", 4150, \"Algorithms\", 3],\n        [\"CS\", 3100, \"Models of Computation\", 3],\n        [\"CS\", 3200, \"Introduction to Scientific Computing\", 3],\n        [\"CS\", 4000, \"Senior Capstone Project - Design Phase\", 3],\n        [\"CS\", 4500, \"Senior Capstone Project\", 3],\n        [\"CS\", 4940, \"Undergraduate Research\", 3],\n        [\"CS\", 4970, \"Computer Science Bachelors Thesis\", 3],\n    ]\n)\n\nTerm.insert(\n    dict(term_year=year, term=term)\n    for year in range(1999, 2019)\n    for term in [\"Spring\", \"Summer\", \"Fall\"]\n)\n\nTerm().fetch(order_by=(\"term_year DESC\", \"term DESC\"), as_dict=True, limit=1)[0]\n\nCurrentTerm().insert1(\n    {**Term().fetch(order_by=(\"term_year DESC\", \"term DESC\"), as_dict=True, limit=1)[0]}\n)\n\n\ndef make_section(prob):\n    for c in (Course * Term).proj():\n        for sec in \"abcd\":\n            if random.random() &lt; prob:\n                break\n            yield {\n                **c,\n                \"section\": sec,\n                \"auditorium\": random.choice(\"ABCDEF\") + str(random.randint(1, 100)),\n            }\n\n\nSection.insert(make_section(0.5))\n</pre> Student.insert(dict(k, student_id=i) for i, k in zip(range(100, 300), yield_students()))  Department.insert(     dict(         dept=dept,         dept_name=name,         dept_address=fake.address(),         dept_phone=fake.phone_number()[:20],     )     for dept, name in [         [\"CS\", \"Computer Science\"],         [\"BIOL\", \"Life Sciences\"],         [\"PHYS\", \"Physics\"],         [\"MATH\", \"Mathematics\"],     ] )  StudentMajor.insert(     {**s, **d, \"declare_date\": fake.date_between(start_date=datetime.date(1999, 1, 1))}     for s, d in zip(         Student.fetch(\"KEY\"), random.choices(Department.fetch(\"KEY\"), k=len(Student()))     )     if random.random() &lt; 0.75 )  # from https://www.utah.edu/ Course.insert(     [         [\"BIOL\", 1006, \"World of Dinosaurs\", 3],         [\"BIOL\", 1010, \"Biology in the 21st Century\", 3],         [\"BIOL\", 1030, \"Human Biology\", 3],         [\"BIOL\", 1210, \"Principles of Biology\", 4],         [\"BIOL\", 2010, \"Evolution &amp; Diversity of Life\", 3],         [\"BIOL\", 2020, \"Principles of Cell Biology\", 3],         [\"BIOL\", 2021, \"Principles of Cell Science\", 4],         [\"BIOL\", 2030, \"Principles of Genetics\", 3],         [\"BIOL\", 2210, \"Human Genetics\", 3],         [\"BIOL\", 2325, \"Human Anatomy\", 4],         [\"BIOL\", 2330, \"Plants &amp; Society\", 3],         [\"BIOL\", 2355, \"Field Botany\", 2],         [\"BIOL\", 2420, \"Human Physiology\", 4],         [\"PHYS\", 2040, \"Classcal Theoretical Physics II\", 4],         [\"PHYS\", 2060, \"Quantum Mechanics\", 3],         [\"PHYS\", 2100, \"General Relativity and Cosmology\", 3],         [\"PHYS\", 2140, \"Statistical Mechanics\", 4],         [\"PHYS\", 2210, \"Physics for Scientists and Engineers I\", 4],         [\"PHYS\", 2220, \"Physics for Scientists and Engineers II\", 4],         [\"PHYS\", 3210, \"Physics for Scientists I (Honors)\", 4],         [\"PHYS\", 3220, \"Physics for Scientists II (Honors)\", 4],         [\"MATH\", 1250, \"Calculus for AP Students I\", 4],         [\"MATH\", 1260, \"Calculus for AP Students II\", 4],         [\"MATH\", 1210, \"Calculus I\", 4],         [\"MATH\", 1220, \"Calculus II\", 4],         [\"MATH\", 2210, \"Calculus III\", 3],         [\"MATH\", 2270, \"Linear Algebra\", 4],         [\"MATH\", 2280, \"Introduction to Differential Equations\", 4],         [\"MATH\", 3210, \"Foundations of Analysis I\", 4],         [\"MATH\", 3220, \"Foundations of Analysis II\", 4],         [\"CS\", 1030, \"Foundations of Computer Science\", 3],         [\"CS\", 1410, \"Introduction to Object-Oriented Programming\", 4],         [\"CS\", 2420, \"Introduction to Algorithms &amp; Data Structures\", 4],         [\"CS\", 2100, \"Discrete Structures\", 3],         [\"CS\", 3500, \"Software Practice\", 4],         [\"CS\", 3505, \"Software Practice II\", 3],         [\"CS\", 3810, \"Computer Organization\", 4],         [\"CS\", 4400, \"Computer Systems\", 4],         [\"CS\", 4150, \"Algorithms\", 3],         [\"CS\", 3100, \"Models of Computation\", 3],         [\"CS\", 3200, \"Introduction to Scientific Computing\", 3],         [\"CS\", 4000, \"Senior Capstone Project - Design Phase\", 3],         [\"CS\", 4500, \"Senior Capstone Project\", 3],         [\"CS\", 4940, \"Undergraduate Research\", 3],         [\"CS\", 4970, \"Computer Science Bachelors Thesis\", 3],     ] )  Term.insert(     dict(term_year=year, term=term)     for year in range(1999, 2019)     for term in [\"Spring\", \"Summer\", \"Fall\"] )  Term().fetch(order_by=(\"term_year DESC\", \"term DESC\"), as_dict=True, limit=1)[0]  CurrentTerm().insert1(     {**Term().fetch(order_by=(\"term_year DESC\", \"term DESC\"), as_dict=True, limit=1)[0]} )   def make_section(prob):     for c in (Course * Term).proj():         for sec in \"abcd\":             if random.random() &lt; prob:                 break             yield {                 **c,                 \"section\": sec,                 \"auditorium\": random.choice(\"ABCDEF\") + str(random.randint(1, 100)),             }   Section.insert(make_section(0.5)) In\u00a0[9]: Copied! <pre># Enrollment\nterms = Term().fetch(\"KEY\")\nquit_prob = 0.1\nfor student in tqdm(Student.fetch(\"KEY\")):\n    start_term = random.randrange(len(terms))\n    for term in terms[start_term:]:\n        if random.random() &lt; quit_prob:\n            break\n        else:\n            sections = ((Section &amp; term) - (Course &amp; (Enroll &amp; student))).fetch(\"KEY\")\n            if sections:\n                Enroll.insert(\n                    {**student, **section}\n                    for section in random.sample(\n                        sections, random.randrange(min(5, len(sections)))\n                    )\n                )\n\n# assign random grades\ngrades = LetterGrade.fetch(\"grade\")\n\ngrade_keys = Enroll.fetch(\"KEY\")\nrandom.shuffle(grade_keys)\ngrade_keys = grade_keys[: len(grade_keys) * 9 // 10]\n\nGrade.insert(\n    {**key, \"grade\": grade}\n    for key, grade in zip(grade_keys, random.choices(grades, k=len(grade_keys)))\n)\n</pre> # Enrollment terms = Term().fetch(\"KEY\") quit_prob = 0.1 for student in tqdm(Student.fetch(\"KEY\")):     start_term = random.randrange(len(terms))     for term in terms[start_term:]:         if random.random() &lt; quit_prob:             break         else:             sections = ((Section &amp; term) - (Course &amp; (Enroll &amp; student))).fetch(\"KEY\")             if sections:                 Enroll.insert(                     {**student, **section}                     for section in random.sample(                         sections, random.randrange(min(5, len(sections)))                     )                 )  # assign random grades grades = LetterGrade.fetch(\"grade\")  grade_keys = Enroll.fetch(\"KEY\") random.shuffle(grade_keys) grade_keys = grade_keys[: len(grade_keys) * 9 // 10]  Grade.insert(     {**key, \"grade\": grade}     for key, grade in zip(grade_keys, random.choices(grades, k=len(grade_keys))) ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:27&lt;00:00,  7.17it/s]\n</pre> In\u00a0[29]: Copied! <pre>(Grade * LetterGrade) &amp; \"term_year='2018'\" &amp; dj.Top(\n    limit=5, order_by=\"points DESC\", offset=5\n)\n</pre> (Grade * LetterGrade) &amp; \"term_year='2018'\" &amp; dj.Top(     limit=5, order_by=\"points DESC\", offset=5 ) Out[29]: <p>student_id</p> university-wide ID number <p>dept</p> abbreviated department name, e.g. BIOL <p>course</p> course number, e.g. 1010 <p>term_year</p> <p>term</p> <p>section</p> <p>grade</p> <p>points</p> 100 MATH 2280 2018 Fall a A- 3.67191 MATH 2210 2018 Spring b A 4.00211 CS 2100 2018 Fall a A 4.00273 PHYS 2100 2018 Spring a A 4.00282 BIOL 2021 2018 Spring d A 4.00 <p>Total: 5</p> In\u00a0[35]: Copied! <pre>(\n    (LetterGrade * Grade)\n    &amp; \"term_year='2018'\"\n    &amp; dj.Top(limit=10, order_by=\"points DESC\", offset=0)\n).make_sql()\n</pre> (     (LetterGrade * Grade)     &amp; \"term_year='2018'\"     &amp; dj.Top(limit=10, order_by=\"points DESC\", offset=0) ).make_sql() Out[35]: <pre>\"SELECT `grade`,`student_id`,`dept`,`course`,`term_year`,`term`,`section`,`points` FROM `university`.`#letter_grade` NATURAL JOIN `university`.`grade` WHERE ( (term_year='2018')) ORDER BY `points` DESC LIMIT 10\"</pre> In\u00a0[44]: Copied! <pre>(\n    (Grade * LetterGrade)\n    &amp; \"term_year='2018'\"\n    &amp; dj.Top(limit=20, order_by=\"points DESC\", offset=0)\n).make_sql()\n</pre> (     (Grade * LetterGrade)     &amp; \"term_year='2018'\"     &amp; dj.Top(limit=20, order_by=\"points DESC\", offset=0) ).make_sql() Out[44]: <pre>\"SELECT `student_id`,`dept`,`course`,`term_year`,`term`,`section`,`grade`,`points` FROM `university`.`grade` NATURAL JOIN `university`.`#letter_grade` WHERE ( (term_year='2018')) ORDER BY `points` DESC LIMIT 20\"</pre> In\u00a0[47]: Copied! <pre>(Grade * LetterGrade) &amp; \"term_year='2018'\" &amp; dj.Top(\n    limit=20, order_by=\"points DESC\", offset=0\n)\n</pre> (Grade * LetterGrade) &amp; \"term_year='2018'\" &amp; dj.Top(     limit=20, order_by=\"points DESC\", offset=0 ) Out[47]: <p>student_id</p> university-wide ID number <p>dept</p> abbreviated department name, e.g. BIOL <p>course</p> course number, e.g. 1010 <p>term_year</p> <p>term</p> <p>section</p> <p>grade</p> <p>points</p> 100 CS 3200 2018 Fall c A 4.00100 MATH 2280 2018 Fall a A- 3.67100 PHYS 2210 2018 Spring d A 4.00122 CS 1030 2018 Fall c B+ 3.33131 BIOL 2030 2018 Spring a A 4.00131 CS 3200 2018 Fall b B+ 3.33136 BIOL 2210 2018 Spring c B+ 3.33136 MATH 2210 2018 Fall b B+ 3.33141 BIOL 2010 2018 Summer c B+ 3.33141 CS 2420 2018 Fall b A 4.00141 CS 3200 2018 Fall b A- 3.67182 CS 1410 2018 Summer c A- 3.67 <p>...</p> <p>Total: 20</p> In\u00a0[41]: Copied! <pre>(LetterGrade * Grade) &amp; \"term_year='2018'\" &amp; dj.Top(\n    limit=20, order_by=\"points DESC\", offset=0\n)\n</pre> (LetterGrade * Grade) &amp; \"term_year='2018'\" &amp; dj.Top(     limit=20, order_by=\"points DESC\", offset=0 ) Out[41]: <p>grade</p> <p>student_id</p> university-wide ID number <p>dept</p> abbreviated department name, e.g. BIOL <p>course</p> course number, e.g. 1010 <p>term_year</p> <p>term</p> <p>section</p> <p>points</p> A 100 CS 3200 2018 Fall c 4.00A 100 PHYS 2210 2018 Spring d 4.00A 131 BIOL 2030 2018 Spring a 4.00A 141 CS 2420 2018 Fall b 4.00A 186 PHYS 2210 2018 Spring a 4.00A 191 MATH 2210 2018 Spring b 4.00A 211 CS 2100 2018 Fall a 4.00A 273 PHYS 2100 2018 Spring a 4.00A 282 BIOL 2021 2018 Spring d 4.00A- 100 MATH 2280 2018 Fall a 3.67A- 141 CS 3200 2018 Fall b 3.67A- 182 CS 1410 2018 Summer c 3.67 <p>...</p> <p>Total: 20</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "tutorials/dj-top/#using-the-djtop-restriction", "title": "Using the dj.Top restriction\u00b6", "text": ""}, {"location": "tutorials/dj-top/#table-definition", "title": "Table Definition\u00b6", "text": ""}, {"location": "tutorials/dj-top/#insert", "title": "Insert\u00b6", "text": ""}, {"location": "tutorials/dj-top/#djtop-restriction", "title": "dj.Top Restriction\u00b6", "text": ""}, {"location": "tutorials/json/", "title": "Using the json type", "text": "<p>\u26a0\ufe0f Note the following before using the <code>json</code> type</p> <ul> <li>Supported only for MySQL &gt;= 8.0 when JSON_VALUE introduced.</li> <li>Equivalent Percona is fully-compatible.</li> <li>MariaDB is not supported since JSON_VALUE does not allow type specification like MySQL's.</li> <li>Not yet supported in DataJoint MATLAB</li> </ul> <p>First you will need to install and connect to a DataJoint data pipeline.</p> <p>Now let's start by importing the <code>datajoint</code> client.</p> In\u00a0[1]: Copied! <pre>import datajoint as dj\n</pre> import datajoint as dj <p>For this exercise, let's imagine we work for an awesome company that is organizing a fun RC car race across various teams in the company. Let's see which team has the fastest car! \ud83c\udfce\ufe0f</p> <p>This establishes 2 important entities: a <code>Team</code> and a <code>Car</code>. Normally the entities are mapped to their own dedicated table, however, let's assume that <code>Team</code> is well-structured but <code>Car</code> is less structured than we'd prefer. In other words, the structure for what makes up a car is varying too much between entries (perhaps because users of the pipeline haven't agreed yet on the definition? \ud83e\udd37).</p> <p>This would make it a good use-case to keep <code>Team</code> as a table but make <code>Car</code> a <code>json</code> type defined within the <code>Team</code> table.</p> <p>Let's begin.</p> In\u00a0[2]: Copied! <pre>schema = dj.Schema(f\"{dj.config['database.user']}_json\")\n</pre> schema = dj.Schema(f\"{dj.config['database.user']}_json\") <pre>[2023-02-12 00:14:33,027][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-02-12 00:14:33,039][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[3]: Copied! <pre>@schema\nclass Team(dj.Lookup):\n    definition = \"\"\"\n    # A team within a company\n    name: varchar(40)  # team name\n    ---\n    car=null: json  # A car belonging to a team (null to allow registering first but specifying car later)\n    \n    unique index(car.length:decimal(4, 1))  # Add an index if this key is frequently accessed\n    \"\"\"\n</pre> @schema class Team(dj.Lookup):     definition = \"\"\"     # A team within a company     name: varchar(40)  # team name     ---     car=null: json  # A car belonging to a team (null to allow registering first but specifying car later)          unique index(car.length:decimal(4, 1))  # Add an index if this key is frequently accessed     \"\"\" <p>Let's suppose that engineering is first up to register their car.</p> In\u00a0[4]: Copied! <pre>Team.insert1(\n    {\n        \"name\": \"engineering\",\n        \"car\": {\n            \"name\": \"Rever\",\n            \"length\": 20.5,\n            \"inspected\": True,\n            \"tire_pressure\": [32, 31, 33, 34],\n            \"headlights\": [\n                {\n                    \"side\": \"left\",\n                    \"hyper_white\": None,\n                },\n                {\n                    \"side\": \"right\",\n                    \"hyper_white\": None,\n                },\n            ],\n        },\n    }\n)\n</pre> Team.insert1(     {         \"name\": \"engineering\",         \"car\": {             \"name\": \"Rever\",             \"length\": 20.5,             \"inspected\": True,             \"tire_pressure\": [32, 31, 33, 34],             \"headlights\": [                 {                     \"side\": \"left\",                     \"hyper_white\": None,                 },                 {                     \"side\": \"right\",                     \"hyper_white\": None,                 },             ],         },     } ) <p>Next, business and marketing teams are up and register their cars.</p> <p>A few points to notice below:</p> <ul> <li>The person signing up on behalf of marketing does not know the specifics of the car during registration but another team member will be updating this soon before the race.</li> <li>Notice how the <code>business</code> and <code>engineering</code> teams appear to specify the same property but refer to it as <code>safety_inspected</code> and <code>inspected</code> respectfully.</li> </ul> In\u00a0[5]: Copied! <pre>Team.insert(\n    [\n        {\n            \"name\": \"marketing\",\n            \"car\": None,\n        },\n        {\n            \"name\": \"business\",\n            \"car\": {\n                \"name\": \"Chaching\",\n                \"length\": 100,\n                \"safety_inspected\": False,\n                \"tire_pressure\": [34, 30, 27, 32],\n                \"headlights\": [\n                    {\n                        \"side\": \"left\",\n                        \"hyper_white\": True,\n                    },\n                    {\n                        \"side\": \"right\",\n                        \"hyper_white\": True,\n                    },\n                ],\n            },\n        },\n    ]\n)\n</pre> Team.insert(     [         {             \"name\": \"marketing\",             \"car\": None,         },         {             \"name\": \"business\",             \"car\": {                 \"name\": \"Chaching\",                 \"length\": 100,                 \"safety_inspected\": False,                 \"tire_pressure\": [34, 30, 27, 32],                 \"headlights\": [                     {                         \"side\": \"left\",                         \"hyper_white\": True,                     },                     {                         \"side\": \"right\",                         \"hyper_white\": True,                     },                 ],             },         },     ] ) <p>We can preview the table data much like normal but notice how the value of <code>car</code> behaves like other BLOB-like attributes.</p> In\u00a0[6]: Copied! <pre>Team()\n</pre> Team() Out[6]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) marketing =BLOB=engineering =BLOB=business =BLOB= <p>Total: 3</p> <p>Now let's see what kinds of queries we can form to demostrate how we can query this pipeline.</p> In\u00a0[7]: Copied! <pre># Which team has a `car` equal to 100 inches long?\nTeam &amp; {\"car.length\": 100}\n</pre> # Which team has a `car` equal to 100 inches long? Team &amp; {\"car.length\": 100} Out[7]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) business =BLOB= <p>Total: 1</p> In\u00a0[8]: Copied! <pre># Which team has a `car` less than 50 inches long?\nTeam &amp; \"car-&gt;&gt;'$.length' &lt; 50\"\n</pre> # Which team has a `car` less than 50 inches long? Team &amp; \"car-&gt;&gt;'$.length' &lt; 50\" Out[8]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= <p>Total: 1</p> In\u00a0[9]: Copied! <pre># Any team that has had their car inspected?\nTeam &amp; [{\"car.inspected:unsigned\": True}, {\"car.safety_inspected:unsigned\": True}]\n</pre> # Any team that has had their car inspected? Team &amp; [{\"car.inspected:unsigned\": True}, {\"car.safety_inspected:unsigned\": True}] Out[9]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= <p>Total: 1</p> In\u00a0[10]: Copied! <pre># Which teams do not have hyper white lights for their first head light?\nTeam &amp; {\"car.headlights[0].hyper_white\": None}\n</pre> # Which teams do not have hyper white lights for their first head light? Team &amp; {\"car.headlights[0].hyper_white\": None} Out[10]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB=marketing =BLOB= <p>Total: 2</p> <p>Notice that the previous query will satisfy the <code>None</code> check if it experiences any of the following scenarious:</p> <ul> <li>if entire record missing (<code>marketing</code> satisfies this)</li> <li>JSON key is missing</li> <li>JSON value is set to JSON <code>null</code> (<code>engineering</code> satisfies this)</li> </ul> <p>Projections can be quite useful with the <code>json</code> type since we can extract out just what we need. This allows greater query flexibility but more importantly, for us to be able to fetch only what is pertinent.</p> In\u00a0[11]: Copied! <pre># Only interested in the car names and the length but let the type be inferred\nq_untyped = Team.proj(\n    car_name=\"car.name\",\n    car_length=\"car.length\",\n)\nq_untyped\n</pre> # Only interested in the car names and the length but let the type be inferred q_untyped = Team.proj(     car_name=\"car.name\",     car_length=\"car.length\", ) q_untyped Out[11]: <p>name</p> team name <p>car_name</p> calculated attribute <p>car_length</p> calculated attribute business Chaching 100engineering Rever 20.5marketing None None <p>Total: 3</p> In\u00a0[12]: Copied! <pre>q_untyped.fetch(as_dict=True)\n</pre> q_untyped.fetch(as_dict=True) Out[12]: <pre>[{'name': 'business', 'car_name': 'Chaching', 'car_length': '100'},\n {'name': 'engineering', 'car_name': 'Rever', 'car_length': '20.5'},\n {'name': 'marketing', 'car_name': None, 'car_length': None}]</pre> In\u00a0[13]: Copied! <pre># Nevermind, I'll specify the type explicitly\nq_typed = Team.proj(\n    car_name=\"car.name\",\n    car_length=\"car.length:float\",\n)\nq_typed\n</pre> # Nevermind, I'll specify the type explicitly q_typed = Team.proj(     car_name=\"car.name\",     car_length=\"car.length:float\", ) q_typed Out[13]: <p>name</p> team name <p>car_name</p> calculated attribute <p>car_length</p> calculated attribute business Chaching 100.0engineering Rever 20.5marketing None None <p>Total: 3</p> In\u00a0[14]: Copied! <pre>q_typed.fetch(as_dict=True)\n</pre> q_typed.fetch(as_dict=True) Out[14]: <pre>[{'name': 'business', 'car_name': 'Chaching', 'car_length': 100.0},\n {'name': 'engineering', 'car_name': 'Rever', 'car_length': 20.5},\n {'name': 'marketing', 'car_name': None, 'car_length': None}]</pre> <p>Lastly, the <code>.describe()</code> function on the <code>Team</code> table can help us generate the table's definition. This is useful if we are connected directly to the pipeline without the original source.</p> In\u00a0[16]: Copied! <pre>rebuilt_definition = Team.describe()\nprint(rebuilt_definition)\n</pre> rebuilt_definition = Team.describe() print(rebuilt_definition) <pre># A team within a company\nname                 : varchar(40)                  # team name\n---\ncar=null             : json                         # A car belonging to a team (null to allow registering first but specifying car later)\nUNIQUE INDEX ((json_value(`car`, _utf8mb4'$.length' returning decimal(4, 1))))\n\n</pre> <p>Finally, let's clean up what we created in this tutorial.</p> In\u00a0[17]: Copied! <pre>schema.drop()\n</pre> schema.drop() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "tutorials/json/#using-the-json-type", "title": "Using the json type\u00b6", "text": ""}, {"location": "tutorials/json/#table-definition", "title": "Table Definition\u00b6", "text": ""}, {"location": "tutorials/json/#insert", "title": "Insert\u00b6", "text": ""}, {"location": "tutorials/json/#restriction", "title": "Restriction\u00b6", "text": ""}, {"location": "tutorials/json/#projection", "title": "Projection\u00b6", "text": ""}, {"location": "tutorials/json/#describe", "title": "Describe\u00b6", "text": ""}, {"location": "tutorials/json/#cleanup", "title": "Cleanup\u00b6", "text": ""}]}