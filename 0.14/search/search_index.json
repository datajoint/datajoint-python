{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "develop/", "text": "Develop \u00b6 Included with the codebase is the recommended development environment configured using DevContainer . Launch Environment \u00b6 Here are some options that provide a great developer experience: Cloud-based IDE : ( recommended ) Launch using GitHub Codespaces using the option Create codespace on master in the codebase repository on your fork. Build time for a 2-Core codespace is ~6m . This is done infrequently and cached for convenience. Start time for a 2-Core codespace is ~2m . This will pull the built codespace from cache when you need it. Tip : GitHub auto names the codespace but you can rename the codespace so that it is easier to identify later. Local IDE : Ensure you have Git Ensure you have Docker Ensure you have VSCode Install the Dev Containers extension git clone the codebase repository and open it in VSCode Use the Dev Containers extension to Reopen in Container (More info in the Getting started included with the extension) You will know your environment has finished loading once you see a terminal open related to Running postStartCommand with a final message: Done . Features \u00b6 Once you've successfully launched the development environment, you'll be able to take advantage of our developer tooling to help improve productivity and quality. Syntax Tests \u00b6 The following will verify that there are no syntax errors. flake8 datajoint --count --select=E9,F63,F7,F82 --show-source --statistics Integration Tests \u00b6 The following will verify there are no regression errors by running our test suite of unit and integration tests. Entire test suite: nosetests -vw tests A single functional test: nosetests -vs --tests=tests.test_external_class:test_insert_and_fetch A single class test: nosetests -vs --tests=tests.test_fetch:TestFetch.test_getattribute_for_fetch1 Style Tests \u00b6 The following will verify that there are no code styling errors. flake8 --ignore=E203,E722,W503 datajoint --count --max-complexity=62 --max-line-length=127 --statistics The following will ensure the codebase has been formatted with black . black datajoint --check -v The following will ensure the test suite has been formatted with black . black tests --check -v Jupyter \u00b6 Jupyter notebooks are supported in this environment. This means that when you import datajoint , it will use the current state of the source. Be sure to see the reference documenation if you are new to running Jupyter notebooks w/ VSCode . Debugger \u00b6 VSCode Debugger is a powerful tool that can really accelerate fixes. Try it as follows: Create a python script of your choice import datajoint (This will use the current state of the source) Add breakpoints by adding red dots next to line numbers Select the Run and Debug tab Start by clicking the button Run and Debug MySQL CLI \u00b6 It is often useful in development to connect to DataJoint's relational database backend directly using the MySQL CLI. Connect as follows to the database running within your developer environment: mysql -hfakeservices.datajoint.io -uroot -psimple Documentation \u00b6 Our documentation is built using MkDocs Material . The easiest way to improve the documentation is by using the docs/docker-compose.yaml environment. The source can be modified in docs/src using markdown. The docs environment can be run using 3 modes: LIVE : ( recommended ) This serves the docs locally. It supports live reloading on saves to docs/src files but does not support the docs version dropdown. Useful to see changes live. MODE=\"LIVE\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build QA : This serves the docs locally. It supports the docs version dropdown but does not support live reloading. Useful as a final check. MODE=\"QA\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build BUILD : This compiles the docs. Most useful for the docs deployment automation. Other modes are more useful to new contributors. MODE=\"BUILD\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build When the docs are served locally, use the VSCode PORTS tab (next to TERMINAL ) to manage access to the forwarded ports. Docs are served on port 8080 .", "title": "Develop"}, {"location": "develop/#develop", "text": "Included with the codebase is the recommended development environment configured using DevContainer .", "title": "Develop"}, {"location": "develop/#launch-environment", "text": "Here are some options that provide a great developer experience: Cloud-based IDE : ( recommended ) Launch using GitHub Codespaces using the option Create codespace on master in the codebase repository on your fork. Build time for a 2-Core codespace is ~6m . This is done infrequently and cached for convenience. Start time for a 2-Core codespace is ~2m . This will pull the built codespace from cache when you need it. Tip : GitHub auto names the codespace but you can rename the codespace so that it is easier to identify later. Local IDE : Ensure you have Git Ensure you have Docker Ensure you have VSCode Install the Dev Containers extension git clone the codebase repository and open it in VSCode Use the Dev Containers extension to Reopen in Container (More info in the Getting started included with the extension) You will know your environment has finished loading once you see a terminal open related to Running postStartCommand with a final message: Done .", "title": "Launch Environment"}, {"location": "develop/#features", "text": "Once you've successfully launched the development environment, you'll be able to take advantage of our developer tooling to help improve productivity and quality.", "title": "Features"}, {"location": "develop/#syntax-tests", "text": "The following will verify that there are no syntax errors. flake8 datajoint --count --select=E9,F63,F7,F82 --show-source --statistics", "title": "Syntax Tests"}, {"location": "develop/#integration-tests", "text": "The following will verify there are no regression errors by running our test suite of unit and integration tests. Entire test suite: nosetests -vw tests A single functional test: nosetests -vs --tests=tests.test_external_class:test_insert_and_fetch A single class test: nosetests -vs --tests=tests.test_fetch:TestFetch.test_getattribute_for_fetch1", "title": "Integration Tests"}, {"location": "develop/#style-tests", "text": "The following will verify that there are no code styling errors. flake8 --ignore=E203,E722,W503 datajoint --count --max-complexity=62 --max-line-length=127 --statistics The following will ensure the codebase has been formatted with black . black datajoint --check -v The following will ensure the test suite has been formatted with black . black tests --check -v", "title": "Style Tests"}, {"location": "develop/#jupyter", "text": "Jupyter notebooks are supported in this environment. This means that when you import datajoint , it will use the current state of the source. Be sure to see the reference documenation if you are new to running Jupyter notebooks w/ VSCode .", "title": "Jupyter"}, {"location": "develop/#debugger", "text": "VSCode Debugger is a powerful tool that can really accelerate fixes. Try it as follows: Create a python script of your choice import datajoint (This will use the current state of the source) Add breakpoints by adding red dots next to line numbers Select the Run and Debug tab Start by clicking the button Run and Debug", "title": "Debugger"}, {"location": "develop/#mysql-cli", "text": "It is often useful in development to connect to DataJoint's relational database backend directly using the MySQL CLI. Connect as follows to the database running within your developer environment: mysql -hfakeservices.datajoint.io -uroot -psimple", "title": "MySQL CLI"}, {"location": "develop/#documentation", "text": "Our documentation is built using MkDocs Material . The easiest way to improve the documentation is by using the docs/docker-compose.yaml environment. The source can be modified in docs/src using markdown. The docs environment can be run using 3 modes: LIVE : ( recommended ) This serves the docs locally. It supports live reloading on saves to docs/src files but does not support the docs version dropdown. Useful to see changes live. MODE=\"LIVE\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build QA : This serves the docs locally. It supports the docs version dropdown but does not support live reloading. Useful as a final check. MODE=\"QA\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build BUILD : This compiles the docs. Most useful for the docs deployment automation. Other modes are more useful to new contributors. MODE=\"BUILD\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build When the docs are served locally, use the VSCode PORTS tab (next to TERMINAL ) to manage access to the forwarded ports. Docs are served on port 8080 .", "title": "Documentation"}, {"location": "about/changelog/", "text": "Release notes \u00b6 0.14.0 -- Feb 13, 2023 \u00b6 Added - json data type ( #245 ) PR #1051 Fixed - Activating a schema requires all tables to exist even if create_tables=False PR #1058 Changed - Populate call with reserve_jobs=True to exclude error and ignore keys - PR #1062 Added - Support for inserting data with CSV files - PR #1067 Changed - Switch testing image from pydev to djtest PR #1012 Added - DevContainer development environment compatible with GH Codespaces PR 1071 Fixed - Convert lingering prints by replacing with logs PR #1073 Changed - table.progress() defaults to no stdout PR #1073 Changed - table.describe() defaults to no stdout PR #1073 Deprecated - table._update() PR #1073 Deprecated - old-style foreign key syntax PR #1073 Deprecated - dj.migrate_dj011_external_blob_storage_to_dj012() PR #1073 Added - Method to set job keys to \"ignore\" status - PR #1068 0.13.8 -- Sep 21, 2022 \u00b6 Added - New documentation structure based on markdown PR #1052 Fixed - Fix queries with backslashes ( #999 ) PR #1052 0.13.7 -- Jul 13, 2022 \u00b6 Fixed - Fix networkx incompatable change by version pinning to 2.6.3 (#1035) PR #1036 Added - Support for serializing numpy datetime64 types (#1022) PR #1036 Changed - Add traceback to default logging PR #1036 0.13.6 -- Jun 13, 2022 \u00b6 Added - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025 Added - Unified package level logger for package (#667) PR #1031 Changed - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031 Fixed - Fix query caching deleting non-datajoint files PR #1027 Changed - Minimum Python version for Datajoint-Python is now 3.7 PR #1027 0.13.5 -- May 19, 2022 \u00b6 Changed - Import ABC from collections.abc for Python 3.10 compatibility Fixed - Fix multiprocessing value error (#1013) PR #1026 0.13.4 -- Mar, 28 2022 \u00b6 Added - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995 Fixed - Add missing jobs argument for multiprocessing PR #997 Added - Test for multiprocessing PR #1008 Fixed - Fix external store key name doesn't allow '-' (#1005) PR #1006 Added - Adopted black formatting into code base PR #998 0.13.3 -- Feb 9, 2022 \u00b6 Fixed - Fix error in listing ancestors, descendants with part tables. Fixed - Fix Python 3.10 compatibility (#983) PR #972 Fixed - Allow renaming non-conforming attributes in proj (#982) PR #972 Added - Expose proxy feature for S3 external stores (#961) PR #962 Added - implement multiprocessing in populate (#695) PR #704, #969 Fixed - Dependencies not properly loaded on populate. (#902) PR #919 Fixed - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939 Fixed - Deletes and drops must include the master of each part. (#151, #374) PR #957 Fixed - ExternalTable.delete should not remove row on error (#953) PR #956 Fixed - Fix error handling of remove_object function in s3.py (#952) PR #955 Fixed - Fix regression issue with DISTINCT clause and GROUP_BY (#914) PR #963 Fixed - Fix sql code generation to comply with sql mode ONLY_FULL_GROUP_BY (#916) PR #965 Fixed - Fix count for left-joined QueryExpressions (#951) PR #966 Fixed - Fix assertion error when performing a union into a join (#930) PR #967 Changed ~jobs.error_stack from blob to mediumblob to allow error stacks >64kB in jobs (#984) PR #986 Fixed - Fix error when performing a union on multiple tables (#926) PR #964 Added - Allow optional keyword arguments for make() in populate() PR #971 0.13.2 -- May 7, 2021 \u00b6 Changed setuptools_certificate dependency to new name otumat Fixed - Explicit calls to dj.Connection throw error due to missing host_input (#895) PR #907 Fixed - Correct count of deleted items. (#897) PR #912 0.13.1 -- Apr 16, 2021 \u00b6 Added None as an alias for IS NULL comparison in dict restrictions (#824) PR #893 Changed - Drop support for MySQL 5.6 since it has reached EOL PR #893 Fixed - schema.list_tables() is not topologically sorted (#838) PR #893 Fixed - Diagram part tables do not show proper class name (#882) PR #893 Fixed - Error in complex restrictions (#892) PR #893 Fixed - WHERE and GROUP BY clases are dropped on joins with aggregation (#898, #899) PR #893 0.13.0 -- Mar 24, 2021 \u00b6 Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754 Re-implement cascading deletes for better performance. PR #839 Add support for deferred schema activation to allow for greater modularity. (#834) PR #839 Add query caching mechanism for offline development (#550) PR #839 Add table method .update1 to update a row in the table with new values (#867) PR #763, #889 Python datatypes are now enabled by default in blobs (#761). PR #859 Added permissive join and restriction operators @ and ^ (#785) PR #754 Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735 Add dj.key_hash alias to dj.hash.key_hash (#804) PR #862 Default enable_python_native_blobs to True Bugfix - Regression error on joins with same attribute name (#857) PR #878 Bugfix - Error when fetch1('KEY') when dj.config['fetch_format']='frame' set (#876) PR #880, #878 Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839 Add deprecation warning for _update . PR #889 Add purge_query_cache utility. PR #889 Add tests for query caching and permissive join and restriction. PR #889 Drop support for Python 3.5 (#829) PR #861 0.12.9 -- Mar 12, 2021 \u00b6 Fix bug with fetch1 with dj.config['fetch_format']=\"frame\" . (#876) PR #880 0.12.8 -- Jan 12, 2021 \u00b6 table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833 Load dependencies before querying dependencies. (#179) PR #833 Fix display of part tables in schema.save . (#821) PR #833 Add schema.list_tables . (#838) PR #844 Fix minio new version regression. PR #847 Add more S3 logging for debugging. (#831) PR #832 Convert testing framework from TravisCI to GitHub Actions (#841) PR #840 0.12.7 -- Oct 27, 2020 \u00b6 Fix case sensitivity issues to adapt to MySQL 8+. PR #819 Fix pymysql regression bug (#814) PR #816 Adapted attribute types now have dtype=object in all recarray results. PR #811 0.12.6 -- May 15, 2020 \u00b6 Add order_by to dj.kill (#668, #779) PR #775, #783 Add explicit S3 bucket and file storage location existence checks (#748) PR #781 Modify _update to allow nullable updates for strings/date (#664) PR #760 Avoid logging events on auxiliary tables (#737) PR #753 Add kill_quick and expand display to include host (#740) PR #741 Bugfix - pandas insert fails due to additional index field (#666) PR #776 Bugfix - delete_external_files=True does not remove from S3 (#686) PR #781 Bugfix - pandas fetch throws error when fetch_format='frame' PR #774 0.12.5 -- Feb 24, 2020 \u00b6 Rename module dj.schema into dj.schemas . dj.schema remains an alias for class dj.Schema . (#731) PR #732 dj.create_virtual_module is now called dj.VirtualModule (#731) PR #732 Bugfix - SSL KeyError on failed connection (#716) PR #725 Bugfix - Unable to run unit tests using nosetests (#723) PR #724 Bugfix - suppress_errors does not suppress loss of connection error (#720) PR #721 0.12.4 -- Jan 14, 2020 \u00b6 Support for simple scalar datatypes in blobs (#690) PR #709 Add support for the serial data type in declarations: alias for bigint unsigned auto_increment PR #713 Improve the log table to avoid primary key collisions PR #713 Improve documentation in README PR #713 0.12.3 -- Nov 22, 2019 \u00b6 Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705 Bugfix - include table definition in doc string and help (#698, #699) PR #706 Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702 0.12.2 -- Nov 11, 2019 \u00b6 Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696 Bugfix - Insert into external does not trim leading slash if defined in dj.config['stores']['<store>']['location'] (#692) PR #693 0.12.1 -- Nov 2, 2019 \u00b6 Bugfix - AttributeAdapter converts into a string (#684) PR #688 0.12.0 -- Oct 31, 2019 \u00b6 Dropped support for Python 3.4 Support secure connections with TLS (aka SSL) PR #620 Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608 Remove expression requirement to have additional attributes (#604) PR #604 Support for filepath datatype (#481) PR #603, #659 Support file attachment datatype (#480, #592, #637) PR #659 Fetch return a dict array when specifying as_dict=True for specified attributes. (#595) PR #593 Support of ellipsis in proj : query_expression.proj(.., '-movie') (#499) PR #578 Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577 Support for alter (#110) PR #573 Support for conda install datajoint via conda-forge channel (#293) dj.conn() accepts a port keyword argument (#563) PR #571 Support for UUID datatype (#562) PR #567 query_expr.fetch(\"KEY\", as_dict=False) returns results as np.recarray (#414) PR #574 dj.ERD is now called dj.Diagram (#255, #546) PR #565 dj.Diagram underlines \"distinguished\" classes (#378) PR #557 Accept alias for supported MySQL datatypes (#544) PR #545 Support for pandas in fetch (#459, #537) PR #534 Support for ordering by \"KEY\" in fetch (#541) PR #534 Add config to enable python native blobs PR #672, #676 Add secure option for external storage (#663) PR #674, #676 Add blob migration utility from DJ011 to DJ012 PR #673 Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532 Increase default display rows (#523) PR #526 Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656) Minor improvements (#538) 0.11.3 -- Jul 26, 2019 \u00b6 Fix incompatibility with pyparsing 2.4.1 (#629) PR #631 0.11.2 -- Jul 25, 2019 \u00b6 Fix #628 - incompatibility with pyparsing 2.4.1 0.11.1 -- Nov 15, 2018 \u00b6 Fix ordering of attributes in proj (#483, #516) Prohibit direct insert into auto-populated tables (#511) 0.11.0 -- Oct 25, 2018 \u00b6 Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507) Rename internal class and module names to comply with terminology in documentation (#494, #500) Full support of secondary indexes (#498, 500) ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500) Full support of unique and nullable dependencies (#254, #301, #493, #495, #500) Improve memory management in populate (#461, #486) Fix query errors and redundancies (#456, #463, #482) 0.10.1 -- Aug 28, 2018 \u00b6 Fix ERD Tooltip message (#431) Networkx 2.0 support (#443) Fix insert from query with skip_duplicates=True (#451) Sped up queries (#458) Bugfix in restriction of the form (A & B) * B (#463) Improved error messages (#466) 0.10.0 -- Jan 10, 2018 \u00b6 Deletes are more efficient (#424) ERD shows table definition on tooltip hover in Jupyter (#422) S3 external storage Garbage collection for external sorage Most operators and methods of tables can be invoked as class methods rather than instance methods (#407) The schema decorator object no longer requires locals() to specify the context Compatibility with pymysql 0.8.0+ More efficient loading of dependencies (#403) 0.9.0 -- Nov 17, 2017 \u00b6 Made graphviz installation optional Implement file-based external storage Implement union operator + Implement file-based external storage 0.8.0 -- Jul 26, 2017 \u00b6 Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io improved the ERD graphics and features using the graphviz libraries (#207, #333) improved password handling logic (#322, #321) the use of the contents property to populate tables now only works in dj.Lookup classes (#310). allow suppressing the display of size of query results through the show_tuple_count configuration option (#309) implemented renamed foreign keys to spec (#333) added the limit keyword argument to populate (#329) reduced the number of displayed messages (#308) added size_on_disk property for dj.Schema() objects (#323) job keys are entered in the jobs table (#316, #243) simplified the fetch and fetch1 syntax, deprecating the fetch[...] syntax (#319) the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317) 0.5.0 (#298) -- Mar 8, 2017 \u00b6 All fetched integers are now 64-bit long and all fetched floats are double precision. Added dj.create_virtual_module 0.4.10 (#286) -- Feb 6, 2017 \u00b6 Removed Vagrant and Readthedocs support Explicit saving of configuration (issue #284) 0.4.9 (#285) -- Feb 2, 2017 \u00b6 Fixed setup.py for pip install 0.4.7 (#281) -- Jan 24, 2017 \u00b6 Fixed issues related to order of attributes in projection. 0.4.6 (#277) -- Dec 22, 2016 \u00b6 Proper handling of interruptions during populate 0.4.5 (#274) -- Dec 20, 2016 \u00b6 Populate reports how many keys remain to be populated at the start. 0.4.3 (#271) -- Dec 6, 2016 \u00b6 Fixed aggregation issues (#270) datajoint no longer attempts to connect to server at import time dropped support of view (reversed #257) more elegant handling of insufficient privileges (#268) 0.4.2 (#267) -- Dec 6, 2016 \u00b6 improved table appearance in Jupyter 0.4.1 (#266) -- Oct 28, 2016 \u00b6 bugfix for very long error messages 0.3.9 -- Sep 27, 2016 \u00b6 Added support for datatype YEAR Fixed issues with dj.U and the aggr operator (#246, #247) 0.3.8 -- Aug 2, 2016 \u00b6 added the _update method in base_relation . It allows updating values in existing tuples. bugfix in reading values of type double. Previously it was cast as float32. 0.3.7 -- Jul 31, 2016 \u00b6 added parameter ignore_extra_fields in insert insert(..., skip_duplicates=True) now relies on SELECT IGNORE . Previously it explicitly checked if tuple already exists. table previews now include blob attributes displaying the string 0.3.6 -- Jul 30, 2016 \u00b6 bugfix in schema.spawn_missing_classes . Previously, spawned part classes would not show in ERDs. dj.key now causes fetch to return as a list of dicts. Previously it was a recarray. 0.3.5 \u00b6 dj.set_password() now asks for user confirmation before changing the password. fixed issue #228 0.3.4 \u00b6 Added method the ERD.add_parts method, which adds the part tables of all tables currently in the ERD. ERD() + arg and ERD() - arg can now accept table classes as arg. 0.3.3 \u00b6 Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example. Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources ERD() no longer text the context argument. ERD.draw() now takes an optional context argument. By default uses the caller's locals. 0.3.2 \u00b6 Fixed issue #223: insert can insert relations without fetching. ERD() now takes the context argument, which specifies in which context to look for classes. The default is taken from the argument (schema or table). ERD.draw() no longer has the prefix argument: class names are shown as found in the context.", "title": "Changelog"}, {"location": "about/changelog/#release-notes", "text": "", "title": "Release notes"}, {"location": "about/changelog/#0140-feb-13-2023", "text": "Added - json data type ( #245 ) PR #1051 Fixed - Activating a schema requires all tables to exist even if create_tables=False PR #1058 Changed - Populate call with reserve_jobs=True to exclude error and ignore keys - PR #1062 Added - Support for inserting data with CSV files - PR #1067 Changed - Switch testing image from pydev to djtest PR #1012 Added - DevContainer development environment compatible with GH Codespaces PR 1071 Fixed - Convert lingering prints by replacing with logs PR #1073 Changed - table.progress() defaults to no stdout PR #1073 Changed - table.describe() defaults to no stdout PR #1073 Deprecated - table._update() PR #1073 Deprecated - old-style foreign key syntax PR #1073 Deprecated - dj.migrate_dj011_external_blob_storage_to_dj012() PR #1073 Added - Method to set job keys to \"ignore\" status - PR #1068", "title": "0.14.0 -- Feb 13, 2023"}, {"location": "about/changelog/#0138-sep-21-2022", "text": "Added - New documentation structure based on markdown PR #1052 Fixed - Fix queries with backslashes ( #999 ) PR #1052", "title": "0.13.8 -- Sep 21, 2022"}, {"location": "about/changelog/#0137-jul-13-2022", "text": "Fixed - Fix networkx incompatable change by version pinning to 2.6.3 (#1035) PR #1036 Added - Support for serializing numpy datetime64 types (#1022) PR #1036 Changed - Add traceback to default logging PR #1036", "title": "0.13.7 -- Jul 13, 2022"}, {"location": "about/changelog/#0136-jun-13-2022", "text": "Added - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025 Added - Unified package level logger for package (#667) PR #1031 Changed - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031 Fixed - Fix query caching deleting non-datajoint files PR #1027 Changed - Minimum Python version for Datajoint-Python is now 3.7 PR #1027", "title": "0.13.6 -- Jun 13, 2022"}, {"location": "about/changelog/#0135-may-19-2022", "text": "Changed - Import ABC from collections.abc for Python 3.10 compatibility Fixed - Fix multiprocessing value error (#1013) PR #1026", "title": "0.13.5 -- May 19, 2022"}, {"location": "about/changelog/#0134-mar-28-2022", "text": "Added - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995 Fixed - Add missing jobs argument for multiprocessing PR #997 Added - Test for multiprocessing PR #1008 Fixed - Fix external store key name doesn't allow '-' (#1005) PR #1006 Added - Adopted black formatting into code base PR #998", "title": "0.13.4 -- Mar, 28 2022"}, {"location": "about/changelog/#0133-feb-9-2022", "text": "Fixed - Fix error in listing ancestors, descendants with part tables. Fixed - Fix Python 3.10 compatibility (#983) PR #972 Fixed - Allow renaming non-conforming attributes in proj (#982) PR #972 Added - Expose proxy feature for S3 external stores (#961) PR #962 Added - implement multiprocessing in populate (#695) PR #704, #969 Fixed - Dependencies not properly loaded on populate. (#902) PR #919 Fixed - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939 Fixed - Deletes and drops must include the master of each part. (#151, #374) PR #957 Fixed - ExternalTable.delete should not remove row on error (#953) PR #956 Fixed - Fix error handling of remove_object function in s3.py (#952) PR #955 Fixed - Fix regression issue with DISTINCT clause and GROUP_BY (#914) PR #963 Fixed - Fix sql code generation to comply with sql mode ONLY_FULL_GROUP_BY (#916) PR #965 Fixed - Fix count for left-joined QueryExpressions (#951) PR #966 Fixed - Fix assertion error when performing a union into a join (#930) PR #967 Changed ~jobs.error_stack from blob to mediumblob to allow error stacks >64kB in jobs (#984) PR #986 Fixed - Fix error when performing a union on multiple tables (#926) PR #964 Added - Allow optional keyword arguments for make() in populate() PR #971", "title": "0.13.3 -- Feb 9, 2022"}, {"location": "about/changelog/#0132-may-7-2021", "text": "Changed setuptools_certificate dependency to new name otumat Fixed - Explicit calls to dj.Connection throw error due to missing host_input (#895) PR #907 Fixed - Correct count of deleted items. (#897) PR #912", "title": "0.13.2 -- May 7, 2021"}, {"location": "about/changelog/#0131-apr-16-2021", "text": "Added None as an alias for IS NULL comparison in dict restrictions (#824) PR #893 Changed - Drop support for MySQL 5.6 since it has reached EOL PR #893 Fixed - schema.list_tables() is not topologically sorted (#838) PR #893 Fixed - Diagram part tables do not show proper class name (#882) PR #893 Fixed - Error in complex restrictions (#892) PR #893 Fixed - WHERE and GROUP BY clases are dropped on joins with aggregation (#898, #899) PR #893", "title": "0.13.1 -- Apr 16, 2021"}, {"location": "about/changelog/#0130-mar-24-2021", "text": "Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754 Re-implement cascading deletes for better performance. PR #839 Add support for deferred schema activation to allow for greater modularity. (#834) PR #839 Add query caching mechanism for offline development (#550) PR #839 Add table method .update1 to update a row in the table with new values (#867) PR #763, #889 Python datatypes are now enabled by default in blobs (#761). PR #859 Added permissive join and restriction operators @ and ^ (#785) PR #754 Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735 Add dj.key_hash alias to dj.hash.key_hash (#804) PR #862 Default enable_python_native_blobs to True Bugfix - Regression error on joins with same attribute name (#857) PR #878 Bugfix - Error when fetch1('KEY') when dj.config['fetch_format']='frame' set (#876) PR #880, #878 Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839 Add deprecation warning for _update . PR #889 Add purge_query_cache utility. PR #889 Add tests for query caching and permissive join and restriction. PR #889 Drop support for Python 3.5 (#829) PR #861", "title": "0.13.0 -- Mar 24, 2021"}, {"location": "about/changelog/#0129-mar-12-2021", "text": "Fix bug with fetch1 with dj.config['fetch_format']=\"frame\" . (#876) PR #880", "title": "0.12.9 -- Mar 12, 2021"}, {"location": "about/changelog/#0128-jan-12-2021", "text": "table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833 Load dependencies before querying dependencies. (#179) PR #833 Fix display of part tables in schema.save . (#821) PR #833 Add schema.list_tables . (#838) PR #844 Fix minio new version regression. PR #847 Add more S3 logging for debugging. (#831) PR #832 Convert testing framework from TravisCI to GitHub Actions (#841) PR #840", "title": "0.12.8 -- Jan 12, 2021"}, {"location": "about/changelog/#0127-oct-27-2020", "text": "Fix case sensitivity issues to adapt to MySQL 8+. PR #819 Fix pymysql regression bug (#814) PR #816 Adapted attribute types now have dtype=object in all recarray results. PR #811", "title": "0.12.7 -- Oct 27, 2020"}, {"location": "about/changelog/#0126-may-15-2020", "text": "Add order_by to dj.kill (#668, #779) PR #775, #783 Add explicit S3 bucket and file storage location existence checks (#748) PR #781 Modify _update to allow nullable updates for strings/date (#664) PR #760 Avoid logging events on auxiliary tables (#737) PR #753 Add kill_quick and expand display to include host (#740) PR #741 Bugfix - pandas insert fails due to additional index field (#666) PR #776 Bugfix - delete_external_files=True does not remove from S3 (#686) PR #781 Bugfix - pandas fetch throws error when fetch_format='frame' PR #774", "title": "0.12.6 -- May 15, 2020"}, {"location": "about/changelog/#0125-feb-24-2020", "text": "Rename module dj.schema into dj.schemas . dj.schema remains an alias for class dj.Schema . (#731) PR #732 dj.create_virtual_module is now called dj.VirtualModule (#731) PR #732 Bugfix - SSL KeyError on failed connection (#716) PR #725 Bugfix - Unable to run unit tests using nosetests (#723) PR #724 Bugfix - suppress_errors does not suppress loss of connection error (#720) PR #721", "title": "0.12.5 -- Feb 24, 2020"}, {"location": "about/changelog/#0124-jan-14-2020", "text": "Support for simple scalar datatypes in blobs (#690) PR #709 Add support for the serial data type in declarations: alias for bigint unsigned auto_increment PR #713 Improve the log table to avoid primary key collisions PR #713 Improve documentation in README PR #713", "title": "0.12.4 -- Jan 14, 2020"}, {"location": "about/changelog/#0123-nov-22-2019", "text": "Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705 Bugfix - include table definition in doc string and help (#698, #699) PR #706 Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702", "title": "0.12.3 -- Nov 22, 2019"}, {"location": "about/changelog/#0122-nov-11-2019", "text": "Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696 Bugfix - Insert into external does not trim leading slash if defined in dj.config['stores']['<store>']['location'] (#692) PR #693", "title": "0.12.2 -- Nov 11, 2019"}, {"location": "about/changelog/#0121-nov-2-2019", "text": "Bugfix - AttributeAdapter converts into a string (#684) PR #688", "title": "0.12.1 -- Nov 2, 2019"}, {"location": "about/changelog/#0120-oct-31-2019", "text": "Dropped support for Python 3.4 Support secure connections with TLS (aka SSL) PR #620 Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608 Remove expression requirement to have additional attributes (#604) PR #604 Support for filepath datatype (#481) PR #603, #659 Support file attachment datatype (#480, #592, #637) PR #659 Fetch return a dict array when specifying as_dict=True for specified attributes. (#595) PR #593 Support of ellipsis in proj : query_expression.proj(.., '-movie') (#499) PR #578 Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577 Support for alter (#110) PR #573 Support for conda install datajoint via conda-forge channel (#293) dj.conn() accepts a port keyword argument (#563) PR #571 Support for UUID datatype (#562) PR #567 query_expr.fetch(\"KEY\", as_dict=False) returns results as np.recarray (#414) PR #574 dj.ERD is now called dj.Diagram (#255, #546) PR #565 dj.Diagram underlines \"distinguished\" classes (#378) PR #557 Accept alias for supported MySQL datatypes (#544) PR #545 Support for pandas in fetch (#459, #537) PR #534 Support for ordering by \"KEY\" in fetch (#541) PR #534 Add config to enable python native blobs PR #672, #676 Add secure option for external storage (#663) PR #674, #676 Add blob migration utility from DJ011 to DJ012 PR #673 Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532 Increase default display rows (#523) PR #526 Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656) Minor improvements (#538)", "title": "0.12.0 -- Oct 31, 2019"}, {"location": "about/changelog/#0113-jul-26-2019", "text": "Fix incompatibility with pyparsing 2.4.1 (#629) PR #631", "title": "0.11.3 -- Jul 26, 2019"}, {"location": "about/changelog/#0112-jul-25-2019", "text": "Fix #628 - incompatibility with pyparsing 2.4.1", "title": "0.11.2 -- Jul 25, 2019"}, {"location": "about/changelog/#0111-nov-15-2018", "text": "Fix ordering of attributes in proj (#483, #516) Prohibit direct insert into auto-populated tables (#511)", "title": "0.11.1 -- Nov 15, 2018"}, {"location": "about/changelog/#0110-oct-25-2018", "text": "Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507) Rename internal class and module names to comply with terminology in documentation (#494, #500) Full support of secondary indexes (#498, 500) ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500) Full support of unique and nullable dependencies (#254, #301, #493, #495, #500) Improve memory management in populate (#461, #486) Fix query errors and redundancies (#456, #463, #482)", "title": "0.11.0 -- Oct 25, 2018"}, {"location": "about/changelog/#0101-aug-28-2018", "text": "Fix ERD Tooltip message (#431) Networkx 2.0 support (#443) Fix insert from query with skip_duplicates=True (#451) Sped up queries (#458) Bugfix in restriction of the form (A & B) * B (#463) Improved error messages (#466)", "title": "0.10.1 -- Aug 28, 2018"}, {"location": "about/changelog/#0100-jan-10-2018", "text": "Deletes are more efficient (#424) ERD shows table definition on tooltip hover in Jupyter (#422) S3 external storage Garbage collection for external sorage Most operators and methods of tables can be invoked as class methods rather than instance methods (#407) The schema decorator object no longer requires locals() to specify the context Compatibility with pymysql 0.8.0+ More efficient loading of dependencies (#403)", "title": "0.10.0 -- Jan 10, 2018"}, {"location": "about/changelog/#090-nov-17-2017", "text": "Made graphviz installation optional Implement file-based external storage Implement union operator + Implement file-based external storage", "title": "0.9.0 -- Nov 17, 2017"}, {"location": "about/changelog/#080-jul-26-2017", "text": "Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io improved the ERD graphics and features using the graphviz libraries (#207, #333) improved password handling logic (#322, #321) the use of the contents property to populate tables now only works in dj.Lookup classes (#310). allow suppressing the display of size of query results through the show_tuple_count configuration option (#309) implemented renamed foreign keys to spec (#333) added the limit keyword argument to populate (#329) reduced the number of displayed messages (#308) added size_on_disk property for dj.Schema() objects (#323) job keys are entered in the jobs table (#316, #243) simplified the fetch and fetch1 syntax, deprecating the fetch[...] syntax (#319) the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317)", "title": "0.8.0 -- Jul 26, 2017"}, {"location": "about/changelog/#050-298-mar-8-2017", "text": "All fetched integers are now 64-bit long and all fetched floats are double precision. Added dj.create_virtual_module", "title": "0.5.0 (#298) -- Mar 8, 2017"}, {"location": "about/changelog/#0410-286-feb-6-2017", "text": "Removed Vagrant and Readthedocs support Explicit saving of configuration (issue #284)", "title": "0.4.10 (#286) -- Feb 6, 2017"}, {"location": "about/changelog/#049-285-feb-2-2017", "text": "Fixed setup.py for pip install", "title": "0.4.9 (#285) -- Feb 2, 2017"}, {"location": "about/changelog/#047-281-jan-24-2017", "text": "Fixed issues related to order of attributes in projection.", "title": "0.4.7 (#281) -- Jan 24, 2017"}, {"location": "about/changelog/#046-277-dec-22-2016", "text": "Proper handling of interruptions during populate", "title": "0.4.6 (#277) -- Dec 22, 2016"}, {"location": "about/changelog/#045-274-dec-20-2016", "text": "Populate reports how many keys remain to be populated at the start.", "title": "0.4.5 (#274) -- Dec 20, 2016"}, {"location": "about/changelog/#043-271-dec-6-2016", "text": "Fixed aggregation issues (#270) datajoint no longer attempts to connect to server at import time dropped support of view (reversed #257) more elegant handling of insufficient privileges (#268)", "title": "0.4.3 (#271) -- Dec 6, 2016"}, {"location": "about/changelog/#042-267-dec-6-2016", "text": "improved table appearance in Jupyter", "title": "0.4.2 (#267) -- Dec 6, 2016"}, {"location": "about/changelog/#041-266-oct-28-2016", "text": "bugfix for very long error messages", "title": "0.4.1 (#266) -- Oct 28, 2016"}, {"location": "about/changelog/#039-sep-27-2016", "text": "Added support for datatype YEAR Fixed issues with dj.U and the aggr operator (#246, #247)", "title": "0.3.9 -- Sep 27, 2016"}, {"location": "about/changelog/#038-aug-2-2016", "text": "added the _update method in base_relation . It allows updating values in existing tuples. bugfix in reading values of type double. Previously it was cast as float32.", "title": "0.3.8 -- Aug 2, 2016"}, {"location": "about/changelog/#037-jul-31-2016", "text": "added parameter ignore_extra_fields in insert insert(..., skip_duplicates=True) now relies on SELECT IGNORE . Previously it explicitly checked if tuple already exists. table previews now include blob attributes displaying the string", "title": "0.3.7 -- Jul 31, 2016"}, {"location": "about/changelog/#036-jul-30-2016", "text": "bugfix in schema.spawn_missing_classes . Previously, spawned part classes would not show in ERDs. dj.key now causes fetch to return as a list of dicts. Previously it was a recarray.", "title": "0.3.6 -- Jul 30, 2016"}, {"location": "about/changelog/#035", "text": "dj.set_password() now asks for user confirmation before changing the password. fixed issue #228", "title": "0.3.5"}, {"location": "about/changelog/#034", "text": "Added method the ERD.add_parts method, which adds the part tables of all tables currently in the ERD. ERD() + arg and ERD() - arg can now accept table classes as arg.", "title": "0.3.4"}, {"location": "about/changelog/#033", "text": "Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example. Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources ERD() no longer text the context argument. ERD.draw() now takes an optional context argument. By default uses the caller's locals.", "title": "0.3.3"}, {"location": "about/changelog/#032", "text": "Fixed issue #223: insert can insert relations without fetching. ERD() now takes the context argument, which specifies in which context to look for classes. The default is taken from the argument (schema or table). ERD.draw() no longer has the prefix argument: class names are shown as found in the context.", "title": "0.3.2"}, {"location": "api/datajoint/__init__/", "text": "DataJoint for Python is a framework for building data piplines using MySQL databases to represent pipeline structure and bulk storage systems for large objects. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, and querying data. The DataJoint data model is described in https://arxiv.org/abs/1807.11104 DataJoint is free software under the LGPL License. In addition, we request that any use of DataJoint leading to a publication be acknowledged in the publication. Please cite: - http://biorxiv.org/content/early/2015/11/14/031658 - http://dx.doi.org/10.1101/031658 AttributeAdapter \u00b6 Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) attribute_type property \u00b6 Returns: Type Description a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" get ( value ) \u00b6 convert value retrieved from the the attribute in a table into the adapted type Parameters: Name Type Description Default value value from the database required Returns: Type Description object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) put ( obj ) \u00b6 convert an object of the adapted type into a value that DataJoint can store in a table attribute Parameters: Name Type Description Default obj an object of the adapted type required Returns: Type Description value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) key_hash ( mapping ) \u00b6 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest () DataJointError \u00b6 Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) suggest ( * args ) \u00b6 regenerate the exception with additional arguments Parameters: Name Type Description Default args addition arguments () Returns: Type Description a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) key \u00b6 object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 19 20 21 22 23 24 25 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass kill ( restriction = None , connection = None , order_by = None ) \u00b6 view and kill database connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() None order_by order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes None Source code in datajoint/admin.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : logger . warn ( \"Process not found\" ) Schema \u00b6 A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part tables.\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared and not assert_declared and self . create_tables : instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] activate ( schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. Parameters: Name Type Description Default schema_name the database schema to associate. schema_name=None is used to assert that the schema has already been activated. None connection Connection object. Defaults to datajoint.conn(). None create_schema If False, do not create the schema and raise an error if missing. None create_tables If False, do not create tables and raise errors when attempting to access missing tables. None add_objects a mapping with additional objects to make available to the context in which table classes are declared. None Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) size_on_disk property \u00b6 Returns: Type Description size of the entire schema in bytes spawn_missing_classes ( context = None ) \u00b6 Creates the appropriate python user table classes from tables in the schema and places them in the context. Parameters: Name Type Description Default context alternative context to place the missing classes into, e.g. locals() None Source code in datajoint/schemas.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) drop ( force = False ) \u00b6 Drop the associated schema if it exists Source code in datajoint/schemas.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) exists property \u00b6 Returns: Type Description true if the associated schema exists on the server jobs property \u00b6 schema.jobs provides a view of the job reservation table for the schema Returns: Type Description jobs table save ( python_filename = None ) \u00b6 Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. Returns: Type Description a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) list_tables () \u00b6 Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job Returns: Type Description A list of table names from the database schema. Source code in datajoint/schemas.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] AndList \u00b6 Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction ) Table \u00b6 Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) elif attr . json : value = json . dumps ( value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert declare ( context = None ) \u00b6 Declare the table in the schema based on self.definition. Parameters: Name Type Description Default context the context for foreign key resolution. If None, foreign keys are not allowed. None Source code in datajoint/table.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) alter ( prompt = True , context = None ) \u00b6 Alter the table definition from self.definition Source code in datajoint/table.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) from_clause () \u00b6 Returns: Type Description the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 149 150 151 152 153 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name get_select_fields ( select_fields = None ) \u00b6 Returns: Type Description the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 155 156 157 158 159 160 161 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) parents ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 Parameters: Name Type Description Default primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes children ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 Parameters: Name Type Description Default primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes descendants ( as_objects = False ) \u00b6 Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables descendants in topological order. Source code in datajoint/table.py 207 208 209 210 211 212 213 214 215 216 217 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] ancestors ( as_objects = False ) \u00b6 Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables ancestors in topological order. Source code in datajoint/table.py 219 220 221 222 223 224 225 226 227 228 229 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] parts ( as_objects = False ) \u00b6 return part tables either as entries in a dict with foreign key informaiton or a list of objects Parameters: Name Type Description Default as_objects if False (default), the output is a dict describing the foreign keys. If True, return table objects. False Source code in datajoint/table.py 231 232 233 234 235 236 237 238 239 240 241 242 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes is_declared property \u00b6 Returns: Type Description True is the table is declared in the schema. full_table_name property \u00b6 Returns: Type Description full table name in the schema update1 ( row ) \u00b6 update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. Parameters: Name Type Description Default row a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default required Source code in datajoint/table.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) insert1 ( row , ** kwargs ) \u00b6 Insert one data record into the table. For kwargs , see insert() . Parameters: Name Type Description Default row a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. required Source code in datajoint/table.py 330 331 332 333 334 335 336 337 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) insert ( rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None ) \u00b6 Insert a collection of rows. Parameters: Name Type Description Default rows Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. required replace If True, replaces the existing tuple. False skip_duplicates If True, silently skip duplicate inserts. False ignore_extra_fields If False, fields that are not in the heading raise error. False allow_direct_insert Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) None Source code in datajoint/table.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) delete_quick ( get_count = False ) \u00b6 Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count delete ( transaction = True , safemode = None , force_parts = False ) \u00b6 Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If True , use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. safemode: If True , prohibit nested transactions and prompt to confirm. Default is dj.config['safemode'] . force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. Source code in datajoint/table.py 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count drop_quick () \u00b6 Drops the table without cascading to dependent tables and without user prompt. Source code in datajoint/table.py 623 624 625 626 627 628 629 630 631 632 633 634 635 def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) drop () \u00b6 Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) size_on_disk property \u00b6 Returns: Type Description size of data and indices in bytes on the storage device describe ( context = None , printout = False ) \u00b6 Returns: Type Description the definition string for the query using DataJoint DDL. Source code in datajoint/table.py 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition Not \u00b6 invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction Diagram \u00b6 Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = cls () . describe ( context = self . context ) . split ( \" \\n \" ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs ) from_sequence ( sequence ) classmethod \u00b6 The join Diagram for all objects in sequence Parameters: Name Type Description Default sequence a sequence (e.g. list, tuple) required Returns: Type Description Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 145 146 147 148 149 150 151 152 153 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) add_parts () \u00b6 Adds to the diagram the part tables of tables already included in the diagram Returns: Type Description Source code in datajoint/diagram.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self topological_sort () \u00b6 Returns: Type Description list of nodes in topological order Source code in datajoint/diagram.py 182 183 184 185 186 187 188 189 190 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) MatCell \u00b6 Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass MatStruct \u00b6 Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ) \u00b6 Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. Parameters: Name Type Description Default host hostname None user mysql user None password mysql password None init_fun initialization function None reset whether the connection should be reset or not False use_tls TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). None Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection Manual \u00b6 Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\" Lookup \u00b6 Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" ) Imported \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\" Connection \u00b6 A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. Parameters: Name Type Description Default host host name, may include port number as hostname:port, in which case it overrides the value in port required user user name required password password required port port number None init_fun connection initialization function (SQL) None use_tls TLS encryption option None Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () connect () \u00b6 Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) set_query_cache ( query_cache = None ) \u00b6 When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. Parameters: Name Type Description Default query_cache a string to initialize the hash for query results None Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache purge_query_cache () \u00b6 Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () ping () \u00b6 Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) is_connected property \u00b6 Return true if the object is connected to the database server. query ( query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ) \u00b6 Execute the specified query and return the tuple generator (cursor). Parameters: Name Type Description Default query SQL query required args additional arguments for the client.cursor () as_dict If as_dict is set to True, the returned cursor objects returns query results as dictionary. False suppress_warnings If True, suppress all warnings arising from underlying query library True reconnect when None, get from config, when True, attempt to reconnect if disconnected None Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor get_user () \u00b6 Returns: Type Description the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] in_transaction property \u00b6 Returns: Type Description True if there is an open transaction. start_transaction () \u00b6 Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) cancel_transaction () \u00b6 Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) commit_transaction () \u00b6 Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) transaction property \u00b6 Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here Computed \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\" Part \u00b6 Bases: UserTable Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) delete ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) drop ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) VirtualModule \u00b6 Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None , ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) list_schemas ( connection = None ) \u00b6 Parameters: Name Type Description Default connection a dj.Connection object None Returns: Type Description list of all accessible schemas on the server Source code in datajoint/schemas.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ] U \u00b6 dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr join ( other , left = False ) \u00b6 Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. Parameters: Name Type Description Default other the other query expression to join with. required left ignored. dj.U always acts as if left=False False Returns: Type Description a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result aggr ( group , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) FreeTable \u00b6 Bases: Table A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. Parameters: Name Type Description Default conn a dj.Connection object required full_table_name in format database . table_name required Source code in datajoint/table.py 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 class FreeTable ( Table ): \"\"\" A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () )", "title": "__init__.py"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter", "text": "Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "AttributeAdapter"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "text": "Returns: Type Description a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"", "title": "attribute_type"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.get", "text": "convert value retrieved from the the attribute in a table into the adapted type Parameters: Name Type Description Default value value from the database required Returns: Type Description object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "get()"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.put", "text": "convert an object of the adapted type into a value that DataJoint can store in a table attribute Parameters: Name Type Description Default obj an object of the adapted type required Returns: Type Description value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "put()"}, {"location": "api/datajoint/__init__/#datajoint.key_hash", "text": "32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest ()", "title": "key_hash()"}, {"location": "api/datajoint/__init__/#datajoint.DataJointError", "text": "Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "DataJointError"}, {"location": "api/datajoint/__init__/#datajoint.errors.DataJointError.suggest", "text": "regenerate the exception with additional arguments Parameters: Name Type Description Default args addition arguments () Returns: Type Description a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "suggest()"}, {"location": "api/datajoint/__init__/#datajoint.key", "text": "object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 19 20 21 22 23 24 25 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass", "title": "key"}, {"location": "api/datajoint/__init__/#datajoint.kill", "text": "view and kill database connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() None order_by order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes None Source code in datajoint/admin.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : logger . warn ( \"Process not found\" )", "title": "kill()"}, {"location": "api/datajoint/__init__/#datajoint.Schema", "text": "A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part tables.\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared and not assert_declared and self . create_tables : instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "Schema"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.activate", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. Parameters: Name Type Description Default schema_name the database schema to associate. schema_name=None is used to assert that the schema has already been activated. None connection Connection object. Defaults to datajoint.conn(). None create_schema If False, do not create the schema and raise an error if missing. None create_tables If False, do not create tables and raise errors when attempting to access missing tables. None add_objects a mapping with additional objects to make available to the context in which table classes are declared. None Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context )", "title": "activate()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.size_on_disk", "text": "Returns: Type Description size of the entire schema in bytes", "title": "size_on_disk"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.spawn_missing_classes", "text": "Creates the appropriate python user table classes from tables in the schema and places them in the context. Parameters: Name Type Description Default context alternative context to place the missing classes into, e.g. locals() None Source code in datajoint/schemas.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class )", "title": "spawn_missing_classes()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.drop", "text": "Drop the associated schema if it exists Source code in datajoint/schemas.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.exists", "text": "Returns: Type Description true if the associated schema exists on the server", "title": "exists"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.jobs", "text": "schema.jobs provides a view of the job reservation table for the schema Returns: Type Description jobs table", "title": "jobs"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.save", "text": "Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. Returns: Type Description a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code )", "title": "save()"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.list_tables", "text": "Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job Returns: Type Description A list of table names from the database schema. Source code in datajoint/schemas.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "list_tables()"}, {"location": "api/datajoint/__init__/#datajoint.AndList", "text": "Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction )", "title": "AndList"}, {"location": "api/datajoint/__init__/#datajoint.Table", "text": "Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) elif attr . json : value = json . dumps ( value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "Table"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.declare", "text": "Declare the table in the schema based on self.definition. Parameters: Name Type Description Default context the context for foreign key resolution. If None, foreign keys are not allowed. None Source code in datajoint/table.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name )", "title": "declare()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.alter", "text": "Alter the table definition from self.definition Source code in datajoint/table.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name )", "title": "alter()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.from_clause", "text": "Returns: Type Description the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 149 150 151 152 153 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name", "title": "from_clause()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.get_select_fields", "text": "Returns: Type Description the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 155 156 157 158 159 160 161 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql )", "title": "get_select_fields()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parents", "text": "Parameters: Name Type Description Default primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "parents()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.children", "text": "Parameters: Name Type Description Default primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "children()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.descendants", "text": "Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables descendants in topological order. Source code in datajoint/table.py 207 208 209 210 211 212 213 214 215 216 217 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ]", "title": "descendants()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.ancestors", "text": "Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables ancestors in topological order. Source code in datajoint/table.py 219 220 221 222 223 224 225 226 227 228 229 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ]", "title": "ancestors()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parts", "text": "return part tables either as entries in a dict with foreign key informaiton or a list of objects Parameters: Name Type Description Default as_objects if False (default), the output is a dict describing the foreign keys. If True, return table objects. False Source code in datajoint/table.py 231 232 233 234 235 236 237 238 239 240 241 242 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes", "title": "parts()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.is_declared", "text": "Returns: Type Description True is the table is declared in the schema.", "title": "is_declared"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.full_table_name", "text": "Returns: Type Description full table name in the schema", "title": "full_table_name"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.update1", "text": "update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. Parameters: Name Type Description Default row a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default required Source code in datajoint/table.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None ))", "title": "update1()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert1", "text": "Insert one data record into the table. For kwargs , see insert() . Parameters: Name Type Description Default row a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. required Source code in datajoint/table.py 330 331 332 333 334 335 336 337 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs )", "title": "insert1()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert", "text": "Insert a collection of rows. Parameters: Name Type Description Default rows Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. required replace If True, replaces the existing tuple. False skip_duplicates If True, silently skip duplicate inserts. False ignore_extra_fields If False, fields that are not in the heading raise error. False allow_direct_insert Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) None Source code in datajoint/table.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" )", "title": "insert()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete_quick", "text": "Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count", "title": "delete_quick()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete", "text": "Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If True , use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. safemode: If True , prohibit nested transactions and prompt to confirm. Default is dj.config['safemode'] . force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. Source code in datajoint/table.py 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count", "title": "delete()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop_quick", "text": "Drops the table without cascading to dependent tables and without user prompt. Source code in datajoint/table.py 623 624 625 626 627 628 629 630 631 632 633 634 635 def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name )", "title": "drop_quick()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop", "text": "Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.size_on_disk", "text": "Returns: Type Description size of data and indices in bytes on the storage device", "title": "size_on_disk"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.describe", "text": "Returns: Type Description the definition string for the query using DataJoint DDL. Source code in datajoint/table.py 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition", "title": "describe()"}, {"location": "api/datajoint/__init__/#datajoint.Not", "text": "invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction", "title": "Not"}, {"location": "api/datajoint/__init__/#datajoint.Diagram", "text": "Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = cls () . describe ( context = self . context ) . split ( \" \\n \" ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs )", "title": "Diagram"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.from_sequence", "text": "The join Diagram for all objects in sequence Parameters: Name Type Description Default sequence a sequence (e.g. list, tuple) required Returns: Type Description Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 145 146 147 148 149 150 151 152 153 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence ))", "title": "from_sequence()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.add_parts", "text": "Adds to the diagram the part tables of tables already included in the diagram Returns: Type Description Source code in datajoint/diagram.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self", "title": "add_parts()"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.topological_sort", "text": "Returns: Type Description list of nodes in topological order Source code in datajoint/diagram.py 182 183 184 185 186 187 188 189 190 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "topological_sort()"}, {"location": "api/datajoint/__init__/#datajoint.MatCell", "text": "Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass", "title": "MatCell"}, {"location": "api/datajoint/__init__/#datajoint.MatStruct", "text": "Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass", "title": "MatStruct"}, {"location": "api/datajoint/__init__/#datajoint.conn", "text": "Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. Parameters: Name Type Description Default host hostname None user mysql user None password mysql password None init_fun initialization function None reset whether the connection should be reset or not False use_tls TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). None Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection", "title": "conn()"}, {"location": "api/datajoint/__init__/#datajoint.Manual", "text": "Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\"", "title": "Manual"}, {"location": "api/datajoint/__init__/#datajoint.Lookup", "text": "Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" )", "title": "Lookup"}, {"location": "api/datajoint/__init__/#datajoint.Imported", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\"", "title": "Imported"}, {"location": "api/datajoint/__init__/#datajoint.Connection", "text": "A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. Parameters: Name Type Description Default host host name, may include port number as hostname:port, in which case it overrides the value in port required user user name required password password required port port number None init_fun connection initialization function (SQL) None use_tls TLS encryption option None Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "Connection"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.connect", "text": "Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True )", "title": "connect()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.set_query_cache", "text": "When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. Parameters: Name Type Description Default query_cache a string to initialize the hash for query results None Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache", "title": "set_query_cache()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.purge_query_cache", "text": "Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink ()", "title": "purge_query_cache()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.ping", "text": "Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False )", "title": "ping()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.is_connected", "text": "Return true if the object is connected to the database server.", "title": "is_connected"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.query", "text": "Execute the specified query and return the tuple generator (cursor). Parameters: Name Type Description Default query SQL query required args additional arguments for the client.cursor () as_dict If as_dict is set to True, the returned cursor objects returns query results as dictionary. False suppress_warnings If True, suppress all warnings arising from underlying query library True reconnect when None, get from config, when True, attempt to reconnect if disconnected None Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor", "title": "query()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.get_user", "text": "Returns: Type Description the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ]", "title": "get_user()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.in_transaction", "text": "Returns: Type Description True if there is an open transaction.", "title": "in_transaction"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.start_transaction", "text": "Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" )", "title": "start_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.cancel_transaction", "text": "Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" )", "title": "cancel_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.commit_transaction", "text": "Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" )", "title": "commit_transaction()"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.transaction", "text": "Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here", "title": "transaction"}, {"location": "api/datajoint/__init__/#datajoint.Computed", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\"", "title": "Computed"}, {"location": "api/datajoint/__init__/#datajoint.Part", "text": "Bases: UserTable Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "Part"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.delete", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" )", "title": "delete()"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.drop", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "drop()"}, {"location": "api/datajoint/__init__/#datajoint.VirtualModule", "text": "Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None , ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "VirtualModule"}, {"location": "api/datajoint/__init__/#datajoint.list_schemas", "text": "Parameters: Name Type Description Default connection a dj.Connection object None Returns: Type Description list of all accessible schemas on the server Source code in datajoint/schemas.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ]", "title": "list_schemas()"}, {"location": "api/datajoint/__init__/#datajoint.U", "text": "dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr", "title": "U"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.join", "text": "Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. Parameters: Name Type Description Default other the other query expression to join with. required left ignored. dj.U always acts as if left=False False Returns: Type Description a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result", "title": "join()"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/__init__/#datajoint.FreeTable", "text": "Bases: Table A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. Parameters: Name Type Description Default conn a dj.Connection object required full_table_name in format database . table_name required Source code in datajoint/table.py 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 class FreeTable ( Table ): \"\"\" A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () )", "title": "FreeTable"}, {"location": "api/datajoint/admin/", "text": "kill ( restriction = None , connection = None , order_by = None ) \u00b6 view and kill database connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() None order_by order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes None Source code in datajoint/admin.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : logger . warn ( \"Process not found\" ) kill_quick ( restriction = None , connection = None ) \u00b6 Kill database connections without prompting. Returns number of terminated connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". None Source code in datajoint/admin.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def kill_quick ( restriction = None , connection = None ): \"\"\" Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". \"\"\" if connection is None : connection = conn () query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) nkill = 0 for process in cur : connection . query ( \"kill %d \" % process [ \"id\" ]) nkill += 1 return nkill", "title": "admin.py"}, {"location": "api/datajoint/admin/#datajoint.admin.kill", "text": "view and kill database connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() None order_by order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes None Source code in datajoint/admin.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def kill ( restriction = None , connection = None , order_by = None ): # pragma: no cover \"\"\" view and kill database connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() :param order_by: order by a single attribute or the list of attributes. defaults to 'id'. Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME > 600') lists only connections in their current state for more than 10 minutes \"\"\" if connection is None : connection = conn () if order_by is not None and not isinstance ( order_by , str ): order_by = \",\" . join ( order_by ) query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) + ( \" ORDER BY %s \" % ( order_by or \"id\" )) ) while True : print ( \" ID USER HOST STATE TIME INFO\" ) print ( \"+--+ +----------+ +-----------+ +-----------+ +-----+\" ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) for process in cur : try : print ( \" {id:>4d} {user:<12s} {host:<12s} {state:<12s} {time:>7d} {info} \" . format ( ** process ) ) except TypeError : print ( process ) response = input ( 'process to kill or \"q\" to quit > ' ) if response == \"q\" : break if response : try : pid = int ( response ) except ValueError : pass # ignore non-numeric input else : try : connection . query ( \"kill %d \" % pid ) except pymysql . err . InternalError : logger . warn ( \"Process not found\" )", "title": "kill()"}, {"location": "api/datajoint/admin/#datajoint.admin.kill_quick", "text": "Kill database connections without prompting. Returns number of terminated connections. Parameters: Name Type Description Default restriction restriction to be applied to processlist None connection a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". None Source code in datajoint/admin.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def kill_quick ( restriction = None , connection = None ): \"\"\" Kill database connections without prompting. Returns number of terminated connections. :param restriction: restriction to be applied to processlist :param connection: a datajoint.Connection object. Default calls datajoint.conn() Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO. Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\". \"\"\" if connection is None : connection = conn () query = ( \"SELECT * FROM information_schema.processlist WHERE id <> CONNECTION_ID()\" + ( \"\" if restriction is None else \" AND ( %s )\" % restriction ) ) cur = ( { k . lower (): v for k , v in elem . items ()} for elem in connection . query ( query , as_dict = True ) ) nkill = 0 for process in cur : connection . query ( \"kill %d \" % process [ \"id\" ]) nkill += 1 return nkill", "title": "kill_quick()"}, {"location": "api/datajoint/attribute_adapter/", "text": "AttributeAdapter \u00b6 Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) attribute_type property \u00b6 Returns: Type Description a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" get ( value ) \u00b6 convert value retrieved from the the attribute in a table into the adapted type Parameters: Name Type Description Default value value from the database required Returns: Type Description object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) put ( obj ) \u00b6 convert an object of the adapted type into a value that DataJoint can store in a table attribute Parameters: Name Type Description Default obj an object of the adapted type required Returns: Type Description value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) get_adapter ( context , adapter_name ) \u00b6 Extract the AttributeAdapter object by its name from the context and validate. Source code in datajoint/attribute_adapter.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_adapter ( context , adapter_name ): \"\"\" Extract the AttributeAdapter object by its name from the context and validate. \"\"\" if not _support_adapted_types (): raise DataJointError ( \"Support for Adapted Attribute types is disabled.\" ) adapter_name = adapter_name . lstrip ( \"<\" ) . rstrip ( \">\" ) try : adapter = ( context [ adapter_name ] if adapter_name in context else type_plugins [ adapter_name ][ \"object\" ] . load () ) except KeyError : raise DataJointError ( \"Attribute adapter ' {adapter_name} ' is not defined.\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter , AttributeAdapter ): raise DataJointError ( \"Attribute adapter ' {adapter_name} ' must be an instance of datajoint.AttributeAdapter\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter . attribute_type , str ) or not re . match ( r \"^\\w\" , adapter . attribute_type ): raise DataJointError ( \"Invalid attribute type {type} in attribute adapter ' {adapter_name} '\" . format ( type = adapter . attribute_type , adapter_name = adapter_name ) ) return adapter", "title": "attribute_adapter.py"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter", "text": "Base class for adapter objects for user-defined attribute types. Source code in datajoint/attribute_adapter.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AttributeAdapter : \"\"\" Base class for adapter objects for user-defined attribute types. \"\"\" @property def attribute_type ( self ): \"\"\" :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\" \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" ) def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "AttributeAdapter"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "text": "Returns: Type Description a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"", "title": "attribute_type"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.get", "text": "convert value retrieved from the the attribute in a table into the adapted type Parameters: Name Type Description Default value value from the database required Returns: Type Description object of the adapted type Source code in datajoint/attribute_adapter.py 18 19 20 21 22 23 24 25 26 def get ( self , value ): \"\"\" convert value retrieved from the the attribute in a table into the adapted type :param value: value from the database :return: object of the adapted type \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "get()"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.put", "text": "convert an object of the adapted type into a value that DataJoint can store in a table attribute Parameters: Name Type Description Default obj an object of the adapted type required Returns: Type Description value to store in the database Source code in datajoint/attribute_adapter.py 28 29 30 31 32 33 34 35 def put ( self , obj ): \"\"\" convert an object of the adapted type into a value that DataJoint can store in a table attribute :param obj: an object of the adapted type :return: value to store in the database \"\"\" raise NotImplementedError ( \"Undefined attribute adapter\" )", "title": "put()"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.get_adapter", "text": "Extract the AttributeAdapter object by its name from the context and validate. Source code in datajoint/attribute_adapter.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_adapter ( context , adapter_name ): \"\"\" Extract the AttributeAdapter object by its name from the context and validate. \"\"\" if not _support_adapted_types (): raise DataJointError ( \"Support for Adapted Attribute types is disabled.\" ) adapter_name = adapter_name . lstrip ( \"<\" ) . rstrip ( \">\" ) try : adapter = ( context [ adapter_name ] if adapter_name in context else type_plugins [ adapter_name ][ \"object\" ] . load () ) except KeyError : raise DataJointError ( \"Attribute adapter ' {adapter_name} ' is not defined.\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter , AttributeAdapter ): raise DataJointError ( \"Attribute adapter ' {adapter_name} ' must be an instance of datajoint.AttributeAdapter\" . format ( adapter_name = adapter_name ) ) if not isinstance ( adapter . attribute_type , str ) or not re . match ( r \"^\\w\" , adapter . attribute_type ): raise DataJointError ( \"Invalid attribute type {type} in attribute adapter ' {adapter_name} '\" . format ( type = adapter . attribute_type , adapter_name = adapter_name ) ) return adapter", "title": "get_adapter()"}, {"location": "api/datajoint/autopopulate/", "text": "This module defines class dj.AutoPopulate AutoPopulate \u00b6 AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property key_source , and must define the callback method make . Source code in datajoint/autopopulate.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class AutoPopulate : \"\"\" AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property `key_source`, and must define the callback method `make`. \"\"\" _key_source = None _allow_insert = False @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self def _job_key ( self , key ): \"\"\" :param key: they key returned for the job from the key source :return: the dict to use to generate the job reservation hash This method allows subclasses to control the job reservation granularity. \"\"\" return key def _jobs_to_do ( self , restrictions ): \"\"\" :return: the query yeilding the keys to be computed (derived from self.key_source) \"\"\" if self . restriction : raise DataJointError ( \"Cannot call populate on a restricted table. \" \"Instead, pass conditions to populate() as arguments.\" ) todo = self . key_source # key_source is a QueryExpression subclass -- trigger instantiation if inspect . isclass ( todo ) and issubclass ( todo , QueryExpression ): todo = todo () if not isinstance ( todo , QueryExpression ): raise DataJointError ( \"Invalid key_source value\" ) try : # check if target lacks any attributes from the primary key of key_source raise DataJointError ( \"The populate target lacks attribute %s \" \"from the primary key of key_source\" % next ( name for name in todo . heading . primary_key if name not in self . target . heading ) ) except StopIteration : pass return ( todo & AndList ( restrictions )) . proj () def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) # exclude \"error\" or \"ignore\" jobs if reserve_jobs : exclude_key_hashes = ( jobs & { \"table_name\" : self . target . table_name } & 'status in (\"error\", \"ignore\")' ) . fetch ( \"key_hash\" ) keys = [ key for key in keys if key_hash ( key ) not in exclude_key_hashes ] if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list def _populate1 ( self , key , jobs , suppress_errors , return_exception_objects , make_kwargs = None ): \"\"\" populates table for one source key, calling self.make inside a transaction. :param jobs: the jobs table or None if not reserve_jobs :param key: dict specifying job to populate :param suppress_errors: bool if errors should be suppressed and returned :param return_exception_objects: if True, errors must be returned as objects :return: (key, error) when suppress_errors=True, otherwise None \"\"\" make = self . _make_tuples if hasattr ( self , \"_make_tuples\" ) else self . make if jobs is None or jobs . reserve ( self . target . table_name , self . _job_key ( key )): self . connection . start_transaction () if key in self . target : # already populated self . connection . cancel_transaction () if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) else : logger . debug ( f \"Making { key } -> { self . target . full_table_name } \" ) self . __class__ . _allow_insert = True try : make ( dict ( key ), ** ( make_kwargs or {})) except ( KeyboardInterrupt , SystemExit , Exception ) as error : try : self . connection . cancel_transaction () except LostConnectionError : pass error_message = \" {exception}{msg} \" . format ( exception = error . __class__ . __name__ , msg = \": \" + str ( error ) if str ( error ) else \"\" , ) logger . debug ( f \"Error making { key } -> { self . target . full_table_name } - { error_message } \" ) if jobs is not None : # show error name and error message (if any) jobs . error ( self . target . table_name , self . _job_key ( key ), error_message = error_message , error_stack = traceback . format_exc (), ) if not suppress_errors or isinstance ( error , SystemExit ): raise else : logger . error ( error ) return key , error if return_exception_objects else error_message else : self . connection . commit_transaction () logger . debug ( f \"Success making { key } -> { self . target . full_table_name } \" ) if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) finally : self . __class__ . _allow_insert = False def progress ( self , * restrictions , display = False ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : logger . info ( \" %-20s \" % self . __class__ . __name__ + \" Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), ) return remaining , total key_source property \u00b6 Returns: Type Description the query expression that yields primary key values to be passed, sequentially, to the make method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. make ( key ) \u00b6 Derived classes must implement method make that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. Source code in datajoint/autopopulate.py 92 93 94 95 96 97 98 99 100 def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) target property \u00b6 Returns: Type Description table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. populate ( * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = 'original' , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None ) \u00b6 table.populate() calls table.make(key) for every primary key in self.key_source for which there is not already a tuple in table. Parameters: Name Type Description Default restrictions a list of restrictions each restrict (table.key_source - target.proj()) () suppress_errors if True, do not terminate execution. False return_exception_objects return error objects instead of just error messages False reserve_jobs if True, reserve jobs to populate in asynchronous fashion False order \"original\"|\"reverse\"|\"random\" - the order of execution 'original' limit if not None, check at most this many keys None max_calls if not None, populate at most this many keys None display_progress if True, report progress_bar False processes number of processes to use. Set to None to use all cores 1 make_kwargs dict, optional Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table. None Source code in datajoint/autopopulate.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) # exclude \"error\" or \"ignore\" jobs if reserve_jobs : exclude_key_hashes = ( jobs & { \"table_name\" : self . target . table_name } & 'status in (\"error\", \"ignore\")' ) . fetch ( \"key_hash\" ) keys = [ key for key in keys if key_hash ( key ) not in exclude_key_hashes ] if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list progress ( * restrictions , display = False ) \u00b6 Report the progress of populating the table. Returns: Type Description (remaining, total) -- numbers of tuples to be populated Source code in datajoint/autopopulate.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def progress ( self , * restrictions , display = False ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : logger . info ( \" %-20s \" % self . __class__ . __name__ + \" Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), ) return remaining , total", "title": "autopopulate.py"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate", "text": "AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property key_source , and must define the callback method make . Source code in datajoint/autopopulate.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 class AutoPopulate : \"\"\" AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property `key_source`, and must define the callback method `make`. \"\"\" _key_source = None _allow_insert = False @property def key_source ( self ): \"\"\" :return: the query expression that yields primary key values to be passed, sequentially, to the ``make`` method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls. \"\"\" def _rename_attributes ( table , props ): return ( table . proj ( ** { attr : ref for attr , ref in props [ \"attr_map\" ] . items () if attr != ref } ) if props [ \"aliased\" ] else table . proj () ) if self . _key_source is None : parents = self . target . parents ( primary = True , as_objects = True , foreign_key_info = True ) if not parents : raise DataJointError ( \"A table must have dependencies \" \"from its primary key for auto-populate to work\" ) self . _key_source = _rename_attributes ( * parents [ 0 ]) for q in parents [ 1 :]: self . _key_source *= _rename_attributes ( * q ) return self . _key_source def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" ) @property def target ( self ): \"\"\" :return: table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self. \"\"\" return self def _job_key ( self , key ): \"\"\" :param key: they key returned for the job from the key source :return: the dict to use to generate the job reservation hash This method allows subclasses to control the job reservation granularity. \"\"\" return key def _jobs_to_do ( self , restrictions ): \"\"\" :return: the query yeilding the keys to be computed (derived from self.key_source) \"\"\" if self . restriction : raise DataJointError ( \"Cannot call populate on a restricted table. \" \"Instead, pass conditions to populate() as arguments.\" ) todo = self . key_source # key_source is a QueryExpression subclass -- trigger instantiation if inspect . isclass ( todo ) and issubclass ( todo , QueryExpression ): todo = todo () if not isinstance ( todo , QueryExpression ): raise DataJointError ( \"Invalid key_source value\" ) try : # check if target lacks any attributes from the primary key of key_source raise DataJointError ( \"The populate target lacks attribute %s \" \"from the primary key of key_source\" % next ( name for name in todo . heading . primary_key if name not in self . target . heading ) ) except StopIteration : pass return ( todo & AndList ( restrictions )) . proj () def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) # exclude \"error\" or \"ignore\" jobs if reserve_jobs : exclude_key_hashes = ( jobs & { \"table_name\" : self . target . table_name } & 'status in (\"error\", \"ignore\")' ) . fetch ( \"key_hash\" ) keys = [ key for key in keys if key_hash ( key ) not in exclude_key_hashes ] if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list def _populate1 ( self , key , jobs , suppress_errors , return_exception_objects , make_kwargs = None ): \"\"\" populates table for one source key, calling self.make inside a transaction. :param jobs: the jobs table or None if not reserve_jobs :param key: dict specifying job to populate :param suppress_errors: bool if errors should be suppressed and returned :param return_exception_objects: if True, errors must be returned as objects :return: (key, error) when suppress_errors=True, otherwise None \"\"\" make = self . _make_tuples if hasattr ( self , \"_make_tuples\" ) else self . make if jobs is None or jobs . reserve ( self . target . table_name , self . _job_key ( key )): self . connection . start_transaction () if key in self . target : # already populated self . connection . cancel_transaction () if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) else : logger . debug ( f \"Making { key } -> { self . target . full_table_name } \" ) self . __class__ . _allow_insert = True try : make ( dict ( key ), ** ( make_kwargs or {})) except ( KeyboardInterrupt , SystemExit , Exception ) as error : try : self . connection . cancel_transaction () except LostConnectionError : pass error_message = \" {exception}{msg} \" . format ( exception = error . __class__ . __name__ , msg = \": \" + str ( error ) if str ( error ) else \"\" , ) logger . debug ( f \"Error making { key } -> { self . target . full_table_name } - { error_message } \" ) if jobs is not None : # show error name and error message (if any) jobs . error ( self . target . table_name , self . _job_key ( key ), error_message = error_message , error_stack = traceback . format_exc (), ) if not suppress_errors or isinstance ( error , SystemExit ): raise else : logger . error ( error ) return key , error if return_exception_objects else error_message else : self . connection . commit_transaction () logger . debug ( f \"Success making { key } -> { self . target . full_table_name } \" ) if jobs is not None : jobs . complete ( self . target . table_name , self . _job_key ( key )) finally : self . __class__ . _allow_insert = False def progress ( self , * restrictions , display = False ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : logger . info ( \" %-20s \" % self . __class__ . __name__ + \" Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), ) return remaining , total", "title": "AutoPopulate"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.key_source", "text": "Returns: Type Description the query expression that yields primary key values to be passed, sequentially, to the make method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls.", "title": "key_source"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.make", "text": "Derived classes must implement method make that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. Source code in datajoint/autopopulate.py 92 93 94 95 96 97 98 99 100 def make ( self , key ): \"\"\" Derived classes must implement method `make` that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self. \"\"\" raise NotImplementedError ( \"Subclasses of AutoPopulate must implement the method `make`\" )", "title": "make()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.target", "text": "Returns: Type Description table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self.", "title": "target"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.populate", "text": "table.populate() calls table.make(key) for every primary key in self.key_source for which there is not already a tuple in table. Parameters: Name Type Description Default restrictions a list of restrictions each restrict (table.key_source - target.proj()) () suppress_errors if True, do not terminate execution. False return_exception_objects return error objects instead of just error messages False reserve_jobs if True, reserve jobs to populate in asynchronous fashion False order \"original\"|\"reverse\"|\"random\" - the order of execution 'original' limit if not None, check at most this many keys None max_calls if not None, populate at most this many keys None display_progress if True, report progress_bar False processes number of processes to use. Set to None to use all cores 1 make_kwargs dict, optional Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table. None Source code in datajoint/autopopulate.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def populate ( self , * restrictions , suppress_errors = False , return_exception_objects = False , reserve_jobs = False , order = \"original\" , limit = None , max_calls = None , display_progress = False , processes = 1 , make_kwargs = None , ): \"\"\" ``table.populate()`` calls ``table.make(key)`` for every primary key in ``self.key_source`` for which there is not already a tuple in table. :param restrictions: a list of restrictions each restrict (table.key_source - target.proj()) :param suppress_errors: if True, do not terminate execution. :param return_exception_objects: return error objects instead of just error messages :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion :param order: \"original\"|\"reverse\"|\"random\" - the order of execution :param limit: if not None, check at most this many keys :param max_calls: if not None, populate at most this many keys :param display_progress: if True, report progress_bar :param processes: number of processes to use. Set to None to use all cores :param make_kwargs: Keyword arguments which do not affect the result of computation to be passed down to each ``make()`` call. Computation arguments should be specified within the pipeline e.g. using a `dj.Lookup` table. :type make_kwargs: dict, optional \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Populate cannot be called during a transaction.\" ) valid_order = [ \"original\" , \"reverse\" , \"random\" ] if order not in valid_order : raise DataJointError ( \"The order argument must be one of %s \" % str ( valid_order ) ) jobs = ( self . connection . schemas [ self . target . database ] . jobs if reserve_jobs else None ) # define and set up signal handler for SIGTERM: if reserve_jobs : def handler ( signum , frame ): logger . info ( \"Populate terminated by SIGTERM\" ) raise SystemExit ( \"SIGTERM received\" ) old_handler = signal . signal ( signal . SIGTERM , handler ) keys = ( self . _jobs_to_do ( restrictions ) - self . target ) . fetch ( \"KEY\" , limit = limit ) # exclude \"error\" or \"ignore\" jobs if reserve_jobs : exclude_key_hashes = ( jobs & { \"table_name\" : self . target . table_name } & 'status in (\"error\", \"ignore\")' ) . fetch ( \"key_hash\" ) keys = [ key for key in keys if key_hash ( key ) not in exclude_key_hashes ] if order == \"reverse\" : keys . reverse () elif order == \"random\" : random . shuffle ( keys ) logger . debug ( \"Found %d keys to populate\" % len ( keys )) keys = keys [: max_calls ] nkeys = len ( keys ) if not nkeys : return processes = min ( _ for _ in ( processes , nkeys , mp . cpu_count ()) if _ ) error_list = [] populate_kwargs = dict ( suppress_errors = suppress_errors , return_exception_objects = return_exception_objects , make_kwargs = make_kwargs , ) if processes == 1 : for key in ( tqdm ( keys , desc = self . __class__ . __name__ ) if display_progress else keys ): error = self . _populate1 ( key , jobs , ** populate_kwargs ) if error is not None : error_list . append ( error ) else : # spawn multiple processes self . connection . close () # disconnect parent process from MySQL server del self . connection . _conn . ctx # SSLContext is not pickleable with mp . Pool ( processes , _initialize_populate , ( self , jobs , populate_kwargs ) ) as pool , ( tqdm ( desc = \"Processes: \" , total = nkeys ) if display_progress else contextlib . nullcontext () ) as progress_bar : for error in pool . imap ( _call_populate1 , keys , chunksize = 1 ): if error is not None : error_list . append ( error ) if display_progress : progress_bar . update () self . connection . connect () # reconnect parent process to MySQL server # restore original signal handler: if reserve_jobs : signal . signal ( signal . SIGTERM , old_handler ) if suppress_errors : return error_list", "title": "populate()"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.progress", "text": "Report the progress of populating the table. Returns: Type Description (remaining, total) -- numbers of tuples to be populated Source code in datajoint/autopopulate.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def progress ( self , * restrictions , display = False ): \"\"\" Report the progress of populating the table. :return: (remaining, total) -- numbers of tuples to be populated \"\"\" todo = self . _jobs_to_do ( restrictions ) total = len ( todo ) remaining = len ( todo - self . target ) if display : logger . info ( \" %-20s \" % self . __class__ . __name__ + \" Completed %d of %d ( %2.1f%% ) %s \" % ( total - remaining , total , 100 - 100 * remaining / ( total + 1e-12 ), datetime . datetime . strftime ( datetime . datetime . now (), \"%Y-%m- %d %H:%M:%S\" ), ), ) return remaining , total", "title": "progress()"}, {"location": "api/datajoint/blob/", "text": "(De)serialization methods for basic datatypes and numpy.ndarrays with provisions for mutual compatibility with Matlab-based serialization implemented by mYm. MatCell \u00b6 Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass MatStruct \u00b6 Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass Blob \u00b6 Source code in datajoint/blob.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 class Blob : def __init__ ( self , squeeze = False ): self . _squeeze = squeeze self . _blob = None self . _pos = 0 self . protocol = None def set_dj0 ( self ): if not config . get ( \"enable_python_native_blobs\" ): raise DataJointError ( \"\"\"v0.12+ python native blobs disabled. See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\" ) self . protocol = b \"dj0 \\0 \" # when using new blob features def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array def unpack ( self , blob ): self . _blob = blob try : # decompress prefix = next ( p for p in compression if self . _blob [ self . _pos :] . startswith ( p ) ) except StopIteration : pass # assume uncompressed but could be unrecognized compression else : self . _pos += len ( prefix ) blob_size = self . read_value () blob = compression [ prefix ]( self . _blob [ self . _pos :]) assert len ( blob ) == blob_size self . _blob = blob self . _pos = 0 blob_format = self . read_zero_terminated_string () if blob_format in ( \"mYm\" , \"dj0\" ): return self . read_blob ( n_bytes = len ( self . _blob ) - self . _pos ) def read_blob ( self , n_bytes = None ): start = self . _pos data_structure_code = chr ( self . read_value ( \"uint8\" )) try : call = { # MATLAB-compatible, inherited from original mYm \"A\" : self . read_array , # matlab-compatible numeric arrays and scalars with ndim==0 \"P\" : self . read_sparse_array , # matlab sparse array -- not supported yet \"S\" : self . read_struct , # matlab struct array \"C\" : self . read_cell_array , # matlab cell array # basic data types \" \\xFF \" : self . read_none , # None \" \\x01 \" : self . read_tuple , # a Sequence (e.g. tuple) \" \\x02 \" : self . read_list , # a MutableSequence (e.g. list) \" \\x03 \" : self . read_set , # a Set \" \\x04 \" : self . read_dict , # a Mapping (e.g. dict) \" \\x05 \" : self . read_string , # a UTF8-encoded string \" \\x06 \" : self . read_bytes , # a ByteString \" \\x0a \" : self . read_int , # unbounded scalar int \" \\x0b \" : self . read_bool , # scalar boolean \" \\x0c \" : self . read_complex , # scalar 128-bit complex number \" \\x0d \" : self . read_float , # scalar 64-bit float \"F\" : self . read_recarray , # numpy array with fields, including recarrays \"d\" : self . read_decimal , # a decimal \"t\" : self . read_datetime , # date, time, or datetime \"u\" : self . read_uuid , # UUID }[ data_structure_code ] except KeyError : raise DataJointError ( 'Unknown data structure code \" %s \". Upgrade datajoint.' % data_structure_code ) v = call () if n_bytes is not None and self . _pos - start != n_bytes : raise DataJointError ( \"Blob length check failed! Invalid blob\" ) return v def pack_blob ( self , obj ): # original mYm-based serialization from datajoint-matlab if isinstance ( obj , MatCell ): return self . pack_cell_array ( obj ) if isinstance ( obj , MatStruct ): return self . pack_struct ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields is None : return self . pack_array ( obj ) # blob types in the expanded dj0 blob format self . set_dj0 () if not isinstance ( obj , ( np . ndarray , np . number )): # python built-in data types if isinstance ( obj , bool ): return self . pack_bool ( obj ) if isinstance ( obj , int ): return self . pack_int ( obj ) if isinstance ( obj , complex ): return self . pack_complex ( obj ) if isinstance ( obj , float ): return self . pack_float ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields : return self . pack_recarray ( np . array ( obj )) if isinstance ( obj , ( np . number , np . datetime64 )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( bool , np . bool_ )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( float , int , complex )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time )): return self . pack_datetime ( obj ) if isinstance ( obj , Decimal ): return self . pack_decimal ( obj ) if isinstance ( obj , uuid . UUID ): return self . pack_uuid ( obj ) if isinstance ( obj , collections . abc . Mapping ): return self . pack_dict ( obj ) if isinstance ( obj , str ): return self . pack_string ( obj ) if isinstance ( obj , collections . abc . ByteString ): return self . pack_bytes ( obj ) if isinstance ( obj , collections . abc . MutableSequence ): return self . pack_list ( obj ) if isinstance ( obj , collections . abc . Sequence ): return self . pack_tuple ( obj ) if isinstance ( obj , collections . abc . Set ): return self . pack_set ( obj ) if obj is None : return self . pack_none () raise DataJointError ( \"Packing object of type %s currently not supported!\" % type ( obj ) ) def read_array ( self ): n_dims = int ( self . read_value ()) shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) dtype_id , is_complex = self . read_value ( \"uint32\" , 2 ) # Get dtype from type id dtype = deserialize_lookup [ dtype_id ][ \"dtype\" ] # Check if name is void if deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"VOID\" : data = np . array ( list ( self . read_blob ( self . read_value ()) for _ in range ( n_elem )), dtype = np . dtype ( \"O\" ), ) # Check if name is char elif deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"CHAR\" : # compensate for MATLAB packing of char arrays data = self . read_value ( dtype , count = 2 * n_elem ) data = data [:: 2 ] . astype ( \"U1\" ) if n_dims == 2 and shape [ 0 ] == 1 or n_dims == 1 : compact = data . squeeze () data = ( compact if compact . shape == () else np . array ( \"\" . join ( data . squeeze ())) ) shape = ( 1 ,) else : data = self . read_value ( dtype , count = n_elem ) if is_complex : data = data + 1 j * self . read_value ( dtype , count = n_elem ) return self . squeeze ( data . reshape ( shape , order = \"F\" )) def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) def read_sparse_array ( self ): raise DataJointError ( \"datajoint-python does not yet support sparse arrays. Issue (#590)\" ) def read_int ( self ): return int . from_bytes ( self . read_binary ( self . read_value ( \"uint16\" )), byteorder = \"little\" , signed = True ) @staticmethod def pack_int ( v ): n_bytes = v . bit_length () // 8 + 1 assert 0 < n_bytes <= 0xFFFF , \"Integers are limited to 65535 bytes\" return ( b \" \\x0a \" + np . uint16 ( n_bytes ) . tobytes () + v . to_bytes ( n_bytes , byteorder = \"little\" , signed = True ) ) def read_bool ( self ): return bool ( self . read_value ( \"bool\" )) @staticmethod def pack_bool ( v ): return b \" \\x0b \" + np . array ( v , dtype = \"bool\" ) . tobytes () def read_complex ( self ): return complex ( self . read_value ( \"complex128\" )) @staticmethod def pack_complex ( v ): return b \" \\x0c \" + np . array ( v , dtype = \"complex128\" ) . tobytes () def read_float ( self ): return float ( self . read_value ( \"float64\" )) @staticmethod def pack_float ( v ): return b \" \\x0d \" + np . array ( v , dtype = \"float64\" ) . tobytes () def read_decimal ( self ): return Decimal ( self . read_string ()) @staticmethod def pack_decimal ( d ): s = str ( d ) return b \"d\" + len_u64 ( s ) + s . encode () def read_string ( self ): return self . read_binary ( self . read_value ()) . decode () @staticmethod def pack_string ( s ): blob = s . encode () return b \" \\5 \" + len_u64 ( blob ) + blob def read_bytes ( self ): return self . read_binary ( self . read_value ()) @staticmethod def pack_bytes ( s ): return b \" \\6 \" + len_u64 ( s ) + s def read_none ( self ): pass @staticmethod def pack_none (): return b \" \\xFF \" def read_tuple ( self ): return tuple ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ()) ) def pack_tuple ( self , t ): return ( b \" \\1 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_list ( self ): return list ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_list ( self , t ): return ( b \" \\2 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_set ( self ): return set ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_set ( self , t ): return ( b \" \\3 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_dict ( self ): return dict ( ( self . read_blob ( self . read_value ()), self . read_blob ( self . read_value ())) for _ in range ( self . read_value ()) ) def pack_dict ( self , d ): return ( b \" \\4 \" + len_u64 ( d ) + b \"\" . join ( b \"\" . join (( len_u64 ( it ) + it ) for it in packed ) for packed in ( map ( self . pack_blob , pair ) for pair in d . items ()) ) ) def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) def pack_cell_array ( self , array ): return ( b \"C\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) ) def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date @staticmethod def pack_datetime ( d ): if isinstance ( d , datetime . datetime ): date , time = d . date (), d . time () elif isinstance ( d , datetime . date ): date , time = d , None else : date , time = None , d return b \"t\" + ( np . int32 ( - 1 if date is None else ( date . year * 100 + date . month ) * 100 + date . day ) . tobytes () + np . int64 ( - 1 if time is None else (( time . hour * 100 + time . minute ) * 100 + time . second ) * 1000000 + time . microsecond ) . tobytes () ) def read_uuid ( self ): q = self . read_binary ( 16 ) return uuid . UUID ( bytes = q ) @staticmethod def pack_uuid ( obj ): return b \"u\" + obj . bytes def read_zero_terminated_string ( self ): target = self . _blob . find ( b \" \\0 \" , self . _pos ) data = self . _blob [ self . _pos : target ] . decode () self . _pos = target + 1 return data def read_value ( self , dtype = None , count = 1 ): if dtype is None : dtype = \"uint32\" if use_32bit_dims else \"uint64\" data = np . frombuffer ( self . _blob , dtype = dtype , count = count , offset = self . _pos ) self . _pos += data . dtype . itemsize * data . size return data [ 0 ] if count == 1 else data def read_binary ( self , size ): self . _pos += int ( size ) return self . _blob [ self . _pos - int ( size ) : self . _pos ] def pack ( self , obj , compress ): self . protocol = b \"mYm \\0 \" # will be replaced with dj0 if new features are used blob = self . pack_blob ( obj ) # this may reset the protocol and must precede protocol evaluation blob = self . protocol + blob if compress and len ( blob ) > 1000 : compressed = b \"ZL123 \\0 \" + len_u64 ( blob ) + zlib . compress ( blob ) if len ( compressed ) < len ( blob ): blob = compressed return blob squeeze ( array , convert_to_scalar = True ) \u00b6 Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars Source code in datajoint/blob.py 101 102 103 104 105 106 107 108 109 def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array pack_array ( array ) \u00b6 Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. Source code in datajoint/blob.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob read_recarray () \u00b6 Serialize an np.ndarray with fields, including recarrays Source code in datajoint/blob.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) pack_recarray ( array ) \u00b6 Serialize a Matlab struct array Source code in datajoint/blob.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) read_struct () \u00b6 deserialize matlab stuct Source code in datajoint/blob.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) pack_struct ( array ) \u00b6 Serialize a Matlab struct array Source code in datajoint/blob.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values read_cell_array () \u00b6 deserialize MATLAB cell array Source code in datajoint/blob.py 487 488 489 490 491 492 493 494 495 496 497 def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) read_datetime () \u00b6 deserialize datetime.date, .time, or .datetime Source code in datajoint/blob.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date", "title": "blob.py"}, {"location": "api/datajoint/blob/#datajoint.blob.MatCell", "text": "Bases: np . ndarray a numpy ndarray representing a Matlab cell array Source code in datajoint/blob.py 73 74 75 76 class MatCell ( np . ndarray ): \"\"\"a numpy ndarray representing a Matlab cell array\"\"\" pass", "title": "MatCell"}, {"location": "api/datajoint/blob/#datajoint.blob.MatStruct", "text": "Bases: np . recarray numpy.recarray representing a Matlab struct array Source code in datajoint/blob.py 79 80 81 82 class MatStruct ( np . recarray ): \"\"\"numpy.recarray representing a Matlab struct array\"\"\" pass", "title": "MatStruct"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob", "text": "Source code in datajoint/blob.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 class Blob : def __init__ ( self , squeeze = False ): self . _squeeze = squeeze self . _blob = None self . _pos = 0 self . protocol = None def set_dj0 ( self ): if not config . get ( \"enable_python_native_blobs\" ): raise DataJointError ( \"\"\"v0.12+ python native blobs disabled. See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\" ) self . protocol = b \"dj0 \\0 \" # when using new blob features def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array def unpack ( self , blob ): self . _blob = blob try : # decompress prefix = next ( p for p in compression if self . _blob [ self . _pos :] . startswith ( p ) ) except StopIteration : pass # assume uncompressed but could be unrecognized compression else : self . _pos += len ( prefix ) blob_size = self . read_value () blob = compression [ prefix ]( self . _blob [ self . _pos :]) assert len ( blob ) == blob_size self . _blob = blob self . _pos = 0 blob_format = self . read_zero_terminated_string () if blob_format in ( \"mYm\" , \"dj0\" ): return self . read_blob ( n_bytes = len ( self . _blob ) - self . _pos ) def read_blob ( self , n_bytes = None ): start = self . _pos data_structure_code = chr ( self . read_value ( \"uint8\" )) try : call = { # MATLAB-compatible, inherited from original mYm \"A\" : self . read_array , # matlab-compatible numeric arrays and scalars with ndim==0 \"P\" : self . read_sparse_array , # matlab sparse array -- not supported yet \"S\" : self . read_struct , # matlab struct array \"C\" : self . read_cell_array , # matlab cell array # basic data types \" \\xFF \" : self . read_none , # None \" \\x01 \" : self . read_tuple , # a Sequence (e.g. tuple) \" \\x02 \" : self . read_list , # a MutableSequence (e.g. list) \" \\x03 \" : self . read_set , # a Set \" \\x04 \" : self . read_dict , # a Mapping (e.g. dict) \" \\x05 \" : self . read_string , # a UTF8-encoded string \" \\x06 \" : self . read_bytes , # a ByteString \" \\x0a \" : self . read_int , # unbounded scalar int \" \\x0b \" : self . read_bool , # scalar boolean \" \\x0c \" : self . read_complex , # scalar 128-bit complex number \" \\x0d \" : self . read_float , # scalar 64-bit float \"F\" : self . read_recarray , # numpy array with fields, including recarrays \"d\" : self . read_decimal , # a decimal \"t\" : self . read_datetime , # date, time, or datetime \"u\" : self . read_uuid , # UUID }[ data_structure_code ] except KeyError : raise DataJointError ( 'Unknown data structure code \" %s \". Upgrade datajoint.' % data_structure_code ) v = call () if n_bytes is not None and self . _pos - start != n_bytes : raise DataJointError ( \"Blob length check failed! Invalid blob\" ) return v def pack_blob ( self , obj ): # original mYm-based serialization from datajoint-matlab if isinstance ( obj , MatCell ): return self . pack_cell_array ( obj ) if isinstance ( obj , MatStruct ): return self . pack_struct ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields is None : return self . pack_array ( obj ) # blob types in the expanded dj0 blob format self . set_dj0 () if not isinstance ( obj , ( np . ndarray , np . number )): # python built-in data types if isinstance ( obj , bool ): return self . pack_bool ( obj ) if isinstance ( obj , int ): return self . pack_int ( obj ) if isinstance ( obj , complex ): return self . pack_complex ( obj ) if isinstance ( obj , float ): return self . pack_float ( obj ) if isinstance ( obj , np . ndarray ) and obj . dtype . fields : return self . pack_recarray ( np . array ( obj )) if isinstance ( obj , ( np . number , np . datetime64 )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( bool , np . bool_ )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( float , int , complex )): return self . pack_array ( np . array ( obj )) if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time )): return self . pack_datetime ( obj ) if isinstance ( obj , Decimal ): return self . pack_decimal ( obj ) if isinstance ( obj , uuid . UUID ): return self . pack_uuid ( obj ) if isinstance ( obj , collections . abc . Mapping ): return self . pack_dict ( obj ) if isinstance ( obj , str ): return self . pack_string ( obj ) if isinstance ( obj , collections . abc . ByteString ): return self . pack_bytes ( obj ) if isinstance ( obj , collections . abc . MutableSequence ): return self . pack_list ( obj ) if isinstance ( obj , collections . abc . Sequence ): return self . pack_tuple ( obj ) if isinstance ( obj , collections . abc . Set ): return self . pack_set ( obj ) if obj is None : return self . pack_none () raise DataJointError ( \"Packing object of type %s currently not supported!\" % type ( obj ) ) def read_array ( self ): n_dims = int ( self . read_value ()) shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) dtype_id , is_complex = self . read_value ( \"uint32\" , 2 ) # Get dtype from type id dtype = deserialize_lookup [ dtype_id ][ \"dtype\" ] # Check if name is void if deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"VOID\" : data = np . array ( list ( self . read_blob ( self . read_value ()) for _ in range ( n_elem )), dtype = np . dtype ( \"O\" ), ) # Check if name is char elif deserialize_lookup [ dtype_id ][ \"scalar_type\" ] == \"CHAR\" : # compensate for MATLAB packing of char arrays data = self . read_value ( dtype , count = 2 * n_elem ) data = data [:: 2 ] . astype ( \"U1\" ) if n_dims == 2 and shape [ 0 ] == 1 or n_dims == 1 : compact = data . squeeze () data = ( compact if compact . shape == () else np . array ( \"\" . join ( data . squeeze ())) ) shape = ( 1 ,) else : data = self . read_value ( dtype , count = n_elem ) if is_complex : data = data + 1 j * self . read_value ( dtype , count = n_elem ) return self . squeeze ( data . reshape ( shape , order = \"F\" )) def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray ) def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) ) def read_sparse_array ( self ): raise DataJointError ( \"datajoint-python does not yet support sparse arrays. Issue (#590)\" ) def read_int ( self ): return int . from_bytes ( self . read_binary ( self . read_value ( \"uint16\" )), byteorder = \"little\" , signed = True ) @staticmethod def pack_int ( v ): n_bytes = v . bit_length () // 8 + 1 assert 0 < n_bytes <= 0xFFFF , \"Integers are limited to 65535 bytes\" return ( b \" \\x0a \" + np . uint16 ( n_bytes ) . tobytes () + v . to_bytes ( n_bytes , byteorder = \"little\" , signed = True ) ) def read_bool ( self ): return bool ( self . read_value ( \"bool\" )) @staticmethod def pack_bool ( v ): return b \" \\x0b \" + np . array ( v , dtype = \"bool\" ) . tobytes () def read_complex ( self ): return complex ( self . read_value ( \"complex128\" )) @staticmethod def pack_complex ( v ): return b \" \\x0c \" + np . array ( v , dtype = \"complex128\" ) . tobytes () def read_float ( self ): return float ( self . read_value ( \"float64\" )) @staticmethod def pack_float ( v ): return b \" \\x0d \" + np . array ( v , dtype = \"float64\" ) . tobytes () def read_decimal ( self ): return Decimal ( self . read_string ()) @staticmethod def pack_decimal ( d ): s = str ( d ) return b \"d\" + len_u64 ( s ) + s . encode () def read_string ( self ): return self . read_binary ( self . read_value ()) . decode () @staticmethod def pack_string ( s ): blob = s . encode () return b \" \\5 \" + len_u64 ( blob ) + blob def read_bytes ( self ): return self . read_binary ( self . read_value ()) @staticmethod def pack_bytes ( s ): return b \" \\6 \" + len_u64 ( s ) + s def read_none ( self ): pass @staticmethod def pack_none (): return b \" \\xFF \" def read_tuple ( self ): return tuple ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ()) ) def pack_tuple ( self , t ): return ( b \" \\1 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_list ( self ): return list ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_list ( self , t ): return ( b \" \\2 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_set ( self ): return set ( self . read_blob ( self . read_value ()) for _ in range ( self . read_value ())) def pack_set ( self , t ): return ( b \" \\3 \" + len_u64 ( t ) + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( i ) for i in t )) ) def read_dict ( self ): return dict ( ( self . read_blob ( self . read_value ()), self . read_blob ( self . read_value ())) for _ in range ( self . read_value ()) ) def pack_dict ( self , d ): return ( b \" \\4 \" + len_u64 ( d ) + b \"\" . join ( b \"\" . join (( len_u64 ( it ) + it ) for it in packed ) for packed in ( map ( self . pack_blob , pair ) for pair in d . items ()) ) ) def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct ) def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell ) def pack_cell_array ( self , array ): return ( b \"C\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) ) def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date @staticmethod def pack_datetime ( d ): if isinstance ( d , datetime . datetime ): date , time = d . date (), d . time () elif isinstance ( d , datetime . date ): date , time = d , None else : date , time = None , d return b \"t\" + ( np . int32 ( - 1 if date is None else ( date . year * 100 + date . month ) * 100 + date . day ) . tobytes () + np . int64 ( - 1 if time is None else (( time . hour * 100 + time . minute ) * 100 + time . second ) * 1000000 + time . microsecond ) . tobytes () ) def read_uuid ( self ): q = self . read_binary ( 16 ) return uuid . UUID ( bytes = q ) @staticmethod def pack_uuid ( obj ): return b \"u\" + obj . bytes def read_zero_terminated_string ( self ): target = self . _blob . find ( b \" \\0 \" , self . _pos ) data = self . _blob [ self . _pos : target ] . decode () self . _pos = target + 1 return data def read_value ( self , dtype = None , count = 1 ): if dtype is None : dtype = \"uint32\" if use_32bit_dims else \"uint64\" data = np . frombuffer ( self . _blob , dtype = dtype , count = count , offset = self . _pos ) self . _pos += data . dtype . itemsize * data . size return data [ 0 ] if count == 1 else data def read_binary ( self , size ): self . _pos += int ( size ) return self . _blob [ self . _pos - int ( size ) : self . _pos ] def pack ( self , obj , compress ): self . protocol = b \"mYm \\0 \" # will be replaced with dj0 if new features are used blob = self . pack_blob ( obj ) # this may reset the protocol and must precede protocol evaluation blob = self . protocol + blob if compress and len ( blob ) > 1000 : compressed = b \"ZL123 \\0 \" + len_u64 ( blob ) + zlib . compress ( blob ) if len ( compressed ) < len ( blob ): blob = compressed return blob", "title": "Blob"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.squeeze", "text": "Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars Source code in datajoint/blob.py 101 102 103 104 105 106 107 108 109 def squeeze ( self , array , convert_to_scalar = True ): \"\"\" Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars \"\"\" if not self . _squeeze : return array array = array . squeeze () return array . item () if array . ndim == 0 and convert_to_scalar else array", "title": "squeeze()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_array", "text": "Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. Source code in datajoint/blob.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def pack_array ( self , array ): \"\"\" Serialize an np.ndarray into bytes. Scalars are encoded with ndim=0. \"\"\" if \"datetime64\" in array . dtype . name : self . set_dj0 () blob = ( b \"A\" + np . uint64 ( array . ndim ) . tobytes () + np . array ( array . shape , dtype = np . uint64 ) . tobytes () ) is_complex = np . iscomplexobj ( array ) if is_complex : array , imaginary = np . real ( array ), np . imag ( array ) try : type_id = serialize_lookup [ array . dtype ][ \"type_id\" ] except KeyError : # U is for unicode string if array . dtype . char == \"U\" : type_id = serialize_lookup [ np . dtype ( \"O\" )][ \"type_id\" ] else : raise DataJointError ( f \"Type { array . dtype } is ambiguous or unknown\" ) blob += np . array ([ type_id , is_complex ], dtype = np . uint32 ) . tobytes () if ( array . dtype . char == \"U\" or serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"VOID\" ): blob += b \"\" . join ( len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for e in array . flatten ( order = \"F\" )) ) self . set_dj0 () # not supported by original mym elif serialize_lookup [ array . dtype ][ \"scalar_type\" ] == \"CHAR\" : blob += ( array . view ( np . uint8 ) . astype ( np . uint16 ) . tobytes () ) # convert to 16-bit chars for MATLAB else : # numeric arrays if array . ndim == 0 : # not supported by original mym self . set_dj0 () blob += array . tobytes ( order = \"F\" ) if is_complex : blob += imaginary . tobytes ( order = \"F\" ) return blob", "title": "pack_array()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_recarray", "text": "Serialize an np.ndarray with fields, including recarrays Source code in datajoint/blob.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def read_recarray ( self ): \"\"\" Serialize an np.ndarray with fields, including recarrays \"\"\" n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] arrays = [ self . read_blob () for _ in range ( n_fields )] rec = np . empty ( arrays [ 0 ] . shape , np . dtype ([( f , t . dtype ) for f , t in zip ( field_names , arrays )]), ) for f , t in zip ( field_names , arrays ): rec [ f ] = t return rec . view ( np . recarray )", "title": "read_recarray()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_recarray", "text": "Serialize a Matlab struct array Source code in datajoint/blob.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def pack_recarray ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"F\" + len_u32 ( array . dtype ) + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names self . pack_recarray ( array [ f ]) if array [ f ] . dtype . fields else self . pack_array ( array [ f ]) for f in array . dtype . names ) )", "title": "pack_recarray()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_struct", "text": "deserialize matlab stuct Source code in datajoint/blob.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def read_struct ( self ): \"\"\"deserialize matlab stuct\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = np . prod ( shape , dtype = int ) n_fields = self . read_value ( \"uint32\" ) if not n_fields : return np . array ( None ) # empty array field_names = [ self . read_zero_terminated_string () for _ in range ( n_fields )] raw_data = [ tuple ( self . read_blob ( n_bytes = int ( self . read_value ())) for _ in range ( n_fields ) ) for __ in range ( n_elem ) ] data = np . array ( raw_data , dtype = list ( zip ( field_names , repeat ( object )))) return self . squeeze ( data . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) . view ( MatStruct )", "title": "read_struct()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_struct", "text": "Serialize a Matlab struct array Source code in datajoint/blob.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def pack_struct ( self , array ): \"\"\"Serialize a Matlab struct array\"\"\" return ( b \"S\" + np . array (( array . ndim ,) + array . shape , dtype = np . uint64 ) . tobytes () + len_u32 ( array . dtype . names ) # dimensionality + \" \\0 \" . join ( array . dtype . names ) . encode () # number of fields + b \" \\0 \" + b \"\" . join ( # field names len_u64 ( it ) + it for it in ( self . pack_blob ( e ) for rec in array . flatten ( order = \"F\" ) for e in rec ) ) ) # values", "title": "pack_struct()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_cell_array", "text": "deserialize MATLAB cell array Source code in datajoint/blob.py 487 488 489 490 491 492 493 494 495 496 497 def read_cell_array ( self ): \"\"\"deserialize MATLAB cell array\"\"\" n_dims = self . read_value () shape = self . read_value ( count = n_dims ) n_elem = int ( np . prod ( shape )) result = [ self . read_blob ( n_bytes = self . read_value ()) for _ in range ( n_elem )] return ( self . squeeze ( np . array ( result ) . reshape ( shape , order = \"F\" ), convert_to_scalar = False ) ) . view ( MatCell )", "title": "read_cell_array()"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_datetime", "text": "deserialize datetime.date, .time, or .datetime Source code in datajoint/blob.py 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def read_datetime ( self ): \"\"\"deserialize datetime.date, .time, or .datetime\"\"\" date , time = self . read_value ( \"int32\" ), self . read_value ( \"int64\" ) date = ( datetime . date ( year = date // 10000 , month = ( date // 100 ) % 100 , day = date % 100 ) if date >= 0 else None ) time = ( datetime . time ( hour = ( time // 10000000000 ) % 100 , minute = ( time // 100000000 ) % 100 , second = ( time // 1000000 ) % 100 , microsecond = time % 1000000 , ) if time >= 0 else None ) return time and date and datetime . datetime . combine ( date , time ) or time or date", "title": "read_datetime()"}, {"location": "api/datajoint/condition/", "text": "methods for generating SQL WHERE clauses from datajoint restriction conditions PromiscuousOperand \u00b6 A container for an operand to ignore join compatibility Source code in datajoint/condition.py 35 36 37 38 39 40 41 class PromiscuousOperand : \"\"\" A container for an operand to ignore join compatibility \"\"\" def __init__ ( self , operand ): self . operand = operand AndList \u00b6 Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction ) Not \u00b6 invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction assert_join_compatibility ( expr1 , expr2 ) \u00b6 Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. Parameters: Name Type Description Default expr1 A QueryExpression object required expr2 A QueryExpression object required Source code in datajoint/condition.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def assert_join_compatibility ( expr1 , expr2 ): \"\"\" Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object \"\"\" from .expression import QueryExpression , U for rel in ( expr1 , expr2 ): if not isinstance ( rel , ( U , QueryExpression )): raise DataJointError ( \"Object %r is not a QueryExpression and cannot be joined.\" % rel ) if not isinstance ( expr1 , U ) and not isinstance ( expr2 , U ): # dj.U is always compatible try : raise DataJointError ( \"Cannot join query expressions on dependent attribute ` %s `\" % next ( r for r in set ( expr1 . heading . secondary_attributes ) . intersection ( expr2 . heading . secondary_attributes ) ) ) except StopIteration : pass # all ok make_condition ( query_expression , condition , columns ) \u00b6 Translate the input condition into the equivalent SQL condition (a string) Parameters: Name Type Description Default query_expression a dj.QueryExpression object to apply condition required condition any valid restriction object. required columns a set passed by reference to collect all column names used in the condition. required Returns: Type Description an SQL condition string or a boolean value. Source code in datajoint/condition.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def make_condition ( query_expression , condition , columns ): \"\"\" Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. \"\"\" from .expression import QueryExpression , Aggregation , U def prep_value ( k , v ): \"\"\"prepare SQL condition\"\"\" key_match , k = translate_attribute ( k ) if key_match [ \"path\" ] is None : k = f \"` { k } `\" if ( query_expression . heading [ key_match [ \"attr\" ]] . json and key_match [ \"path\" ] is not None and isinstance ( v , dict ) ): return f \" { k } =' { json . dumps ( v ) } '\" if v is None : return f \" { k } IS NULL\" if query_expression . heading [ key_match [ \"attr\" ]] . uuid : if not isinstance ( v , uuid . UUID ): try : v = uuid . UUID ( v ) except ( AttributeError , ValueError ): raise DataJointError ( \"Badly formed UUID {v} in restriction by ` {k} `\" . format ( k = k , v = v ) ) return f \" { k } =X' { v . bytes . hex () } '\" if isinstance ( v , ( datetime . date , datetime . datetime , datetime . time , decimal . Decimal , list , ), ): return f ' { k } =\" { v } \"' if isinstance ( v , str ): v = v . replace ( \"%\" , \" %% \" ) . replace ( \" \\\\ \" , \" \\\\\\\\ \" ) return f ' { k } =\" { v } \"' return f \" { k } = { v } \" def combine_conditions ( negate , conditions ): return f \" { 'NOT ' if negate else '' } ( { ')AND(' . join ( conditions ) } )\" negate = False while isinstance ( condition , Not ): negate = not negate condition = condition . restriction # restrict by string if isinstance ( condition , str ): columns . update ( extract_column_names ( condition )) return combine_conditions ( negate , conditions = [ condition . strip () . replace ( \"%\" , \" %% \" )] ) # escape %, see issue #376 # restrict by AndList if isinstance ( condition , AndList ): # omit all conditions that evaluate to True items = [ item for item in ( make_condition ( query_expression , cond , columns ) for cond in condition ) if item is not True ] if any ( item is False for item in items ): return negate # if any item is False, the whole thing is False if not items : return not negate # and empty AndList is True return combine_conditions ( negate , conditions = items ) # restriction by dj.U evaluates to True if isinstance ( condition , U ): return not negate # restrict by boolean if isinstance ( condition , bool ): return negate != condition # restrict by a mapping/dict -- convert to an AndList of string equality conditions if isinstance ( condition , collections . abc . Mapping ): common_attributes = set ( c . split ( \".\" , 1 )[ 0 ] for c in condition ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluates to True columns . update ( common_attributes ) return combine_conditions ( negate , conditions = [ prep_value ( k , v ) for k , v in condition . items () if k . split ( \".\" , 1 )[ 0 ] in common_attributes # handle json indexing ], ) # restrict by a numpy record -- convert to an AndList of string equality conditions if isinstance ( condition , numpy . void ): common_attributes = set ( condition . dtype . fields ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluate to True columns . update ( common_attributes ) return combine_conditions ( negate , conditions = [ prep_value ( k , condition [ k ]) for k in common_attributes ], ) # restrict by a QueryExpression subclass -- trigger instantiation and move on if inspect . isclass ( condition ) and issubclass ( condition , QueryExpression ): condition = condition () # restrict by another expression (aka semijoin and antijoin) check_compatibility = True if isinstance ( condition , PromiscuousOperand ): condition = condition . operand check_compatibility = False if isinstance ( condition , QueryExpression ): if check_compatibility : assert_join_compatibility ( query_expression , condition ) common_attributes = [ q for q in condition . heading . names if q in query_expression . heading . names ] columns . update ( common_attributes ) if isinstance ( condition , Aggregation ): condition = condition . make_subquery () return ( # without common attributes, any non-empty set matches everything ( not negate if condition else negate ) if not common_attributes else \"( {fields} ) {not_} in ( {subquery} )\" . format ( fields = \"`\" + \"`,`\" . join ( common_attributes ) + \"`\" , not_ = \"not \" if negate else \"\" , subquery = condition . make_sql ( common_attributes ), ) ) # restrict by pandas.DataFrames if isinstance ( condition , pandas . DataFrame ): condition = condition . to_records () # convert to numpy.recarray and move on # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList try : or_list = [ make_condition ( query_expression , q , columns ) for q in condition ] except TypeError : raise DataJointError ( \"Invalid restriction type %r \" % condition ) else : or_list = [ item for item in or_list if item is not False ] # ignore False conditions if any ( item is True for item in or_list ): # if any item is True, entirely True return not negate return ( f \" { 'NOT ' if negate else '' } ( { ' OR ' . join ( or_list ) } )\" if or_list else negate ) extract_column_names ( sql_expression ) \u00b6 extract all presumed column names from an sql expression such as the WHERE clause, for example. Parameters: Name Type Description Default sql_expression a string containing an SQL expression required Returns: Type Description set of extracted column names This may be MySQL-specific for now. Source code in datajoint/condition.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def extract_column_names ( sql_expression ): \"\"\" extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. \"\"\" assert isinstance ( sql_expression , str ) result = set () s = sql_expression # for terseness # remove escaped quotes s = re . sub ( r \"( \\\\\\\" )|( \\\\ \\')\" , \"\" , s ) # remove quoted text s = re . sub ( r \"'[^']*'\" , \"\" , s ) s = re . sub ( r '\"[^\"]*\"' , \"\" , s ) # find all tokens in back quotes and remove them result . update ( re . findall ( r \"`([a-z][a-z_0-9]*)`\" , s )) s = re . sub ( r \"`[a-z][a-z_0-9]*`\" , \"\" , s ) # remove space before parentheses s = re . sub ( r \"\\s*\\(\" , \"(\" , s ) # remove tokens followed by ( since they must be functions s = re . sub ( r \"(\\b[a-z][a-z_0-9]*)\\(\" , \"(\" , s ) remaining_tokens = set ( re . findall ( r \"\\b[a-z][a-z_0-9]*\\b\" , s )) # update result removing reserved words result . update ( remaining_tokens - { \"is\" , \"in\" , \"between\" , \"like\" , \"and\" , \"or\" , \"null\" , \"not\" , \"interval\" , \"second\" , \"minute\" , \"hour\" , \"day\" , \"month\" , \"week\" , \"year\" , } ) return result", "title": "condition.py"}, {"location": "api/datajoint/condition/#datajoint.condition.PromiscuousOperand", "text": "A container for an operand to ignore join compatibility Source code in datajoint/condition.py 35 36 37 38 39 40 41 class PromiscuousOperand : \"\"\" A container for an operand to ignore join compatibility \"\"\" def __init__ ( self , operand ): self . operand = operand", "title": "PromiscuousOperand"}, {"location": "api/datajoint/condition/#datajoint.condition.AndList", "text": "Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction )", "title": "AndList"}, {"location": "api/datajoint/condition/#datajoint.condition.Not", "text": "invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction", "title": "Not"}, {"location": "api/datajoint/condition/#datajoint.condition.assert_join_compatibility", "text": "Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. Parameters: Name Type Description Default expr1 A QueryExpression object required expr2 A QueryExpression object required Source code in datajoint/condition.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def assert_join_compatibility ( expr1 , expr2 ): \"\"\" Determine if expressions expr1 and expr2 are join-compatible. To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible. :param expr1: A QueryExpression object :param expr2: A QueryExpression object \"\"\" from .expression import QueryExpression , U for rel in ( expr1 , expr2 ): if not isinstance ( rel , ( U , QueryExpression )): raise DataJointError ( \"Object %r is not a QueryExpression and cannot be joined.\" % rel ) if not isinstance ( expr1 , U ) and not isinstance ( expr2 , U ): # dj.U is always compatible try : raise DataJointError ( \"Cannot join query expressions on dependent attribute ` %s `\" % next ( r for r in set ( expr1 . heading . secondary_attributes ) . intersection ( expr2 . heading . secondary_attributes ) ) ) except StopIteration : pass # all ok", "title": "assert_join_compatibility()"}, {"location": "api/datajoint/condition/#datajoint.condition.make_condition", "text": "Translate the input condition into the equivalent SQL condition (a string) Parameters: Name Type Description Default query_expression a dj.QueryExpression object to apply condition required condition any valid restriction object. required columns a set passed by reference to collect all column names used in the condition. required Returns: Type Description an SQL condition string or a boolean value. Source code in datajoint/condition.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def make_condition ( query_expression , condition , columns ): \"\"\" Translate the input condition into the equivalent SQL condition (a string) :param query_expression: a dj.QueryExpression object to apply condition :param condition: any valid restriction object. :param columns: a set passed by reference to collect all column names used in the condition. :return: an SQL condition string or a boolean value. \"\"\" from .expression import QueryExpression , Aggregation , U def prep_value ( k , v ): \"\"\"prepare SQL condition\"\"\" key_match , k = translate_attribute ( k ) if key_match [ \"path\" ] is None : k = f \"` { k } `\" if ( query_expression . heading [ key_match [ \"attr\" ]] . json and key_match [ \"path\" ] is not None and isinstance ( v , dict ) ): return f \" { k } =' { json . dumps ( v ) } '\" if v is None : return f \" { k } IS NULL\" if query_expression . heading [ key_match [ \"attr\" ]] . uuid : if not isinstance ( v , uuid . UUID ): try : v = uuid . UUID ( v ) except ( AttributeError , ValueError ): raise DataJointError ( \"Badly formed UUID {v} in restriction by ` {k} `\" . format ( k = k , v = v ) ) return f \" { k } =X' { v . bytes . hex () } '\" if isinstance ( v , ( datetime . date , datetime . datetime , datetime . time , decimal . Decimal , list , ), ): return f ' { k } =\" { v } \"' if isinstance ( v , str ): v = v . replace ( \"%\" , \" %% \" ) . replace ( \" \\\\ \" , \" \\\\\\\\ \" ) return f ' { k } =\" { v } \"' return f \" { k } = { v } \" def combine_conditions ( negate , conditions ): return f \" { 'NOT ' if negate else '' } ( { ')AND(' . join ( conditions ) } )\" negate = False while isinstance ( condition , Not ): negate = not negate condition = condition . restriction # restrict by string if isinstance ( condition , str ): columns . update ( extract_column_names ( condition )) return combine_conditions ( negate , conditions = [ condition . strip () . replace ( \"%\" , \" %% \" )] ) # escape %, see issue #376 # restrict by AndList if isinstance ( condition , AndList ): # omit all conditions that evaluate to True items = [ item for item in ( make_condition ( query_expression , cond , columns ) for cond in condition ) if item is not True ] if any ( item is False for item in items ): return negate # if any item is False, the whole thing is False if not items : return not negate # and empty AndList is True return combine_conditions ( negate , conditions = items ) # restriction by dj.U evaluates to True if isinstance ( condition , U ): return not negate # restrict by boolean if isinstance ( condition , bool ): return negate != condition # restrict by a mapping/dict -- convert to an AndList of string equality conditions if isinstance ( condition , collections . abc . Mapping ): common_attributes = set ( c . split ( \".\" , 1 )[ 0 ] for c in condition ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluates to True columns . update ( common_attributes ) return combine_conditions ( negate , conditions = [ prep_value ( k , v ) for k , v in condition . items () if k . split ( \".\" , 1 )[ 0 ] in common_attributes # handle json indexing ], ) # restrict by a numpy record -- convert to an AndList of string equality conditions if isinstance ( condition , numpy . void ): common_attributes = set ( condition . dtype . fields ) . intersection ( query_expression . heading . names ) if not common_attributes : return not negate # no matching attributes -> evaluate to True columns . update ( common_attributes ) return combine_conditions ( negate , conditions = [ prep_value ( k , condition [ k ]) for k in common_attributes ], ) # restrict by a QueryExpression subclass -- trigger instantiation and move on if inspect . isclass ( condition ) and issubclass ( condition , QueryExpression ): condition = condition () # restrict by another expression (aka semijoin and antijoin) check_compatibility = True if isinstance ( condition , PromiscuousOperand ): condition = condition . operand check_compatibility = False if isinstance ( condition , QueryExpression ): if check_compatibility : assert_join_compatibility ( query_expression , condition ) common_attributes = [ q for q in condition . heading . names if q in query_expression . heading . names ] columns . update ( common_attributes ) if isinstance ( condition , Aggregation ): condition = condition . make_subquery () return ( # without common attributes, any non-empty set matches everything ( not negate if condition else negate ) if not common_attributes else \"( {fields} ) {not_} in ( {subquery} )\" . format ( fields = \"`\" + \"`,`\" . join ( common_attributes ) + \"`\" , not_ = \"not \" if negate else \"\" , subquery = condition . make_sql ( common_attributes ), ) ) # restrict by pandas.DataFrames if isinstance ( condition , pandas . DataFrame ): condition = condition . to_records () # convert to numpy.recarray and move on # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList try : or_list = [ make_condition ( query_expression , q , columns ) for q in condition ] except TypeError : raise DataJointError ( \"Invalid restriction type %r \" % condition ) else : or_list = [ item for item in or_list if item is not False ] # ignore False conditions if any ( item is True for item in or_list ): # if any item is True, entirely True return not negate return ( f \" { 'NOT ' if negate else '' } ( { ' OR ' . join ( or_list ) } )\" if or_list else negate )", "title": "make_condition()"}, {"location": "api/datajoint/condition/#datajoint.condition.extract_column_names", "text": "extract all presumed column names from an sql expression such as the WHERE clause, for example. Parameters: Name Type Description Default sql_expression a string containing an SQL expression required Returns: Type Description set of extracted column names This may be MySQL-specific for now. Source code in datajoint/condition.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def extract_column_names ( sql_expression ): \"\"\" extract all presumed column names from an sql expression such as the WHERE clause, for example. :param sql_expression: a string containing an SQL expression :return: set of extracted column names This may be MySQL-specific for now. \"\"\" assert isinstance ( sql_expression , str ) result = set () s = sql_expression # for terseness # remove escaped quotes s = re . sub ( r \"( \\\\\\\" )|( \\\\ \\')\" , \"\" , s ) # remove quoted text s = re . sub ( r \"'[^']*'\" , \"\" , s ) s = re . sub ( r '\"[^\"]*\"' , \"\" , s ) # find all tokens in back quotes and remove them result . update ( re . findall ( r \"`([a-z][a-z_0-9]*)`\" , s )) s = re . sub ( r \"`[a-z][a-z_0-9]*`\" , \"\" , s ) # remove space before parentheses s = re . sub ( r \"\\s*\\(\" , \"(\" , s ) # remove tokens followed by ( since they must be functions s = re . sub ( r \"(\\b[a-z][a-z_0-9]*)\\(\" , \"(\" , s ) remaining_tokens = set ( re . findall ( r \"\\b[a-z][a-z_0-9]*\\b\" , s )) # update result removing reserved words result . update ( remaining_tokens - { \"is\" , \"in\" , \"between\" , \"like\" , \"and\" , \"or\" , \"null\" , \"not\" , \"interval\" , \"second\" , \"minute\" , \"hour\" , \"day\" , \"month\" , \"week\" , \"year\" , } ) return result", "title": "extract_column_names()"}, {"location": "api/datajoint/connection/", "text": "This module contains the Connection class that manages the connection to the database, and the conn function that provides access to a persistent connection in datajoint. translate_query_error ( client_error , query ) \u00b6 Take client error and original query and return the corresponding DataJoint exception. Parameters: Name Type Description Default client_error the exception raised by the client interface required query sql query with placeholders required Returns: Type Description an instance of the corresponding subclass of datajoint.errors.DataJointError Source code in datajoint/connection.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def translate_query_error ( client_error , query ): \"\"\" Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError \"\"\" logger . debug ( \"type: {} , args: {} \" . format ( type ( client_error ), client_error . args )) err , * args = client_error . args # Loss of connection errors if err in ( 0 , \"(0, '')\" ): return errors . LostConnectionError ( \"Server connection lost due to an interface error.\" , * args ) if err == 2006 : return errors . LostConnectionError ( \"Connection timed out\" , * args ) if err == 2013 : return errors . LostConnectionError ( \"Server connection lost\" , * args ) # Access errors if err in ( 1044 , 1142 ): return errors . AccessError ( \"Insufficient privileges.\" , args [ 0 ], query ) # Integrity errors if err == 1062 : return errors . DuplicateError ( * args ) if err == 1451 : return errors . IntegrityError ( * args ) if err == 1452 : return errors . IntegrityError ( * args ) # Syntax errors if err == 1064 : return errors . QuerySyntaxError ( args [ 0 ], query ) # Existence errors if err == 1146 : return errors . MissingTableError ( args [ 0 ], query ) if err == 1364 : return errors . MissingAttributeError ( * args ) if err == 1054 : return errors . UnknownAttributeError ( * args ) # all the other errors are re-raised in original form return client_error conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ) \u00b6 Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. Parameters: Name Type Description Default host hostname None user mysql user None password mysql password None init_fun initialization function None reset whether the connection should be reset or not False use_tls TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). None Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection EmulatedCursor \u00b6 acts like a cursor Source code in datajoint/connection.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class EmulatedCursor : \"\"\"acts like a cursor\"\"\" def __init__ ( self , data ): self . _data = data self . _iter = iter ( self . _data ) def __iter__ ( self ): return self def __next__ ( self ): return next ( self . _iter ) def fetchall ( self ): return self . _data def fetchone ( self ): return next ( self . _iter ) @property def rowcount ( self ): return len ( self . _data ) Connection \u00b6 A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. Parameters: Name Type Description Default host host name, may include port number as hostname:port, in which case it overrides the value in port required user user name required password password required port port number None init_fun connection initialization function (SQL) None use_tls TLS encryption option None Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction () connect () \u00b6 Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) set_query_cache ( query_cache = None ) \u00b6 When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. Parameters: Name Type Description Default query_cache a string to initialize the hash for query results None Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache purge_query_cache () \u00b6 Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () ping () \u00b6 Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) is_connected property \u00b6 Return true if the object is connected to the database server. query ( query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ) \u00b6 Execute the specified query and return the tuple generator (cursor). Parameters: Name Type Description Default query SQL query required args additional arguments for the client.cursor () as_dict If as_dict is set to True, the returned cursor objects returns query results as dictionary. False suppress_warnings If True, suppress all warnings arising from underlying query library True reconnect when None, get from config, when True, attempt to reconnect if disconnected None Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor get_user () \u00b6 Returns: Type Description the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] in_transaction property \u00b6 Returns: Type Description True if there is an open transaction. start_transaction () \u00b6 Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) cancel_transaction () \u00b6 Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) commit_transaction () \u00b6 Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) transaction property \u00b6 Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here", "title": "connection.py"}, {"location": "api/datajoint/connection/#datajoint.connection.translate_query_error", "text": "Take client error and original query and return the corresponding DataJoint exception. Parameters: Name Type Description Default client_error the exception raised by the client interface required query sql query with placeholders required Returns: Type Description an instance of the corresponding subclass of datajoint.errors.DataJointError Source code in datajoint/connection.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def translate_query_error ( client_error , query ): \"\"\" Take client error and original query and return the corresponding DataJoint exception. :param client_error: the exception raised by the client interface :param query: sql query with placeholders :return: an instance of the corresponding subclass of datajoint.errors.DataJointError \"\"\" logger . debug ( \"type: {} , args: {} \" . format ( type ( client_error ), client_error . args )) err , * args = client_error . args # Loss of connection errors if err in ( 0 , \"(0, '')\" ): return errors . LostConnectionError ( \"Server connection lost due to an interface error.\" , * args ) if err == 2006 : return errors . LostConnectionError ( \"Connection timed out\" , * args ) if err == 2013 : return errors . LostConnectionError ( \"Server connection lost\" , * args ) # Access errors if err in ( 1044 , 1142 ): return errors . AccessError ( \"Insufficient privileges.\" , args [ 0 ], query ) # Integrity errors if err == 1062 : return errors . DuplicateError ( * args ) if err == 1451 : return errors . IntegrityError ( * args ) if err == 1452 : return errors . IntegrityError ( * args ) # Syntax errors if err == 1064 : return errors . QuerySyntaxError ( args [ 0 ], query ) # Existence errors if err == 1146 : return errors . MissingTableError ( args [ 0 ], query ) if err == 1364 : return errors . MissingAttributeError ( * args ) if err == 1054 : return errors . UnknownAttributeError ( * args ) # all the other errors are re-raised in original form return client_error", "title": "translate_query_error()"}, {"location": "api/datajoint/connection/#datajoint.connection.conn", "text": "Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. Parameters: Name Type Description Default host hostname None user mysql user None password mysql password None init_fun initialization function None reset whether the connection should be reset or not False use_tls TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). None Source code in datajoint/connection.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def conn ( host = None , user = None , password = None , * , init_fun = None , reset = False , use_tls = None ): \"\"\" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not :param use_tls: TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options). \"\"\" if not hasattr ( conn , \"connection\" ) or reset : host = host if host is not None else config [ \"database.host\" ] user = user if user is not None else config [ \"database.user\" ] password = password if password is not None else config [ \"database.password\" ] if user is None : # pragma: no cover user = input ( \"Please enter DataJoint username: \" ) if password is None : # pragma: no cover password = getpass ( prompt = \"Please enter DataJoint password: \" ) init_fun = ( init_fun if init_fun is not None else config [ \"connection.init_function\" ] ) use_tls = use_tls if use_tls is not None else config [ \"database.use_tls\" ] conn . connection = Connection ( host , user , password , None , init_fun , use_tls ) return conn . connection", "title": "conn()"}, {"location": "api/datajoint/connection/#datajoint.connection.EmulatedCursor", "text": "acts like a cursor Source code in datajoint/connection.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class EmulatedCursor : \"\"\"acts like a cursor\"\"\" def __init__ ( self , data ): self . _data = data self . _iter = iter ( self . _data ) def __iter__ ( self ): return self def __next__ ( self ): return next ( self . _iter ) def fetchall ( self ): return self . _data def fetchone ( self ): return next ( self . _iter ) @property def rowcount ( self ): return len ( self . _data )", "title": "EmulatedCursor"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection", "text": "A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. Parameters: Name Type Description Default host host name, may include port number as hostname:port, in which case it overrides the value in port required user user name required password password required port port number None init_fun connection initialization function (SQL) None use_tls TLS encryption option None Source code in datajoint/connection.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 class Connection : \"\"\" A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys). Most of the parameters below should be set in the local configuration file. :param host: host name, may include port number as hostname:port, in which case it overrides the value in port :param user: user name :param password: password :param port: port number :param init_fun: connection initialization function (SQL) :param use_tls: TLS encryption option \"\"\" def __init__ ( self , host , user , password , port = None , init_fun = None , use_tls = None ): host_input , host = ( host , get_host_hook ( host )) if \":\" in host : # the port in the hostname overrides the port argument host , port = host . split ( \":\" ) port = int ( port ) elif port is None : port = config [ \"database.port\" ] self . conn_info = dict ( host = host , port = port , user = user , passwd = password ) if use_tls is not False : self . conn_info [ \"ssl\" ] = ( use_tls if isinstance ( use_tls , dict ) else { \"ssl\" : {}} ) self . conn_info [ \"ssl_input\" ] = use_tls self . conn_info [ \"host_input\" ] = host_input self . init_fun = init_fun logger . info ( \"Connecting {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . _conn = None self . _query_cache = None connect_host_hook ( self ) if self . is_connected : logger . info ( \"Connected {user} @ {host} : {port} \" . format ( ** self . conn_info )) self . connection_id = self . query ( \"SELECT connection_id()\" ) . fetchone ()[ 0 ] else : raise errors . LostConnectionError ( \"Connection failed.\" ) self . _in_transaction = False self . schemas = dict () self . dependencies = Dependencies ( self ) def __eq__ ( self , other ): return self . conn_info == other . conn_info def __repr__ ( self ): connected = \"connected\" if self . is_connected else \"disconnected\" return \"DataJoint connection ( {connected} ) {user} @ {host} : {port} \" . format ( connected = connected , ** self . conn_info ) def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True ) def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink () def close ( self ): self . _conn . close () def register ( self , schema ): self . schemas [ schema . database ] = schema self . dependencies . clear () def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False ) @property def is_connected ( self ): \"\"\"Return true if the object is connected to the database server.\"\"\" try : self . ping () except : return False return True @staticmethod def _execute_query ( cursor , query , args , suppress_warnings ): try : with warnings . catch_warnings (): if suppress_warnings : # suppress all warnings arising from underlying SQL library warnings . simplefilter ( \"ignore\" ) cursor . execute ( query , args ) except client . err . Error as err : raise translate_query_error ( err , query ) def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ] # ---------- transaction processing @property def in_transaction ( self ): \"\"\" :return: True if there is an open transaction. \"\"\" self . _in_transaction = self . _in_transaction and self . is_connected return self . _in_transaction def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" ) def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" ) def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" ) # -------- context manager for transactions @property @contextmanager def transaction ( self ): \"\"\" Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: >>> import datajoint as dj >>> with dj.conn().transaction as conn: >>> # transaction is open here \"\"\" try : self . start_transaction () yield self except : self . cancel_transaction () raise else : self . commit_transaction ()", "title": "Connection"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.connect", "text": "Connect to the database server. Source code in datajoint/connection.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 def connect ( self ): \"\"\"Connect to the database server.\"\"\" with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , \".*deprecated.*\" ) try : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if k not in [ \"ssl_input\" , \"host_input\" ] }, ) except client . err . InternalError : self . _conn = client . connect ( init_command = self . init_fun , sql_mode = \"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\" \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\" , charset = config [ \"connection.charset\" ], ** { k : v for k , v in self . conn_info . items () if not ( k in [ \"ssl_input\" , \"host_input\" ] or k == \"ssl\" and self . conn_info [ \"ssl_input\" ] is None ) }, ) self . _conn . autocommit ( True )", "title": "connect()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.set_query_cache", "text": "When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. Parameters: Name Type Description Default query_cache a string to initialize the hash for query results None Source code in datajoint/connection.py 246 247 248 249 250 251 252 253 254 255 def set_query_cache ( self , query_cache = None ): \"\"\" When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states. :param query_cache: a string to initialize the hash for query results \"\"\" self . _query_cache = query_cache", "title": "set_query_cache()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.purge_query_cache", "text": "Purges all query cache. Source code in datajoint/connection.py 257 258 259 260 261 262 263 264 265 def purge_query_cache ( self ): \"\"\"Purges all query cache.\"\"\" if ( isinstance ( config . get ( cache_key ), str ) and pathlib . Path ( config [ cache_key ]) . is_dir () ): for path in pathlib . Path ( config [ cache_key ]) . iterdir (): if not path . is_dir (): path . unlink ()", "title": "purge_query_cache()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.ping", "text": "Ping the connection or raises an exception if the connection is closed. Source code in datajoint/connection.py 274 275 276 def ping ( self ): \"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\" self . _conn . ping ( reconnect = False )", "title": "ping()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.is_connected", "text": "Return true if the object is connected to the database server.", "title": "is_connected"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.query", "text": "Execute the specified query and return the tuple generator (cursor). Parameters: Name Type Description Default query SQL query required args additional arguments for the client.cursor () as_dict If as_dict is set to True, the returned cursor objects returns query results as dictionary. False suppress_warnings If True, suppress all warnings arising from underlying query library True reconnect when None, get from config, when True, attempt to reconnect if disconnected None Source code in datajoint/connection.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def query ( self , query , args = (), * , as_dict = False , suppress_warnings = True , reconnect = None ): \"\"\" Execute the specified query and return the tuple generator (cursor). :param query: SQL query :param args: additional arguments for the client.cursor :param as_dict: If as_dict is set to True, the returned cursor objects returns query results as dictionary. :param suppress_warnings: If True, suppress all warnings arising from underlying query library :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected \"\"\" # check cache first: use_query_cache = bool ( self . _query_cache ) if use_query_cache and not re . match ( r \"\\s*(SELECT|SHOW)\" , query ): raise errors . DataJointError ( \"Only SELECT queries are allowed when query caching is on.\" ) if use_query_cache : if not config [ cache_key ]: raise errors . DataJointError ( f \"Provide filepath dj.config[' { cache_key } '] when using query caching.\" ) hash_ = uuid_from_buffer ( ( str ( self . _query_cache ) + re . sub ( r \"`\\$\\w+`\" , \"\" , query )) . encode () + pack ( args ) ) cache_path = pathlib . Path ( config [ cache_key ]) / str ( hash_ ) try : buffer = cache_path . read_bytes () except FileNotFoundError : pass # proceed to query the database else : return EmulatedCursor ( unpack ( buffer )) if reconnect is None : reconnect = config [ \"database.reconnect\" ] logger . debug ( \"Executing SQL:\" + query [: query_log_max_length ]) cursor_class = client . cursors . DictCursor if as_dict else client . cursors . Cursor cursor = self . _conn . cursor ( cursor = cursor_class ) try : self . _execute_query ( cursor , query , args , suppress_warnings ) except errors . LostConnectionError : if not reconnect : raise logger . warning ( \"MySQL server has gone away. Reconnecting to the server.\" ) connect_host_hook ( self ) if self . _in_transaction : self . cancel_transaction () raise errors . LostConnectionError ( \"Connection was lost during a transaction.\" ) logger . debug ( \"Re-executing\" ) cursor = self . _conn . cursor ( cursor = cursor_class ) self . _execute_query ( cursor , query , args , suppress_warnings ) if use_query_cache : data = cursor . fetchall () cache_path . write_bytes ( pack ( data )) return EmulatedCursor ( data ) return cursor", "title": "query()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.get_user", "text": "Returns: Type Description the user name and host name provided by the client to the server. Source code in datajoint/connection.py 362 363 364 365 366 def get_user ( self ): \"\"\" :return: the user name and host name provided by the client to the server. \"\"\" return self . query ( \"SELECT user()\" ) . fetchone ()[ 0 ]", "title": "get_user()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.in_transaction", "text": "Returns: Type Description True if there is an open transaction.", "title": "in_transaction"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.start_transaction", "text": "Starts a transaction error. Source code in datajoint/connection.py 377 378 379 380 381 382 383 384 385 def start_transaction ( self ): \"\"\" Starts a transaction error. \"\"\" if self . in_transaction : raise errors . DataJointError ( \"Nested connections are not supported.\" ) self . query ( \"START TRANSACTION WITH CONSISTENT SNAPSHOT\" ) self . _in_transaction = True logger . debug ( \"Transaction started\" )", "title": "start_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.cancel_transaction", "text": "Cancels the current transaction and rolls back all changes made during the transaction. Source code in datajoint/connection.py 387 388 389 390 391 392 393 def cancel_transaction ( self ): \"\"\" Cancels the current transaction and rolls back all changes made during the transaction. \"\"\" self . query ( \"ROLLBACK\" ) self . _in_transaction = False logger . debug ( \"Transaction cancelled. Rolling back ...\" )", "title": "cancel_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.commit_transaction", "text": "Commit all changes made during the transaction and close it. Source code in datajoint/connection.py 395 396 397 398 399 400 401 402 def commit_transaction ( self ): \"\"\" Commit all changes made during the transaction and close it. \"\"\" self . query ( \"COMMIT\" ) self . _in_transaction = False logger . debug ( \"Transaction committed and closed.\" )", "title": "commit_transaction()"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.transaction", "text": "Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again. Example: import datajoint as dj with dj.conn().transaction as conn: # transaction is open here", "title": "transaction"}, {"location": "api/datajoint/declare/", "text": "This module hosts functions to convert DataJoint table definitions into mysql table definitions, and to declare the corresponding mysql tables. is_foreign_key ( line ) \u00b6 Parameters: Name Type Description Default line a line from the table definition required Returns: Type Description true if the line appears to be a foreign key definition Source code in datajoint/declare.py 139 140 141 142 143 144 145 146 def is_foreign_key ( line ): \"\"\" :param line: a line from the table definition :return: true if the line appears to be a foreign key definition \"\"\" arrow_position = line . find ( \"->\" ) return arrow_position >= 0 and not any ( c in line [: arrow_position ] for c in \" \\\" #'\" ) compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ) \u00b6 Parameters: Name Type Description Default line a line from a table definition required context namespace containing referenced objects required attributes list of attribute names already in the declaration -- to be updated by this function required primary_key None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function required attr_sql list of sql statements defining attributes -- to be updated by this function. required foreign_key_sql list of sql statements specifying foreign key constraints -- to be updated by this function. required index_sql list of INDEX declaration statements, duplicate or redundant indexes are ok. required Source code in datajoint/declare.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ): \"\"\" :param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. \"\"\" # Parse and validate from .table import Table from .expression import QueryExpression try : result = foreign_key_parser . parseString ( line ) except pp . ParseException as err : raise DataJointError ( 'Parsing error in line \" %s \". %s .' % ( line , err )) try : ref = eval ( result . ref_table , context ) except Exception : raise DataJointError ( \"Foreign key reference %s could not be resolved\" % result . ref_table ) options = [ opt . upper () for opt in result . options ] for opt in options : # check for invalid options if opt not in { \"NULLABLE\" , \"UNIQUE\" }: raise DataJointError ( 'Invalid foreign key option \" {opt} \"' . format ( opt = opt )) is_nullable = \"NULLABLE\" in options is_unique = \"UNIQUE\" in options if is_nullable and primary_key is not None : raise DataJointError ( 'Primary dependencies cannot be nullable in line \" {line} \"' . format ( line = line ) ) if isinstance ( ref , type ) and issubclass ( ref , Table ): ref = ref () # check that dependency is of a supported type if ( not isinstance ( ref , QueryExpression ) or len ( ref . restriction ) or len ( ref . support ) != 1 or not isinstance ( ref . support [ 0 ], str ) ): raise DataJointError ( 'Dependency \" %s \" is not supported (yet). Use a base table or its projection.' % result . ref_table ) # declare new foreign key attributes for attr in ref . primary_key : if attr not in attributes : attributes . append ( attr ) if primary_key is not None : primary_key . append ( attr ) attr_sql . append ( ref . heading [ attr ] . sql . replace ( \"NOT NULL \" , \"\" , int ( is_nullable )) ) # declare the foreign key foreign_key_sql . append ( \"FOREIGN KEY (` {fk} `) REFERENCES {ref} (` {pk} `) ON UPDATE CASCADE ON DELETE RESTRICT\" . format ( fk = \"`,`\" . join ( ref . primary_key ), pk = \"`,`\" . join ( ref . heading [ name ] . original_name for name in ref . primary_key ), ref = ref . support [ 0 ], ) ) # declare unique index if is_unique : index_sql . append ( \"UNIQUE INDEX ( {attrs} )\" . format ( attrs = \",\" . join ( \"` %s `\" % attr for attr in ref . primary_key ) ) ) declare ( full_table_name , definition , context ) \u00b6 Parse declaration and generate the SQL CREATE TABLE code Parameters: Name Type Description Default full_table_name full name of the table required definition DataJoint table definition required context dictionary of objects that might be referred to in the table required Returns: Type Description SQL CREATE TABLE statement, list of external stores used Source code in datajoint/declare.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def declare ( full_table_name , definition , context ): \"\"\" Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used \"\"\" table_name = full_table_name . strip ( \"`\" ) . split ( \".\" )[ 1 ] if len ( table_name ) > MAX_TABLE_NAME_LENGTH : raise DataJointError ( \"Table name ` {name} ` exceeds the max length of {max_length} \" . format ( name = table_name , max_length = MAX_TABLE_NAME_LENGTH ) ) ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) if not primary_key : raise DataJointError ( \"Table must have a primary key\" ) return ( \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % full_table_name + \", \\n \" . join ( attribute_sql + [ \"PRIMARY KEY (`\" + \"`,`\" . join ( primary_key ) + \"`)\" ] + foreign_key_sql + index_sql ) + ' \\n ) ENGINE=InnoDB, COMMENT \" %s \"' % table_comment ), external_stores alter ( definition , old_definition , context ) \u00b6 Parameters: Name Type Description Default definition new table definition required old_definition current table definition required context the context in which to evaluate foreign key definitions required Returns: Type Description string SQL ALTER command, list of new stores used for external storage Source code in datajoint/declare.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def alter ( definition , old_definition , context ): \"\"\" :param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage \"\"\" ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) ( table_comment_ , primary_key_ , attribute_sql_ , foreign_key_sql_ , index_sql_ , external_stores_ , ) = prepare_declare ( old_definition , context ) # analyze differences between declarations sql = list () if primary_key != primary_key_ : raise NotImplementedError ( \"table.alter cannot alter the primary key (yet).\" ) if foreign_key_sql != foreign_key_sql_ : raise NotImplementedError ( \"table.alter cannot alter foreign keys (yet).\" ) if index_sql != index_sql_ : raise NotImplementedError ( \"table.alter cannot alter indexes (yet)\" ) if attribute_sql != attribute_sql_ : sql . extend ( _make_attribute_alter ( attribute_sql , attribute_sql_ , primary_key )) if table_comment != table_comment_ : sql . append ( 'COMMENT=\" %s \"' % table_comment ) return sql , [ e for e in external_stores if e not in external_stores_ ] substitute_special_type ( match , category , foreign_key_sql , context ) \u00b6 Parameters: Name Type Description Default match dict containing with keys \"type\" and \"comment\" -- will be modified in place required category attribute type category from TYPE_PATTERN required foreign_key_sql list of foreign key declarations to add to required context context for looking up user-defined attribute_type adapters required Source code in datajoint/declare.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def substitute_special_type ( match , category , foreign_key_sql , context ): \"\"\" :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters \"\"\" if category == \"UUID\" : match [ \"type\" ] = UUID_DATA_TYPE elif category == \"INTERNAL_ATTACH\" : match [ \"type\" ] = \"LONGBLOB\" elif category in EXTERNAL_TYPES : if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) match [ \"store\" ] = match [ \"type\" ] . split ( \"@\" , 1 )[ 1 ] match [ \"type\" ] = UUID_DATA_TYPE foreign_key_sql . append ( \"FOREIGN KEY (` {name} `) REFERENCES `{{database}}`.` {external_table_root} _ {store} ` (`hash`) \" \"ON UPDATE RESTRICT ON DELETE RESTRICT\" . format ( external_table_root = EXTERNAL_TABLE_ROOT , ** match ) ) elif category == \"ADAPTED\" : adapter = get_adapter ( context , match [ \"type\" ]) match [ \"type\" ] = adapter . attribute_type category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : # recursive redefinition from user-defined datatypes. substitute_special_type ( match , category , foreign_key_sql , context ) else : assert False , \"Unknown special type\" compile_attribute ( line , in_key , foreign_key_sql , context ) \u00b6 Convert attribute definition from DataJoint format to SQL Parameters: Name Type Description Default line attribution line required in_key set to True if attribute is in primary key set required foreign_key_sql the list of foreign key declarations to add to required context context in which to look up user-defined attribute type adapterss required Returns: Type Description (name, sql, is_external) -- attribute name and sql code for its declaration Source code in datajoint/declare.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def compile_attribute ( line , in_key , foreign_key_sql , context ): \"\"\" Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration \"\"\" try : match = attribute_parser . parseString ( line + \"#\" , parseAll = True ) except pp . ParseException as err : raise DataJointError ( \"Declaration error in position {pos} in line: \\n {line} \\n {msg} \" . format ( line = err . args [ 0 ], pos = err . args [ 1 ], msg = err . args [ 2 ] ) ) match [ \"comment\" ] = match [ \"comment\" ] . rstrip ( \"#\" ) if \"default\" not in match : match [ \"default\" ] = \"\" match = { k : v . strip () for k , v in match . items ()} match [ \"nullable\" ] = match [ \"default\" ] . lower () == \"null\" if match [ \"nullable\" ]: if in_key : raise DataJointError ( 'Primary key attributes cannot be nullable in line \" %s \"' % line ) match [ \"default\" ] = \"DEFAULT NULL\" # nullable attributes default to null else : if match [ \"default\" ]: quote = ( match [ \"default\" ] . split ( \"(\" )[ 0 ] . upper () not in CONSTANT_LITERALS and match [ \"default\" ][ 0 ] not in \" \\\" '\" ) match [ \"default\" ] = ( \"NOT NULL DEFAULT \" + ( '\" %s \"' if quote else \" %s \" ) % match [ \"default\" ] ) else : match [ \"default\" ] = \"NOT NULL\" match [ \"comment\" ] = match [ \"comment\" ] . replace ( '\"' , ' \\\\ \"' ) # escape double quotes in comment if match [ \"comment\" ] . startswith ( \":\" ): raise DataJointError ( 'An attribute comment must not start with a colon in comment \" {comment} \"' . format ( ** match ) ) category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : match [ \"comment\" ] = \": {type} : {comment} \" . format ( ** match ) # insert custom type into comment substitute_special_type ( match , category , foreign_key_sql , context ) if category in SERIALIZED_TYPES and match [ \"default\" ] not in { \"DEFAULT NULL\" , \"NOT NULL\" , }: raise DataJointError ( \"The default value for a blob or attachment attributes can only be NULL in: \\n {line} \" . format ( line = line ) ) sql = ( \"` {name} ` {type} {default} \" + ( ' COMMENT \" {comment} \"' if match [ \"comment\" ] else \"\" ) ) . format ( ** match ) return match [ \"name\" ], sql , match . get ( \"store\" )", "title": "declare.py"}, {"location": "api/datajoint/declare/#datajoint.declare.is_foreign_key", "text": "Parameters: Name Type Description Default line a line from the table definition required Returns: Type Description true if the line appears to be a foreign key definition Source code in datajoint/declare.py 139 140 141 142 143 144 145 146 def is_foreign_key ( line ): \"\"\" :param line: a line from the table definition :return: true if the line appears to be a foreign key definition \"\"\" arrow_position = line . find ( \"->\" ) return arrow_position >= 0 and not any ( c in line [: arrow_position ] for c in \" \\\" #'\" )", "title": "is_foreign_key()"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_foreign_key", "text": "Parameters: Name Type Description Default line a line from a table definition required context namespace containing referenced objects required attributes list of attribute names already in the declaration -- to be updated by this function required primary_key None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function required attr_sql list of sql statements defining attributes -- to be updated by this function. required foreign_key_sql list of sql statements specifying foreign key constraints -- to be updated by this function. required index_sql list of INDEX declaration statements, duplicate or redundant indexes are ok. required Source code in datajoint/declare.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def compile_foreign_key ( line , context , attributes , primary_key , attr_sql , foreign_key_sql , index_sql ): \"\"\" :param line: a line from a table definition :param context: namespace containing referenced objects :param attributes: list of attribute names already in the declaration -- to be updated by this function :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function :param attr_sql: list of sql statements defining attributes -- to be updated by this function. :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function. :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok. \"\"\" # Parse and validate from .table import Table from .expression import QueryExpression try : result = foreign_key_parser . parseString ( line ) except pp . ParseException as err : raise DataJointError ( 'Parsing error in line \" %s \". %s .' % ( line , err )) try : ref = eval ( result . ref_table , context ) except Exception : raise DataJointError ( \"Foreign key reference %s could not be resolved\" % result . ref_table ) options = [ opt . upper () for opt in result . options ] for opt in options : # check for invalid options if opt not in { \"NULLABLE\" , \"UNIQUE\" }: raise DataJointError ( 'Invalid foreign key option \" {opt} \"' . format ( opt = opt )) is_nullable = \"NULLABLE\" in options is_unique = \"UNIQUE\" in options if is_nullable and primary_key is not None : raise DataJointError ( 'Primary dependencies cannot be nullable in line \" {line} \"' . format ( line = line ) ) if isinstance ( ref , type ) and issubclass ( ref , Table ): ref = ref () # check that dependency is of a supported type if ( not isinstance ( ref , QueryExpression ) or len ( ref . restriction ) or len ( ref . support ) != 1 or not isinstance ( ref . support [ 0 ], str ) ): raise DataJointError ( 'Dependency \" %s \" is not supported (yet). Use a base table or its projection.' % result . ref_table ) # declare new foreign key attributes for attr in ref . primary_key : if attr not in attributes : attributes . append ( attr ) if primary_key is not None : primary_key . append ( attr ) attr_sql . append ( ref . heading [ attr ] . sql . replace ( \"NOT NULL \" , \"\" , int ( is_nullable )) ) # declare the foreign key foreign_key_sql . append ( \"FOREIGN KEY (` {fk} `) REFERENCES {ref} (` {pk} `) ON UPDATE CASCADE ON DELETE RESTRICT\" . format ( fk = \"`,`\" . join ( ref . primary_key ), pk = \"`,`\" . join ( ref . heading [ name ] . original_name for name in ref . primary_key ), ref = ref . support [ 0 ], ) ) # declare unique index if is_unique : index_sql . append ( \"UNIQUE INDEX ( {attrs} )\" . format ( attrs = \",\" . join ( \"` %s `\" % attr for attr in ref . primary_key ) ) )", "title": "compile_foreign_key()"}, {"location": "api/datajoint/declare/#datajoint.declare.declare", "text": "Parse declaration and generate the SQL CREATE TABLE code Parameters: Name Type Description Default full_table_name full name of the table required definition DataJoint table definition required context dictionary of objects that might be referred to in the table required Returns: Type Description SQL CREATE TABLE statement, list of external stores used Source code in datajoint/declare.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def declare ( full_table_name , definition , context ): \"\"\" Parse declaration and generate the SQL CREATE TABLE code :param full_table_name: full name of the table :param definition: DataJoint table definition :param context: dictionary of objects that might be referred to in the table :return: SQL CREATE TABLE statement, list of external stores used \"\"\" table_name = full_table_name . strip ( \"`\" ) . split ( \".\" )[ 1 ] if len ( table_name ) > MAX_TABLE_NAME_LENGTH : raise DataJointError ( \"Table name ` {name} ` exceeds the max length of {max_length} \" . format ( name = table_name , max_length = MAX_TABLE_NAME_LENGTH ) ) ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) if not primary_key : raise DataJointError ( \"Table must have a primary key\" ) return ( \"CREATE TABLE IF NOT EXISTS %s ( \\n \" % full_table_name + \", \\n \" . join ( attribute_sql + [ \"PRIMARY KEY (`\" + \"`,`\" . join ( primary_key ) + \"`)\" ] + foreign_key_sql + index_sql ) + ' \\n ) ENGINE=InnoDB, COMMENT \" %s \"' % table_comment ), external_stores", "title": "declare()"}, {"location": "api/datajoint/declare/#datajoint.declare.alter", "text": "Parameters: Name Type Description Default definition new table definition required old_definition current table definition required context the context in which to evaluate foreign key definitions required Returns: Type Description string SQL ALTER command, list of new stores used for external storage Source code in datajoint/declare.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def alter ( definition , old_definition , context ): \"\"\" :param definition: new table definition :param old_definition: current table definition :param context: the context in which to evaluate foreign key definitions :return: string SQL ALTER command, list of new stores used for external storage \"\"\" ( table_comment , primary_key , attribute_sql , foreign_key_sql , index_sql , external_stores , ) = prepare_declare ( definition , context ) ( table_comment_ , primary_key_ , attribute_sql_ , foreign_key_sql_ , index_sql_ , external_stores_ , ) = prepare_declare ( old_definition , context ) # analyze differences between declarations sql = list () if primary_key != primary_key_ : raise NotImplementedError ( \"table.alter cannot alter the primary key (yet).\" ) if foreign_key_sql != foreign_key_sql_ : raise NotImplementedError ( \"table.alter cannot alter foreign keys (yet).\" ) if index_sql != index_sql_ : raise NotImplementedError ( \"table.alter cannot alter indexes (yet)\" ) if attribute_sql != attribute_sql_ : sql . extend ( _make_attribute_alter ( attribute_sql , attribute_sql_ , primary_key )) if table_comment != table_comment_ : sql . append ( 'COMMENT=\" %s \"' % table_comment ) return sql , [ e for e in external_stores if e not in external_stores_ ]", "title": "alter()"}, {"location": "api/datajoint/declare/#datajoint.declare.substitute_special_type", "text": "Parameters: Name Type Description Default match dict containing with keys \"type\" and \"comment\" -- will be modified in place required category attribute type category from TYPE_PATTERN required foreign_key_sql list of foreign key declarations to add to required context context for looking up user-defined attribute_type adapters required Source code in datajoint/declare.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def substitute_special_type ( match , category , foreign_key_sql , context ): \"\"\" :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place :param category: attribute type category from TYPE_PATTERN :param foreign_key_sql: list of foreign key declarations to add to :param context: context for looking up user-defined attribute_type adapters \"\"\" if category == \"UUID\" : match [ \"type\" ] = UUID_DATA_TYPE elif category == \"INTERNAL_ATTACH\" : match [ \"type\" ] = \"LONGBLOB\" elif category in EXTERNAL_TYPES : if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) match [ \"store\" ] = match [ \"type\" ] . split ( \"@\" , 1 )[ 1 ] match [ \"type\" ] = UUID_DATA_TYPE foreign_key_sql . append ( \"FOREIGN KEY (` {name} `) REFERENCES `{{database}}`.` {external_table_root} _ {store} ` (`hash`) \" \"ON UPDATE RESTRICT ON DELETE RESTRICT\" . format ( external_table_root = EXTERNAL_TABLE_ROOT , ** match ) ) elif category == \"ADAPTED\" : adapter = get_adapter ( context , match [ \"type\" ]) match [ \"type\" ] = adapter . attribute_type category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : # recursive redefinition from user-defined datatypes. substitute_special_type ( match , category , foreign_key_sql , context ) else : assert False , \"Unknown special type\"", "title": "substitute_special_type()"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_attribute", "text": "Convert attribute definition from DataJoint format to SQL Parameters: Name Type Description Default line attribution line required in_key set to True if attribute is in primary key set required foreign_key_sql the list of foreign key declarations to add to required context context in which to look up user-defined attribute type adapterss required Returns: Type Description (name, sql, is_external) -- attribute name and sql code for its declaration Source code in datajoint/declare.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def compile_attribute ( line , in_key , foreign_key_sql , context ): \"\"\" Convert attribute definition from DataJoint format to SQL :param line: attribution line :param in_key: set to True if attribute is in primary key set :param foreign_key_sql: the list of foreign key declarations to add to :param context: context in which to look up user-defined attribute type adapterss :returns: (name, sql, is_external) -- attribute name and sql code for its declaration \"\"\" try : match = attribute_parser . parseString ( line + \"#\" , parseAll = True ) except pp . ParseException as err : raise DataJointError ( \"Declaration error in position {pos} in line: \\n {line} \\n {msg} \" . format ( line = err . args [ 0 ], pos = err . args [ 1 ], msg = err . args [ 2 ] ) ) match [ \"comment\" ] = match [ \"comment\" ] . rstrip ( \"#\" ) if \"default\" not in match : match [ \"default\" ] = \"\" match = { k : v . strip () for k , v in match . items ()} match [ \"nullable\" ] = match [ \"default\" ] . lower () == \"null\" if match [ \"nullable\" ]: if in_key : raise DataJointError ( 'Primary key attributes cannot be nullable in line \" %s \"' % line ) match [ \"default\" ] = \"DEFAULT NULL\" # nullable attributes default to null else : if match [ \"default\" ]: quote = ( match [ \"default\" ] . split ( \"(\" )[ 0 ] . upper () not in CONSTANT_LITERALS and match [ \"default\" ][ 0 ] not in \" \\\" '\" ) match [ \"default\" ] = ( \"NOT NULL DEFAULT \" + ( '\" %s \"' if quote else \" %s \" ) % match [ \"default\" ] ) else : match [ \"default\" ] = \"NOT NULL\" match [ \"comment\" ] = match [ \"comment\" ] . replace ( '\"' , ' \\\\ \"' ) # escape double quotes in comment if match [ \"comment\" ] . startswith ( \":\" ): raise DataJointError ( 'An attribute comment must not start with a colon in comment \" {comment} \"' . format ( ** match ) ) category = match_type ( match [ \"type\" ]) if category in SPECIAL_TYPES : match [ \"comment\" ] = \": {type} : {comment} \" . format ( ** match ) # insert custom type into comment substitute_special_type ( match , category , foreign_key_sql , context ) if category in SERIALIZED_TYPES and match [ \"default\" ] not in { \"DEFAULT NULL\" , \"NOT NULL\" , }: raise DataJointError ( \"The default value for a blob or attachment attributes can only be NULL in: \\n {line} \" . format ( line = line ) ) sql = ( \"` {name} ` {type} {default} \" + ( ' COMMENT \" {comment} \"' if match [ \"comment\" ] else \"\" ) ) . format ( ** match ) return match [ \"name\" ], sql , match . get ( \"store\" )", "title": "compile_attribute()"}, {"location": "api/datajoint/dependencies/", "text": "unite_master_parts ( lst ) \u00b6 re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( [' s . a ', ' s . a__q ', ' s . b ', ' s . c ', ' s . c__q ', ' s . b__q ', ' s . d ', ' s . a__r ']) -> [' s . a ', ' s . a__q ', ' s . a__r ', ' s . b ', ' s . b__q ', ' s . c ', ' s . c__q ', ' s . d '] Source code in datajoint/dependencies.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def unite_master_parts ( lst ): \"\"\" re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -> ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`'] \"\"\" for i in range ( 2 , len ( lst )): name = lst [ i ] match = re . match ( r \"(?P<master>`\\w+`.`#?\\w+)__\\w+`\" , name ) if match : # name is a part table master = match . group ( \"master\" ) for j in range ( i - 1 , - 1 , - 1 ): if lst [ j ] == master + \"`\" or lst [ j ] . startswith ( master + \"__\" ): # move from the ith position to the (j+1)th position lst [ j + 1 : i + 1 ] = [ name ] + lst [ j + 1 : i ] break return lst Dependencies \u00b6 Bases: nx . DiGraph The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 Source code in datajoint/dependencies.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Dependencies ( nx . DiGraph ): \"\"\" The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 \"\"\" def __init__ ( self , connection = None ): self . _conn = connection self . _node_alias_count = itertools . count () self . _loaded = False super () . __init__ ( self ) def clear ( self ): self . _loaded = False super () . clear () def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) ) load ( force = True ) \u00b6 Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. Source code in datajoint/dependencies.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True parents ( table_name , primary = None ) \u00b6 Parameters: Name Type Description Default table_name schema . table required primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. None Returns: Type Description dict of tables referenced by the foreign keys of table Source code in datajoint/dependencies.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } children ( table_name , primary = None ) \u00b6 Parameters: Name Type Description Default table_name schema . table required primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. None Returns: Type Description dict of tables referencing the table through foreign keys Source code in datajoint/dependencies.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } descendants ( full_table_name ) \u00b6 Parameters: Name Type Description Default full_table_name In form schema . table_name required Returns: Type Description all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 164 165 166 167 168 169 170 171 172 173 def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) ancestors ( full_table_name ) \u00b6 Parameters: Name Type Description Default full_table_name In form schema . table_name required Returns: Type Description all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) )", "title": "dependencies.py"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.unite_master_parts", "text": "re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( [' s . a ', ' s . a__q ', ' s . b ', ' s . c ', ' s . c__q ', ' s . b__q ', ' s . d ', ' s . a__r ']) -> [' s . a ', ' s . a__q ', ' s . a__r ', ' s . b ', ' s . b__q ', ' s . c ', ' s . c__q ', ' s . d '] Source code in datajoint/dependencies.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def unite_master_parts ( lst ): \"\"\" re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts( ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -> ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`'] \"\"\" for i in range ( 2 , len ( lst )): name = lst [ i ] match = re . match ( r \"(?P<master>`\\w+`.`#?\\w+)__\\w+`\" , name ) if match : # name is a part table master = match . group ( \"master\" ) for j in range ( i - 1 , - 1 , - 1 ): if lst [ j ] == master + \"`\" or lst [ j ] . startswith ( master + \"__\" ): # move from the ith position to the (j+1)th position lst [ j + 1 : i + 1 ] = [ name ] + lst [ j + 1 : i ] break return lst", "title": "unite_master_parts()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies", "text": "Bases: nx . DiGraph The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 Source code in datajoint/dependencies.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class Dependencies ( nx . DiGraph ): \"\"\" The graph of dependencies (foreign keys) between loaded tables. Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443 \"\"\" def __init__ ( self , connection = None ): self . _conn = connection self . _node_alias_count = itertools . count () self . _loaded = False super () . __init__ ( self ) def clear ( self ): self . _loaded = False super () . clear () def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary } def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) ) def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) )", "title": "Dependencies"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.load", "text": "Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. Source code in datajoint/dependencies.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def load ( self , force = True ): \"\"\" Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress. \"\"\" # reload from scratch to prevent duplication of renamed edges if self . _loaded and not force : return self . clear () # load primary key info keys = self . _conn . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as tab, column_name FROM information_schema.key_column_usage WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\" \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ) ) pks = defaultdict ( set ) for key in keys : pks [ key [ 0 ]] . add ( key [ 1 ]) # add nodes to the graph for n , pk in pks . items (): self . add_node ( n , primary_key = pk ) # load foreign keys keys = ( { k . lower (): v for k , v in elem . items ()} for elem in self . _conn . query ( \"\"\" SELECT constraint_name, concat('`', table_schema, '`.`', table_name, '`') as referencing_table, concat('`', referenced_table_schema, '`.`', referenced_table_name, '`') as referenced_table, column_name, referenced_column_name FROM information_schema.key_column_usage WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR referenced_table_schema is not NULL AND table_schema in ('{schemas}')) \"\"\" . format ( schemas = \"','\" . join ( self . _conn . schemas ) ), as_dict = True , ) ) fks = defaultdict ( lambda : dict ( attr_map = dict ())) for key in keys : d = fks [ ( key [ \"constraint_name\" ], key [ \"referencing_table\" ], key [ \"referenced_table\" ], ) ] d [ \"referencing_table\" ] = key [ \"referencing_table\" ] d [ \"referenced_table\" ] = key [ \"referenced_table\" ] d [ \"attr_map\" ][ key [ \"column_name\" ]] = key [ \"referenced_column_name\" ] # add edges to the graph for fk in fks . values (): props = dict ( primary = set ( fk [ \"attr_map\" ]) <= set ( pks [ fk [ \"referencing_table\" ]]), attr_map = fk [ \"attr_map\" ], aliased = any ( k != v for k , v in fk [ \"attr_map\" ] . items ()), multi = set ( fk [ \"attr_map\" ]) != set ( pks [ fk [ \"referencing_table\" ]]), ) if not props [ \"aliased\" ]: self . add_edge ( fk [ \"referenced_table\" ], fk [ \"referencing_table\" ], ** props ) else : # for aliased dependencies, add an extra node in the format '1', '2', etc alias_node = \" %d \" % next ( self . _node_alias_count ) self . add_node ( alias_node ) self . add_edge ( fk [ \"referenced_table\" ], alias_node , ** props ) self . add_edge ( alias_node , fk [ \"referencing_table\" ], ** props ) if not nx . is_directed_acyclic_graph ( self ): # pragma: no cover raise DataJointError ( \"DataJoint can only work with acyclic dependencies\" ) self . _loaded = True", "title": "load()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.parents", "text": "Parameters: Name Type Description Default table_name schema . table required primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. None Returns: Type Description dict of tables referenced by the foreign keys of table Source code in datajoint/dependencies.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def parents ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referenced by the foreign keys of table \"\"\" self . load ( force = False ) return { p [ 0 ]: p [ 2 ] for p in self . in_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary }", "title": "parents()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.children", "text": "Parameters: Name Type Description Default table_name schema . table required primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. None Returns: Type Description dict of tables referencing the table through foreign keys Source code in datajoint/dependencies.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def children ( self , table_name , primary = None ): \"\"\" :param table_name: `schema`.`table` :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, the only foreign keys including at least one non-primary attribute are considered. :return: dict of tables referencing the table through foreign keys \"\"\" self . load ( force = False ) return { p [ 1 ]: p [ 2 ] for p in self . out_edges ( table_name , data = True ) if primary is None or p [ 2 ][ \"primary\" ] == primary }", "title": "children()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.descendants", "text": "Parameters: Name Type Description Default full_table_name In form schema . table_name required Returns: Type Description all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 164 165 166 167 168 169 170 171 172 173 def descendants ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . descendants ( self , full_table_name )) return unite_master_parts ( [ full_table_name ] + list ( nx . algorithms . dag . topological_sort ( nodes )) )", "title": "descendants()"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.ancestors", "text": "Parameters: Name Type Description Default full_table_name In form schema . table_name required Returns: Type Description all dependent tables sorted in topological order. Self is included. Source code in datajoint/dependencies.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def ancestors ( self , full_table_name ): \"\"\" :param full_table_name: In form `schema`.`table_name` :return: all dependent tables sorted in topological order. Self is included. \"\"\" self . load ( force = False ) nodes = self . subgraph ( nx . algorithms . dag . ancestors ( self , full_table_name )) return list ( reversed ( unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nodes )) + [ full_table_name ] ) ) )", "title": "ancestors()"}, {"location": "api/datajoint/diagram/", "text": "Diagram \u00b6 Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = cls () . describe ( context = self . context ) . split ( \" \\n \" ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs ) from_sequence ( sequence ) classmethod \u00b6 The join Diagram for all objects in sequence Parameters: Name Type Description Default sequence a sequence (e.g. list, tuple) required Returns: Type Description Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 145 146 147 148 149 150 151 152 153 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) add_parts () \u00b6 Adds to the diagram the part tables of tables already included in the diagram Returns: Type Description Source code in datajoint/diagram.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self topological_sort () \u00b6 Returns: Type Description list of nodes in topological order Source code in datajoint/diagram.py 182 183 184 185 186 187 188 189 190 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "diagram.py"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram", "text": "Bases: nx . DiGraph Entity relationship diagram. Usage: diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed Source code in datajoint/diagram.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 class Diagram ( nx . DiGraph ): \"\"\" Entity relationship diagram. Usage: >>> diag = Diagram(source) source can be a base table object, a base table class, a schema, or a module that has a schema. >>> diag.draw() draws the diagram using pyplot diag1 + diag2 - combines the two diagrams. diag + n - expands n levels of successors diag - n - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table Note that diagram + 1 - 1 may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed \"\"\" def __init__ ( self , source , context = None ): if isinstance ( source , Diagram ): # copy constructor self . nodes_to_show = set ( source . nodes_to_show ) self . context = source . context super () . __init__ ( source ) return # get the caller's context if context is None : frame = inspect . currentframe () . f_back self . context = dict ( frame . f_globals , ** frame . f_locals ) del frame else : self . context = context # find connection in the source try : connection = source . connection except AttributeError : try : connection = source . schema . connection except AttributeError : raise DataJointError ( \"Could not find database connection in %s \" % repr ( source [ 0 ]) ) # initialize graph from dependencies connection . dependencies . load () super () . __init__ ( connection . dependencies ) # Enumerate nodes from all the items in the list self . nodes_to_show = set () try : self . nodes_to_show . add ( source . full_table_name ) except AttributeError : try : database = source . database except AttributeError : try : database = source . schema . database except AttributeError : raise DataJointError ( \"Cannot plot Diagram for %s \" % repr ( source ) ) for node in self : if node . startswith ( \"` %s `\" % database ): self . nodes_to_show . add ( node ) @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence )) def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) ) def __add__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Union of the diagrams when arg is another Diagram or an expansion downstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . add ( arg . full_table_name ) except AttributeError : for i in range ( arg ): new = nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( self , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __sub__ ( self , arg ): \"\"\" :param arg: either another Diagram or a positive integer. :return: Difference of the diagrams when arg is another Diagram or an expansion upstream when arg is a positive integer. \"\"\" self = Diagram ( self ) # copy try : self . nodes_to_show . difference_update ( arg . nodes_to_show ) except AttributeError : try : self . nodes_to_show . remove ( arg . full_table_name ) except AttributeError : for i in range ( arg ): graph = nx . DiGraph ( self ) . reverse () new = nx . algorithms . boundary . node_boundary ( graph , self . nodes_to_show ) if not new : break # add nodes referenced by aliased nodes new . update ( nx . algorithms . boundary . node_boundary ( graph , ( a for a in new if a . isdigit ()) ) ) self . nodes_to_show . update ( new ) return self def __mul__ ( self , arg ): \"\"\" Intersection of two diagrams :param arg: another Diagram :return: a new Diagram comprising nodes that are present in both operands. \"\"\" self = Diagram ( self ) # copy self . nodes_to_show . intersection_update ( arg . nodes_to_show ) return self def _make_graph ( self ): \"\"\" Make the self.graph - a graph object ready for drawing \"\"\" # mark \"distinguished\" tables, i.e. those that introduce new primary key # attributes for name in self . nodes_to_show : foreign_attributes = set ( attr for p in self . in_edges ( name , data = True ) for attr in p [ 2 ][ \"attr_map\" ] if p [ 2 ][ \"primary\" ] ) self . nodes [ name ][ \"distinguished\" ] = ( \"primary_key\" in self . nodes [ name ] and foreign_attributes < self . nodes [ name ][ \"primary_key\" ] ) # include aliased nodes that are sandwiched between two displayed nodes gaps = set ( nx . algorithms . boundary . node_boundary ( self , self . nodes_to_show ) ) . intersection ( nx . algorithms . boundary . node_boundary ( nx . DiGraph ( self ) . reverse (), self . nodes_to_show ) ) nodes = self . nodes_to_show . union ( a for a in gaps if a . isdigit ) # construct subgraph and rename nodes to class names graph = nx . DiGraph ( nx . DiGraph ( self ) . subgraph ( nodes )) nx . set_node_attributes ( graph , name = \"node_type\" , values = { n : _get_tier ( n ) for n in graph } ) # relabel nodes to class names mapping = { node : lookup_class_name ( node , self . context ) or node for node in graph . nodes () } new_names = [ mapping . values ()] if len ( new_names ) > len ( set ( new_names )): raise DataJointError ( \"Some classes have identical names. The Diagram cannot be plotted.\" ) nx . relabel_nodes ( graph , mapping , copy = False ) return graph def make_dot ( self ): graph = self . _make_graph () graph . nodes () scale = 1.2 # scaling factor for fonts and boxes label_props = { # http://matplotlib.org/examples/color/named_colors.html None : dict ( shape = \"circle\" , color = \"#FFFF0040\" , fontcolor = \"yellow\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), _AliasNode : dict ( shape = \"circle\" , color = \"#FF880080\" , fontcolor = \"#FF880080\" , fontsize = round ( scale * 0 ), size = 0.05 * scale , fixed = True , ), Manual : dict ( shape = \"box\" , color = \"#00FF0030\" , fontcolor = \"darkgreen\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Lookup : dict ( shape = \"plaintext\" , color = \"#00000020\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.4 * scale , fixed = False , ), Computed : dict ( shape = \"ellipse\" , color = \"#FF000020\" , fontcolor = \"#7F0000A0\" , fontsize = round ( scale * 10 ), size = 0.3 * scale , fixed = True , ), Imported : dict ( shape = \"ellipse\" , color = \"#00007F40\" , fontcolor = \"#00007FA0\" , fontsize = round ( scale * 10 ), size = 0.4 * scale , fixed = False , ), Part : dict ( shape = \"plaintext\" , color = \"#0000000\" , fontcolor = \"black\" , fontsize = round ( scale * 8 ), size = 0.1 * scale , fixed = False , ), } node_props = { node : label_props [ d [ \"node_type\" ]] for node , d in dict ( graph . nodes ( data = True )) . items () } dot = nx . drawing . nx_pydot . to_pydot ( graph ) for node in dot . get_nodes (): node . set_shape ( \"circle\" ) name = node . get_name () . strip ( '\"' ) props = node_props [ name ] node . set_fontsize ( props [ \"fontsize\" ]) node . set_fontcolor ( props [ \"fontcolor\" ]) node . set_shape ( props [ \"shape\" ]) node . set_fontname ( \"arial\" ) node . set_fixedsize ( \"shape\" if props [ \"fixed\" ] else False ) node . set_width ( props [ \"size\" ]) node . set_height ( props [ \"size\" ]) if name . split ( \".\" )[ 0 ] in self . context : cls = eval ( name , self . context ) assert issubclass ( cls , Table ) description = cls () . describe ( context = self . context ) . split ( \" \\n \" ) description = ( \"-\" * 30 if q . startswith ( \"---\" ) else q . replace ( \"->\" , \"&#8594;\" ) if \"->\" in q else q . split ( \":\" )[ 0 ] for q in description if not q . startswith ( \"#\" ) ) node . set_tooltip ( \"&#13;\" . join ( description )) node . set_label ( \"<<u>\" + name + \"</u>>\" if node . get ( \"distinguished\" ) == \"True\" else name ) node . set_color ( props [ \"color\" ]) node . set_style ( \"filled\" ) for edge in dot . get_edges (): # see https://graphviz.org/doc/info/attrs.html src = edge . get_source () . strip ( '\"' ) dest = edge . get_destination () . strip ( '\"' ) props = graph . get_edge_data ( src , dest ) edge . set_color ( \"#00000040\" ) edge . set_style ( \"solid\" if props [ \"primary\" ] else \"dashed\" ) master_part = graph . nodes [ dest ][ \"node_type\" ] is Part and dest . startswith ( src + \".\" ) edge . set_weight ( 3 if master_part else 1 ) edge . set_arrowhead ( \"none\" ) edge . set_penwidth ( 0.75 if props [ \"multi\" ] else 2 ) return dot def make_svg ( self ): from IPython.display import SVG return SVG ( self . make_dot () . create_svg ()) def make_png ( self ): return io . BytesIO ( self . make_dot () . create_png ()) def make_image ( self ): if plot_active : return plt . imread ( self . make_png ()) else : raise DataJointError ( \"pyplot was not imported\" ) def _repr_svg_ ( self ): return self . make_svg () . _repr_svg_ () def draw ( self ): if plot_active : plt . imshow ( self . make_image ()) plt . gca () . axis ( \"off\" ) plt . show () else : raise DataJointError ( \"pyplot was not imported\" ) def save ( self , filename , format = None ): if format is None : if filename . lower () . endswith ( \".png\" ): format = \"png\" elif filename . lower () . endswith ( \".svg\" ): format = \"svg\" if format . lower () == \"png\" : with open ( filename , \"wb\" ) as f : f . write ( self . make_png () . getbuffer () . tobytes ()) elif format . lower () == \"svg\" : with open ( filename , \"w\" ) as f : f . write ( self . make_svg () . data ) else : raise DataJointError ( \"Unsupported file format\" ) @staticmethod def _layout ( graph , ** kwargs ): return pydot_layout ( graph , prog = \"dot\" , ** kwargs )", "title": "Diagram"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.from_sequence", "text": "The join Diagram for all objects in sequence Parameters: Name Type Description Default sequence a sequence (e.g. list, tuple) required Returns: Type Description Diagram(arg1) + ... + Diagram(argn) Source code in datajoint/diagram.py 145 146 147 148 149 150 151 152 153 @classmethod def from_sequence ( cls , sequence ): \"\"\" The join Diagram for all objects in sequence :param sequence: a sequence (e.g. list, tuple) :return: Diagram(arg1) + ... + Diagram(argn) \"\"\" return functools . reduce ( lambda x , y : x + y , map ( Diagram , sequence ))", "title": "from_sequence()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.add_parts", "text": "Adds to the diagram the part tables of tables already included in the diagram Returns: Type Description Source code in datajoint/diagram.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def add_parts ( self ): \"\"\" Adds to the diagram the part tables of tables already included in the diagram :return: \"\"\" def is_part ( part , master ): \"\"\" :param part: `database`.`table_name` :param master: `database`.`table_name` :return: True if part is part of master. \"\"\" part = [ s . strip ( \"`\" ) for s in part . split ( \".\" )] master = [ s . strip ( \"`\" ) for s in master . split ( \".\" )] return ( master [ 0 ] == part [ 0 ] and master [ 1 ] + \"__\" == part [ 1 ][: len ( master [ 1 ]) + 2 ] ) self = Diagram ( self ) # copy self . nodes_to_show . update ( n for n in self . nodes () if any ( is_part ( n , m ) for m in self . nodes_to_show ) ) return self", "title": "add_parts()"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.topological_sort", "text": "Returns: Type Description list of nodes in topological order Source code in datajoint/diagram.py 182 183 184 185 186 187 188 189 190 def topological_sort ( self ): \"\"\":return: list of nodes in topological order\"\"\" return unite_master_parts ( list ( nx . algorithms . dag . topological_sort ( nx . DiGraph ( self ) . subgraph ( self . nodes_to_show ) ) ) )", "title": "topological_sort()"}, {"location": "api/datajoint/errors/", "text": "Exception classes for the DataJoint library DataJointError \u00b6 Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) suggest ( * args ) \u00b6 regenerate the exception with additional arguments Parameters: Name Type Description Default args addition arguments () Returns: Type Description a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args )) LostConnectionError \u00b6 Bases: DataJointError Loss of server connection Source code in datajoint/errors.py 45 46 47 48 class LostConnectionError ( DataJointError ): \"\"\" Loss of server connection \"\"\" QueryError \u00b6 Bases: DataJointError Errors arising from queries to the database Source code in datajoint/errors.py 51 52 53 54 class QueryError ( DataJointError ): \"\"\" Errors arising from queries to the database \"\"\" QuerySyntaxError \u00b6 Bases: QueryError Errors arising from incorrect query syntax Source code in datajoint/errors.py 58 59 60 61 class QuerySyntaxError ( QueryError ): \"\"\" Errors arising from incorrect query syntax \"\"\" AccessError \u00b6 Bases: QueryError User access error: insufficient privileges. Source code in datajoint/errors.py 64 65 66 67 class AccessError ( QueryError ): \"\"\" User access error: insufficient privileges. \"\"\" MissingTableError \u00b6 Bases: DataJointError Query on a table that has not been declared Source code in datajoint/errors.py 70 71 72 73 class MissingTableError ( DataJointError ): \"\"\" Query on a table that has not been declared \"\"\" DuplicateError \u00b6 Bases: QueryError An integrity error caused by a duplicate entry into a unique key Source code in datajoint/errors.py 76 77 78 79 class DuplicateError ( QueryError ): \"\"\" An integrity error caused by a duplicate entry into a unique key \"\"\" IntegrityError \u00b6 Bases: QueryError An integrity error triggered by foreign key constraints Source code in datajoint/errors.py 82 83 84 85 class IntegrityError ( QueryError ): \"\"\" An integrity error triggered by foreign key constraints \"\"\" UnknownAttributeError \u00b6 Bases: QueryError User requests an attribute name not found in query heading Source code in datajoint/errors.py 88 89 90 91 class UnknownAttributeError ( QueryError ): \"\"\" User requests an attribute name not found in query heading \"\"\" MissingAttributeError \u00b6 Bases: QueryError An error arising when a required attribute value is not provided in INSERT Source code in datajoint/errors.py 94 95 96 97 class MissingAttributeError ( QueryError ): \"\"\" An error arising when a required attribute value is not provided in INSERT \"\"\" MissingExternalFile \u00b6 Bases: DataJointError Error raised when an external file managed by DataJoint is no longer accessible Source code in datajoint/errors.py 100 101 102 103 class MissingExternalFile ( DataJointError ): \"\"\" Error raised when an external file managed by DataJoint is no longer accessible \"\"\" BucketInaccessible \u00b6 Bases: DataJointError Error raised when a S3 bucket is inaccessible Source code in datajoint/errors.py 106 107 108 109 class BucketInaccessible ( DataJointError ): \"\"\" Error raised when a S3 bucket is inaccessible \"\"\"", "title": "errors.py"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError", "text": "Bases: Exception Base class for errors specific to DataJoint internal operation. Source code in datajoint/errors.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class DataJointError ( Exception ): \"\"\" Base class for errors specific to DataJoint internal operation. \"\"\" def __init__ ( self , * args ): from .plugin import connection_plugins , type_plugins self . __cause__ = ( PluginWarning ( \"Unverified DataJoint plugin detected.\" ) if any ( [ any ([ not plugins [ k ][ \"verified\" ] for k in plugins ]) for plugins in [ connection_plugins , type_plugins ] if plugins ] ) else None ) def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "DataJointError"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError.suggest", "text": "regenerate the exception with additional arguments Parameters: Name Type Description Default args addition arguments () Returns: Type Description a new exception of the same type with the additional arguments Source code in datajoint/errors.py 34 35 36 37 38 39 40 41 def suggest ( self , * args ): \"\"\" regenerate the exception with additional arguments :param args: addition arguments :return: a new exception of the same type with the additional arguments \"\"\" return self . __class__ ( * ( self . args + args ))", "title": "suggest()"}, {"location": "api/datajoint/errors/#datajoint.errors.LostConnectionError", "text": "Bases: DataJointError Loss of server connection Source code in datajoint/errors.py 45 46 47 48 class LostConnectionError ( DataJointError ): \"\"\" Loss of server connection \"\"\"", "title": "LostConnectionError"}, {"location": "api/datajoint/errors/#datajoint.errors.QueryError", "text": "Bases: DataJointError Errors arising from queries to the database Source code in datajoint/errors.py 51 52 53 54 class QueryError ( DataJointError ): \"\"\" Errors arising from queries to the database \"\"\"", "title": "QueryError"}, {"location": "api/datajoint/errors/#datajoint.errors.QuerySyntaxError", "text": "Bases: QueryError Errors arising from incorrect query syntax Source code in datajoint/errors.py 58 59 60 61 class QuerySyntaxError ( QueryError ): \"\"\" Errors arising from incorrect query syntax \"\"\"", "title": "QuerySyntaxError"}, {"location": "api/datajoint/errors/#datajoint.errors.AccessError", "text": "Bases: QueryError User access error: insufficient privileges. Source code in datajoint/errors.py 64 65 66 67 class AccessError ( QueryError ): \"\"\" User access error: insufficient privileges. \"\"\"", "title": "AccessError"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingTableError", "text": "Bases: DataJointError Query on a table that has not been declared Source code in datajoint/errors.py 70 71 72 73 class MissingTableError ( DataJointError ): \"\"\" Query on a table that has not been declared \"\"\"", "title": "MissingTableError"}, {"location": "api/datajoint/errors/#datajoint.errors.DuplicateError", "text": "Bases: QueryError An integrity error caused by a duplicate entry into a unique key Source code in datajoint/errors.py 76 77 78 79 class DuplicateError ( QueryError ): \"\"\" An integrity error caused by a duplicate entry into a unique key \"\"\"", "title": "DuplicateError"}, {"location": "api/datajoint/errors/#datajoint.errors.IntegrityError", "text": "Bases: QueryError An integrity error triggered by foreign key constraints Source code in datajoint/errors.py 82 83 84 85 class IntegrityError ( QueryError ): \"\"\" An integrity error triggered by foreign key constraints \"\"\"", "title": "IntegrityError"}, {"location": "api/datajoint/errors/#datajoint.errors.UnknownAttributeError", "text": "Bases: QueryError User requests an attribute name not found in query heading Source code in datajoint/errors.py 88 89 90 91 class UnknownAttributeError ( QueryError ): \"\"\" User requests an attribute name not found in query heading \"\"\"", "title": "UnknownAttributeError"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingAttributeError", "text": "Bases: QueryError An error arising when a required attribute value is not provided in INSERT Source code in datajoint/errors.py 94 95 96 97 class MissingAttributeError ( QueryError ): \"\"\" An error arising when a required attribute value is not provided in INSERT \"\"\"", "title": "MissingAttributeError"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingExternalFile", "text": "Bases: DataJointError Error raised when an external file managed by DataJoint is no longer accessible Source code in datajoint/errors.py 100 101 102 103 class MissingExternalFile ( DataJointError ): \"\"\" Error raised when an external file managed by DataJoint is no longer accessible \"\"\"", "title": "MissingExternalFile"}, {"location": "api/datajoint/errors/#datajoint.errors.BucketInaccessible", "text": "Bases: DataJointError Error raised when a S3 bucket is inaccessible Source code in datajoint/errors.py 106 107 108 109 class BucketInaccessible ( DataJointError ): \"\"\" Error raised when a S3 bucket is inaccessible \"\"\"", "title": "BucketInaccessible"}, {"location": "api/datajoint/expression/", "text": "QueryExpression \u00b6 QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property heading (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property support is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. Source code in datajoint/expression.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 class QueryExpression : \"\"\" QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property `heading` (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property `support` is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. \"\"\" _restriction = None _restriction_attributes = None _left = [] # list of booleans True for left joins, False for inner joins _original_heading = None # heading before projections # subclasses or instantiators must provide values _connection = None _heading = None _support = None # If the query will be using distinct _distinct = False @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes @property def primary_key ( self ): return self . heading . primary_key _subquery_alias_count = count () # count for alias names used in the FROM clause def from_clause ( self ): support = ( \"(\" + src . make_sql () + \") as `$ %x `\" % next ( self . _subquery_alias_count ) if isinstance ( src , QueryExpression ) else src for src in self . support ) clause = next ( support ) for s , left in zip ( support , self . _left ): clause += \" NATURAL {left} JOIN {clause} \" . format ( left = \" LEFT\" if left else \"\" , clause = s ) return clause def where_clause ( self ): return ( \"\" if not self . restriction else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . restriction ) ) def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) # --------- query operators ----------- def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result def restrict_in_place ( self , restriction ): self . __dict__ . update ( self . restrict ( restriction ) . __dict__ ) def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction ) def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction )) def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction )) def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self ) def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other ) def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False ) def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other ) def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" named_attributes = { k : translate_attribute ( v )[ 1 ] for k , v in named_attributes . items () } # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) aggregate = aggr # alias for aggr # ---------- Fetch operators -------------------- @property def fetch1 ( self ): return Fetch1 ( self ) @property def fetch ( self ): return Fetch ( self ) def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] ) def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self ) def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () ) def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) def _repr_html_ ( self ): \"\"\":return: HTML to display table in Jupyter notebook.\"\"\" return repr_html ( self ) connection property \u00b6 a dj.Connection object support property \u00b6 A list of table names or subqueries to from the FROM clause heading property \u00b6 a dj.Heading object, reflects the effects of the projection operator .proj original_heading property \u00b6 a dj.Heading object reflecting the attributes before projection restriction property \u00b6 a AndList object of restrictions applied to input to produce the result restriction_attributes property \u00b6 the set of attribute names invoked in the WHERE clause make_sql ( fields = None ) \u00b6 Make the SQL SELECT statement. Parameters: Name Type Description Default fields used to explicitly set the select attributes None Source code in datajoint/expression.py 122 123 124 125 126 127 128 129 130 131 132 133 def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) make_subquery () \u00b6 create a new SELECT statement where self is the FROM clause Source code in datajoint/expression.py 136 137 138 139 140 141 142 def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result restrict ( restriction ) \u00b6 Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() Parameters: Name Type Description Default restriction a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. required Source code in datajoint/expression.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result join ( other , semantic_check = True , left = False ) \u00b6 create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. Source code in datajoint/expression.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result proj ( * attributes , ** named_attributes ) \u00b6 Projection operator. Parameters: Name Type Description Default attributes attributes to be included in the result. (The primary key is already included). () named_attributes new attributes computed or renamed from existing attributes. {} Returns: Type Description the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. Source code in datajoint/expression.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" named_attributes = { k : translate_attribute ( v )[ 1 ] for k , v in named_attributes . items () } # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result aggr ( group , * attributes , keep_all_rows = False , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required keep_all_rows True=keep all the rows from self. False=keep only rows that match entries in group. False named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) head ( limit = 25 , ** fetch_kwargs ) \u00b6 shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) Parameters: Name Type Description Default limit number of entries 25 fetch_kwargs kwargs for fetch {} Returns: Type Description query result Source code in datajoint/expression.py 516 517 518 519 520 521 522 523 524 525 def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) tail ( limit = 25 , ** fetch_kwargs ) \u00b6 shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] Parameters: Name Type Description Default limit number of entries 25 fetch_kwargs kwargs for fetch {} Returns: Type Description query result Source code in datajoint/expression.py 527 528 529 530 531 532 533 534 535 536 def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] cursor ( offset = 0 , limit = None , order_by = None , as_dict = False ) \u00b6 See expression.fetch() for input description. Returns: Type Description query cursor Source code in datajoint/expression.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) preview ( limit = None , width = None ) \u00b6 Returns: Type Description a string of preview of the contents of the query. Source code in datajoint/expression.py 649 650 651 def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) AndList \u00b6 Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction ) Not \u00b6 invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction Aggregation \u00b6 Bases: QueryExpression Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. Source code in datajoint/expression.py 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 class Aggregation ( QueryExpression ): \"\"\" Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. \"\"\" _left_restrict = None # the pre-GROUP BY conditions for the WHERE clause _subquery_alias_count = count () @classmethod def create ( cls , arg , group , keep_all_rows = False ): if inspect . isclass ( group ) and issubclass ( group , QueryExpression ): group = group () # instantiate if a class assert isinstance ( group , QueryExpression ) if keep_all_rows and len ( group . support ) > 1 or group . heading . new_attributes : group = group . make_subquery () # subquery if left joining a join join = arg . join ( group , left = keep_all_rows ) # reuse the join logic result = cls () result . _connection = join . connection result . _heading = join . heading . set_primary_key ( arg . primary_key ) # use left operand's primary key result . _support = join . support result . _left = join . _left result . _left_restrict = join . restriction # WHERE clause applied before GROUP BY result . _grouping_attributes = result . primary_key return result def where_clause ( self ): return ( \"\" if not self . _left_restrict else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . _left_restrict ) ) def make_sql ( self , fields = None ): fields = self . heading . as_sql ( fields or self . heading . names ) assert self . _grouping_attributes or not self . restriction distinct = set ( self . heading . names ) == set ( self . primary_key ) return \"SELECT {distinct}{fields} FROM {from_}{where}{group_by} \" . format ( distinct = \"DISTINCT \" if distinct else \"\" , fields = fields , from_ = self . from_clause (), where = self . where_clause (), group_by = \"\" if not self . primary_key else ( \" GROUP BY ` %s `\" % \"`,`\" . join ( self . _grouping_attributes ) + ( \"\" if not self . restriction else \" HAVING ( %s )\" % \")AND(\" . join ( self . restriction ) ) ), ) def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( self . _subquery_alias_count ) ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) ) Union \u00b6 Bases: QueryExpression Union is the private DataJoint class that implements the union operator. Source code in datajoint/expression.py 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 class Union ( QueryExpression ): \"\"\" Union is the private DataJoint class that implements the union operator. \"\"\" __count = count () @classmethod def create ( cls , arg1 , arg2 ): if inspect . isclass ( arg2 ) and issubclass ( arg2 , QueryExpression ): arg2 = arg2 () # instantiate if a class if not isinstance ( arg2 , QueryExpression ): raise DataJointError ( \"A QueryExpression can only be unioned with another QueryExpression\" ) if arg1 . connection != arg2 . connection : raise DataJointError ( \"Cannot operate on QueryExpressions originating from different connections.\" ) if set ( arg1 . primary_key ) != set ( arg2 . primary_key ): raise DataJointError ( \"The operands of a union must share the same primary key.\" ) if set ( arg1 . heading . secondary_attributes ) & set ( arg2 . heading . secondary_attributes ): raise DataJointError ( \"The operands of a union must not share any secondary attributes.\" ) result = cls () result . _connection = arg1 . connection result . _heading = arg1 . heading . join ( arg2 . heading ) result . _support = [ arg1 , arg2 ] return result def make_sql ( self ): arg1 , arg2 = self . _support if ( not arg1 . heading . secondary_attributes and not arg2 . heading . secondary_attributes ): # no secondary attributes: use UNION DISTINCT fields = arg1 . primary_key return \"SELECT * FROM (( {sql1} ) UNION ( {sql2} )) as `_u {alias} `\" . format ( sql1 = arg1 . make_sql () if isinstance ( arg1 , Union ) else arg1 . make_sql ( fields ), sql2 = arg2 . make_sql () if isinstance ( arg2 , Union ) else arg2 . make_sql ( fields ), alias = next ( self . __count ), ) # with secondary attributes, use union of left join with antijoin fields = self . heading . names sql1 = arg1 . join ( arg2 , left = True ) . make_sql ( fields ) sql2 = ( ( arg2 - arg1 ) . proj ( ... , ** { k : \"NULL\" for k in arg1 . heading . secondary_attributes }) . make_sql ( fields ) ) return \"( {sql1} ) UNION ( {sql2} )\" . format ( sql1 = sql1 , sql2 = sql2 ) def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( QueryExpression . _subquery_alias_count ), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) ) from_clause () \u00b6 The union does not use a FROM clause Source code in datajoint/expression.py 794 795 796 def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False where_clause () \u00b6 The union does not use a WHERE clause Source code in datajoint/expression.py 798 799 800 def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False U \u00b6 dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr join ( other , left = False ) \u00b6 Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. Parameters: Name Type Description Default other the other query expression to join with. required left ignored. dj.U always acts as if left=False False Returns: Type Description a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result aggr ( group , ** named_attributes ) \u00b6 Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes )", "title": "expression.py"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression", "text": "QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property heading (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property support is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. Source code in datajoint/expression.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 class QueryExpression : \"\"\" QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union. A QueryExpression object has a support, a restriction (an AndList), and heading. Property `heading` (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj. Property `support` is the list of table names or other QueryExpressions to be joined. The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute. Application of operators does not always lead to the creation of a subquery. A subquery is generated when: 1. A restriction is applied on any computed or renamed attributes 2. A projection is applied remapping remapped attributes 3. Subclasses: Join, Aggregation, and Union have additional specific rules. \"\"\" _restriction = None _restriction_attributes = None _left = [] # list of booleans True for left joins, False for inner joins _original_heading = None # heading before projections # subclasses or instantiators must provide values _connection = None _heading = None _support = None # If the query will be using distinct _distinct = False @property def connection ( self ): \"\"\"a dj.Connection object\"\"\" assert self . _connection is not None return self . _connection @property def support ( self ): \"\"\"A list of table names or subqueries to from the FROM clause\"\"\" assert self . _support is not None return self . _support @property def heading ( self ): \"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\" return self . _heading @property def original_heading ( self ): \"\"\"a dj.Heading object reflecting the attributes before projection\"\"\" return self . _original_heading or self . heading @property def restriction ( self ): \"\"\"a AndList object of restrictions applied to input to produce the result\"\"\" if self . _restriction is None : self . _restriction = AndList () return self . _restriction @property def restriction_attributes ( self ): \"\"\"the set of attribute names invoked in the WHERE clause\"\"\" if self . _restriction_attributes is None : self . _restriction_attributes = set () return self . _restriction_attributes @property def primary_key ( self ): return self . heading . primary_key _subquery_alias_count = count () # count for alias names used in the FROM clause def from_clause ( self ): support = ( \"(\" + src . make_sql () + \") as `$ %x `\" % next ( self . _subquery_alias_count ) if isinstance ( src , QueryExpression ) else src for src in self . support ) clause = next ( support ) for s , left in zip ( support , self . _left ): clause += \" NATURAL {left} JOIN {clause} \" . format ( left = \" LEFT\" if left else \"\" , clause = s ) return clause def where_clause ( self ): return ( \"\" if not self . restriction else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . restriction ) ) def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), ) # --------- query operators ----------- def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result def restrict_in_place ( self , restriction ): self . __dict__ . update ( self . restrict ( restriction ) . __dict__ ) def __and__ ( self , restriction ): \"\"\" Restriction operator e.g. ``q1 & q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( restriction ) def __xor__ ( self , restriction ): \"\"\" Permissive restriction operator ignoring compatibility check e.g. ``q1 ^ q2``. \"\"\" if inspect . isclass ( restriction ) and issubclass ( restriction , QueryExpression ): restriction = restriction () if isinstance ( restriction , Not ): return self . restrict ( Not ( PromiscuousOperand ( restriction . restriction ))) return self . restrict ( PromiscuousOperand ( restriction )) def __sub__ ( self , restriction ): \"\"\" Inverted restriction e.g. ``q1 - q2``. :return: a restricted copy of the input argument See QueryExpression.restrict for more detail. \"\"\" return self . restrict ( Not ( restriction )) def __neg__ ( self ): \"\"\" Convert between restriction and inverted restriction e.g. ``-q1``. :return: target restriction See QueryExpression.restrict for more detail. \"\"\" if isinstance ( self , Not ): return self . restriction return Not ( self ) def __mul__ ( self , other ): \"\"\" join of query expressions `self` and `other` e.g. ``q1 * q2``. \"\"\" return self . join ( other ) def __matmul__ ( self , other ): \"\"\" Permissive join of query expressions `self` and `other` ignoring compatibility check e.g. ``q1 @ q2``. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate return self . join ( other , semantic_check = False ) def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result def __add__ ( self , other ): \"\"\"union e.g. ``q1 + q2``.\"\"\" return Union . create ( self , other ) def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" named_attributes = { k : translate_attribute ( v )[ 1 ] for k , v in named_attributes . items () } # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes ) aggregate = aggr # alias for aggr # ---------- Fetch operators -------------------- @property def fetch1 ( self ): return Fetch1 ( self ) @property def fetch ( self ): return Fetch ( self ) def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs ) def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ] def __len__ ( self ): \"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\" return self . connection . query ( \"SELECT {select_} FROM {from_}{where} \" . format ( select_ = ( \"count(*)\" if any ( self . _left ) else \"count(DISTINCT {fields} )\" . format ( fields = self . heading . as_sql ( self . primary_key , include_aliases = False ) ) ), from_ = self . from_clause (), where = self . where_clause (), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): \"\"\" :return: True if the result is not empty. Equivalent to len(self) > 0 but often faster e.g. ``bool(q1)``. \"\"\" return bool ( self . connection . query ( \"SELECT EXISTS(SELECT 1 FROM {from_}{where} )\" . format ( from_ = self . from_clause (), where = self . where_clause () ) ) . fetchone ()[ 0 ] ) def __contains__ ( self , item ): \"\"\" returns True if the restriction in item matches any entries in self e.g. ``restriction in q1``. :param item: any restriction (item in query_expression) is equivalent to bool(query_expression & item) but may be executed more efficiently. \"\"\" return bool ( self & item ) # May be optimized e.g. using an EXISTS query def __iter__ ( self ): \"\"\" returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``. :param self: iterator-compatible QueryExpression object \"\"\" self . _iter_only_key = all ( v . in_key for v in self . heading . attributes . values ()) self . _iter_keys = self . fetch ( \"KEY\" ) return self def __next__ ( self ): \"\"\" returns the next record on an iterator-compatible QueryExpression object e.g. ``next(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: dict \"\"\" try : key = self . _iter_keys . pop ( 0 ) except AttributeError : # self._iter_keys is missing because __iter__ has not been called. raise TypeError ( \"A QueryExpression object is not an iterator. \" \"Use iter(obj) to create an iterator.\" ) except IndexError : raise StopIteration else : if self . _iter_only_key : return key else : try : return ( self & key ) . fetch1 () except DataJointError : # The data may have been deleted since the moment the keys were fetched # -- move on to next entry. return next ( self ) def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict ) def __repr__ ( self ): \"\"\" returns the string representation of a QueryExpression object e.g. ``str(q1)``. :param self: A query expression :type self: :class:`QueryExpression` :rtype: str \"\"\" return ( super () . __repr__ () if config [ \"loglevel\" ] . lower () == \"debug\" else self . preview () ) def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width ) def _repr_html_ ( self ): \"\"\":return: HTML to display table in Jupyter notebook.\"\"\" return repr_html ( self )", "title": "QueryExpression"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.connection", "text": "a dj.Connection object", "title": "connection"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.support", "text": "A list of table names or subqueries to from the FROM clause", "title": "support"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.heading", "text": "a dj.Heading object, reflects the effects of the projection operator .proj", "title": "heading"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.original_heading", "text": "a dj.Heading object reflecting the attributes before projection", "title": "original_heading"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction", "text": "a AndList object of restrictions applied to input to produce the result", "title": "restriction"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction_attributes", "text": "the set of attribute names invoked in the WHERE clause", "title": "restriction_attributes"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_sql", "text": "Make the SQL SELECT statement. Parameters: Name Type Description Default fields used to explicitly set the select attributes None Source code in datajoint/expression.py 122 123 124 125 126 127 128 129 130 131 132 133 def make_sql ( self , fields = None ): \"\"\" Make the SQL SELECT statement. :param fields: used to explicitly set the select attributes \"\"\" return \"SELECT {distinct}{fields} FROM {from_}{where} \" . format ( distinct = \"DISTINCT \" if self . _distinct else \"\" , fields = self . heading . as_sql ( fields or self . heading . names ), from_ = self . from_clause (), where = self . where_clause (), )", "title": "make_sql()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_subquery", "text": "create a new SELECT statement where self is the FROM clause Source code in datajoint/expression.py 136 137 138 139 140 141 142 def make_subquery ( self ): \"\"\"create a new SELECT statement where self is the FROM clause\"\"\" result = QueryExpression () result . _connection = self . connection result . _support = [ self ] result . _heading = self . heading . make_subquery_heading () return result", "title": "make_subquery()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restrict", "text": "Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() Parameters: Name Type Description Default restriction a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. required Source code in datajoint/expression.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def restrict ( self , restriction ): \"\"\" Produces a new expression with the new restriction applied. rel.restrict(restriction) is equivalent to rel & restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r & a & b is equivalent to r & AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class. The expressions in each row equivalent: rel & True rel rel & False the empty entity set rel & 'TRUE' rel rel & 'FALSE' the empty entity set rel - cond rel & Not(cond) rel - 'TRUE' rel & False rel - 'FALSE' rel rel & AndList((cond1,cond2)) rel & cond1 & cond2 rel & AndList() rel rel & [cond1, cond2] rel & OrList((cond1, cond2)) rel & [] rel & False rel & None rel & False rel & any_empty_entity_set rel & False rel - AndList((cond1,cond2)) rel & [Not(cond1), Not(cond2)] rel - [cond1, cond2] rel & Not(cond1) & Not(cond2) rel - AndList() rel & False rel - [] rel rel - None rel rel - any_empty_entity_set rel When arg is another QueryExpression, the restriction rel & arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised. QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict() :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList. \"\"\" attributes = set () new_condition = make_condition ( self , restriction , attributes ) if new_condition is True : return self # restriction has no effect, return the same object # check that all attributes in condition are present in the query try : raise DataJointError ( \"Attribute ` %s ` is not found in query.\" % next ( attr for attr in attributes if attr not in self . heading . names ) ) except StopIteration : pass # all ok # If the new condition uses any new attributes, a subquery is required. # However, Aggregation's HAVING statement works fine with aliased attributes. need_subquery = isinstance ( self , Union ) or ( not isinstance ( self , Aggregation ) and self . heading . new_attributes ) if need_subquery : result = self . make_subquery () else : result = copy . copy ( self ) result . _restriction = AndList ( self . restriction ) # copy to preserve the original result . restriction . append ( new_condition ) result . restriction_attributes . update ( attributes ) return result", "title": "restrict()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.join", "text": "create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. Source code in datajoint/expression.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def join ( self , other , semantic_check = True , left = False ): \"\"\" create the joined QueryExpression. a * b is short for A.join(B) a @ b is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join. \"\"\" # trigger subqueries if joining on renamed attributes if isinstance ( other , U ): return other * self if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if not isinstance ( other , QueryExpression ): raise DataJointError ( \"The argument of join must be a QueryExpression\" ) if semantic_check : assert_join_compatibility ( self , other ) join_attributes = set ( n for n in self . heading . names if n in other . heading . names ) # needs subquery if self's FROM clause has common attributes with other's FROM clause need_subquery1 = need_subquery2 = bool ( ( set ( self . original_heading . names ) & set ( other . original_heading . names )) - join_attributes ) # need subquery if any of the join attributes are derived need_subquery1 = ( need_subquery1 or isinstance ( self , Aggregation ) or any ( n in self . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) need_subquery2 = ( need_subquery2 or isinstance ( other , Aggregation ) or any ( n in other . heading . new_attributes for n in join_attributes ) or isinstance ( self , Union ) ) if need_subquery1 : self = self . make_subquery () if need_subquery2 : other = other . make_subquery () result = QueryExpression () result . _connection = self . connection result . _support = self . support + other . support result . _left = self . _left + [ left ] + other . _left result . _heading = self . heading . join ( other . heading ) result . _restriction = AndList ( self . restriction ) result . _restriction . append ( other . restriction ) result . _original_heading = self . original_heading . join ( other . original_heading ) assert len ( result . support ) == len ( result . _left ) + 1 return result", "title": "join()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.proj", "text": "Projection operator. Parameters: Name Type Description Default attributes attributes to be included in the result. (The primary key is already included). () named_attributes new attributes computed or renamed from existing attributes. {} Returns: Type Description the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. Source code in datajoint/expression.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def proj ( self , * attributes , ** named_attributes ): \"\"\" Projection operator. :param attributes: attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2') -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2') -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once. \"\"\" named_attributes = { k : translate_attribute ( v )[ 1 ] for k , v in named_attributes . items () } # new attributes in parentheses are included again with the new name without removing original duplication_pattern = re . compile ( rf '^\\s*\\(\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*\\)\\s*$' ) # attributes without parentheses renamed rename_pattern = re . compile ( rf '^\\s*(?! { \"|\" . join ( CONSTANT_LITERALS ) } )(?P<name>[a-zA-Z_]\\w*)\\s*$' ) replicate_map = { k : m . group ( \"name\" ) for k , m in ( ( k , duplication_pattern . match ( v )) for k , v in named_attributes . items () ) if m } rename_map = { k : m . group ( \"name\" ) for k , m in ( ( k , rename_pattern . match ( v )) for k , v in named_attributes . items () ) if m } compute_map = { k : v for k , v in named_attributes . items () if not duplication_pattern . match ( v ) and not rename_pattern . match ( v ) } attributes = set ( attributes ) # include primary key attributes . update (( k for k in self . primary_key if k not in rename_map . values ())) # include all secondary attributes with Ellipsis if Ellipsis in attributes : attributes . discard ( Ellipsis ) attributes . update ( ( a for a in self . heading . secondary_attributes if a not in attributes and a not in rename_map . values () ) ) try : raise DataJointError ( \" %s is not a valid data type for an attribute in .proj\" % next ( a for a in attributes if not isinstance ( a , str )) ) except StopIteration : pass # normal case # remove excluded attributes, specified as `-attr' excluded = set ( a for a in attributes if a . strip () . startswith ( \"-\" )) attributes . difference_update ( excluded ) excluded = set ( a . lstrip ( \"-\" ) . strip () for a in excluded ) attributes . difference_update ( excluded ) try : raise DataJointError ( \"Cannot exclude primary key attribute %s \" , next ( a for a in excluded if a in self . primary_key ), ) except StopIteration : pass # all ok # check that all attributes exist in heading try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( a for a in attributes if a not in self . heading . names ) ) except StopIteration : pass # all ok # check that all mentioned names are present in heading mentions = attributes . union ( replicate_map . values ()) . union ( rename_map . values ()) try : raise DataJointError ( \"Attribute ' %s ' not found.\" % next ( a for a in mentions if not self . heading . names ) ) except StopIteration : pass # all ok # check that newly created attributes do not clash with any other selected attributes try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in rename_map if a in attributes . union ( compute_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in compute_map if a in attributes . union ( rename_map ) . union ( replicate_map ) ) ) except StopIteration : pass # all ok try : raise DataJointError ( \"Attribute ` %s ` already exists\" % next ( a for a in replicate_map if a in attributes . union ( rename_map ) . union ( compute_map ) ) ) except StopIteration : pass # all ok # need a subquery if the projection remaps any remapped attributes used = set ( q for v in compute_map . values () for q in extract_column_names ( v )) used . update ( rename_map . values ()) used . update ( replicate_map . values ()) used . intersection_update ( self . heading . names ) need_subquery = isinstance ( self , Union ) or any ( self . heading [ name ] . attribute_expression is not None for name in used ) if not need_subquery and self . restriction : # need a subquery if the restriction applies to attributes that have been renamed need_subquery = any ( name in self . restriction_attributes for name in self . heading . new_attributes ) result = self . make_subquery () if need_subquery else copy . copy ( self ) result . _original_heading = result . original_heading result . _heading = result . heading . select ( attributes , rename_map = dict ( ** rename_map , ** replicate_map ), compute_map = compute_map , ) return result", "title": "proj()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required keep_all_rows True=keep all the rows from self. False=keep only rows that match entries in group. False named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 def aggr ( self , group , * attributes , keep_all_rows = False , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if Ellipsis in attributes : # expand ellipsis to include only attributes from the left table attributes = set ( attributes ) attributes . discard ( Ellipsis ) attributes . update ( self . heading . secondary_attributes ) return Aggregation . create ( self , group = group , keep_all_rows = keep_all_rows ) . proj ( * attributes , ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.head", "text": "shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) Parameters: Name Type Description Default limit number of entries 25 fetch_kwargs kwargs for fetch {} Returns: Type Description query result Source code in datajoint/expression.py 516 517 518 519 520 521 522 523 524 525 def head ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25) :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY\" , limit = limit , ** fetch_kwargs )", "title": "head()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.tail", "text": "shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] Parameters: Name Type Description Default limit number of entries 25 fetch_kwargs kwargs for fetch {} Returns: Type Description query result Source code in datajoint/expression.py 527 528 529 530 531 532 533 534 535 536 def tail ( self , limit = 25 , ** fetch_kwargs ): \"\"\" shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1] :param limit: number of entries :param fetch_kwargs: kwargs for fetch :return: query result \"\"\" return self . fetch ( order_by = \"KEY DESC\" , limit = limit , ** fetch_kwargs )[:: - 1 ]", "title": "tail()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.cursor", "text": "See expression.fetch() for input description. Returns: Type Description query cursor Source code in datajoint/expression.py 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def cursor ( self , offset = 0 , limit = None , order_by = None , as_dict = False ): \"\"\" See expression.fetch() for input description. :return: query cursor \"\"\" if offset and limit is None : raise DataJointError ( \"limit is required when offset is set\" ) sql = self . make_sql () if order_by is not None : sql += \" ORDER BY \" + \", \" . join ( order_by ) if limit is not None : sql += \" LIMIT %d \" % limit + ( \" OFFSET %d \" % offset if offset else \"\" ) logger . debug ( sql ) return self . connection . query ( sql , as_dict = as_dict )", "title": "cursor()"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.preview", "text": "Returns: Type Description a string of preview of the contents of the query. Source code in datajoint/expression.py 649 650 651 def preview ( self , limit = None , width = None ): \"\"\":return: a string of preview of the contents of the query.\"\"\" return preview ( self , limit , width )", "title": "preview()"}, {"location": "api/datajoint/expression/#datajoint.expression.AndList", "text": "Bases: list A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 Source code in datajoint/condition.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AndList ( list ): \"\"\" A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR). Example: expr2 = expr & dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr & cond1 & cond2 & cond3 \"\"\" def append ( self , restriction ): if isinstance ( restriction , AndList ): # extend to reduce nesting self . extend ( restriction ) else : super () . append ( restriction )", "title": "AndList"}, {"location": "api/datajoint/expression/#datajoint.expression.Not", "text": "invert restriction Source code in datajoint/condition.py 64 65 66 67 68 class Not : \"\"\"invert restriction\"\"\" def __init__ ( self , restriction ): self . restriction = restriction", "title": "Not"}, {"location": "api/datajoint/expression/#datajoint.expression.Aggregation", "text": "Bases: QueryExpression Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. Source code in datajoint/expression.py 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 class Aggregation ( QueryExpression ): \"\"\" Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn') yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users. \"\"\" _left_restrict = None # the pre-GROUP BY conditions for the WHERE clause _subquery_alias_count = count () @classmethod def create ( cls , arg , group , keep_all_rows = False ): if inspect . isclass ( group ) and issubclass ( group , QueryExpression ): group = group () # instantiate if a class assert isinstance ( group , QueryExpression ) if keep_all_rows and len ( group . support ) > 1 or group . heading . new_attributes : group = group . make_subquery () # subquery if left joining a join join = arg . join ( group , left = keep_all_rows ) # reuse the join logic result = cls () result . _connection = join . connection result . _heading = join . heading . set_primary_key ( arg . primary_key ) # use left operand's primary key result . _support = join . support result . _left = join . _left result . _left_restrict = join . restriction # WHERE clause applied before GROUP BY result . _grouping_attributes = result . primary_key return result def where_clause ( self ): return ( \"\" if not self . _left_restrict else \" WHERE ( %s )\" % \")AND(\" . join ( str ( s ) for s in self . _left_restrict ) ) def make_sql ( self , fields = None ): fields = self . heading . as_sql ( fields or self . heading . names ) assert self . _grouping_attributes or not self . restriction distinct = set ( self . heading . names ) == set ( self . primary_key ) return \"SELECT {distinct}{fields} FROM {from_}{where}{group_by} \" . format ( distinct = \"DISTINCT \" if distinct else \"\" , fields = fields , from_ = self . from_clause (), where = self . where_clause (), group_by = \"\" if not self . primary_key else ( \" GROUP BY ` %s `\" % \"`,`\" . join ( self . _grouping_attributes ) + ( \"\" if not self . restriction else \" HAVING ( %s )\" % \")AND(\" . join ( self . restriction ) ) ), ) def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( self . _subquery_alias_count ) ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) )", "title": "Aggregation"}, {"location": "api/datajoint/expression/#datajoint.expression.Union", "text": "Bases: QueryExpression Union is the private DataJoint class that implements the union operator. Source code in datajoint/expression.py 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 class Union ( QueryExpression ): \"\"\" Union is the private DataJoint class that implements the union operator. \"\"\" __count = count () @classmethod def create ( cls , arg1 , arg2 ): if inspect . isclass ( arg2 ) and issubclass ( arg2 , QueryExpression ): arg2 = arg2 () # instantiate if a class if not isinstance ( arg2 , QueryExpression ): raise DataJointError ( \"A QueryExpression can only be unioned with another QueryExpression\" ) if arg1 . connection != arg2 . connection : raise DataJointError ( \"Cannot operate on QueryExpressions originating from different connections.\" ) if set ( arg1 . primary_key ) != set ( arg2 . primary_key ): raise DataJointError ( \"The operands of a union must share the same primary key.\" ) if set ( arg1 . heading . secondary_attributes ) & set ( arg2 . heading . secondary_attributes ): raise DataJointError ( \"The operands of a union must not share any secondary attributes.\" ) result = cls () result . _connection = arg1 . connection result . _heading = arg1 . heading . join ( arg2 . heading ) result . _support = [ arg1 , arg2 ] return result def make_sql ( self ): arg1 , arg2 = self . _support if ( not arg1 . heading . secondary_attributes and not arg2 . heading . secondary_attributes ): # no secondary attributes: use UNION DISTINCT fields = arg1 . primary_key return \"SELECT * FROM (( {sql1} ) UNION ( {sql2} )) as `_u {alias} `\" . format ( sql1 = arg1 . make_sql () if isinstance ( arg1 , Union ) else arg1 . make_sql ( fields ), sql2 = arg2 . make_sql () if isinstance ( arg2 , Union ) else arg2 . make_sql ( fields ), alias = next ( self . __count ), ) # with secondary attributes, use union of left join with antijoin fields = self . heading . names sql1 = arg1 . join ( arg2 , left = True ) . make_sql ( fields ) sql2 = ( ( arg2 - arg1 ) . proj ( ... , ** { k : \"NULL\" for k in arg1 . heading . secondary_attributes }) . make_sql ( fields ) ) return \"( {sql1} ) UNION ( {sql2} )\" . format ( sql1 = sql1 , sql2 = sql2 ) def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False def __len__ ( self ): return self . connection . query ( \"SELECT count(1) FROM ( {subquery} ) `$ {alias:x} `\" . format ( subquery = self . make_sql (), alias = next ( QueryExpression . _subquery_alias_count ), ) ) . fetchone ()[ 0 ] def __bool__ ( self ): return bool ( self . connection . query ( \"SELECT EXISTS( {sql} )\" . format ( sql = self . make_sql ())) )", "title": "Union"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.from_clause", "text": "The union does not use a FROM clause Source code in datajoint/expression.py 794 795 796 def from_clause ( self ): \"\"\"The union does not use a FROM clause\"\"\" assert False", "title": "from_clause()"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.where_clause", "text": "The union does not use a WHERE clause Source code in datajoint/expression.py 798 799 800 def where_clause ( self ): \"\"\"The union does not use a WHERE clause\"\"\" assert False", "title": "where_clause()"}, {"location": "api/datajoint/expression/#datajoint.expression.U", "text": "dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the stimulus set: dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute s containing the total number of elements in query expression expr : dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number n of distinct values of attribute attr in query expressio expr . dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute s containing the sum of values of attribute attr over entire result set of expression expr : dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes attr1 , attr2 and the number of their occurrences in the result set of query expression expr . dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression expr has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as expr but attr1 and attr2 are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if attr is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename attr in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. Source code in datajoint/expression.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 class U : \"\"\" dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes. Restriction: dj.U can be used to enumerate unique combinations of values of attributes from other expressions. The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set: >>> dj.U('contrast', 'brightness') & stimulus Aggregation: In aggregation, dj.U is used for summary calculation over an entire set: The following expression yields one element with one attribute `s` containing the total number of elements in query expression `expr`: >>> dj.U().aggr(expr, n='count(*)') The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in query expressio `expr`. >>> dj.U().aggr(expr, n='count(distinct attr)') >>> dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)') The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr` over entire result set of expression `expr`: >>> dj.U().aggr(expr, s='sum(attr)') The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of their occurrences in the result set of query expression `expr`. >>> dj.U(attr1,attr2).aggr(expr, n='count(*)') Joins: If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as `expr` but `attr1` and `attr2` are promoted to the the primary key. This is useful for producing a join on non-primary key attributes. For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename `attr` in one of the operands. The expression dj.U('attr') * rel1 * rel2 overrides this constraint. \"\"\" def __init__ ( self , * primary_key ): self . _primary_key = primary_key @property def primary_key ( self ): return self . _primary_key def __and__ ( self , other ): if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be restricted with a QueryExpression.\" ) result = copy . copy ( other ) result . _distinct = True result . _heading = result . heading . set_primary_key ( self . primary_key ) result = result . proj () return result def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result def __mul__ ( self , other ): \"\"\"shorthand for join\"\"\" return self . join ( other ) def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes ) aggregate = aggr # alias for aggr", "title": "U"}, {"location": "api/datajoint/expression/#datajoint.expression.U.join", "text": "Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. Parameters: Name Type Description Default other the other query expression to join with. required left ignored. dj.U always acts as if left=False False Returns: Type Description a copy of the other query expression with the primary key extended. Source code in datajoint/expression.py 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def join ( self , other , left = False ): \"\"\" Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression. :param other: the other query expression to join with. :param left: ignored. dj.U always acts as if left=False :return: a copy of the other query expression with the primary key extended. \"\"\" if inspect . isclass ( other ) and issubclass ( other , QueryExpression ): other = other () # instantiate if a class if not isinstance ( other , QueryExpression ): raise DataJointError ( \"Set U can only be joined with a QueryExpression.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found\" % next ( k for k in self . primary_key if k not in other . heading . names ) ) except StopIteration : pass # all ok result = copy . copy ( other ) result . _heading = result . heading . set_primary_key ( other . primary_key + [ k for k in self . primary_key if k not in other . primary_key ] ) return result", "title": "join()"}, {"location": "api/datajoint/expression/#datajoint.expression.U.aggr", "text": "Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of group . Parameters: Name Type Description Default group The query expression to be aggregated. required named_attributes computations of the form new_attribute=\"sql expression on attributes of group\" {} Returns: Type Description The derived query expression Source code in datajoint/expression.py 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 def aggr ( self , group , ** named_attributes ): \"\"\" Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`. :param group: The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression \"\"\" if named_attributes . get ( \"keep_all_rows\" , False ): raise DataJointError ( \"Cannot set keep_all_rows=True when aggregating on a universal set.\" ) return Aggregation . create ( self , group = group , keep_all_rows = False ) . proj ( ** named_attributes )", "title": "aggr()"}, {"location": "api/datajoint/external/", "text": "subfold ( name , folds ) \u00b6 subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] Source code in datajoint/external.py 23 24 25 26 27 28 29 30 31 def subfold ( name , folds ): \"\"\" subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] \"\"\" return ( ( name [: folds [ 0 ]] . lower (),) + subfold ( name [ folds [ 0 ] :], folds [ 1 :]) if folds else () ) ExternalTable \u00b6 Bases: Table The table tracking externally stored objects. Declare as ExternalTable(connection, database) Source code in datajoint/external.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 class ExternalTable ( Table ): \"\"\" The table tracking externally stored objects. Declare as ExternalTable(connection, database) \"\"\" def __init__ ( self , connection , store , database ): self . store = store self . spec = config . get_store_spec ( store ) self . _s3 = None self . database = database self . _connection = connection self . _heading = Heading ( table_info = dict ( conn = connection , database = database , table_name = self . table_name , context = None , ) ) self . _support = [ self . full_table_name ] if not self . is_declared : self . declare () self . _s3 = None if self . spec [ \"protocol\" ] == \"file\" and not Path ( self . spec [ \"location\" ]) . is_dir (): raise FileNotFoundError ( \"Inaccessible local directory %s \" % self . spec [ \"location\" ] ) from None @property def definition ( self ): return \"\"\" # external storage tracking hash : uuid # hash of contents (blob), of filename + contents (attach), or relative filepath (filepath) --- size :bigint unsigned # size of object in bytes attachment_name=null : varchar(255) # the filename of an attachment filepath=null : varchar(1000) # relative filepath or attachment filename contents_hash=null : uuid # used for the filepath datatype timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" @property def table_name ( self ): return f \" { EXTERNAL_TABLE_ROOT } _ { self . store } \" @property def s3 ( self ): if self . _s3 is None : self . _s3 = s3 . Folder ( ** self . spec ) return self . _s3 # - low-level operations - private def _make_external_filepath ( self , relative_filepath ): \"\"\"resolve the complete external path based on the relative path\"\"\" # Strip root if self . spec [ \"protocol\" ] == \"s3\" : posix_path = PurePosixPath ( PureWindowsPath ( self . spec [ \"location\" ])) location_path = ( Path ( * posix_path . parts [ 1 :]) if len ( self . spec [ \"location\" ]) > 0 and any ( case in posix_path . parts [ 0 ] for case in ( \" \\\\ \" , \":\" )) else Path ( posix_path ) ) return PurePosixPath ( location_path , relative_filepath ) # Preserve root elif self . spec [ \"protocol\" ] == \"file\" : return PurePosixPath ( Path ( self . spec [ \"location\" ]), relative_filepath ) else : assert False def _make_uuid_path ( self , uuid , suffix = \"\" ): \"\"\"create external path based on the uuid hash\"\"\" return self . _make_external_filepath ( PurePosixPath ( self . database , \"/\" . join ( subfold ( uuid . hex , self . spec [ \"subfolding\" ])), uuid . hex , ) . with_suffix ( suffix ) ) def _upload_file ( self , local_path , external_path , metadata = None ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fput ( local_path , external_path , metadata ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( local_path , external_path , overwrite = True ) else : assert False def _download_file ( self , external_path , download_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fget ( external_path , download_path ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( external_path , download_path ) else : assert False def _upload_buffer ( self , buffer , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . put ( external_path , buffer ) elif self . spec [ \"protocol\" ] == \"file\" : safe_write ( external_path , buffer ) else : assert False def _download_buffer ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . get ( external_path ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_path ) . read_bytes () assert False def _remove_external_file ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . remove_object ( external_path ) elif self . spec [ \"protocol\" ] == \"file\" : try : Path ( external_path ) . unlink () except FileNotFoundError : pass def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False # --- BLOBS ---- def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob # --- ATTACHMENTS --- def upload_attachment ( self , local_path ): attachment_name = Path ( local_path ) . name uuid = uuid_from_file ( local_path , init_string = attachment_name + \" \\0 \" ) external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _upload_file ( local_path , external_path ) # insert tracking info self . connection . query ( \"\"\" INSERT INTO {tab} (hash, size, attachment_name) VALUES (%s, {size}, \"{attachment_name}\") ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\" . format ( tab = self . full_table_name , size = Path ( local_path ) . stat () . st_size , attachment_name = attachment_name , ), args = [ uuid . bytes ], ) return uuid def get_attachment_name ( self , uuid ): return ( self & { \"hash\" : uuid }) . fetch1 ( \"attachment_name\" ) def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) # --- FILEPATH --- def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash # --- UTILITIES --- @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) ) def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list exists ( external_filepath ) \u00b6 Returns: Type Description True if the external file is accessible Source code in datajoint/external.py 156 157 158 159 160 161 162 163 164 def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False put ( blob ) \u00b6 put a binary string (blob) in external store Source code in datajoint/external.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid get ( uuid ) \u00b6 get an object from external store. Source code in datajoint/external.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob download_attachment ( uuid , attachment_name , download_path ) \u00b6 save attachment from memory buffer into the save_path Source code in datajoint/external.py 245 246 247 248 def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) upload_filepath ( local_filepath ) \u00b6 Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur Source code in datajoint/external.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid download_filepath ( filepath_hash ) \u00b6 sync a file from external store to the local stage Parameters: Name Type Description Default filepath_hash The hash (UUID) of the relative_path required Returns: Type Description hash (UUID) of the contents of the downloaded file or Nones Source code in datajoint/external.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash references property \u00b6 Returns: Type Description generator of referencing table names and their referencing columns fetch_external_paths ( ** fetch_kwargs ) \u00b6 generate complete external filepaths from the query. Each element is a tuple: (uuid, path) Parameters: Name Type Description Default fetch_kwargs keyword arguments to pass to fetch {} Source code in datajoint/external.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths unused () \u00b6 query expression for unused hashes Returns: Type Description self restricted to elements that are not in use by any tables in the schema Source code in datajoint/external.py 388 389 390 391 392 393 394 395 396 397 398 399 def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] used () \u00b6 query expression for used hashes Returns: Type Description self restricted to elements that in use by tables in the schema Source code in datajoint/external.py 401 402 403 404 405 406 407 408 409 410 411 412 def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] delete ( * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True ) \u00b6 Parameters: Name Type Description Default delete_external_files True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. None errors_as_string If True any errors returned when deleting from external files will be strings True limit (integer) limit the number of items to delete None display_progress if True, display progress as files are cleaned up True Returns: Type Description if deleting external files, returns errors Source code in datajoint/external.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list ExternalMapping \u00b6 Bases: Mapping The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] Source code in datajoint/external.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class ExternalMapping ( Mapping ): \"\"\" The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] \"\"\" def __init__ ( self , schema ): self . schema = schema self . _tables = {} def __repr__ ( self ): return \"External file tables for schema ` {schema} `: \\n \" . format ( schema = self . schema . database ) + \" \\n \" . join ( '\" {store} \" {protocol} : {location} ' . format ( store = k , ** v . spec ) for k , v in self . items () ) def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ] def __len__ ( self ): return len ( self . _tables ) def __iter__ ( self ): return iter ( self . _tables )", "title": "external.py"}, {"location": "api/datajoint/external/#datajoint.external.subfold", "text": "subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] Source code in datajoint/external.py 23 24 25 26 27 28 29 30 31 def subfold ( name , folds ): \"\"\" subfolding for external storage: e.g. subfold('aBCdefg', (2, 3)) --> ['ab','cde'] \"\"\" return ( ( name [: folds [ 0 ]] . lower (),) + subfold ( name [ folds [ 0 ] :], folds [ 1 :]) if folds else () )", "title": "subfold()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable", "text": "Bases: Table The table tracking externally stored objects. Declare as ExternalTable(connection, database) Source code in datajoint/external.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 class ExternalTable ( Table ): \"\"\" The table tracking externally stored objects. Declare as ExternalTable(connection, database) \"\"\" def __init__ ( self , connection , store , database ): self . store = store self . spec = config . get_store_spec ( store ) self . _s3 = None self . database = database self . _connection = connection self . _heading = Heading ( table_info = dict ( conn = connection , database = database , table_name = self . table_name , context = None , ) ) self . _support = [ self . full_table_name ] if not self . is_declared : self . declare () self . _s3 = None if self . spec [ \"protocol\" ] == \"file\" and not Path ( self . spec [ \"location\" ]) . is_dir (): raise FileNotFoundError ( \"Inaccessible local directory %s \" % self . spec [ \"location\" ] ) from None @property def definition ( self ): return \"\"\" # external storage tracking hash : uuid # hash of contents (blob), of filename + contents (attach), or relative filepath (filepath) --- size :bigint unsigned # size of object in bytes attachment_name=null : varchar(255) # the filename of an attachment filepath=null : varchar(1000) # relative filepath or attachment filename contents_hash=null : uuid # used for the filepath datatype timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" @property def table_name ( self ): return f \" { EXTERNAL_TABLE_ROOT } _ { self . store } \" @property def s3 ( self ): if self . _s3 is None : self . _s3 = s3 . Folder ( ** self . spec ) return self . _s3 # - low-level operations - private def _make_external_filepath ( self , relative_filepath ): \"\"\"resolve the complete external path based on the relative path\"\"\" # Strip root if self . spec [ \"protocol\" ] == \"s3\" : posix_path = PurePosixPath ( PureWindowsPath ( self . spec [ \"location\" ])) location_path = ( Path ( * posix_path . parts [ 1 :]) if len ( self . spec [ \"location\" ]) > 0 and any ( case in posix_path . parts [ 0 ] for case in ( \" \\\\ \" , \":\" )) else Path ( posix_path ) ) return PurePosixPath ( location_path , relative_filepath ) # Preserve root elif self . spec [ \"protocol\" ] == \"file\" : return PurePosixPath ( Path ( self . spec [ \"location\" ]), relative_filepath ) else : assert False def _make_uuid_path ( self , uuid , suffix = \"\" ): \"\"\"create external path based on the uuid hash\"\"\" return self . _make_external_filepath ( PurePosixPath ( self . database , \"/\" . join ( subfold ( uuid . hex , self . spec [ \"subfolding\" ])), uuid . hex , ) . with_suffix ( suffix ) ) def _upload_file ( self , local_path , external_path , metadata = None ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fput ( local_path , external_path , metadata ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( local_path , external_path , overwrite = True ) else : assert False def _download_file ( self , external_path , download_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . fget ( external_path , download_path ) elif self . spec [ \"protocol\" ] == \"file\" : safe_copy ( external_path , download_path ) else : assert False def _upload_buffer ( self , buffer , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . put ( external_path , buffer ) elif self . spec [ \"protocol\" ] == \"file\" : safe_write ( external_path , buffer ) else : assert False def _download_buffer ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . get ( external_path ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_path ) . read_bytes () assert False def _remove_external_file ( self , external_path ): if self . spec [ \"protocol\" ] == \"s3\" : self . s3 . remove_object ( external_path ) elif self . spec [ \"protocol\" ] == \"file\" : try : Path ( external_path ) . unlink () except FileNotFoundError : pass def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False # --- BLOBS ---- def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob # --- ATTACHMENTS --- def upload_attachment ( self , local_path ): attachment_name = Path ( local_path ) . name uuid = uuid_from_file ( local_path , init_string = attachment_name + \" \\0 \" ) external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _upload_file ( local_path , external_path ) # insert tracking info self . connection . query ( \"\"\" INSERT INTO {tab} (hash, size, attachment_name) VALUES (%s, {size}, \"{attachment_name}\") ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\" . format ( tab = self . full_table_name , size = Path ( local_path ) . stat () . st_size , attachment_name = attachment_name , ), args = [ uuid . bytes ], ) return uuid def get_attachment_name ( self , uuid ): return ( self & { \"hash\" : uuid }) . fetch1 ( \"attachment_name\" ) def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path ) # --- FILEPATH --- def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash # --- UTILITIES --- @property def references ( self ): \"\"\" :return: generator of referencing table names and their referencing columns \"\"\" return ( { k . lower (): v for k , v in elem . items ()} for elem in self . connection . query ( \"\"\" SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name FROM information_schema.key_column_usage WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\" \"\"\" . format ( tab = self . table_name , db = self . database ), as_dict = True , ) ) def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ] def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list", "title": "ExternalTable"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.exists", "text": "Returns: Type Description True if the external file is accessible Source code in datajoint/external.py 156 157 158 159 160 161 162 163 164 def exists ( self , external_filepath ): \"\"\" :return: True if the external file is accessible \"\"\" if self . spec [ \"protocol\" ] == \"s3\" : return self . s3 . exists ( external_filepath ) if self . spec [ \"protocol\" ] == \"file\" : return Path ( external_filepath ) . is_file () assert False", "title": "exists()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.put", "text": "put a binary string (blob) in external store Source code in datajoint/external.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def put ( self , blob ): \"\"\" put a binary string (blob) in external store \"\"\" uuid = uuid_from_buffer ( blob ) self . _upload_buffer ( blob , self . _make_uuid_path ( uuid )) # insert tracking info self . connection . query ( \"INSERT INTO {tab} (hash, size) VALUES ( %s , {size} ) ON DUPLICATE KEY \" \"UPDATE timestamp=CURRENT_TIMESTAMP\" . format ( tab = self . full_table_name , size = len ( blob ) ), args = ( uuid . bytes ,), ) return uuid", "title": "put()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.get", "text": "get an object from external store. Source code in datajoint/external.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get ( self , uuid ): \"\"\" get an object from external store. \"\"\" if uuid is None : return None # attempt to get object from cache blob = None cache_folder = config . get ( \"cache\" , None ) if cache_folder : try : cache_path = Path ( cache_folder , * subfold ( uuid . hex , CACHE_SUBFOLDING )) cache_file = Path ( cache_path , uuid . hex ) blob = cache_file . read_bytes () except FileNotFoundError : pass # not cached # download blob from external store if blob is None : try : blob = self . _download_buffer ( self . _make_uuid_path ( uuid )) except MissingExternalFile : if not SUPPORT_MIGRATED_BLOBS : raise # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths relative_filepath , contents_hash = ( self & { \"hash\" : uuid }) . fetch1 ( \"filepath\" , \"contents_hash\" ) if relative_filepath is None : raise blob = self . _download_buffer ( self . _make_external_filepath ( relative_filepath ) ) if cache_folder : cache_path . mkdir ( parents = True , exist_ok = True ) safe_write ( cache_path / uuid . hex , blob ) return blob", "title": "get()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_attachment", "text": "save attachment from memory buffer into the save_path Source code in datajoint/external.py 245 246 247 248 def download_attachment ( self , uuid , attachment_name , download_path ): \"\"\"save attachment from memory buffer into the save_path\"\"\" external_path = self . _make_uuid_path ( uuid , \".\" + attachment_name ) self . _download_file ( external_path , download_path )", "title": "download_attachment()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.upload_filepath", "text": "Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur Source code in datajoint/external.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def upload_filepath ( self , local_filepath ): \"\"\" Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur \"\"\" local_filepath = Path ( local_filepath ) try : relative_filepath = str ( local_filepath . relative_to ( self . spec [ \"stage\" ]) . as_posix () ) except ValueError : raise DataJointError ( \"The path {path} is not in stage {stage} \" . format ( path = local_filepath . parent , ** self . spec ) ) uuid = uuid_from_buffer ( init_string = relative_filepath ) # hash relative path, not contents contents_hash = uuid_from_file ( local_filepath ) # check if the remote file already exists and verify that it matches check_hash = ( self & { \"hash\" : uuid }) . fetch ( \"contents_hash\" ) if check_hash : # the tracking entry exists, check that it's the same file as before if contents_hash != check_hash [ 0 ]: raise DataJointError ( f \"A different version of ' { relative_filepath } ' has already been placed.\" ) else : # upload the file and create its tracking entry self . _upload_file ( local_filepath , self . _make_external_filepath ( relative_filepath ), metadata = { \"contents_hash\" : str ( contents_hash )}, ) self . connection . query ( \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES ( %s , {size} , ' {filepath} ', %s )\" . format ( tab = self . full_table_name , size = Path ( local_filepath ) . stat () . st_size , filepath = relative_filepath , ), args = ( uuid . bytes , contents_hash . bytes ), ) return uuid", "title": "upload_filepath()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_filepath", "text": "sync a file from external store to the local stage Parameters: Name Type Description Default filepath_hash The hash (UUID) of the relative_path required Returns: Type Description hash (UUID) of the contents of the downloaded file or Nones Source code in datajoint/external.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def download_filepath ( self , filepath_hash ): \"\"\" sync a file from external store to the local stage :param filepath_hash: The hash (UUID) of the relative_path :return: hash (UUID) of the contents of the downloaded file or Nones \"\"\" def _need_checksum ( local_filepath , expected_size ): limit = config . get ( \"filepath_checksum_size_limit\" ) actual_size = Path ( local_filepath ) . stat () . st_size if expected_size != actual_size : # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but size did not match.\" ) return limit is None or actual_size < limit if filepath_hash is not None : relative_filepath , contents_hash , size = ( self & { \"hash\" : filepath_hash } ) . fetch1 ( \"filepath\" , \"contents_hash\" , \"size\" ) external_path = self . _make_external_filepath ( relative_filepath ) local_filepath = Path ( self . spec [ \"stage\" ]) . absolute () / relative_filepath file_exists = Path ( local_filepath ) . is_file () and ( not _need_checksum ( local_filepath , size ) or uuid_from_file ( local_filepath ) == contents_hash ) if not file_exists : self . _download_file ( external_path , local_filepath ) if ( _need_checksum ( local_filepath , size ) and uuid_from_file ( local_filepath ) != contents_hash ): # this should never happen without outside interference raise DataJointError ( f \"' { local_filepath } ' downloaded but did not pass checksum.\" ) if not _need_checksum ( local_filepath , size ): logger . warning ( f \"Skipped checksum for file with hash: { contents_hash } , and path: { local_filepath } \" ) return str ( local_filepath ), contents_hash", "title": "download_filepath()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.references", "text": "Returns: Type Description generator of referencing table names and their referencing columns", "title": "references"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.fetch_external_paths", "text": "generate complete external filepaths from the query. Each element is a tuple: (uuid, path) Parameters: Name Type Description Default fetch_kwargs keyword arguments to pass to fetch {} Source code in datajoint/external.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def fetch_external_paths ( self , ** fetch_kwargs ): \"\"\" generate complete external filepaths from the query. Each element is a tuple: (uuid, path) :param fetch_kwargs: keyword arguments to pass to fetch \"\"\" fetch_kwargs . update ( as_dict = True ) paths = [] for item in self . fetch ( \"hash\" , \"attachment_name\" , \"filepath\" , ** fetch_kwargs ): if item [ \"attachment_name\" ]: # attachments path = self . _make_uuid_path ( item [ \"hash\" ], \".\" + item [ \"attachment_name\" ]) elif item [ \"filepath\" ]: # external filepaths path = self . _make_external_filepath ( item [ \"filepath\" ]) else : # blobs path = self . _make_uuid_path ( item [ \"hash\" ]) paths . append (( item [ \"hash\" ], path )) return paths", "title": "fetch_external_paths()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.unused", "text": "query expression for unused hashes Returns: Type Description self restricted to elements that are not in use by any tables in the schema Source code in datajoint/external.py 388 389 390 391 392 393 394 395 396 397 398 399 def unused ( self ): \"\"\" query expression for unused hashes :return: self restricted to elements that are not in use by any tables in the schema \"\"\" return self - [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ]", "title": "unused()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.used", "text": "query expression for used hashes Returns: Type Description self restricted to elements that in use by tables in the schema Source code in datajoint/external.py 401 402 403 404 405 406 407 408 409 410 411 412 def used ( self ): \"\"\" query expression for used hashes :return: self restricted to elements that in use by tables in the schema \"\"\" return self & [ FreeTable ( self . connection , ref [ \"referencing_table\" ]) . proj ( hash = ref [ \"column_name\" ] ) for ref in self . references ]", "title": "used()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.delete", "text": "Parameters: Name Type Description Default delete_external_files True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. None errors_as_string If True any errors returned when deleting from external files will be strings True limit (integer) limit the number of items to delete None display_progress if True, display progress as files are cleaned up True Returns: Type Description if deleting external files, returns errors Source code in datajoint/external.py 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def delete ( self , * , delete_external_files = None , limit = None , display_progress = True , errors_as_string = True , ): \"\"\" :param delete_external_files: True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too. :param errors_as_string: If True any errors returned when deleting from external files will be strings :param limit: (integer) limit the number of items to delete :param display_progress: if True, display progress as files are cleaned up :return: if deleting external files, returns errors \"\"\" if delete_external_files not in ( True , False ): raise DataJointError ( \"The delete_external_files argument must be set to either \" \"True or False in delete()\" ) if not delete_external_files : self . unused () . delete_quick () else : items = self . unused () . fetch_external_paths ( limit = limit ) if display_progress : items = tqdm ( items ) # delete items one by one, close to transaction-safe error_list = [] for uuid , external_path in items : row = ( self & { \"hash\" : uuid }) . fetch () if row . size : try : ( self & { \"hash\" : uuid }) . delete_quick () except Exception : pass # if delete failed, do not remove the external file else : try : self . _remove_external_file ( external_path ) except Exception as error : # adding row back into table after failed delete self . insert1 ( row [ 0 ], skip_duplicates = True ) error_list . append ( ( uuid , external_path , str ( error ) if errors_as_string else error , ) ) return error_list", "title": "delete()"}, {"location": "api/datajoint/external/#datajoint.external.ExternalMapping", "text": "Bases: Mapping The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] Source code in datajoint/external.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 class ExternalMapping ( Mapping ): \"\"\" The external manager contains all the tables for all external stores for a given schema :Example: e = ExternalMapping(schema) external_table = e[store] \"\"\" def __init__ ( self , schema ): self . schema = schema self . _tables = {} def __repr__ ( self ): return \"External file tables for schema ` {schema} `: \\n \" . format ( schema = self . schema . database ) + \" \\n \" . join ( '\" {store} \" {protocol} : {location} ' . format ( store = k , ** v . spec ) for k , v in self . items () ) def __getitem__ ( self , store ): \"\"\" Triggers the creation of an external table. Should only be used when ready to save or read from external storage. :param store: the name of the store :return: the ExternalTable object for the store \"\"\" if store not in self . _tables : self . _tables [ store ] = ExternalTable ( connection = self . schema . connection , store = store , database = self . schema . database , ) return self . _tables [ store ] def __len__ ( self ): return len ( self . _tables ) def __iter__ ( self ): return iter ( self . _tables )", "title": "ExternalMapping"}, {"location": "api/datajoint/fetch/", "text": "key \u00b6 object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 19 20 21 22 23 24 25 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass to_dicts ( recarray ) \u00b6 convert record array to a dictionaries Source code in datajoint/fetch.py 32 33 34 35 def to_dicts ( recarray ): \"\"\"convert record array to a dictionaries\"\"\" for rec in recarray : yield dict ( zip ( recarray . dtype . names , rec . tolist ())) Fetch \u00b6 A fetch object that handles retrieving elements from the table expression. Parameters: Name Type Description Default expression the QueryExpression object to fetch from. required Source code in datajoint/fetch.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 class Fetch : \"\"\" A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this table. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the table in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret Fetch1 \u00b6 Fetch object for fetching the result of a query yielding one row. Parameters: Name Type Description Default expression a query expression to fetch from. required Source code in datajoint/fetch.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 class Fetch1 : \"\"\" Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the table in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret", "title": "fetch.py"}, {"location": "api/datajoint/fetch/#datajoint.fetch.key", "text": "object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key Source code in datajoint/fetch.py 19 20 21 22 23 24 25 class key : \"\"\" object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key \"\"\" pass", "title": "key"}, {"location": "api/datajoint/fetch/#datajoint.fetch.to_dicts", "text": "convert record array to a dictionaries Source code in datajoint/fetch.py 32 33 34 35 def to_dicts ( recarray ): \"\"\"convert record array to a dictionaries\"\"\" for rec in recarray : yield dict ( zip ( recarray . dtype . names , rec . tolist ()))", "title": "to_dicts()"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch", "text": "A fetch object that handles retrieving elements from the table expression. Parameters: Name Type Description Default expression the QueryExpression object to fetch from. required Source code in datajoint/fetch.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 class Fetch : \"\"\" A fetch object that handles retrieving elements from the table expression. :param expression: the QueryExpression object to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , offset = None , limit = None , order_by = None , format = None , as_dict = None , squeeze = False , download_path = \".\" ): \"\"\" Fetches the expression results from the database into an np.array or list of dictionaries and unpacks blob attributes. :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this table. If provided, returns tuples with an entry for each attribute. :param offset: the number of tuples to skip in the returned result :param limit: the maximum number of tuples to return :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\", \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\" :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or 'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. . :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to True for .fetch('KEY') :param squeeze: if True, remove extra dimensions from arrays :param download_path: for fetches that download data, e.g. attachments :return: the contents of the table in the form of a structured numpy.array or a dict list \"\"\" if order_by is not None : # if 'order_by' passed in a string, make into list if isinstance ( order_by , str ): order_by = [ order_by ] # expand \"KEY\" or \"KEY DESC\" order_by = list ( _flatten_attribute_list ( self . _expression . primary_key , order_by ) ) attrs_as_dict = as_dict and attrs if attrs_as_dict : # absorb KEY into attrs and prepare to return attributes as dict (issue #595) if any ( is_key ( k ) for k in attrs ): attrs = list ( self . _expression . primary_key ) + [ a for a in attrs if a not in self . _expression . primary_key ] if as_dict is None : as_dict = bool ( attrs ) # default to True for \"KEY\" and False otherwise # format should not be specified with attrs or is_dict=True if format is not None and ( as_dict or attrs ): raise DataJointError ( \"Cannot specify output format when as_dict=True or \" \"when attributes are selected to be fetched separately.\" ) if format not in { None , \"array\" , \"frame\" }: raise DataJointError ( \"Fetch output format must be in \" '{{\"array\", \"frame\"}} but \" {} \" was given' . format ( format ) ) if not ( attrs or as_dict ) and format is None : format = config [ \"fetch_format\" ] # default to array if format not in { \"array\" , \"frame\" }: raise DataJointError ( 'Invalid entry \" {} \" in datajoint.config[\"fetch_format\"]: ' 'use \"array\" or \"frame\"' . format ( format ) ) if limit is None and offset is not None : logger . warning ( \"Offset set, but no limit. Setting limit to a large number. \" \"Consider setting a limit explicitly.\" ) limit = 8000000000 # just a very large number to effect no limit get = partial ( _get , self . _expression . connection , squeeze = squeeze , download_path = download_path , ) if attrs : # a list of attributes provided attributes = [ a for a in attrs if not is_key ( a )] ret = self . _expression . proj ( * attributes ) ret = ret . fetch ( offset = offset , limit = limit , order_by = order_by , as_dict = False , squeeze = squeeze , download_path = download_path , format = \"array\" , ) if attrs_as_dict : ret = [ { k : v for k , v in zip ( ret . dtype . names , x ) if k in attrs } for x in ret ] else : return_values = [ list ( ( to_dicts if as_dict else lambda x : x )( ret [ self . _expression . primary_key ] ) ) if is_key ( attribute ) else ret [ attribute ] for attribute in attrs ] ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values else : # fetch all attributes as a numpy.record_array or pandas.DataFrame cur = self . _expression . cursor ( as_dict = as_dict , limit = limit , offset = offset , order_by = order_by ) heading = self . _expression . heading if as_dict : ret = [ dict (( name , get ( heading [ name ], d [ name ])) for name in heading . names ) for d in cur ] else : ret = list ( cur . fetchall ()) record_type = ( heading . as_dtype if not ret else np . dtype ( [ ( name , type ( value ), ) # use the first element to determine blob type if heading [ name ] . is_blob and isinstance ( value , numbers . Number ) else ( name , heading . as_dtype [ name ]) for value , name in zip ( ret [ 0 ], heading . as_dtype . names ) ] ) ) try : ret = np . array ( ret , dtype = record_type ) except Exception as e : raise e for name in heading : # unpack blobs and externals ret [ name ] = list ( map ( partial ( get , heading [ name ]), ret [ name ])) if format == \"frame\" : ret = pandas . DataFrame ( ret ) . set_index ( heading . primary_key ) return ret", "title": "Fetch"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch1", "text": "Fetch object for fetching the result of a query yielding one row. Parameters: Name Type Description Default expression a query expression to fetch from. required Source code in datajoint/fetch.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 class Fetch1 : \"\"\" Fetch object for fetching the result of a query yielding one row. :param expression: a query expression to fetch from. \"\"\" def __init__ ( self , expression ): self . _expression = expression def __call__ ( self , * attrs , squeeze = False , download_path = \".\" ): \"\"\" Fetches the result of a query expression that yields one entry. If no attributes are specified, returns the result as a dict. If attributes are specified returns the corresponding results as a tuple. Examples: d = rel.fetch1() # as a dictionary a, b = rel.fetch1('a', 'b') # as a tuple :params *attrs: attributes to return when expanding into a tuple. If attrs is empty, the return result is a dict :param squeeze: When true, remove extra dimensions from arrays in attributes :param download_path: for fetches that download data, e.g. attachments :return: the one tuple in the table in the form of a dict \"\"\" heading = self . _expression . heading if not attrs : # fetch all attributes, return as ordered dict cur = self . _expression . cursor ( as_dict = True ) ret = cur . fetchone () if not ret or cur . fetchone (): raise DataJointError ( \"fetch1 requires exactly one tuple in the input set.\" ) ret = dict ( ( name , _get ( self . _expression . connection , heading [ name ], ret [ name ], squeeze = squeeze , download_path = download_path , ), ) for name in heading . names ) else : # fetch some attributes, return as tuple attributes = [ a for a in attrs if not is_key ( a )] result = self . _expression . proj ( * attributes ) . fetch ( squeeze = squeeze , download_path = download_path , format = \"array\" ) if len ( result ) != 1 : raise DataJointError ( \"fetch1 should only return one tuple. %d tuples found\" % len ( result ) ) return_values = tuple ( next ( to_dicts ( result [ self . _expression . primary_key ])) if is_key ( attribute ) else result [ attribute ][ 0 ] for attribute in attrs ) ret = return_values [ 0 ] if len ( attrs ) == 1 else return_values return ret", "title": "Fetch1"}, {"location": "api/datajoint/hash/", "text": "key_hash ( mapping ) \u00b6 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest () uuid_from_stream ( stream , * , init_string = '' ) \u00b6 :stream: stream object or open file handle :init_string: string to initialize the checksum Returns: Type Description 16-byte digest of stream data Source code in datajoint/hash.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def uuid_from_stream ( stream , * , init_string = \"\" ): \"\"\" :return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum \"\"\" hashed = hashlib . md5 ( init_string . encode ()) chunk = True chunk_size = 1 << 14 while chunk : chunk = stream . read ( chunk_size ) hashed . update ( chunk ) return uuid . UUID ( bytes = hashed . digest ())", "title": "hash.py"}, {"location": "api/datajoint/hash/#datajoint.hash.key_hash", "text": "32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. Source code in datajoint/hash.py 7 8 9 10 11 12 13 14 15 16 def key_hash ( mapping ): \"\"\" 32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables. \"\"\" hashed = hashlib . md5 () for k , v in sorted ( mapping . items ()): hashed . update ( str ( v ) . encode ()) return hashed . hexdigest ()", "title": "key_hash()"}, {"location": "api/datajoint/hash/#datajoint.hash.uuid_from_stream", "text": ":stream: stream object or open file handle :init_string: string to initialize the checksum Returns: Type Description 16-byte digest of stream data Source code in datajoint/hash.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def uuid_from_stream ( stream , * , init_string = \"\" ): \"\"\" :return: 16-byte digest of stream data :stream: stream object or open file handle :init_string: string to initialize the checksum \"\"\" hashed = hashlib . md5 ( init_string . encode ()) chunk = True chunk_size = 1 << 14 while chunk : chunk = stream . read ( chunk_size ) hashed . update ( chunk ) return uuid . UUID ( bytes = hashed . digest ())", "title": "uuid_from_stream()"}, {"location": "api/datajoint/heading/", "text": "Attribute \u00b6 Bases: namedtuple ( _Attribute , default_attribute_properties ) Properties of a table column (attribute) Source code in datajoint/heading.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class Attribute ( namedtuple ( \"_Attribute\" , default_attribute_properties )): \"\"\" Properties of a table column (attribute) \"\"\" def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment ) @property def original_name ( self ): if self . attribute_expression is None : return self . name assert self . attribute_expression . startswith ( \"`\" ) return self . attribute_expression . strip ( \"`\" ) todict () \u00b6 Convert namedtuple to dict. Source code in datajoint/heading.py 51 52 53 def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) sql_type property \u00b6 Returns: Type Description datatype (as string) in database. In most cases, it is the same as self.type sql_comment property \u00b6 Returns: Type Description full comment for the SQL declaration. Includes custom type specification sql property \u00b6 Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables Returns: Type Description SQL code for attribute declaration Heading \u00b6 Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. Source code in datajoint/heading.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 class Heading : \"\"\" Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. \"\"\" def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) ) def __len__ ( self ): return 0 if self . attributes is None else len ( self . attributes ) @property def table_status ( self ): if self . table_info is None : return None if self . _table_status is None : self . _init_from_database () return self . _table_status @property def attributes ( self ): if self . _attributes is None : self . _init_from_database () # lazy loading from database return self . _attributes @property def names ( self ): return [ k for k in self . attributes ] @property def primary_key ( self ): return [ k for k , v in self . attributes . items () if v . in_key ] @property def secondary_attributes ( self ): return [ k for k , v in self . attributes . items () if not v . in_key ] @property def blobs ( self ): return [ k for k , v in self . attributes . items () if v . is_blob ] @property def non_blobs ( self ): return [ k for k , v in self . attributes . items () if not ( v . is_blob or v . is_attachment or v . is_filepath or v . json ) ] @property def new_attributes ( self ): return [ k for k , v in self . attributes . items () if v . attribute_expression is not None ] def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ] def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret @property def has_autoincrement ( self ): return any ( e . autoincrement for e in self . attributes . values ()) @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) ) def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) def __iter__ ( self ): return iter ( self . attributes ) def _init_from_database ( self ): \"\"\"initialize heading from an existing database table.\"\"\" conn , database , table_name , context = ( self . table_info [ k ] for k in ( \"conn\" , \"database\" , \"table_name\" , \"context\" ) ) info = conn . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE name=\" {table_name} \"' . format ( table_name = table_name , database = database ), as_dict = True , ) . fetchone () if info is None : if table_name == \"~log\" : logger . warning ( \"Could not create the ~log table\" ) return raise DataJointError ( \"The table ` {database} `.` {table_name} ` is not defined.\" . format ( table_name = table_name , database = database ) ) self . _table_status = { k . lower (): v for k , v in info . items ()} cur = conn . query ( \"SHOW FULL COLUMNS FROM ` {table_name} ` IN ` {database} `\" . format ( table_name = table_name , database = database ), as_dict = True , ) attributes = cur . fetchall () rename_map = { \"Field\" : \"name\" , \"Type\" : \"type\" , \"Null\" : \"nullable\" , \"Default\" : \"default\" , \"Key\" : \"in_key\" , \"Comment\" : \"comment\" , } fields_to_drop = ( \"Privileges\" , \"Collation\" ) # rename and drop attributes attributes = [ { rename_map [ k ] if k in rename_map else k : v for k , v in x . items () if k not in fields_to_drop } for x in attributes ] numeric_types = { ( \"float\" , False ): np . float64 , ( \"float\" , True ): np . float64 , ( \"double\" , False ): np . float64 , ( \"double\" , True ): np . float64 , ( \"tinyint\" , False ): np . int64 , ( \"tinyint\" , True ): np . int64 , ( \"smallint\" , False ): np . int64 , ( \"smallint\" , True ): np . int64 , ( \"mediumint\" , False ): np . int64 , ( \"mediumint\" , True ): np . int64 , ( \"int\" , False ): np . int64 , ( \"int\" , True ): np . int64 , ( \"bigint\" , False ): np . int64 , ( \"bigint\" , True ): np . uint64 , } sql_literals = [ \"CURRENT_TIMESTAMP\" ] # additional attribute properties for attr in attributes : attr . update ( in_key = ( attr [ \"in_key\" ] == \"PRI\" ), database = database , nullable = attr [ \"nullable\" ] == \"YES\" , autoincrement = bool ( re . search ( r \"auto_increment\" , attr [ \"Extra\" ], flags = re . I ) ), numeric = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"DECIMAL\" , \"INTEGER\" , \"FLOAT\" ) ), string = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"ENUM\" , \"TEMPORAL\" , \"STRING\" ) ), is_blob = bool ( TYPE_PATTERN [ \"INTERNAL_BLOB\" ] . match ( attr [ \"type\" ])), uuid = False , json = bool ( TYPE_PATTERN [ \"JSON\" ] . match ( attr [ \"type\" ])), is_attachment = False , is_filepath = False , adapter = None , store = None , is_external = False , attribute_expression = None , ) if any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"INTEGER\" , \"FLOAT\" )): attr [ \"type\" ] = re . sub ( r \"\\(\\d+\\)\" , \"\" , attr [ \"type\" ], count = 1 ) # strip size off integers and floats attr [ \"unsupported\" ] = not any ( ( attr [ \"is_blob\" ], attr [ \"numeric\" ], attr [ \"numeric\" ]) ) attr . pop ( \"Extra\" ) # process custom DataJoint types special = re . match ( r \":(?P<type>[^:]+):(?P<comment>.*)\" , attr [ \"comment\" ]) if special : special = special . groupdict () attr . update ( special ) # process adapted attribute types if special and TYPE_PATTERN [ \"ADAPTED\" ] . match ( attr [ \"type\" ]): assert context is not None , \"Declaration context is not set\" adapter_name = special [ \"type\" ] try : attr . update ( adapter = get_adapter ( context , adapter_name )) except DataJointError : # if no adapter, then delay the error until the first invocation attr . update ( adapter = AttributeAdapter ()) else : attr . update ( type = attr [ \"adapter\" ] . attribute_type ) if not any ( r . match ( attr [ \"type\" ]) for r in TYPE_PATTERN . values ()): raise DataJointError ( \"Invalid attribute type ' {type} ' in adapter object < {adapter_name} >.\" . format ( adapter_name = adapter_name , ** attr ) ) special = not any ( TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) for c in NATIVE_TYPES ) if special : try : category = next ( c for c in SPECIAL_TYPES if TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) ) except StopIteration : if attr [ \"type\" ] . startswith ( \"external\" ): url = ( \"https://docs.datajoint.io/python/admin/5-blob-config.html\" \"#migration-between-datajoint-v0-11-and-v0-12\" ) raise DataJointError ( \"Legacy datatype ` {type} `. Migrate your external stores to \" \"datajoint 0.12: {url} \" . format ( url = url , ** attr ) ) raise DataJointError ( \"Unknown attribute type ` {type} `\" . format ( ** attr ) ) if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) attr . update ( unsupported = False , is_attachment = category in ( \"INTERNAL_ATTACH\" , \"EXTERNAL_ATTACH\" ), is_filepath = category == \"FILEPATH\" , # INTERNAL_BLOB is not a custom type but is included for completeness is_blob = category in ( \"INTERNAL_BLOB\" , \"EXTERNAL_BLOB\" ), uuid = category == \"UUID\" , is_external = category in EXTERNAL_TYPES , store = attr [ \"type\" ] . split ( \"@\" )[ 1 ] if category in EXTERNAL_TYPES else None , ) if attr [ \"in_key\" ] and any ( ( attr [ \"is_blob\" ], attr [ \"is_attachment\" ], attr [ \"is_filepath\" ], attr [ \"json\" ], ) ): raise DataJointError ( \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\" ) if ( attr [ \"string\" ] and attr [ \"default\" ] is not None and attr [ \"default\" ] not in sql_literals ): attr [ \"default\" ] = '\" %s \"' % attr [ \"default\" ] if attr [ \"nullable\" ]: # nullable fields always default to null attr [ \"default\" ] = \"null\" # fill out dtype. All floats and non-nullable integers are turned into specific dtypes attr [ \"dtype\" ] = object if attr [ \"numeric\" ] and not attr [ \"adapter\" ]: is_integer = TYPE_PATTERN [ \"INTEGER\" ] . match ( attr [ \"type\" ]) is_float = TYPE_PATTERN [ \"FLOAT\" ] . match ( attr [ \"type\" ]) if is_integer and not attr [ \"nullable\" ] or is_float : is_unsigned = bool ( re . match ( \"sunsigned\" , attr [ \"type\" ], flags = re . I )) t = re . sub ( r \"\\(.*\\)\" , \"\" , attr [ \"type\" ]) # remove parentheses t = re . sub ( r \" unsigned$\" , \"\" , t ) # remove unsigned assert ( t , is_unsigned ) in numeric_types , ( \"dtype not found for type %s \" % t ) attr [ \"dtype\" ] = numeric_types [( t , is_unsigned )] if attr [ \"adapter\" ]: # restore adapted type name attr [ \"type\" ] = adapter_name self . _attributes = dict ((( q [ \"name\" ], Attribute ( ** q )) for q in attributes )) # Read and tabulate secondary indexes keys = defaultdict ( dict ) for item in conn . query ( \"SHOW KEYS FROM ` {db} `.` {tab} `\" . format ( db = database , tab = table_name ), as_dict = True , ): if item [ \"Key_name\" ] != \"PRIMARY\" : keys [ item [ \"Key_name\" ]][ item [ \"Seq_in_index\" ]] = dict ( column = item [ \"Column_name\" ] or f \"( { item [ 'Expression' ] } )\" . replace ( r \"\\'\" , \"'\" ), unique = ( item [ \"Non_unique\" ] == 0 ), nullable = item [ \"Null\" ] . lower () == \"yes\" , ) self . indexes = { tuple ( item [ k ][ \"column\" ] for k in sorted ( item . keys ())): dict ( unique = item [ 1 ][ \"unique\" ], nullable = any ( v [ \"nullable\" ] for v in item . values ()), ) for item in keys . values () } def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) ) def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () ) as_dtype property \u00b6 represent the heading as a numpy dtype as_sql ( fields , include_aliases = True ) \u00b6 represent heading as the SQL SELECT clause. Source code in datajoint/heading.py 191 192 193 194 195 196 197 198 199 200 201 def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) select ( select_list , rename_map = None , compute_map = None ) \u00b6 derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. Parameters: Name Type Description Default select_list the full list of existing attributes to include required rename_map dictionary of renamed attributes: keys=new names, values=old names None compute_map a direction of computed attributes This low-level method performs no error checking. None Source code in datajoint/heading.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) join ( other ) \u00b6 Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. Source code in datajoint/heading.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) set_primary_key ( primary_key ) \u00b6 Create a new heading with the specified primary key. This low-level method performs no error checking. Source code in datajoint/heading.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) ) make_subquery_heading () \u00b6 Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. Source code in datajoint/heading.py 518 519 520 521 522 523 524 525 526 def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () )", "title": "heading.py"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute", "text": "Bases: namedtuple ( _Attribute , default_attribute_properties ) Properties of a table column (attribute) Source code in datajoint/heading.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class Attribute ( namedtuple ( \"_Attribute\" , default_attribute_properties )): \"\"\" Properties of a table column (attribute) \"\"\" def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields )) @property def sql_type ( self ): \"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\" return UUID_DATA_TYPE if self . uuid else self . type @property def sql_comment ( self ): \"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\" return ( \":uuid:\" if self . uuid else \"\" ) + self . comment @property def sql ( self ): \"\"\" Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables :return: SQL code for attribute declaration \"\"\" return '` {name} ` {type} NOT NULL COMMENT \" {comment} \"' . format ( name = self . name , type = self . sql_type , comment = self . sql_comment ) @property def original_name ( self ): if self . attribute_expression is None : return self . name assert self . attribute_expression . startswith ( \"`\" ) return self . attribute_expression . strip ( \"`\" )", "title": "Attribute"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.todict", "text": "Convert namedtuple to dict. Source code in datajoint/heading.py 51 52 53 def todict ( self ): \"\"\"Convert namedtuple to dict.\"\"\" return dict (( name , self [ i ]) for i , name in enumerate ( self . _fields ))", "title": "todict()"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_type", "text": "Returns: Type Description datatype (as string) in database. In most cases, it is the same as self.type", "title": "sql_type"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_comment", "text": "Returns: Type Description full comment for the SQL declaration. Includes custom type specification", "title": "sql_comment"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql", "text": "Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables Returns: Type Description SQL code for attribute declaration", "title": "sql"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading", "text": "Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. Source code in datajoint/heading.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 class Heading : \"\"\" Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes. \"\"\" def __init__ ( self , attribute_specs = None , table_info = None ): \"\"\" :param attribute_specs: a list of dicts with the same keys as Attribute :param table_info: a dict with information to load the heading from the database \"\"\" self . indexes = None self . table_info = table_info self . _table_status = None self . _attributes = ( None if attribute_specs is None else dict (( q [ \"name\" ], Attribute ( ** q )) for q in attribute_specs ) ) def __len__ ( self ): return 0 if self . attributes is None else len ( self . attributes ) @property def table_status ( self ): if self . table_info is None : return None if self . _table_status is None : self . _init_from_database () return self . _table_status @property def attributes ( self ): if self . _attributes is None : self . _init_from_database () # lazy loading from database return self . _attributes @property def names ( self ): return [ k for k in self . attributes ] @property def primary_key ( self ): return [ k for k , v in self . attributes . items () if v . in_key ] @property def secondary_attributes ( self ): return [ k for k , v in self . attributes . items () if not v . in_key ] @property def blobs ( self ): return [ k for k , v in self . attributes . items () if v . is_blob ] @property def non_blobs ( self ): return [ k for k , v in self . attributes . items () if not ( v . is_blob or v . is_attachment or v . is_filepath or v . json ) ] @property def new_attributes ( self ): return [ k for k , v in self . attributes . items () if v . attribute_expression is not None ] def __getitem__ ( self , name ): \"\"\"shortcut to the attribute\"\"\" return self . attributes [ name ] def __repr__ ( self ): \"\"\" :return: heading representation in DataJoint declaration format but without foreign key expansion \"\"\" in_key = True ret = \"\" if self . _table_status is not None : ret += \"# \" + self . table_status [ \"comment\" ] + \" \\n \" for v in self . attributes . values (): if in_key and not v . in_key : ret += \"--- \\n \" in_key = False ret += \" %-20s : %-28s # %s \\n \" % ( v . name if v . default is None else \" %s = %s \" % ( v . name , v . default ), \" %s%s \" % ( v . type , \"auto_increment\" if v . autoincrement else \"\" ), v . comment , ) return ret @property def has_autoincrement ( self ): return any ( e . autoincrement for e in self . attributes . values ()) @property def as_dtype ( self ): \"\"\" represent the heading as a numpy dtype \"\"\" return np . dtype ( dict ( names = self . names , formats = [ v . dtype for v in self . attributes . values ()]) ) def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields ) def __iter__ ( self ): return iter ( self . attributes ) def _init_from_database ( self ): \"\"\"initialize heading from an existing database table.\"\"\" conn , database , table_name , context = ( self . table_info [ k ] for k in ( \"conn\" , \"database\" , \"table_name\" , \"context\" ) ) info = conn . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE name=\" {table_name} \"' . format ( table_name = table_name , database = database ), as_dict = True , ) . fetchone () if info is None : if table_name == \"~log\" : logger . warning ( \"Could not create the ~log table\" ) return raise DataJointError ( \"The table ` {database} `.` {table_name} ` is not defined.\" . format ( table_name = table_name , database = database ) ) self . _table_status = { k . lower (): v for k , v in info . items ()} cur = conn . query ( \"SHOW FULL COLUMNS FROM ` {table_name} ` IN ` {database} `\" . format ( table_name = table_name , database = database ), as_dict = True , ) attributes = cur . fetchall () rename_map = { \"Field\" : \"name\" , \"Type\" : \"type\" , \"Null\" : \"nullable\" , \"Default\" : \"default\" , \"Key\" : \"in_key\" , \"Comment\" : \"comment\" , } fields_to_drop = ( \"Privileges\" , \"Collation\" ) # rename and drop attributes attributes = [ { rename_map [ k ] if k in rename_map else k : v for k , v in x . items () if k not in fields_to_drop } for x in attributes ] numeric_types = { ( \"float\" , False ): np . float64 , ( \"float\" , True ): np . float64 , ( \"double\" , False ): np . float64 , ( \"double\" , True ): np . float64 , ( \"tinyint\" , False ): np . int64 , ( \"tinyint\" , True ): np . int64 , ( \"smallint\" , False ): np . int64 , ( \"smallint\" , True ): np . int64 , ( \"mediumint\" , False ): np . int64 , ( \"mediumint\" , True ): np . int64 , ( \"int\" , False ): np . int64 , ( \"int\" , True ): np . int64 , ( \"bigint\" , False ): np . int64 , ( \"bigint\" , True ): np . uint64 , } sql_literals = [ \"CURRENT_TIMESTAMP\" ] # additional attribute properties for attr in attributes : attr . update ( in_key = ( attr [ \"in_key\" ] == \"PRI\" ), database = database , nullable = attr [ \"nullable\" ] == \"YES\" , autoincrement = bool ( re . search ( r \"auto_increment\" , attr [ \"Extra\" ], flags = re . I ) ), numeric = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"DECIMAL\" , \"INTEGER\" , \"FLOAT\" ) ), string = any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"ENUM\" , \"TEMPORAL\" , \"STRING\" ) ), is_blob = bool ( TYPE_PATTERN [ \"INTERNAL_BLOB\" ] . match ( attr [ \"type\" ])), uuid = False , json = bool ( TYPE_PATTERN [ \"JSON\" ] . match ( attr [ \"type\" ])), is_attachment = False , is_filepath = False , adapter = None , store = None , is_external = False , attribute_expression = None , ) if any ( TYPE_PATTERN [ t ] . match ( attr [ \"type\" ]) for t in ( \"INTEGER\" , \"FLOAT\" )): attr [ \"type\" ] = re . sub ( r \"\\(\\d+\\)\" , \"\" , attr [ \"type\" ], count = 1 ) # strip size off integers and floats attr [ \"unsupported\" ] = not any ( ( attr [ \"is_blob\" ], attr [ \"numeric\" ], attr [ \"numeric\" ]) ) attr . pop ( \"Extra\" ) # process custom DataJoint types special = re . match ( r \":(?P<type>[^:]+):(?P<comment>.*)\" , attr [ \"comment\" ]) if special : special = special . groupdict () attr . update ( special ) # process adapted attribute types if special and TYPE_PATTERN [ \"ADAPTED\" ] . match ( attr [ \"type\" ]): assert context is not None , \"Declaration context is not set\" adapter_name = special [ \"type\" ] try : attr . update ( adapter = get_adapter ( context , adapter_name )) except DataJointError : # if no adapter, then delay the error until the first invocation attr . update ( adapter = AttributeAdapter ()) else : attr . update ( type = attr [ \"adapter\" ] . attribute_type ) if not any ( r . match ( attr [ \"type\" ]) for r in TYPE_PATTERN . values ()): raise DataJointError ( \"Invalid attribute type ' {type} ' in adapter object < {adapter_name} >.\" . format ( adapter_name = adapter_name , ** attr ) ) special = not any ( TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) for c in NATIVE_TYPES ) if special : try : category = next ( c for c in SPECIAL_TYPES if TYPE_PATTERN [ c ] . match ( attr [ \"type\" ]) ) except StopIteration : if attr [ \"type\" ] . startswith ( \"external\" ): url = ( \"https://docs.datajoint.io/python/admin/5-blob-config.html\" \"#migration-between-datajoint-v0-11-and-v0-12\" ) raise DataJointError ( \"Legacy datatype ` {type} `. Migrate your external stores to \" \"datajoint 0.12: {url} \" . format ( url = url , ** attr ) ) raise DataJointError ( \"Unknown attribute type ` {type} `\" . format ( ** attr ) ) if category == \"FILEPATH\" and not _support_filepath_types (): raise DataJointError ( \"\"\" The filepath data type is disabled until complete validation. To turn it on as experimental feature, set the environment variable {env} = TRUE or upgrade datajoint. \"\"\" . format ( env = FILEPATH_FEATURE_SWITCH ) ) attr . update ( unsupported = False , is_attachment = category in ( \"INTERNAL_ATTACH\" , \"EXTERNAL_ATTACH\" ), is_filepath = category == \"FILEPATH\" , # INTERNAL_BLOB is not a custom type but is included for completeness is_blob = category in ( \"INTERNAL_BLOB\" , \"EXTERNAL_BLOB\" ), uuid = category == \"UUID\" , is_external = category in EXTERNAL_TYPES , store = attr [ \"type\" ] . split ( \"@\" )[ 1 ] if category in EXTERNAL_TYPES else None , ) if attr [ \"in_key\" ] and any ( ( attr [ \"is_blob\" ], attr [ \"is_attachment\" ], attr [ \"is_filepath\" ], attr [ \"json\" ], ) ): raise DataJointError ( \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\" ) if ( attr [ \"string\" ] and attr [ \"default\" ] is not None and attr [ \"default\" ] not in sql_literals ): attr [ \"default\" ] = '\" %s \"' % attr [ \"default\" ] if attr [ \"nullable\" ]: # nullable fields always default to null attr [ \"default\" ] = \"null\" # fill out dtype. All floats and non-nullable integers are turned into specific dtypes attr [ \"dtype\" ] = object if attr [ \"numeric\" ] and not attr [ \"adapter\" ]: is_integer = TYPE_PATTERN [ \"INTEGER\" ] . match ( attr [ \"type\" ]) is_float = TYPE_PATTERN [ \"FLOAT\" ] . match ( attr [ \"type\" ]) if is_integer and not attr [ \"nullable\" ] or is_float : is_unsigned = bool ( re . match ( \"sunsigned\" , attr [ \"type\" ], flags = re . I )) t = re . sub ( r \"\\(.*\\)\" , \"\" , attr [ \"type\" ]) # remove parentheses t = re . sub ( r \" unsigned$\" , \"\" , t ) # remove unsigned assert ( t , is_unsigned ) in numeric_types , ( \"dtype not found for type %s \" % t ) attr [ \"dtype\" ] = numeric_types [( t , is_unsigned )] if attr [ \"adapter\" ]: # restore adapted type name attr [ \"type\" ] = adapter_name self . _attributes = dict ((( q [ \"name\" ], Attribute ( ** q )) for q in attributes )) # Read and tabulate secondary indexes keys = defaultdict ( dict ) for item in conn . query ( \"SHOW KEYS FROM ` {db} `.` {tab} `\" . format ( db = database , tab = table_name ), as_dict = True , ): if item [ \"Key_name\" ] != \"PRIMARY\" : keys [ item [ \"Key_name\" ]][ item [ \"Seq_in_index\" ]] = dict ( column = item [ \"Column_name\" ] or f \"( { item [ 'Expression' ] } )\" . replace ( r \"\\'\" , \"'\" ), unique = ( item [ \"Non_unique\" ] == 0 ), nullable = item [ \"Null\" ] . lower () == \"yes\" , ) self . indexes = { tuple ( item [ k ][ \"column\" ] for k in sorted ( item . keys ())): dict ( unique = item [ 1 ][ \"unique\" ], nullable = any ( v [ \"nullable\" ] for v in item . values ()), ) for item in keys . values () } def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs )) def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] ) def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) ) def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () )", "title": "Heading"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_dtype", "text": "represent the heading as a numpy dtype", "title": "as_dtype"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_sql", "text": "represent heading as the SQL SELECT clause. Source code in datajoint/heading.py 191 192 193 194 195 196 197 198 199 200 201 def as_sql ( self , fields , include_aliases = True ): \"\"\" represent heading as the SQL SELECT clause. \"\"\" return \",\" . join ( \"` %s `\" % name if self . attributes [ name ] . attribute_expression is None else self . attributes [ name ] . attribute_expression + ( \" as ` %s `\" % name if include_aliases else \"\" ) for name in fields )", "title": "as_sql()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.select", "text": "derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. Parameters: Name Type Description Default select_list the full list of existing attributes to include required rename_map dictionary of renamed attributes: keys=new names, values=old names None compute_map a direction of computed attributes This low-level method performs no error checking. None Source code in datajoint/heading.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def select ( self , select_list , rename_map = None , compute_map = None ): \"\"\" derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend. :param select_list: the full list of existing attributes to include :param rename_map: dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking. \"\"\" rename_map = rename_map or {} compute_map = compute_map or {} copy_attrs = list () for name in self . attributes : if name in select_list : copy_attrs . append ( self . attributes [ name ] . todict ()) copy_attrs . extend ( ( dict ( self . attributes [ old_name ] . todict (), name = new_name , attribute_expression = \"` %s `\" % old_name , ) for new_name , old_name in rename_map . items () if old_name == name ) ) compute_attrs = ( dict ( default_attribute_properties , name = new_name , attribute_expression = expr ) for new_name , expr in compute_map . items () ) return Heading ( chain ( copy_attrs , compute_attrs ))", "title": "select()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.join", "text": "Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. Source code in datajoint/heading.py 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 def join ( self , other ): \"\"\" Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes. \"\"\" return Heading ( [ self . attributes [ name ] . todict () for name in self . primary_key ] + [ other . attributes [ name ] . todict () for name in other . primary_key if name not in self . primary_key ] + [ self . attributes [ name ] . todict () for name in self . secondary_attributes if name not in other . primary_key ] + [ other . attributes [ name ] . todict () for name in other . secondary_attributes if name not in self . primary_key ] )", "title": "join()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.set_primary_key", "text": "Create a new heading with the specified primary key. This low-level method performs no error checking. Source code in datajoint/heading.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def set_primary_key ( self , primary_key ): \"\"\" Create a new heading with the specified primary key. This low-level method performs no error checking. \"\"\" return Heading ( chain ( ( dict ( self . attributes [ name ] . todict (), in_key = True ) for name in primary_key ), ( dict ( self . attributes [ name ] . todict (), in_key = False ) for name in self . names if name not in primary_key ), ) )", "title": "set_primary_key()"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.make_subquery_heading", "text": "Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. Source code in datajoint/heading.py 518 519 520 521 522 523 524 525 526 def make_subquery_heading ( self ): \"\"\" Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions. \"\"\" return Heading ( dict ( v . todict (), attribute_expression = None ) for v in self . attributes . values () )", "title": "make_subquery_heading()"}, {"location": "api/datajoint/jobs/", "text": "JobTable \u00b6 Bases: Table A base table with no definition. Allows reserving jobs Source code in datajoint/jobs.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class JobTable ( Table ): \"\"\" A base table with no definition. Allows reserving jobs \"\"\" def __init__ ( self , conn , database ): self . database = database self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # job reservation table for ` {database} ` table_name :varchar(255) # className of the table key_hash :char(32) # key hash --- status :enum('reserved','error','ignore') # if tuple is missing, the job is available key=null :blob # structure containing the key error_message=\"\" :varchar( {error_message_length} ) # error message returned if failed error_stack=null :mediumblob # error stack if failed user=\"\" :varchar(255) # database user host=\"\" :varchar(255) # system hostname pid=0 :int unsigned # system process id connection_id = 0 : bigint unsigned # connection_id() timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" . format ( database = database , error_message_length = ERROR_MESSAGE_LENGTH ) if not self . is_declared : self . declare () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition @property def table_name ( self ): return \"~jobs\" def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def ignore ( self , table_name , key ): \"\"\" Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - `database`.`table_name` key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"ignore\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , ) delete () \u00b6 bypass interactive prompts and dependencies Source code in datajoint/jobs.py 56 57 58 def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () drop () \u00b6 bypass interactive prompts and dependencies Source code in datajoint/jobs.py 60 61 62 def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () reserve ( table_name , key ) \u00b6 Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required Returns: Type Description True if reserved job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True ignore ( table_name , key ) \u00b6 Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - database . table_name key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def ignore ( self , table_name , key ): \"\"\" Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - `database`.`table_name` key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"ignore\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True complete ( table_name , key ) \u00b6 Log a completed job. When a job is completed, its reservation entry is deleted. Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required Source code in datajoint/jobs.py 121 122 123 124 125 126 127 128 129 def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () error ( table_name , key , error_message , error_stack = None ) \u00b6 Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required error_message string error message required error_stack stack trace None Source code in datajoint/jobs.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , )", "title": "jobs.py"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable", "text": "Bases: Table A base table with no definition. Allows reserving jobs Source code in datajoint/jobs.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class JobTable ( Table ): \"\"\" A base table with no definition. Allows reserving jobs \"\"\" def __init__ ( self , conn , database ): self . database = database self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # job reservation table for ` {database} ` table_name :varchar(255) # className of the table key_hash :char(32) # key hash --- status :enum('reserved','error','ignore') # if tuple is missing, the job is available key=null :blob # structure containing the key error_message=\"\" :varchar( {error_message_length} ) # error message returned if failed error_stack=null :mediumblob # error stack if failed user=\"\" :varchar(255) # database user host=\"\" :varchar(255) # system hostname pid=0 :int unsigned # system process id connection_id = 0 : bigint unsigned # connection_id() timestamp=CURRENT_TIMESTAMP :timestamp # automatic timestamp \"\"\" . format ( database = database , error_message_length = ERROR_MESSAGE_LENGTH ) if not self . is_declared : self . declare () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition @property def table_name ( self ): return \"~jobs\" def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick () def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick () def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def ignore ( self , table_name , key ): \"\"\" Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - `database`.`table_name` key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"ignore\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick () def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , )", "title": "JobTable"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.delete", "text": "bypass interactive prompts and dependencies Source code in datajoint/jobs.py 56 57 58 def delete ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . delete_quick ()", "title": "delete()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.drop", "text": "bypass interactive prompts and dependencies Source code in datajoint/jobs.py 60 61 62 def drop ( self ): \"\"\"bypass interactive prompts and dependencies\"\"\" self . drop_quick ()", "title": "drop()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.reserve", "text": "Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required Returns: Type Description True if reserved job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def reserve ( self , table_name , key ): \"\"\" Reserve a job for computation. When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :return: True if reserved job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"reserved\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True", "title": "reserve()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.ignore", "text": "Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - database . table_name key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken Source code in datajoint/jobs.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def ignore ( self , table_name , key ): \"\"\" Set a job to be ignored for computation. When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\". Args: table_name: Table name (str) - `database`.`table_name` key: The dict of the job's primary key Returns: True if ignore job successfully. False = the jobs is already taken \"\"\" job = dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"ignore\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , key = key , user = self . _user , ) try : with config ( enable_python_native_blobs = True ): self . insert1 ( job , ignore_extra_fields = True ) except DuplicateError : return False return True", "title": "ignore()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.complete", "text": "Log a completed job. When a job is completed, its reservation entry is deleted. Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required Source code in datajoint/jobs.py 121 122 123 124 125 126 127 128 129 def complete ( self , table_name , key ): \"\"\" Log a completed job. When a job is completed, its reservation entry is deleted. :param table_name: `database`.`table_name` :param key: the dict of the job's primary key \"\"\" job_key = dict ( table_name = table_name , key_hash = key_hash ( key )) ( self & job_key ) . delete_quick ()", "title": "complete()"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.error", "text": "Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem Parameters: Name Type Description Default table_name database . table_name required key the dict of the job's primary key required error_message string error message required error_stack stack trace None Source code in datajoint/jobs.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def error ( self , table_name , key , error_message , error_stack = None ): \"\"\" Log an error message. The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem :param table_name: `database`.`table_name` :param key: the dict of the job's primary key :param error_message: string error message :param error_stack: stack trace \"\"\" if len ( error_message ) > ERROR_MESSAGE_LENGTH : error_message = ( error_message [: ERROR_MESSAGE_LENGTH - len ( TRUNCATION_APPENDIX )] + TRUNCATION_APPENDIX ) with config ( enable_python_native_blobs = True ): self . insert1 ( dict ( table_name = table_name , key_hash = key_hash ( key ), status = \"error\" , host = platform . node (), pid = os . getpid (), connection_id = self . connection . connection_id , user = self . _user , key = key , error_message = error_message , error_stack = error_stack , ), replace = True , ignore_extra_fields = True , )", "title": "error()"}, {"location": "api/datajoint/logging/", "text": "", "title": "logging.py"}, {"location": "api/datajoint/plugin/", "text": "", "title": "plugin.py"}, {"location": "api/datajoint/preview/", "text": "methods for generating previews of query expression results in python command line and Jupyter", "title": "preview.py"}, {"location": "api/datajoint/s3/", "text": "AWS S3 operations Folder \u00b6 A Folder instance manipulates a flat folder of objects within an S3-compatible object store Source code in datajoint/s3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class Folder : \"\"\" A Folder instance manipulates a flat folder of objects within an S3-compatible object store \"\"\" def __init__ ( self , endpoint , bucket , access_key , secret_key , * , secure = False , proxy_server = None , ** _ ): # from https://docs.min.io/docs/python-client-api-reference self . client = minio . Minio ( endpoint , access_key = access_key , secret_key = secret_key , secure = secure , http_client = ( urllib3 . ProxyManager ( proxy_server , timeout = urllib3 . Timeout . DEFAULT_TIMEOUT , cert_reqs = \"CERT_REQUIRED\" , retries = urllib3 . Retry ( total = 5 , backoff_factor = 0.2 , status_forcelist = [ 500 , 502 , 503 , 504 ], ), ) if proxy_server else None ), ) self . bucket = bucket if not self . client . bucket_exists ( bucket ): raise errors . BucketInaccessible ( \"Inaccessible s3 bucket %s \" % bucket ) def put ( self , name , buffer ): logger . debug ( \"put: {} : {} \" . format ( self . bucket , name )) return self . client . put_object ( self . bucket , str ( name ), BytesIO ( buffer ), length = len ( buffer ) ) def fput ( self , local_file , name , metadata = None ): logger . debug ( \"fput: {} -> {} : {} \" . format ( self . bucket , local_file , name )) return self . client . fput_object ( self . bucket , str ( name ), str ( local_file ), metadata = metadata ) def get ( self , name ): logger . debug ( \"get: {} : {} \" . format ( self . bucket , name )) try : return self . client . get_object ( self . bucket , str ( name )) . data except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile ( \"Missing s3 key %s \" % name ) else : raise e def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ]) def exists ( self , name ): logger . debug ( \"exists: {} : {} \" . format ( self . bucket , name )) try : self . client . stat_object ( self . bucket , str ( name )) except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : return False else : raise e return True def get_size ( self , name ): logger . debug ( \"get_size: {} : {} \" . format ( self . bucket , name )) try : return self . client . stat_object ( self . bucket , str ( name )) . size except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile raise e def remove_object ( self , name ): logger . debug ( \"remove_object: {} : {} \" . format ( self . bucket , name )) try : self . client . remove_object ( self . bucket , str ( name )) except minio . error . MinioException : raise errors . DataJointError ( \"Failed to delete %s from s3 storage\" % name ) fget ( name , local_filepath ) \u00b6 get file from object name to local filepath Source code in datajoint/s3.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ])", "title": "s3.py"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder", "text": "A Folder instance manipulates a flat folder of objects within an S3-compatible object store Source code in datajoint/s3.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class Folder : \"\"\" A Folder instance manipulates a flat folder of objects within an S3-compatible object store \"\"\" def __init__ ( self , endpoint , bucket , access_key , secret_key , * , secure = False , proxy_server = None , ** _ ): # from https://docs.min.io/docs/python-client-api-reference self . client = minio . Minio ( endpoint , access_key = access_key , secret_key = secret_key , secure = secure , http_client = ( urllib3 . ProxyManager ( proxy_server , timeout = urllib3 . Timeout . DEFAULT_TIMEOUT , cert_reqs = \"CERT_REQUIRED\" , retries = urllib3 . Retry ( total = 5 , backoff_factor = 0.2 , status_forcelist = [ 500 , 502 , 503 , 504 ], ), ) if proxy_server else None ), ) self . bucket = bucket if not self . client . bucket_exists ( bucket ): raise errors . BucketInaccessible ( \"Inaccessible s3 bucket %s \" % bucket ) def put ( self , name , buffer ): logger . debug ( \"put: {} : {} \" . format ( self . bucket , name )) return self . client . put_object ( self . bucket , str ( name ), BytesIO ( buffer ), length = len ( buffer ) ) def fput ( self , local_file , name , metadata = None ): logger . debug ( \"fput: {} -> {} : {} \" . format ( self . bucket , local_file , name )) return self . client . fput_object ( self . bucket , str ( name ), str ( local_file ), metadata = metadata ) def get ( self , name ): logger . debug ( \"get: {} : {} \" . format ( self . bucket , name )) try : return self . client . get_object ( self . bucket , str ( name )) . data except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile ( \"Missing s3 key %s \" % name ) else : raise e def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ]) def exists ( self , name ): logger . debug ( \"exists: {} : {} \" . format ( self . bucket , name )) try : self . client . stat_object ( self . bucket , str ( name )) except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : return False else : raise e return True def get_size ( self , name ): logger . debug ( \"get_size: {} : {} \" . format ( self . bucket , name )) try : return self . client . stat_object ( self . bucket , str ( name )) . size except minio . error . S3Error as e : if e . code == \"NoSuchKey\" : raise errors . MissingExternalFile raise e def remove_object ( self , name ): logger . debug ( \"remove_object: {} : {} \" . format ( self . bucket , name )) try : self . client . remove_object ( self . bucket , str ( name )) except minio . error . MinioException : raise errors . DataJointError ( \"Failed to delete %s from s3 storage\" % name )", "title": "Folder"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder.fget", "text": "get file from object name to local filepath Source code in datajoint/s3.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def fget ( self , name , local_filepath ): \"\"\"get file from object name to local filepath\"\"\" logger . debug ( \"fget: {} : {} \" . format ( self . bucket , name )) name = str ( name ) stat = self . client . stat_object ( self . bucket , name ) meta = { k . lower () . lstrip ( \"x-amz-meta\" ): v for k , v in stat . metadata . items ()} data = self . client . get_object ( self . bucket , name ) local_filepath = Path ( local_filepath ) local_filepath . parent . mkdir ( parents = True , exist_ok = True ) with local_filepath . open ( \"wb\" ) as f : for d in data . stream ( 1 << 16 ): f . write ( d ) if \"contents_hash\" in meta : return uuid . UUID ( meta [ \"contents_hash\" ])", "title": "fget()"}, {"location": "api/datajoint/schemas/", "text": "ordered_dir ( class_ ) \u00b6 List (most) attributes of the class including inherited ones, similar to dir build-in function, but respects order of attribute declaration as much as possible. Parameters: Name Type Description Default class_ class to list members for required Returns: Type Description a list of attributes declared in class_ and its superclasses Source code in datajoint/schemas.py 22 23 24 25 26 27 28 29 30 31 32 33 def ordered_dir ( class_ ): \"\"\" List (most) attributes of the class including inherited ones, similar to `dir` build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses \"\"\" attr_list = list () for c in reversed ( class_ . mro ()): attr_list . extend ( e for e in c . __dict__ if e not in attr_list ) return attr_list Schema \u00b6 A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part tables.\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared and not assert_declared and self . create_tables : instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] activate ( schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None ) \u00b6 Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. Parameters: Name Type Description Default schema_name the database schema to associate. schema_name=None is used to assert that the schema has already been activated. None connection Connection object. Defaults to datajoint.conn(). None create_schema If False, do not create the schema and raise an error if missing. None create_tables If False, do not create tables and raise errors when attempting to access missing tables. None add_objects a mapping with additional objects to make available to the context in which table classes are declared. None Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) size_on_disk property \u00b6 Returns: Type Description size of the entire schema in bytes spawn_missing_classes ( context = None ) \u00b6 Creates the appropriate python user table classes from tables in the schema and places them in the context. Parameters: Name Type Description Default context alternative context to place the missing classes into, e.g. locals() None Source code in datajoint/schemas.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) drop ( force = False ) \u00b6 Drop the associated schema if it exists Source code in datajoint/schemas.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) exists property \u00b6 Returns: Type Description true if the associated schema exists on the server jobs property \u00b6 schema.jobs provides a view of the job reservation table for the schema Returns: Type Description jobs table save ( python_filename = None ) \u00b6 Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. Returns: Type Description a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) list_tables () \u00b6 Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job Returns: Type Description A list of table names from the database schema. Source code in datajoint/schemas.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ] VirtualModule \u00b6 Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None , ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ ) list_schemas ( connection = None ) \u00b6 Parameters: Name Type Description Default connection a dj.Connection object None Returns: Type Description list of all accessible schemas on the server Source code in datajoint/schemas.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ]", "title": "schemas.py"}, {"location": "api/datajoint/schemas/#datajoint.schemas.ordered_dir", "text": "List (most) attributes of the class including inherited ones, similar to dir build-in function, but respects order of attribute declaration as much as possible. Parameters: Name Type Description Default class_ class to list members for required Returns: Type Description a list of attributes declared in class_ and its superclasses Source code in datajoint/schemas.py 22 23 24 25 26 27 28 29 30 31 32 33 def ordered_dir ( class_ ): \"\"\" List (most) attributes of the class including inherited ones, similar to `dir` build-in function, but respects order of attribute declaration as much as possible. :param class_: class to list members for :return: a list of attributes declared in class_ and its superclasses \"\"\" attr_list = list () for c in reversed ( class_ . mro ()): attr_list . extend ( e for e in c . __dict__ if e not in attr_list ) return attr_list", "title": "ordered_dir()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema", "text": "A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace context in which other UserTable classes are defined. Source code in datajoint/schemas.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 class Schema : \"\"\" A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace `context` in which other UserTable classes are defined. \"\"\" def __init__ ( self , schema_name = None , context = None , * , connection = None , create_schema = True , create_tables = True , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. If the schema_name is omitted, then schema.activate(..) must be called later to associate with the database. :param schema_name: the database schema to associate. :param context: dictionary for looking up foreign key references, leave None to use local context. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: When False, do not create the schema and raise an error if missing. :param create_tables: When False, do not create tables and raise errors when accessing missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" self . _log = None self . connection = connection self . database = None self . context = context self . create_schema = create_schema self . create_tables = create_tables self . _jobs = None self . external = ExternalMapping ( self ) self . add_objects = add_objects self . declare_list = [] if schema_name : self . activate ( schema_name ) def is_activated ( self ): return self . database is not None def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context ) def _assert_exists ( self , message = None ): if not self . exists : raise DataJointError ( message or \"Schema ` {db} ` has not been created.\" . format ( db = self . database ) ) def __call__ ( self , cls , * , context = None ): \"\"\" Binds the supplied class to a schema. This is intended to be used as a decorator. :param cls: class to decorate. :param context: supplied when called from spawn_missing_classes \"\"\" context = context or self . context or inspect . currentframe () . f_back . f_locals if issubclass ( cls , Part ): raise DataJointError ( \"The schema decorator should not be applied to Part tables.\" ) if self . is_activated (): self . _decorate_master ( cls , context ) else : self . declare_list . append (( cls , context )) return cls def _decorate_master ( self , cls , context ): \"\"\" :param cls: the master class to process :param context: the class' declaration context \"\"\" self . _decorate_table ( cls , context = dict ( context , self = cls , ** { cls . __name__ : cls }) ) # Process part tables for part in ordered_dir ( cls ): if part [ 0 ] . isupper (): part = getattr ( cls , part ) if inspect . isclass ( part ) and issubclass ( part , Part ): part . _master = cls # allow addressing master by name or keyword 'master' self . _decorate_table ( part , context = dict ( context , master = cls , self = part , ** { cls . __name__ : cls } ), ) def _decorate_table ( self , table_class , context , assert_declared = False ): \"\"\" assign schema properties to the table class and declare the table \"\"\" table_class . database = self . database table_class . _connection = self . connection table_class . _heading = Heading ( table_info = dict ( conn = self . connection , database = self . database , table_name = table_class . table_name , context = context , ) ) table_class . _support = [ table_class . full_table_name ] table_class . declaration_context = context # instantiate the class, declare the table if not already instance = table_class () is_declared = instance . is_declared if not is_declared and not assert_declared and self . create_tables : instance . declare ( context ) self . connection . dependencies . clear () is_declared = is_declared or instance . is_declared # add table definition to the doc string if isinstance ( table_class . definition , str ): table_class . __doc__ = ( ( table_class . __doc__ or \"\" ) + \" \\n Table definition: \\n\\n \" + table_class . definition ) # fill values in Lookup tables from their contents property if ( isinstance ( instance , Lookup ) and hasattr ( instance , \"contents\" ) and is_declared ): contents = list ( instance . contents ) if len ( contents ) > len ( instance ): if instance . heading . has_autoincrement : warnings . warn ( ( \"Contents has changed but cannot be inserted because \" \" {table} has autoincrement.\" ) . format ( table = instance . __class__ . __name__ ) ) else : instance . insert ( contents , skip_duplicates = True ) @property def log ( self ): self . _assert_exists () if self . _log is None : self . _log = Log ( self . connection , self . database ) return self . _log def __repr__ ( self ): return \"Schema ` {name} ` \\n \" . format ( name = self . database ) @property def size_on_disk ( self ): \"\"\" :return: size of the entire schema in bytes \"\"\" self . _assert_exists () return int ( self . connection . query ( \"\"\" SELECT SUM(data_length + index_length) FROM information_schema.tables WHERE table_schema='{db}' \"\"\" . format ( db = self . database ) ) . fetchone ()[ 0 ] ) def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class ) def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) ) @property def exists ( self ): \"\"\" :return: true if the associated schema exists on the server \"\"\" if self . database is None : raise DataJointError ( \"Schema must be activated first.\" ) return bool ( self . connection . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" \"WHERE schema_name = ' {database} '\" . format ( database = self . database ) ) . rowcount ) @property def jobs ( self ): \"\"\" schema.jobs provides a view of the job reservation table for the schema :return: jobs table \"\"\" self . _assert_exists () if self . _jobs is None : self . _jobs = JobTable ( self . connection , self . database ) return self . _jobs @property def code ( self ): self . _assert_exists () return self . save () def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code ) def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "Schema"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.activate", "text": "Associate database schema schema_name . If the schema does not exist, attempt to create it on the server. Parameters: Name Type Description Default schema_name the database schema to associate. schema_name=None is used to assert that the schema has already been activated. None connection Connection object. Defaults to datajoint.conn(). None create_schema If False, do not create the schema and raise an error if missing. None create_tables If False, do not create tables and raise errors when attempting to access missing tables. None add_objects a mapping with additional objects to make available to the context in which table classes are declared. None Source code in datajoint/schemas.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def activate ( self , schema_name = None , * , connection = None , create_schema = None , create_tables = None , add_objects = None , ): \"\"\" Associate database schema `schema_name`. If the schema does not exist, attempt to create it on the server. :param schema_name: the database schema to associate. schema_name=None is used to assert that the schema has already been activated. :param connection: Connection object. Defaults to datajoint.conn(). :param create_schema: If False, do not create the schema and raise an error if missing. :param create_tables: If False, do not create tables and raise errors when attempting to access missing tables. :param add_objects: a mapping with additional objects to make available to the context in which table classes are declared. \"\"\" if schema_name is None : if self . exists : return raise DataJointError ( \"Please provide a schema_name to activate the schema.\" ) if self . database is not None and self . exists : if self . database == schema_name : # already activated return raise DataJointError ( \"The schema is already activated for schema {db} .\" . format ( db = self . database ) ) if connection is not None : self . connection = connection if self . connection is None : self . connection = conn () self . database = schema_name if create_schema is not None : self . create_schema = create_schema if create_tables is not None : self . create_tables = create_tables if add_objects : self . add_objects = add_objects if not self . exists : if not self . create_schema or not self . database : raise DataJointError ( \"Database ` {name} ` has not yet been declared. \" \"Set argument create_schema=True to create it.\" . format ( name = schema_name ) ) # create database logger . debug ( \"Creating schema ` {name} `.\" . format ( name = schema_name )) try : self . connection . query ( \"CREATE DATABASE ` {name} `\" . format ( name = schema_name ) ) except AccessError : raise DataJointError ( \"Schema ` {name} ` does not exist and could not be created. \" \"Check permissions.\" . format ( name = schema_name ) ) else : self . log ( \"created\" ) self . connection . register ( self ) # decorate all tables already decorated for cls , context in self . declare_list : if self . add_objects : context = dict ( context , ** self . add_objects ) self . _decorate_master ( cls , context )", "title": "activate()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.size_on_disk", "text": "Returns: Type Description size of the entire schema in bytes", "title": "size_on_disk"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.spawn_missing_classes", "text": "Creates the appropriate python user table classes from tables in the schema and places them in the context. Parameters: Name Type Description Default context alternative context to place the missing classes into, e.g. locals() None Source code in datajoint/schemas.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def spawn_missing_classes ( self , context = None ): \"\"\" Creates the appropriate python user table classes from tables in the schema and places them in the context. :param context: alternative context to place the missing classes into, e.g. locals() \"\"\" self . _assert_exists () if context is None : if self . context is not None : context = self . context else : # if context is missing, use the calling namespace frame = inspect . currentframe () . f_back context = frame . f_locals del frame tables = [ row [ 0 ] for row in self . connection . query ( \"SHOW TABLES in ` %s `\" % self . database ) if lookup_class_name ( \"` {db} `.` {tab} `\" . format ( db = self . database , tab = row [ 0 ]), context , 0 ) is None ] master_classes = ( Lookup , Manual , Imported , Computed ) part_tables = [] for table_name in tables : class_name = to_camel_case ( table_name ) if class_name not in context : try : cls = next ( cls for cls in master_classes if re . fullmatch ( cls . tier_regexp , table_name ) ) except StopIteration : if re . fullmatch ( Part . tier_regexp , table_name ): part_tables . append ( table_name ) else : # declare and decorate master table classes context [ class_name ] = self ( type ( class_name , ( cls ,), dict ()), context = context ) # attach parts to masters for table_name in part_tables : groups = re . fullmatch ( Part . tier_regexp , table_name ) . groupdict () class_name = to_camel_case ( groups [ \"part\" ]) try : master_class = context [ to_camel_case ( groups [ \"master\" ])] except KeyError : raise DataJointError ( \"The table %s does not follow DataJoint naming conventions\" % table_name ) part_class = type ( class_name , ( Part ,), dict ( definition =... )) part_class . _master = master_class self . _decorate_table ( part_class , context = context , assert_declared = True ) setattr ( master_class , class_name , part_class )", "title": "spawn_missing_classes()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.drop", "text": "Drop the associated schema if it exists Source code in datajoint/schemas.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def drop ( self , force = False ): \"\"\" Drop the associated schema if it exists \"\"\" if not self . exists : logger . info ( \"Schema named ` {database} ` does not exist. Doing nothing.\" . format ( database = self . database ) ) elif ( not config [ \"safemode\" ] or force or user_choice ( \"Proceed to delete entire schema ` %s `?\" % self . database , default = \"no\" ) == \"yes\" ): logger . debug ( \"Dropping ` {database} `.\" . format ( database = self . database )) try : self . connection . query ( \"DROP DATABASE ` {database} `\" . format ( database = self . database ) ) logger . debug ( \"Schema ` {database} ` was dropped successfully.\" . format ( database = self . database ) ) except AccessError : raise AccessError ( \"An attempt to drop schema ` {database} ` \" \"has failed. Check permissions.\" . format ( database = self . database ) )", "title": "drop()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.exists", "text": "Returns: Type Description true if the associated schema exists on the server", "title": "exists"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.jobs", "text": "schema.jobs provides a view of the job reservation table for the schema Returns: Type Description jobs table", "title": "jobs"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.save", "text": "Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. Returns: Type Description a string containing the body of a complete Python module defining this schema. Source code in datajoint/schemas.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def save ( self , python_filename = None ): \"\"\" Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported. :return: a string containing the body of a complete Python module defining this schema. \"\"\" self . _assert_exists () module_count = itertools . count () # add virtual modules for referenced modules with names vmod0, vmod1, ... module_lookup = collections . defaultdict ( lambda : \"vmod\" + str ( next ( module_count )) ) db = self . database def make_class_definition ( table ): tier = _get_tier ( table ) . __name__ class_name = table . split ( \".\" )[ 1 ] . strip ( \"`\" ) indent = \"\" if tier == \"Part\" : class_name = class_name . split ( \"__\" )[ - 1 ] indent += \" \" class_name = to_camel_case ( class_name ) def replace ( s ): d , tabs = s . group ( 1 ), s . group ( 2 ) return ( \"\" if d == db else ( module_lookup [ d ] + \".\" )) + \".\" . join ( to_camel_case ( tab ) for tab in tabs . lstrip ( \"__\" ) . split ( \"__\" ) ) return ( \"\" if tier == \"Part\" else \" \\n @schema \\n \" ) + ( \" {indent} class {class_name} (dj. {tier} ): \\n \" ' {indent} definition = \"\"\" \\n ' ' {indent} {defi} \"\"\"' ) . format ( class_name = class_name , indent = indent , tier = tier , defi = re . sub ( r \"`([^`]+)`.`([^`]+)`\" , replace , FreeTable ( self . connection , table ) . describe (), ) . replace ( \" \\n \" , \" \\n \" + indent ), ) diagram = Diagram ( self ) body = \" \\n\\n \" . join ( make_class_definition ( table ) for table in diagram . topological_sort () ) python_code = \" \\n\\n \" . join ( ( '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"' , \"import datajoint as dj \\n\\n schema = dj.Schema(' {db} ')\" . format ( db = db ), \" \\n \" . join ( \" {module} = dj.VirtualModule(' {module} ', ' {schema_name} ')\" . format ( module = v , schema_name = k ) for k , v in module_lookup . items () ), body , ) ) if python_filename is None : return python_code with open ( python_filename , \"wt\" ) as f : f . write ( python_code )", "title": "save()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.list_tables", "text": "Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job Returns: Type Description A list of table names from the database schema. Source code in datajoint/schemas.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 def list_tables ( self ): \"\"\" Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job :return: A list of table names from the database schema. \"\"\" return [ t for d , t in ( full_t . replace ( \"`\" , \"\" ) . split ( \".\" ) for full_t in Diagram ( self ) . topological_sort () ) if d == self . database ]", "title": "list_tables()"}, {"location": "api/datajoint/schemas/#datajoint.schemas.VirtualModule", "text": "Bases: types . ModuleType A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. Source code in datajoint/schemas.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 class VirtualModule ( types . ModuleType ): \"\"\" A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table. \"\"\" def __init__ ( self , module_name , schema_name , * , create_schema = False , create_tables = False , connection = None , add_objects = None , ): \"\"\" Creates a python module with the given name from the name of a schema on the server and automatically adds classes to it corresponding to the tables in the schema. :param module_name: displayed module name :param schema_name: name of the database in mysql :param create_schema: if True, create the schema on the database server :param create_tables: if True, module.schema can be used as the decorator for declaring new :param connection: a dj.Connection object to pass into the schema :param add_objects: additional objects to add to the module :return: the python module containing classes from the schema object and the table classes \"\"\" super ( VirtualModule , self ) . __init__ ( name = module_name ) _schema = Schema ( schema_name , create_schema = create_schema , create_tables = create_tables , connection = connection , ) if add_objects : self . __dict__ . update ( add_objects ) self . __dict__ [ \"schema\" ] = _schema _schema . spawn_missing_classes ( context = self . __dict__ )", "title": "VirtualModule"}, {"location": "api/datajoint/schemas/#datajoint.schemas.list_schemas", "text": "Parameters: Name Type Description Default connection a dj.Connection object None Returns: Type Description list of all accessible schemas on the server Source code in datajoint/schemas.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def list_schemas ( connection = None ): \"\"\" :param connection: a dj.Connection object :return: list of all accessible schemas on the server \"\"\" return [ r [ 0 ] for r in ( connection or conn ()) . query ( \"SELECT schema_name \" \"FROM information_schema.schemata \" 'WHERE schema_name <> \"information_schema\"' ) ]", "title": "list_schemas()"}, {"location": "api/datajoint/settings/", "text": "Settings for DataJoint. Config \u00b6 Bases: collections . abc . MutableMapping Source code in datajoint/settings.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class Config ( collections . abc . MutableMapping ): instance = None def __init__ ( self , * args , ** kwargs ): if not Config . instance : Config . instance = Config . __Config ( * args , ** kwargs ) else : Config . instance . _conf . update ( dict ( * args , ** kwargs )) def __getattr__ ( self , name ): return getattr ( self . instance , name ) def __getitem__ ( self , item ): return self . instance . __getitem__ ( item ) def __setitem__ ( self , item , value ): self . instance . __setitem__ ( item , value ) def __str__ ( self ): return pprint . pformat ( self . instance . _conf , indent = 4 ) def __repr__ ( self ): return self . __str__ () def __delitem__ ( self , key ): del self . instance . _conf [ key ] def __iter__ ( self ): return iter ( self . instance . _conf ) def __len__ ( self ): return len ( self . instance . _conf ) def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose ) def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key )) save ( filename , verbose = False ) \u00b6 Saves the settings in JSON format to the given file path. Parameters: Name Type Description Default filename filename of the local JSON settings file. required verbose report having saved the settings file False Source code in datajoint/settings.py 97 98 99 100 101 102 103 104 105 106 107 def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) load ( filename ) \u00b6 Updates the setting from config file in JSON format. Parameters: Name Type Description Default filename filename of the local JSON settings file. If None, the local config file is used. required Source code in datajoint/settings.py 109 110 111 112 113 114 115 116 117 118 def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) save_local ( verbose = False ) \u00b6 saves the settings in the local config file Source code in datajoint/settings.py 120 121 122 123 124 def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose ) save_global ( verbose = False ) \u00b6 saves the settings in the global config file Source code in datajoint/settings.py 126 127 128 129 130 def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) get_store_spec ( store ) \u00b6 find configuration of external stores for blobs and attachments Source code in datajoint/settings.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec", "title": "settings.py"}, {"location": "api/datajoint/settings/#datajoint.settings.Config", "text": "Bases: collections . abc . MutableMapping Source code in datajoint/settings.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class Config ( collections . abc . MutableMapping ): instance = None def __init__ ( self , * args , ** kwargs ): if not Config . instance : Config . instance = Config . __Config ( * args , ** kwargs ) else : Config . instance . _conf . update ( dict ( * args , ** kwargs )) def __getattr__ ( self , name ): return getattr ( self . instance , name ) def __getitem__ ( self , item ): return self . instance . __getitem__ ( item ) def __setitem__ ( self , item , value ): self . instance . __setitem__ ( item , value ) def __str__ ( self ): return pprint . pformat ( self . instance . _conf , indent = 4 ) def __repr__ ( self ): return self . __str__ () def __delitem__ ( self , key ): del self . instance . _conf [ key ] def __iter__ ( self ): return iter ( self . instance . _conf ) def __len__ ( self ): return len ( self . instance . _conf ) def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename ) def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid )) def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose ) def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose ) def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec @contextmanager def __call__ ( self , ** kwargs ): \"\"\" The config object can also be used in a with statement to change the state of the configuration temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a double underscore '__'. The context manager yields the changed config object. Example: >>> import datajoint as dj >>> with dj.config(safemode=False, database__host=\"localhost\") as cfg: >>> # do dangerous stuff here \"\"\" try : backup = self . instance self . instance = Config . __Config ( self . instance . _conf ) new = { k . replace ( \"__\" , \".\" ): v for k , v in kwargs . items ()} self . instance . _conf . update ( new ) yield self except : self . instance = backup raise else : self . instance = backup class __Config : \"\"\" Stores datajoint settings. Behaves like a dictionary, but applies validator functions when certain keys are set. The default parameters are stored in datajoint.settings.default . If a local config file exists, the settings specified in this file override the default settings. \"\"\" def __init__ ( self , * args , ** kwargs ): self . _conf = dict ( default ) self . _conf . update ( dict ( * args , ** kwargs )) # use the free update to set keys def __getitem__ ( self , key ): return self . _conf [ key ] def __setitem__ ( self , key , value ): logger . debug ( \"Setting {0:s} to {1:s} \" . format ( str ( key ), str ( value ))) if validators [ key ]( value ): self . _conf [ key ] = value else : raise DataJointError ( \"Validator for {0:s} did not pass\" . format ( key ))", "title": "Config"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save", "text": "Saves the settings in JSON format to the given file path. Parameters: Name Type Description Default filename filename of the local JSON settings file. required verbose report having saved the settings file False Source code in datajoint/settings.py 97 98 99 100 101 102 103 104 105 106 107 def save ( self , filename , verbose = False ): \"\"\" Saves the settings in JSON format to the given file path. :param filename: filename of the local JSON settings file. :param verbose: report having saved the settings file \"\"\" with open ( filename , \"w\" ) as fid : json . dump ( self . _conf , fid , indent = 4 ) if verbose : logger . info ( \"Saved settings in \" + filename )", "title": "save()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.load", "text": "Updates the setting from config file in JSON format. Parameters: Name Type Description Default filename filename of the local JSON settings file. If None, the local config file is used. required Source code in datajoint/settings.py 109 110 111 112 113 114 115 116 117 118 def load ( self , filename ): \"\"\" Updates the setting from config file in JSON format. :param filename: filename of the local JSON settings file. If None, the local config file is used. \"\"\" if filename is None : filename = LOCALCONFIG with open ( filename , \"r\" ) as fid : self . _conf . update ( json . load ( fid ))", "title": "load()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_local", "text": "saves the settings in the local config file Source code in datajoint/settings.py 120 121 122 123 124 def save_local ( self , verbose = False ): \"\"\" saves the settings in the local config file \"\"\" self . save ( LOCALCONFIG , verbose )", "title": "save_local()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_global", "text": "saves the settings in the global config file Source code in datajoint/settings.py 126 127 128 129 130 def save_global ( self , verbose = False ): \"\"\" saves the settings in the global config file \"\"\" self . save ( os . path . expanduser ( os . path . join ( \"~\" , GLOBALCONFIG )), verbose )", "title": "save_global()"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.get_store_spec", "text": "find configuration of external stores for blobs and attachments Source code in datajoint/settings.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def get_store_spec ( self , store ): \"\"\" find configuration of external stores for blobs and attachments \"\"\" try : spec = self [ \"stores\" ][ store ] except KeyError : raise DataJointError ( \"Storage {store} is requested but not configured\" . format ( store = store ) ) spec [ \"subfolding\" ] = spec . get ( \"subfolding\" , DEFAULT_SUBFOLDING ) spec_keys = { # REQUIRED in uppercase and allowed in lowercase \"file\" : ( \"PROTOCOL\" , \"LOCATION\" , \"subfolding\" , \"stage\" ), \"s3\" : ( \"PROTOCOL\" , \"ENDPOINT\" , \"BUCKET\" , \"ACCESS_KEY\" , \"SECRET_KEY\" , \"LOCATION\" , \"secure\" , \"subfolding\" , \"stage\" , \"proxy_server\" , ), } try : spec_keys = spec_keys [ spec . get ( \"protocol\" , \"\" ) . lower ()] except KeyError : raise DataJointError ( 'Missing or invalid protocol in dj.config[\"stores\"][\" {store} \"]' . format ( store = store ) ) # check that all required keys are present in spec try : raise DataJointError ( 'dj.config[\"stores\"][\" {store} \"] is missing \" {k} \"' . format ( store = store , k = next ( k . lower () for k in spec_keys if k . isupper () and k . lower () not in spec ), ) ) except StopIteration : pass # check that only allowed keys are present in spec try : raise DataJointError ( 'Invalid key \" {k} \" in dj.config[\"stores\"][\" {store} \"]' . format ( store = store , k = next ( k for k in spec if k . upper () not in spec_keys and k . lower () not in spec_keys ), ) ) except StopIteration : pass # no invalid keys return spec", "title": "get_store_spec()"}, {"location": "api/datajoint/table/", "text": "Table \u00b6 Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) elif attr . json : value = json . dumps ( value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert declare ( context = None ) \u00b6 Declare the table in the schema based on self.definition. Parameters: Name Type Description Default context the context for foreign key resolution. If None, foreign keys are not allowed. None Source code in datajoint/table.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) alter ( prompt = True , context = None ) \u00b6 Alter the table definition from self.definition Source code in datajoint/table.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) from_clause () \u00b6 Returns: Type Description the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 149 150 151 152 153 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name get_select_fields ( select_fields = None ) \u00b6 Returns: Type Description the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 155 156 157 158 159 160 161 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) parents ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 Parameters: Name Type Description Default primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes children ( primary = None , as_objects = False , foreign_key_info = False ) \u00b6 Parameters: Name Type Description Default primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes descendants ( as_objects = False ) \u00b6 Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables descendants in topological order. Source code in datajoint/table.py 207 208 209 210 211 212 213 214 215 216 217 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] ancestors ( as_objects = False ) \u00b6 Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables ancestors in topological order. Source code in datajoint/table.py 219 220 221 222 223 224 225 226 227 228 229 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] parts ( as_objects = False ) \u00b6 return part tables either as entries in a dict with foreign key informaiton or a list of objects Parameters: Name Type Description Default as_objects if False (default), the output is a dict describing the foreign keys. If True, return table objects. False Source code in datajoint/table.py 231 232 233 234 235 236 237 238 239 240 241 242 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes is_declared property \u00b6 Returns: Type Description True is the table is declared in the schema. full_table_name property \u00b6 Returns: Type Description full table name in the schema update1 ( row ) \u00b6 update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. Parameters: Name Type Description Default row a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default required Source code in datajoint/table.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) insert1 ( row , ** kwargs ) \u00b6 Insert one data record into the table. For kwargs , see insert() . Parameters: Name Type Description Default row a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. required Source code in datajoint/table.py 330 331 332 333 334 335 336 337 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) insert ( rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None ) \u00b6 Insert a collection of rows. Parameters: Name Type Description Default rows Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. required replace If True, replaces the existing tuple. False skip_duplicates If True, silently skip duplicate inserts. False ignore_extra_fields If False, fields that are not in the heading raise error. False allow_direct_insert Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) None Source code in datajoint/table.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) delete_quick ( get_count = False ) \u00b6 Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count delete ( transaction = True , safemode = None , force_parts = False ) \u00b6 Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If True , use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. safemode: If True , prohibit nested transactions and prompt to confirm. Default is dj.config['safemode'] . force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. Source code in datajoint/table.py 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count drop_quick () \u00b6 Drops the table without cascading to dependent tables and without user prompt. Source code in datajoint/table.py 623 624 625 626 627 628 629 630 631 632 633 634 635 def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) drop () \u00b6 Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) size_on_disk property \u00b6 Returns: Type Description size of data and indices in bytes on the storage device describe ( context = None , printout = False ) \u00b6 Returns: Type Description the definition string for the query using DataJoint DDL. Source code in datajoint/table.py 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition lookup_class_name ( name , context , depth = 3 ) \u00b6 given a table name in the form schema_name . table_name , find its class in the context. Parameters: Name Type Description Default name schema_name . table_name required context dictionary representing the namespace required depth search depth into imported modules, helps avoid infinite recursion. 3 Returns: Type Description class name found in the context or None if not found Source code in datajoint/table.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 def lookup_class_name ( name , context , depth = 3 ): \"\"\" given a table name in the form `schema_name`.`table_name`, find its class in the context. :param name: `schema_name`.`table_name` :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found \"\"\" # breadth-first search nodes = [ dict ( context = context , context_name = \"\" , depth = depth )] while nodes : node = nodes . pop ( 0 ) for member_name , member in node [ \"context\" ] . items (): if not member_name . startswith ( \"_\" ): # skip IPython's implicit variables if inspect . isclass ( member ) and issubclass ( member , Table ): if member . full_table_name == name : # found it! return \".\" . join ([ node [ \"context_name\" ], member_name ]) . lstrip ( \".\" ) try : # look for part tables parts = member . __dict__ except AttributeError : pass # not a UserTable -- cannot have part tables. else : for part in ( getattr ( member , p ) for p in parts if p [ 0 ] . isupper () and hasattr ( member , p ) ): if ( inspect . isclass ( part ) and issubclass ( part , Table ) and part . full_table_name == name ): return \".\" . join ( [ node [ \"context_name\" ], member_name , part . __name__ ] ) . lstrip ( \".\" ) elif ( node [ \"depth\" ] > 0 and inspect . ismodule ( member ) and member . __name__ != \"datajoint\" ): try : nodes . append ( dict ( context = dict ( inspect . getmembers ( member )), context_name = node [ \"context_name\" ] + \".\" + member_name , depth = node [ \"depth\" ] - 1 , ) ) except ImportError : pass # could not import, so do not attempt return None FreeTable \u00b6 Bases: Table A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. Parameters: Name Type Description Default conn a dj.Connection object required full_table_name in format database . table_name required Source code in datajoint/table.py 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 class FreeTable ( Table ): \"\"\" A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () ) Log \u00b6 Bases: Table The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. Parameters: Name Type Description Default skip_logging if True, then log entry is skipped by default. See call False Source code in datajoint/table.py 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 class Log ( Table ): \"\"\" The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See __call__ \"\"\" _table_name = \"~log\" def __init__ ( self , conn , database , skip_logging = False ): self . database = database self . skip_logging = skip_logging self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # event logging table for ` {database} ` id :int unsigned auto_increment # event order id --- timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp version :varchar(12) # datajoint version user :varchar(255) # user@host host=\"\" :varchar(255) # system hostname event=\"\" :varchar(255) # event message \"\"\" . format ( database = database ) super () . __init__ () if not self . is_declared : self . declare () self . connection . dependencies . clear () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" ) def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick () delete () \u00b6 bypass interactive prompts and cascading dependencies Returns: Type Description number of deleted items Source code in datajoint/table.py 1063 1064 1065 1066 1067 1068 1069 def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) drop () \u00b6 bypass interactive prompts and cascading dependencies Source code in datajoint/table.py 1071 1072 1073 def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick ()", "title": "table.py"}, {"location": "api/datajoint/table/#datajoint.table.Table", "text": "Bases: QueryExpression Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. Source code in datajoint/table.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 class Table ( QueryExpression ): \"\"\" Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition. \"\"\" _table_name = None # must be defined in subclass _log_ = None # placeholder for the Log table object # These properties must be set by the schema decorator (schemas.py) at class level # or by FreeTable at instance level database = None declaration_context = None @property def table_name ( self ): return self . _table_name @property def definition ( self ): raise NotImplementedError ( \"Subclasses of Table must implement the `definition` property\" ) def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name ) def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name ) def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql ) def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ] def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ] def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes @property def is_declared ( self ): \"\"\" :return: True is the table is declared in the schema. \"\"\" return ( self . connection . query ( 'SHOW TABLES in ` {database} ` LIKE \" {table_name} \"' . format ( database = self . database , table_name = self . table_name ) ) . rowcount > 0 ) @property def full_table_name ( self ): \"\"\" :return: full table name in the schema \"\"\" return r \"` {0:s} `.` {1:s} `\" . format ( self . database , self . table_name ) @property def _log ( self ): if self . _log_ is None : self . _log_ = Log ( self . connection , database = self . database , skip_logging = self . table_name . startswith ( \"~\" ), ) return self . _log_ @property def external ( self ): return self . connection . schemas [ self . database ] . external def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None )) def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs ) def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" ) def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name ) def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" ) @property def size_on_disk ( self ): \"\"\" :return: size of data and indices in bytes on the storage device \"\"\" ret = self . connection . query ( 'SHOW TABLE STATUS FROM ` {database} ` WHERE NAME=\" {table} \"' . format ( database = self . database , table = self . table_name ), as_dict = True , ) . fetchone () return ret [ \"Data_length\" ] + ret [ \"Index_length\" ] def show_definition ( self ): raise AttributeError ( \"show_definition is deprecated. Use the describe method instead.\" ) def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition # --- private helper functions ---- def __make_placeholder ( self , name , value , ignore_extra_fields = False ): \"\"\" For a given attribute `name` with `value`, return its processed value or value placeholder as a string to be included in the query and the value, if any, to be submitted for processing by mysql API. :param name: name of attribute to be inserted :param value: value of attribute to be inserted \"\"\" if ignore_extra_fields and name not in self . heading : return None attr = self . heading [ name ] if attr . adapter : value = attr . adapter . put ( value ) if value is None or ( attr . numeric and ( value == \"\" or np . isnan ( float ( value )))): # set default value placeholder , value = \"DEFAULT\" , None else : # not NULL placeholder = \" %s \" if attr . uuid : if not isinstance ( value , uuid . UUID ): try : value = uuid . UUID ( value ) except ( AttributeError , ValueError ): raise DataJointError ( \"badly formed UUID value {v} for attribute ` {n} `\" . format ( v = value , n = name ) ) value = value . bytes elif attr . is_blob : value = blob . pack ( value ) value = ( self . external [ attr . store ] . put ( value ) . bytes if attr . is_external else value ) elif attr . is_attachment : attachment_path = Path ( value ) if attr . is_external : # value is hash of contents value = ( self . external [ attr . store ] . upload_attachment ( attachment_path ) . bytes ) else : # value is filename + contents value = ( str . encode ( attachment_path . name ) + b \" \\0 \" + attachment_path . read_bytes () ) elif attr . is_filepath : value = self . external [ attr . store ] . upload_filepath ( value ) . bytes elif attr . numeric : value = str ( int ( value ) if isinstance ( value , bool ) else value ) elif attr . json : value = json . dumps ( value ) return name , placeholder , value def __make_row_to_insert ( self , row , field_list , ignore_extra_fields ): \"\"\" Helper function for insert and update :param row: A tuple to insert :return: a dict with fields 'names', 'placeholders', 'values' \"\"\" def check_fields ( fields ): \"\"\" Validates that all items in `fields` are valid attributes in the heading :param fields: field names of a tuple \"\"\" if not field_list : if not ignore_extra_fields : for field in fields : if field not in self . heading : raise KeyError ( \"` {0:s} ` is not in the table heading\" . format ( field ) ) elif set ( field_list ) != set ( fields ) . intersection ( self . heading . names ): raise DataJointError ( \"Attempt to insert rows with different fields.\" ) if isinstance ( row , np . void ): # np.array check_fields ( row . dtype . fields ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row . dtype . fields ] elif isinstance ( row , collections . abc . Mapping ): # dict-based check_fields ( row ) attributes = [ self . __make_placeholder ( name , row [ name ], ignore_extra_fields ) for name in self . heading if name in row ] else : # positional try : if len ( row ) != len ( self . heading ): raise DataJointError ( \"Invalid insert argument. Incorrect number of attributes: \" \" {given} given; {expected} expected\" . format ( given = len ( row ), expected = len ( self . heading ) ) ) except TypeError : raise DataJointError ( \"Datatype %s cannot be inserted\" % type ( row )) else : attributes = [ self . __make_placeholder ( name , value , ignore_extra_fields ) for name , value in zip ( self . heading , row ) ] if ignore_extra_fields : attributes = [ a for a in attributes if a is not None ] assert len ( attributes ), \"Empty tuple\" row_to_insert = dict ( zip (( \"names\" , \"placeholders\" , \"values\" ), zip ( * attributes ))) if not field_list : # first row sets the composition of the field list field_list . extend ( row_to_insert [ \"names\" ]) else : # reorder attributes in row_to_insert to match field_list order = list ( row_to_insert [ \"names\" ] . index ( field ) for field in field_list ) row_to_insert [ \"names\" ] = list ( row_to_insert [ \"names\" ][ i ] for i in order ) row_to_insert [ \"placeholders\" ] = list ( row_to_insert [ \"placeholders\" ][ i ] for i in order ) row_to_insert [ \"values\" ] = list ( row_to_insert [ \"values\" ][ i ] for i in order ) return row_to_insert", "title": "Table"}, {"location": "api/datajoint/table/#datajoint.table.Table.declare", "text": "Declare the table in the schema based on self.definition. Parameters: Name Type Description Default context the context for foreign key resolution. If None, foreign keys are not allowed. None Source code in datajoint/table.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def declare ( self , context = None ): \"\"\" Declare the table in the schema based on self.definition. :param context: the context for foreign key resolution. If None, foreign keys are not allowed. \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot declare new tables inside a transaction, \" \"e.g. from inside a populate/make call\" ) sql , external_stores = declare ( self . full_table_name , self . definition , context ) sql = sql . format ( database = self . database ) try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : self . _log ( \"Declared \" + self . full_table_name )", "title": "declare()"}, {"location": "api/datajoint/table/#datajoint.table.Table.alter", "text": "Alter the table definition from self.definition Source code in datajoint/table.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def alter ( self , prompt = True , context = None ): \"\"\" Alter the table definition from self.definition \"\"\" if self . connection . in_transaction : raise DataJointError ( \"Cannot update table declaration inside a transaction, \" \"e.g. from inside a populate/make call\" ) if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame old_definition = self . describe ( context = context ) sql , external_stores = alter ( self . definition , old_definition , context ) if not sql : if prompt : logger . warn ( \"Nothing to alter.\" ) else : sql = \"ALTER TABLE {tab} \\n\\t \" . format ( tab = self . full_table_name ) + \", \\n\\t \" . join ( sql ) if not prompt or user_choice ( sql + \" \\n\\n Execute?\" ) == \"yes\" : try : # declare all external tables before declaring main table for store in external_stores : self . connection . schemas [ self . database ] . external [ store ] self . connection . query ( sql ) except AccessError : # skip if no create privilege pass else : # reset heading self . __class__ . _heading = Heading ( table_info = self . heading . table_info ) if prompt : logger . info ( \"Table altered\" ) self . _log ( \"Altered \" + self . full_table_name )", "title": "alter()"}, {"location": "api/datajoint/table/#datajoint.table.Table.from_clause", "text": "Returns: Type Description the FROM clause of SQL SELECT statements. Source code in datajoint/table.py 149 150 151 152 153 def from_clause ( self ): \"\"\" :return: the FROM clause of SQL SELECT statements. \"\"\" return self . full_table_name", "title": "from_clause()"}, {"location": "api/datajoint/table/#datajoint.table.Table.get_select_fields", "text": "Returns: Type Description the selected attributes from the SQL SELECT statement. Source code in datajoint/table.py 155 156 157 158 159 160 161 def get_select_fields ( self , select_fields = None ): \"\"\" :return: the selected attributes from the SQL SELECT statement. \"\"\" return ( \"*\" if select_fields is None else self . heading . project ( select_fields ) . as_sql )", "title": "get_select_fields()"}, {"location": "api/datajoint/table/#datajoint.table.Table.parents", "text": "Parameters: Name Type Description Default primary if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of parents as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def parents ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . parents nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "parents()"}, {"location": "api/datajoint/table/#datajoint.table.Table.children", "text": "Parameters: Name Type Description Default primary if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. None as_objects if False, return table names. If True, return table objects. False foreign_key_info if True, each element in result also includes foreign key info. False Returns: Type Description list of children as table names or table objects with (optional) foreign key information. Source code in datajoint/table.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def children ( self , primary = None , as_objects = False , foreign_key_info = False ): \"\"\" :param primary: if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered. If False, return foreign keys including at least one secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects with (optional) foreign key information. \"\"\" get_edge = self . connection . dependencies . children nodes = [ next ( iter ( get_edge ( name ) . items ())) if name . isdigit () else ( name , props ) for name , props in get_edge ( self . full_table_name , primary ) . items () ] if as_objects : nodes = [( FreeTable ( self . connection , name ), props ) for name , props in nodes ] if not foreign_key_info : nodes = [ name for name , props in nodes ] return nodes", "title": "children()"}, {"location": "api/datajoint/table/#datajoint.table.Table.descendants", "text": "Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables descendants in topological order. Source code in datajoint/table.py 207 208 209 210 211 212 213 214 215 216 217 def descendants ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . descendants ( self . full_table_name ) if not node . isdigit () ]", "title": "descendants()"}, {"location": "api/datajoint/table/#datajoint.table.Table.ancestors", "text": "Parameters: Name Type Description Default as_objects False - a list of table names; True - a list of table objects. False Returns: Type Description list of tables ancestors in topological order. Source code in datajoint/table.py 219 220 221 222 223 224 225 226 227 228 229 def ancestors ( self , as_objects = False ): \"\"\" :param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order. \"\"\" return [ FreeTable ( self . connection , node ) if as_objects else node for node in self . connection . dependencies . ancestors ( self . full_table_name ) if not node . isdigit () ]", "title": "ancestors()"}, {"location": "api/datajoint/table/#datajoint.table.Table.parts", "text": "return part tables either as entries in a dict with foreign key informaiton or a list of objects Parameters: Name Type Description Default as_objects if False (default), the output is a dict describing the foreign keys. If True, return table objects. False Source code in datajoint/table.py 231 232 233 234 235 236 237 238 239 240 241 242 def parts ( self , as_objects = False ): \"\"\" return part tables either as entries in a dict with foreign key informaiton or a list of objects :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects. \"\"\" nodes = [ node for node in self . connection . dependencies . nodes if not node . isdigit () and node . startswith ( self . full_table_name [: - 1 ] + \"__\" ) ] return [ FreeTable ( self . connection , c ) for c in nodes ] if as_objects else nodes", "title": "parts()"}, {"location": "api/datajoint/table/#datajoint.table.Table.is_declared", "text": "Returns: Type Description True is the table is declared in the schema.", "title": "is_declared"}, {"location": "api/datajoint/table/#datajoint.table.Table.full_table_name", "text": "Returns: Type Description full table name in the schema", "title": "full_table_name"}, {"location": "api/datajoint/table/#datajoint.table.Table.update1", "text": "update1 updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to insert and delete entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. Parameters: Name Type Description Default row a dict containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default required Source code in datajoint/table.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def update1 ( self , row ): \"\"\" ``update1`` updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and ``delete`` entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions. :param row: a ``dict`` containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any). The primary key attributes must always be provided. Examples: >>> table.update1({'id': 1, 'value': 3}) # update value in record with id=1 >>> table.update1({'id': 1, 'value': None}) # reset value to default \"\"\" # argument validations if not isinstance ( row , collections . abc . Mapping ): raise DataJointError ( \"The argument of update1 must be dict-like.\" ) if not set ( row ) . issuperset ( self . primary_key ): raise DataJointError ( \"The argument of update1 must supply all primary key values.\" ) try : raise DataJointError ( \"Attribute ` %s ` not found.\" % next ( k for k in row if k not in self . heading . names ) ) except StopIteration : pass # ok if len ( self . restriction ): raise DataJointError ( \"Update cannot be applied to a restricted table.\" ) key = { k : row [ k ] for k in self . primary_key } if len ( self & key ) != 1 : raise DataJointError ( \"Update can only be applied to one existing entry.\" ) # UPDATE query row = [ self . __make_placeholder ( k , v ) for k , v in row . items () if k not in self . primary_key ] query = \"UPDATE {table} SET {assignments} WHERE {where} \" . format ( table = self . full_table_name , assignments = \",\" . join ( \"` %s `= %s \" % r [: 2 ] for r in row ), where = make_condition ( self , key , set ()), ) self . connection . query ( query , args = list ( r [ 2 ] for r in row if r [ 2 ] is not None ))", "title": "update1()"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert1", "text": "Insert one data record into the table. For kwargs , see insert() . Parameters: Name Type Description Default row a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. required Source code in datajoint/table.py 330 331 332 333 334 335 336 337 def insert1 ( self , row , ** kwargs ): \"\"\" Insert one data record into the table. For ``kwargs``, see ``insert()``. :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted as one row. \"\"\" self . insert (( row ,), ** kwargs )", "title": "insert1()"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert", "text": "Insert a collection of rows. Parameters: Name Type Description Default rows Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. required replace If True, replaces the existing tuple. False skip_duplicates If True, silently skip duplicate inserts. False ignore_extra_fields If False, fields that are not in the heading raise error. False allow_direct_insert Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) None Source code in datajoint/table.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def insert ( self , rows , replace = False , skip_duplicates = False , ignore_extra_fields = False , allow_direct_insert = None , ): \"\"\" Insert a collection of rows. :param rows: Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback. Example: >>> Table.insert([ >>> dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), >>> dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")]) \"\"\" if isinstance ( rows , pandas . DataFrame ): # drop 'extra' synthetic index for 1-field index case - # frames with more advanced indices should be prepared by user. rows = rows . reset_index ( drop = len ( rows . index . names ) == 1 and not rows . index . names [ 0 ] ) . to_records ( index = False ) if isinstance ( rows , Path ): with open ( rows , newline = \"\" ) as data_file : rows = list ( csv . DictReader ( data_file , delimiter = \",\" )) # prohibit direct inserts into auto-populated tables if not allow_direct_insert and not getattr ( self , \"_allow_insert\" , True ): raise DataJointError ( \"Inserts into an auto-populated table can only be done inside \" \"its make method during a populate call.\" \" To override, set keyword argument allow_direct_insert=True.\" ) if inspect . isclass ( rows ) and issubclass ( rows , QueryExpression ): rows = rows () # instantiate if a class if isinstance ( rows , QueryExpression ): # insert from select if not ignore_extra_fields : try : raise DataJointError ( \"Attribute %s not found. To ignore extra attributes in insert, \" \"set ignore_extra_fields=True.\" % next ( name for name in rows . heading if name not in self . heading ) ) except StopIteration : pass fields = list ( name for name in rows . heading if name in self . heading ) query = \" {command} INTO {table} ( {fields} ) {select}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , fields = \"`\" + \"`,`\" . join ( fields ) + \"`\" , table = self . full_table_name , select = rows . make_sql ( fields ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `= {table} .` {pk} `\" . format ( table = self . full_table_name , pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query ) return field_list = [] # collects the field list from first row (passed by reference) rows = list ( self . __make_row_to_insert ( row , field_list , ignore_extra_fields ) for row in rows ) if rows : try : query = \" {command} INTO {destination} (` {fields} `) VALUES {placeholders}{duplicate} \" . format ( command = \"REPLACE\" if replace else \"INSERT\" , destination = self . from_clause (), fields = \"`,`\" . join ( field_list ), placeholders = \",\" . join ( \"(\" + \",\" . join ( row [ \"placeholders\" ]) + \")\" for row in rows ), duplicate = ( \" ON DUPLICATE KEY UPDATE ` {pk} `=` {pk} `\" . format ( pk = self . primary_key [ 0 ] ) if skip_duplicates else \"\" ), ) self . connection . query ( query , args = list ( itertools . chain . from_iterable ( ( v for v in r [ \"values\" ] if v is not None ) for r in rows ) ), ) except UnknownAttributeError as err : raise err . suggest ( \"To ignore extra fields in insert, set ignore_extra_fields=True\" ) except DuplicateError as err : raise err . suggest ( \"To ignore duplicate entries in insert, set skip_duplicates=True\" )", "title": "insert()"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete_quick", "text": "Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. Source code in datajoint/table.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def delete_quick ( self , get_count = False ): \"\"\" Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail. \"\"\" query = \"DELETE FROM \" + self . full_table_name + self . where_clause () self . connection . query ( query ) count = ( self . connection . query ( \"SELECT ROW_COUNT()\" ) . fetchone ()[ 0 ] if get_count else None ) self . _log ( query [: 255 ]) return count", "title": "delete_quick()"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete", "text": "Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If True , use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to False if this delete is nested within another transaction. safemode: If True , prohibit nested transactions and prompt to confirm. Default is dj.config['safemode'] . force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. Source code in datajoint/table.py 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 def delete ( self , transaction : bool = True , safemode : Union [ bool , None ] = None , force_parts : bool = False , ) -> int : \"\"\" Deletes the contents of the table and its dependent tables, recursively. Args: transaction: If `True`, use of the entire delete becomes an atomic transaction. This is the default and recommended behavior. Set to `False` if this delete is nested within another transaction. safemode: If `True`, prohibit nested transactions and prompt to confirm. Default is `dj.config['safemode']`. force_parts: Delete from parts even when not deleting from their masters. Returns: Number of deleted rows (excluding those from dependent tables). Raises: DataJointError: Delete exceeds maximum number of delete attempts. DataJointError: When deleting within an existing transaction. DataJointError: Deleting a part table before its master. \"\"\" deleted = set () def cascade ( table ): \"\"\"service function to perform cascading deletes recursively.\"\"\" max_attempts = 50 for _ in range ( max_attempts ): try : delete_count = table . delete_quick ( get_count = True ) except IntegrityError as error : match = foreign_key_error_regexp . match ( error . args [ 0 ]) . groupdict () if \"`.`\" not in match [ \"child\" ]: # if schema name missing, use table match [ \"child\" ] = \" {} . {} \" . format ( table . full_table_name . split ( \".\" )[ 0 ], match [ \"child\" ] ) if ( match [ \"pk_attrs\" ] is not None ): # fully matched, adjusting the keys match [ \"fk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"fk_attrs\" ] . split ( \",\" ) ] match [ \"pk_attrs\" ] = [ k . strip ( \"`\" ) for k in match [ \"pk_attrs\" ] . split ( \",\" ) ] else : # only partially matched, querying with constraint to determine keys match [ \"fk_attrs\" ], match [ \"parent\" ], match [ \"pk_attrs\" ] = list ( map ( list , zip ( * table . connection . query ( constraint_info_query , args = ( match [ \"name\" ] . strip ( \"`\" ), * [ _ . strip ( \"`\" ) for _ in match [ \"child\" ] . split ( \"`.`\" ) ], ), ) . fetchall () ), ) ) match [ \"parent\" ] = match [ \"parent\" ][ 0 ] # Restrict child by table if # 1. if table's restriction attributes are not in child's primary key # 2. if child renames any attributes # Otherwise restrict child by table's restriction. child = FreeTable ( table . connection , match [ \"child\" ]) if ( set ( table . restriction_attributes ) <= set ( child . primary_key ) and match [ \"fk_attrs\" ] == match [ \"pk_attrs\" ] ): child . _restriction = table . _restriction elif match [ \"fk_attrs\" ] != match [ \"pk_attrs\" ]: child &= table . proj ( ** dict ( zip ( match [ \"fk_attrs\" ], match [ \"pk_attrs\" ])) ) else : child &= table . proj () cascade ( child ) else : deleted . add ( table . full_table_name ) logger . info ( \"Deleting {count} rows from {table} \" . format ( count = delete_count , table = table . full_table_name ) ) break else : raise DataJointError ( \"Exceeded maximum number of delete attempts.\" ) return delete_count safemode = config [ \"safemode\" ] if safemode is None else safemode # Start transaction if transaction : if not self . connection . in_transaction : self . connection . start_transaction () else : if not safemode : transaction = False else : raise DataJointError ( \"Delete cannot use a transaction within an ongoing transaction. \" \"Set transaction=False or safemode=False).\" ) # Cascading delete try : delete_count = cascade ( self ) except : if transaction : self . connection . cancel_transaction () raise if not force_parts : # Avoid deleting from child before master (See issue #151) for part in deleted : master = get_master ( part ) if master and master not in deleted : if transaction : self . connection . cancel_transaction () raise DataJointError ( \"Attempt to delete part table {part} before deleting from \" \"its master {master} first.\" . format ( part = part , master = master ) ) # Confirm and commit if delete_count == 0 : if safemode : logger . warn ( \"Nothing to delete.\" ) if transaction : self . connection . cancel_transaction () else : if not safemode or user_choice ( \"Commit deletes?\" , default = \"no\" ) == \"yes\" : if transaction : self . connection . commit_transaction () if safemode : logger . info ( \"Deletes committed.\" ) else : if transaction : self . connection . cancel_transaction () if safemode : logger . warn ( \"Deletes cancelled\" ) return delete_count", "title": "delete()"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop_quick", "text": "Drops the table without cascading to dependent tables and without user prompt. Source code in datajoint/table.py 623 624 625 626 627 628 629 630 631 632 633 634 635 def drop_quick ( self ): \"\"\" Drops the table without cascading to dependent tables and without user prompt. \"\"\" if self . is_declared : query = \"DROP TABLE %s \" % self . full_table_name self . connection . query ( query ) logger . info ( \"Dropped table %s \" % self . full_table_name ) self . _log ( query [: 255 ]) else : logger . info ( \"Nothing to drop: table %s is not declared\" % self . full_table_name )", "title": "drop_quick()"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop", "text": "Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. Source code in datajoint/table.py 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 def drop ( self ): \"\"\" Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True. \"\"\" if self . restriction : raise DataJointError ( \"A table with an applied restriction cannot be dropped.\" \" Call drop() on the unrestricted Table.\" ) self . connection . dependencies . load () do_drop = True tables = [ table for table in self . connection . dependencies . descendants ( self . full_table_name ) if not table . isdigit () ] # avoid dropping part tables without their masters: See issue #374 for part in tables : master = get_master ( part ) if master and master not in tables : raise DataJointError ( \"Attempt to drop part table {part} before dropping \" \"its master. Drop {master} first.\" . format ( part = part , master = master ) ) if config [ \"safemode\" ]: for table in tables : logger . info ( table + \" ( %d tuples)\" % len ( FreeTable ( self . connection , table )) ) do_drop = user_choice ( \"Proceed?\" , default = \"no\" ) == \"yes\" if do_drop : for table in reversed ( tables ): FreeTable ( self . connection , table ) . drop_quick () logger . info ( \"Tables dropped. Restart kernel.\" )", "title": "drop()"}, {"location": "api/datajoint/table/#datajoint.table.Table.size_on_disk", "text": "Returns: Type Description size of data and indices in bytes on the storage device", "title": "size_on_disk"}, {"location": "api/datajoint/table/#datajoint.table.Table.describe", "text": "Returns: Type Description the definition string for the query using DataJoint DDL. Source code in datajoint/table.py 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 def describe ( self , context = None , printout = False ): \"\"\" :return: the definition string for the query using DataJoint DDL. \"\"\" if context is None : frame = inspect . currentframe () . f_back context = dict ( frame . f_globals , ** frame . f_locals ) del frame if self . full_table_name not in self . connection . dependencies : self . connection . dependencies . load () parents = self . parents ( foreign_key_info = True ) in_key = True definition = ( \"# \" + self . heading . table_status [ \"comment\" ] + \" \\n \" if self . heading . table_status [ \"comment\" ] else \"\" ) attributes_thus_far = set () attributes_declared = set () indexes = self . heading . indexes . copy () for attr in self . heading . attributes . values (): if in_key and not attr . in_key : definition += \"--- \\n \" in_key = False attributes_thus_far . add ( attr . name ) do_include = True for parent_name , fk_props in parents : if attr . name in fk_props [ \"attr_map\" ]: do_include = False if attributes_thus_far . issuperset ( fk_props [ \"attr_map\" ]): # foreign key properties try : index_props = indexes . pop ( tuple ( fk_props [ \"attr_map\" ])) except KeyError : index_props = \"\" else : index_props = [ k for k , v in index_props . items () if v ] index_props = ( \" [ {} ]\" . format ( \", \" . join ( index_props )) if index_props else \"\" ) if not fk_props [ \"aliased\" ]: # simple foreign key definition += \"-> {props} {class_name} \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , ) else : # projected foreign key definition += ( \"-> {props} {class_name} .proj( {proj_list} ) \\n \" . format ( props = index_props , class_name = lookup_class_name ( parent_name , context ) or parent_name , proj_list = \",\" . join ( ' {} =\" {} \"' . format ( attr , ref ) for attr , ref in fk_props [ \"attr_map\" ] . items () if ref != attr ), ) ) attributes_declared . update ( fk_props [ \"attr_map\" ]) if do_include : attributes_declared . add ( attr . name ) definition += \" %-20s : %-28s %s \\n \" % ( attr . name if attr . default is None else \" %s = %s \" % ( attr . name , attr . default ), \" %s%s \" % ( attr . type , \" auto_increment\" if attr . autoincrement else \"\" ), \"# \" + attr . comment if attr . comment else \"\" , ) # add remaining indexes for k , v in indexes . items (): definition += \" {unique} INDEX ( {attrs} ) \\n \" . format ( unique = \"UNIQUE \" if v [ \"unique\" ] else \"\" , attrs = \", \" . join ( k ) ) if printout : logger . info ( \" \\n \" + definition ) return definition", "title": "describe()"}, {"location": "api/datajoint/table/#datajoint.table.lookup_class_name", "text": "given a table name in the form schema_name . table_name , find its class in the context. Parameters: Name Type Description Default name schema_name . table_name required context dictionary representing the namespace required depth search depth into imported modules, helps avoid infinite recursion. 3 Returns: Type Description class name found in the context or None if not found Source code in datajoint/table.py 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 def lookup_class_name ( name , context , depth = 3 ): \"\"\" given a table name in the form `schema_name`.`table_name`, find its class in the context. :param name: `schema_name`.`table_name` :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found \"\"\" # breadth-first search nodes = [ dict ( context = context , context_name = \"\" , depth = depth )] while nodes : node = nodes . pop ( 0 ) for member_name , member in node [ \"context\" ] . items (): if not member_name . startswith ( \"_\" ): # skip IPython's implicit variables if inspect . isclass ( member ) and issubclass ( member , Table ): if member . full_table_name == name : # found it! return \".\" . join ([ node [ \"context_name\" ], member_name ]) . lstrip ( \".\" ) try : # look for part tables parts = member . __dict__ except AttributeError : pass # not a UserTable -- cannot have part tables. else : for part in ( getattr ( member , p ) for p in parts if p [ 0 ] . isupper () and hasattr ( member , p ) ): if ( inspect . isclass ( part ) and issubclass ( part , Table ) and part . full_table_name == name ): return \".\" . join ( [ node [ \"context_name\" ], member_name , part . __name__ ] ) . lstrip ( \".\" ) elif ( node [ \"depth\" ] > 0 and inspect . ismodule ( member ) and member . __name__ != \"datajoint\" ): try : nodes . append ( dict ( context = dict ( inspect . getmembers ( member )), context_name = node [ \"context_name\" ] + \".\" + member_name , depth = node [ \"depth\" ] - 1 , ) ) except ImportError : pass # could not import, so do not attempt return None", "title": "lookup_class_name()"}, {"location": "api/datajoint/table/#datajoint.table.FreeTable", "text": "Bases: Table A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. Parameters: Name Type Description Default conn a dj.Connection object required full_table_name in format database . table_name required Source code in datajoint/table.py 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 class FreeTable ( Table ): \"\"\" A base table without a dedicated class. Each instance is associated with a table specified by full_table_name. :param conn: a dj.Connection object :param full_table_name: in format `database`.`table_name` \"\"\" def __init__ ( self , conn , full_table_name ): self . database , self . _table_name = ( s . strip ( \"`\" ) for s in full_table_name . split ( \".\" ) ) self . _connection = conn self . _support = [ full_table_name ] self . _heading = Heading ( table_info = dict ( conn = conn , database = self . database , table_name = self . table_name , context = None , ) ) def __repr__ ( self ): return ( \"FreeTable(` %s `.` %s `) \\n \" % ( self . database , self . _table_name ) + super () . __repr__ () )", "title": "FreeTable"}, {"location": "api/datajoint/table/#datajoint.table.Log", "text": "Bases: Table The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. Parameters: Name Type Description Default skip_logging if True, then log entry is skipped by default. See call False Source code in datajoint/table.py 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 class Log ( Table ): \"\"\" The log table for each schema. Instances are callable. Calls log the time and identifying information along with the event. :param skip_logging: if True, then log entry is skipped by default. See __call__ \"\"\" _table_name = \"~log\" def __init__ ( self , conn , database , skip_logging = False ): self . database = database self . skip_logging = skip_logging self . _connection = conn self . _heading = Heading ( table_info = dict ( conn = conn , database = database , table_name = self . table_name , context = None ) ) self . _support = [ self . full_table_name ] self . _definition = \"\"\" # event logging table for ` {database} ` id :int unsigned auto_increment # event order id --- timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp version :varchar(12) # datajoint version user :varchar(255) # user@host host=\"\" :varchar(255) # system hostname event=\"\" :varchar(255) # event message \"\"\" . format ( database = database ) super () . __init__ () if not self . is_declared : self . declare () self . connection . dependencies . clear () self . _user = self . connection . get_user () @property def definition ( self ): return self . _definition def __call__ ( self , event , skip_logging = None ): \"\"\" :param event: string to write into the log table :param skip_logging: If True then do not log. If None, then use self.skip_logging \"\"\" skip_logging = self . skip_logging if skip_logging is None else skip_logging if not skip_logging : try : self . insert1 ( dict ( user = self . _user , version = version + \"py\" , host = platform . uname () . node , event = event , ), skip_duplicates = True , ignore_extra_fields = True , ) except DataJointError : logger . info ( \"could not log event in table ~log\" ) def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True ) def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick ()", "title": "Log"}, {"location": "api/datajoint/table/#datajoint.table.Log.delete", "text": "bypass interactive prompts and cascading dependencies Returns: Type Description number of deleted items Source code in datajoint/table.py 1063 1064 1065 1066 1067 1068 1069 def delete ( self ): \"\"\" bypass interactive prompts and cascading dependencies :return: number of deleted items \"\"\" return self . delete_quick ( get_count = True )", "title": "delete()"}, {"location": "api/datajoint/table/#datajoint.table.Log.drop", "text": "bypass interactive prompts and cascading dependencies Source code in datajoint/table.py 1071 1072 1073 def drop ( self ): \"\"\"bypass interactive prompts and cascading dependencies\"\"\" self . drop_quick ()", "title": "drop()"}, {"location": "api/datajoint/user_tables/", "text": "Hosts the table tiers, user tables should be derived from. TableMeta \u00b6 Bases: type TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). Source code in datajoint/user_tables.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class TableMeta ( type ): \"\"\" TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). \"\"\" def __getattribute__ ( cls , name ): # trigger instantiation for supported class attrs return ( cls () . __getattribute__ ( name ) if name in supported_class_attrs else super () . __getattribute__ ( name ) ) def __and__ ( cls , arg ): return cls () & arg def __xor__ ( cls , arg ): return cls () ^ arg def __sub__ ( cls , arg ): return cls () - arg def __neg__ ( cls ): return - cls () def __mul__ ( cls , arg ): return cls () * arg def __matmul__ ( cls , arg ): return cls () @ arg def __add__ ( cls , arg ): return cls () + arg def __iter__ ( cls ): return iter ( cls ()) UserTable \u00b6 Bases: Table A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema(). Source code in datajoint/user_tables.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class UserTable ( Table , metaclass = TableMeta ): \"\"\" A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema(). \"\"\" # set by @schema _connection = None _heading = None _support = None # set by subclass tier_regexp = None _prefix = None @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ ) @ClassProperty def full_table_name ( cls ): if cls not in { Manual , Imported , Lookup , Computed , Part , UserTable }: # for derived classes only if cls . database is None : raise DataJointError ( \"Class %s is not properly declared (schema decorator not applied?)\" % cls . __name__ ) return r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) definition property \u00b6 Returns: Type Description a string containing the table definition using the DataJoint DDL. table_name () \u00b6 Returns: Type Description the table name of the table formatted for mysql. Source code in datajoint/user_tables.py 112 113 114 115 116 117 118 119 @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ ) Manual \u00b6 Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\" Lookup \u00b6 Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" ) Imported \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\" Computed \u00b6 Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\" Part \u00b6 Bases: UserTable Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" ) delete ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) drop ( force = False ) \u00b6 unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "user_tables.py"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.TableMeta", "text": "Bases: type TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). Source code in datajoint/user_tables.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class TableMeta ( type ): \"\"\" TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch(). \"\"\" def __getattribute__ ( cls , name ): # trigger instantiation for supported class attrs return ( cls () . __getattribute__ ( name ) if name in supported_class_attrs else super () . __getattribute__ ( name ) ) def __and__ ( cls , arg ): return cls () & arg def __xor__ ( cls , arg ): return cls () ^ arg def __sub__ ( cls , arg ): return cls () - arg def __neg__ ( cls ): return - cls () def __mul__ ( cls , arg ): return cls () * arg def __matmul__ ( cls , arg ): return cls () @ arg def __add__ ( cls , arg ): return cls () + arg def __iter__ ( cls ): return iter ( cls ())", "title": "TableMeta"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable", "text": "Bases: Table A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema(). Source code in datajoint/user_tables.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class UserTable ( Table , metaclass = TableMeta ): \"\"\" A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema(). \"\"\" # set by @schema _connection = None _heading = None _support = None # set by subclass tier_regexp = None _prefix = None @property def definition ( self ): \"\"\" :return: a string containing the table definition using the DataJoint DDL. \"\"\" raise NotImplementedError ( 'Subclasses of Table must implement the property \"definition\"' ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ ) @ClassProperty def full_table_name ( cls ): if cls not in { Manual , Imported , Lookup , Computed , Part , UserTable }: # for derived classes only if cls . database is None : raise DataJointError ( \"Class %s is not properly declared (schema decorator not applied?)\" % cls . __name__ ) return r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name )", "title": "UserTable"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.definition", "text": "Returns: Type Description a string containing the table definition using the DataJoint DDL.", "title": "definition"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.table_name", "text": "Returns: Type Description the table name of the table formatted for mysql. Source code in datajoint/user_tables.py 112 113 114 115 116 117 118 119 @ClassProperty def table_name ( cls ): \"\"\" :return: the table name of the table formatted for mysql. \"\"\" if cls . _prefix is None : raise AttributeError ( \"Class prefix is not defined!\" ) return cls . _prefix + from_camel_case ( cls . __name__ )", "title": "table_name()"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Manual", "text": "Bases: UserTable Inherit from this class if the table's values are entered manually. Source code in datajoint/user_tables.py 133 134 135 136 137 138 139 class Manual ( UserTable ): \"\"\" Inherit from this class if the table's values are entered manually. \"\"\" _prefix = r \"\" tier_regexp = r \"(?P<manual>\" + _prefix + _base_regexp + \")\"", "title": "Manual"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Lookup", "text": "Bases: UserTable Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. Source code in datajoint/user_tables.py 142 143 144 145 146 147 148 149 150 151 152 class Lookup ( UserTable ): \"\"\" Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only. \"\"\" _prefix = \"#\" tier_regexp = ( r \"(?P<lookup>\" + _prefix + _base_regexp . replace ( \"TIER\" , \"lookup\" ) + \")\" )", "title": "Lookup"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Imported", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 155 156 157 158 159 160 161 162 class Imported ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"_\" tier_regexp = r \"(?P<imported>\" + _prefix + _base_regexp + \")\"", "title": "Imported"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Computed", "text": "Bases: UserTable , AutoPopulate Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function _make_tuples . Source code in datajoint/user_tables.py 165 166 167 168 169 170 171 172 class Computed ( UserTable , AutoPopulate ): \"\"\" Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function `_make_tuples`. \"\"\" _prefix = \"__\" tier_regexp = r \"(?P<computed>\" + _prefix + _base_regexp + \")\"", "title": "Computed"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part", "text": "Bases: UserTable Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. Source code in datajoint/user_tables.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class Part ( UserTable ): \"\"\" Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes. \"\"\" _connection = None _master = None tier_regexp = ( r \"(?P<master>\" + \"|\" . join ([ c . tier_regexp for c in ( Manual , Lookup , Imported , Computed )]) + r \"){1,1}\" + \"__\" + r \"(?P<part>\" + _base_regexp + \")\" ) @ClassProperty def connection ( cls ): return cls . _connection @ClassProperty def full_table_name ( cls ): return ( None if cls . database is None or cls . table_name is None else r \"` {0:s} `.` {1:s} `\" . format ( cls . database , cls . table_name ) ) @ClassProperty def master ( cls ): return cls . _master @ClassProperty def table_name ( cls ): return ( None if cls . master is None else cls . master . table_name + \"__\" + from_camel_case ( cls . __name__ ) ) def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" ) def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "Part"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.delete", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 220 221 222 223 224 225 226 227 228 229 def delete ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . delete ( force_parts = True ) else : raise DataJointError ( \"Cannot delete from a Part directly. Delete from master instead\" )", "title": "delete()"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.drop", "text": "unless force is True, prohibits direct deletes from parts. Source code in datajoint/user_tables.py 231 232 233 234 235 236 237 238 239 240 def drop ( self , force = False ): \"\"\" unless force is True, prohibits direct deletes from parts. \"\"\" if force : super () . drop () else : raise DataJointError ( \"Cannot drop a Part directly. Delete from master instead\" )", "title": "drop()"}, {"location": "api/datajoint/utils/", "text": "General-purpose utilities user_choice ( prompt , choices = ( 'yes' , 'no' ), default = None ) \u00b6 Prompts the user for confirmation. The default value, if any, is capitalized. Parameters: Name Type Description Default prompt Information to display to the user. required choices an iterable of possible choices. ('yes', 'no') default default choice None Returns: Type Description the user's choice Source code in datajoint/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def user_choice ( prompt , choices = ( \"yes\" , \"no\" ), default = None ): \"\"\" Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice \"\"\" assert default is None or default in choices choice_list = \", \" . join ( ( choice . title () if choice == default else choice for choice in choices ) ) response = None while response not in choices : response = input ( prompt + \" [\" + choice_list + \"]: \" ) response = response . lower () if response else default return response get_master ( full_table_name ) \u00b6 If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + __ . Example: ephys . session -- master ephys . session__recording -- part Parameters: Name Type Description Default full_table_name str Full table name including part. required Returns: Type Description str Supposed master full table name or empty string if not a part table name. Source code in datajoint/utils.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_master ( full_table_name : str ) -> str : \"\"\" If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + ``__``. Example: `ephys`.`session` -- master `ephys`.`session__recording` -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str \"\"\" match = re . match ( r \"(?P<master>`\\w+`.`\\w+)__(?P<part>\\w+)`\" , full_table_name ) return match [ \"master\" ] + \"`\" if match else \"\" to_camel_case ( s ) \u00b6 Convert names with under score (_) separation into camel case names. Parameters: Name Type Description Default s string in under_score notation required Returns: Type Description string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" Source code in datajoint/utils.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def to_camel_case ( s ): \"\"\" Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" \"\"\" def to_upper ( match ): return match . group ( 0 )[ - 1 ] . upper () return re . sub ( r \"(^|[_\\W])+[a-zA-Z]\" , to_upper , s ) from_camel_case ( s ) \u00b6 Convert names in camel case into underscore (_) separated names Parameters: Name Type Description Default s string in CamelCase notation required Returns: Type Description string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" Source code in datajoint/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def from_camel_case ( s ): \"\"\" Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" \"\"\" def convert ( match ): return ( \"_\" if match . groups ()[ 0 ] else \"\" ) + match . group ( 0 ) . lower () if not re . match ( r \"[A-Z][a-zA-Z0-9]*\" , s ): raise DataJointError ( \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\" ) return re . sub ( r \"(\\B[A-Z])|(\\b[A-Z])\" , convert , s ) safe_write ( filepath , blob ) \u00b6 A two-step write. Parameters: Name Type Description Default filename full path required blob binary data required Source code in datajoint/utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 def safe_write ( filepath , blob ): \"\"\" A two-step write. :param filename: full path :param blob: binary data \"\"\" filepath = Path ( filepath ) if not filepath . is_file (): filepath . parent . mkdir ( parents = True , exist_ok = True ) temp_file = filepath . with_suffix ( filepath . suffix + \".saving\" ) temp_file . write_bytes ( blob ) temp_file . rename ( filepath ) safe_copy ( src , dest , overwrite = False ) \u00b6 Copy the contents of src file into dest file as a two-step process. Skip if dest exists already Source code in datajoint/utils.py 107 108 109 110 111 112 113 114 115 116 def safe_copy ( src , dest , overwrite = False ): \"\"\" Copy the contents of src file into dest file as a two-step process. Skip if dest exists already \"\"\" src , dest = Path ( src ), Path ( dest ) if not ( dest . exists () and src . samefile ( dest )) and ( overwrite or not dest . is_file ()): dest . parent . mkdir ( parents = True , exist_ok = True ) temp_file = dest . with_suffix ( dest . suffix + \".copying\" ) shutil . copyfile ( str ( src ), str ( temp_file )) temp_file . rename ( dest ) parse_sql ( filepath ) \u00b6 yield SQL statements from an SQL file Source code in datajoint/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def parse_sql ( filepath ): \"\"\" yield SQL statements from an SQL file \"\"\" delimiter = \";\" statement = [] with Path ( filepath ) . open ( \"rt\" ) as f : for line in f : line = line . strip () if not line . startswith ( \"--\" ) and len ( line ) > 1 : if line . startswith ( \"delimiter\" ): delimiter = line . split ()[ 1 ] else : statement . append ( line ) if line . endswith ( delimiter ): yield \" \" . join ( statement ) statement = []", "title": "utils.py"}, {"location": "api/datajoint/utils/#datajoint.utils.user_choice", "text": "Prompts the user for confirmation. The default value, if any, is capitalized. Parameters: Name Type Description Default prompt Information to display to the user. required choices an iterable of possible choices. ('yes', 'no') default default choice None Returns: Type Description the user's choice Source code in datajoint/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def user_choice ( prompt , choices = ( \"yes\" , \"no\" ), default = None ): \"\"\" Prompts the user for confirmation. The default value, if any, is capitalized. :param prompt: Information to display to the user. :param choices: an iterable of possible choices. :param default: default choice :return: the user's choice \"\"\" assert default is None or default in choices choice_list = \", \" . join ( ( choice . title () if choice == default else choice for choice in choices ) ) response = None while response not in choices : response = input ( prompt + \" [\" + choice_list + \"]: \" ) response = response . lower () if response else default return response", "title": "user_choice()"}, {"location": "api/datajoint/utils/#datajoint.utils.get_master", "text": "If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + __ . Example: ephys . session -- master ephys . session__recording -- part Parameters: Name Type Description Default full_table_name str Full table name including part. required Returns: Type Description str Supposed master full table name or empty string if not a part table name. Source code in datajoint/utils.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_master ( full_table_name : str ) -> str : \"\"\" If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + ``__``. Example: `ephys`.`session` -- master `ephys`.`session__recording` -- part :param full_table_name: Full table name including part. :type full_table_name: str :return: Supposed master full table name or empty string if not a part table name. :rtype: str \"\"\" match = re . match ( r \"(?P<master>`\\w+`.`\\w+)__(?P<part>\\w+)`\" , full_table_name ) return match [ \"master\" ] + \"`\" if match else \"\"", "title": "get_master()"}, {"location": "api/datajoint/utils/#datajoint.utils.to_camel_case", "text": "Convert names with under score (_) separation into camel case names. Parameters: Name Type Description Default s string in under_score notation required Returns: Type Description string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" Source code in datajoint/utils.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def to_camel_case ( s ): \"\"\" Convert names with under score (_) separation into camel case names. :param s: string in under_score notation :returns: string in CamelCase notation Example: >>> to_camel_case(\"table_name\") # returns \"TableName\" \"\"\" def to_upper ( match ): return match . group ( 0 )[ - 1 ] . upper () return re . sub ( r \"(^|[_\\W])+[a-zA-Z]\" , to_upper , s )", "title": "to_camel_case()"}, {"location": "api/datajoint/utils/#datajoint.utils.from_camel_case", "text": "Convert names in camel case into underscore (_) separated names Parameters: Name Type Description Default s string in CamelCase notation required Returns: Type Description string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" Source code in datajoint/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def from_camel_case ( s ): \"\"\" Convert names in camel case into underscore (_) separated names :param s: string in CamelCase notation :returns: string in under_score notation Example: >>> from_camel_case(\"TableName\") # yields \"table_name\" \"\"\" def convert ( match ): return ( \"_\" if match . groups ()[ 0 ] else \"\" ) + match . group ( 0 ) . lower () if not re . match ( r \"[A-Z][a-zA-Z0-9]*\" , s ): raise DataJointError ( \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\" ) return re . sub ( r \"(\\B[A-Z])|(\\b[A-Z])\" , convert , s )", "title": "from_camel_case()"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_write", "text": "A two-step write. Parameters: Name Type Description Default filename full path required blob binary data required Source code in datajoint/utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 def safe_write ( filepath , blob ): \"\"\" A two-step write. :param filename: full path :param blob: binary data \"\"\" filepath = Path ( filepath ) if not filepath . is_file (): filepath . parent . mkdir ( parents = True , exist_ok = True ) temp_file = filepath . with_suffix ( filepath . suffix + \".saving\" ) temp_file . write_bytes ( blob ) temp_file . rename ( filepath )", "title": "safe_write()"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_copy", "text": "Copy the contents of src file into dest file as a two-step process. Skip if dest exists already Source code in datajoint/utils.py 107 108 109 110 111 112 113 114 115 116 def safe_copy ( src , dest , overwrite = False ): \"\"\" Copy the contents of src file into dest file as a two-step process. Skip if dest exists already \"\"\" src , dest = Path ( src ), Path ( dest ) if not ( dest . exists () and src . samefile ( dest )) and ( overwrite or not dest . is_file ()): dest . parent . mkdir ( parents = True , exist_ok = True ) temp_file = dest . with_suffix ( dest . suffix + \".copying\" ) shutil . copyfile ( str ( src ), str ( temp_file )) temp_file . rename ( dest )", "title": "safe_copy()"}, {"location": "api/datajoint/utils/#datajoint.utils.parse_sql", "text": "yield SQL statements from an SQL file Source code in datajoint/utils.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def parse_sql ( filepath ): \"\"\" yield SQL statements from an SQL file \"\"\" delimiter = \";\" statement = [] with Path ( filepath ) . open ( \"rt\" ) as f : for line in f : line = line . strip () if not line . startswith ( \"--\" ) and len ( line ) > 1 : if line . startswith ( \"delimiter\" ): delimiter = line . split ()[ 1 ] else : statement . append ( line ) if line . endswith ( delimiter ): yield \" \" . join ( statement ) statement = []", "title": "parse_sql()"}, {"location": "api/datajoint/version/", "text": "", "title": "version.py"}, {"location": "concepts/existing-pipelines/", "text": "Existing Pipelines \u00b6 This section describes how to work with database schemas without access to the original code that generated the schema. These situations often arise when the database is created by another user who has not shared the generating code yet or when the database schema is created from a programming language other than Python. Loading Classes \u00b6 Typically, a DataJoint schema is created as a dedicated Python module. This module defines a schema object that is used to link classes declared in the module to tables in the database schema. With the module installed, you can simply import it to interact with its tables: import datajoint as dj from element_calcium_imaging import scan # (1) This and other DataJoint Elements are installable via pip or downloadable via their respective GitHub repositories. To visualize an unfamiliar schema, see commands for generating diagrams . Spawning Missing Classes \u00b6 Now, imagine we do not have access to the Python definition of Scan , or we're unsure if the version on our server matches the definition available. We can use the dj.list_schemas function to list the available database schemas. import datajoint as dj dj . conn () # (1) dj . list_schemas () # (2) dj . Schema ( 'schema_name' ) . list_tables () # (3) Establish a connection to the server. List the available schemas on the server. List the tables for a given schema from the previous step. These will appear in their raw database form, with underscores instead of camelcase and special characters for Part tables. Just as with a new schema, we can create a schema object to connect to the chosen database schema. If the schema already exists, dj.Schema is initialized as usual. If a diagram will shows a mixture of class names and database table names, the spawn_missing_classes method will spawn classes into the local namespace for any tables missing their classes. This will allow us to interact with all tables as if they were declared in the current namespace. schema . spawn_missing_classes () Virtual Modules \u00b6 While spawn_missing_classes creates the new classes in the local namespace, it is often more convenient to import a schema with its Python module, equivalent to the Python command. We can mimmick this import without having access to the schema using the VirtualModule class object: import datajoint as dj subject = dj . create_virtual_module ( module_name = 'subject' , schema_name = 'db_subject' ) Now, subject behaves as an imported module complete with the schema object and all the table classes. The class object VirtualModule of the dj.Schema class provides access to virtual modules. It creates a python module with the given name from the name of a schema on the server, automatically adds classes to it corresponding to the tables in the schema. The function can take several parameters: module_name : displayed module name. schema_name : name of the database in MySQL. create_schema : if True , create the schema on the database server if it does not already exist; if False (default), raise an error when the schema is not found. create_tables : if True , module.schema can be used as the decorator for declaring new classes; if False , such use will raise an error stating that the module is intend only to work with existing tables. The function returns the Python module containing classes from the schema object with all the table classes already declared inside it. create_schema=False may be useful if we want to make sure that the schema already exists. If none exists, create_schema=True will create an empty schema. dj . VirtualModule ( 'what' , 'nonexistent' ) Returns DataJointError : Database named ` nonexistent ` was not defined . Set argument create_schema = True to create it . create_tables=False prevents the use of the schema object of the virtual module for creating new tables in the existing schema. This is a precautionary measure since virtual modules are often used for completed schemas. create_tables=True will new tables to the existing schema. A more common approach in this scenario would be to create a new schema object and to use the spawn_missing_classes function to make the classes available. However, you if do decide to create new tables in an existing tables using the virtual module, you may do so by using the schema object from the module as the decorator for declaring new tables: uni = dj . VirtualModule ( 'university.py' , 'dimitri_university' , create_tables = True ) @uni . schema class Example ( dj . Manual ): definition = \"\"\" -> uni.Student --- example : varchar(255) \"\"\" dj . Diagram ( uni )", "title": "Existing Pipelines"}, {"location": "concepts/existing-pipelines/#existing-pipelines", "text": "This section describes how to work with database schemas without access to the original code that generated the schema. These situations often arise when the database is created by another user who has not shared the generating code yet or when the database schema is created from a programming language other than Python.", "title": "Existing Pipelines"}, {"location": "concepts/existing-pipelines/#loading-classes", "text": "Typically, a DataJoint schema is created as a dedicated Python module. This module defines a schema object that is used to link classes declared in the module to tables in the database schema. With the module installed, you can simply import it to interact with its tables: import datajoint as dj from element_calcium_imaging import scan # (1) This and other DataJoint Elements are installable via pip or downloadable via their respective GitHub repositories. To visualize an unfamiliar schema, see commands for generating diagrams .", "title": "Loading Classes"}, {"location": "concepts/existing-pipelines/#spawning-missing-classes", "text": "Now, imagine we do not have access to the Python definition of Scan , or we're unsure if the version on our server matches the definition available. We can use the dj.list_schemas function to list the available database schemas. import datajoint as dj dj . conn () # (1) dj . list_schemas () # (2) dj . Schema ( 'schema_name' ) . list_tables () # (3) Establish a connection to the server. List the available schemas on the server. List the tables for a given schema from the previous step. These will appear in their raw database form, with underscores instead of camelcase and special characters for Part tables. Just as with a new schema, we can create a schema object to connect to the chosen database schema. If the schema already exists, dj.Schema is initialized as usual. If a diagram will shows a mixture of class names and database table names, the spawn_missing_classes method will spawn classes into the local namespace for any tables missing their classes. This will allow us to interact with all tables as if they were declared in the current namespace. schema . spawn_missing_classes ()", "title": "Spawning Missing Classes"}, {"location": "concepts/existing-pipelines/#virtual-modules", "text": "While spawn_missing_classes creates the new classes in the local namespace, it is often more convenient to import a schema with its Python module, equivalent to the Python command. We can mimmick this import without having access to the schema using the VirtualModule class object: import datajoint as dj subject = dj . create_virtual_module ( module_name = 'subject' , schema_name = 'db_subject' ) Now, subject behaves as an imported module complete with the schema object and all the table classes. The class object VirtualModule of the dj.Schema class provides access to virtual modules. It creates a python module with the given name from the name of a schema on the server, automatically adds classes to it corresponding to the tables in the schema. The function can take several parameters: module_name : displayed module name. schema_name : name of the database in MySQL. create_schema : if True , create the schema on the database server if it does not already exist; if False (default), raise an error when the schema is not found. create_tables : if True , module.schema can be used as the decorator for declaring new classes; if False , such use will raise an error stating that the module is intend only to work with existing tables. The function returns the Python module containing classes from the schema object with all the table classes already declared inside it. create_schema=False may be useful if we want to make sure that the schema already exists. If none exists, create_schema=True will create an empty schema. dj . VirtualModule ( 'what' , 'nonexistent' ) Returns DataJointError : Database named ` nonexistent ` was not defined . Set argument create_schema = True to create it . create_tables=False prevents the use of the schema object of the virtual module for creating new tables in the existing schema. This is a precautionary measure since virtual modules are often used for completed schemas. create_tables=True will new tables to the existing schema. A more common approach in this scenario would be to create a new schema object and to use the spawn_missing_classes function to make the classes available. However, you if do decide to create new tables in an existing tables using the virtual module, you may do so by using the schema object from the module as the decorator for declaring new tables: uni = dj . VirtualModule ( 'university.py' , 'dimitri_university' , create_tables = True ) @uni . schema class Example ( dj . Manual ): definition = \"\"\" -> uni.Student --- example : varchar(255) \"\"\" dj . Diagram ( uni )", "title": "Virtual Modules"}, {"location": "getting-started/", "text": "Getting Started \u00b6 Installation \u00b6 First, please install Python version 3.7 or later. We recommend 3.8. Next, please install DataJoint via one of the following: conda pip + pip + pip + Pre-Requisites Ensure you have conda installed. To add the conda-forge channel: conda config --add channels conda-forge To install: conda install -c conda-forge datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint Connection \u00b6 Note Although you may connect to any MySQL server of your choice, the DataJoint company offers an online tutorial environment. Simply sign up for a free DataJoint account . You will be granted privileges to create schemas that are prefixed as {user}_ . environment variables memory file Before using datajoint , set the following environment variables like so: 1 2 3 DJ_HOST = tutorial-db.datajoint.io DJ_USER ={ user } DJ_PASS ={ password } To set connection settings within Python, perform: 1 2 3 4 5 import datajoint as dj dj . config [ \"database.host\" ] = \"tutorial-db.datajoint.io\" dj . config [ \"database.user\" ] = \" {user} \" dj . config [ \"database.password\" ] = \" {password} \" These configuration settings can be saved either locally or system-wide using one of the following commands: dj . config . save_local () dj . config . save_global () Before using datajoint , create a file named dj_local_conf.json in the current directory like so: 1 2 3 4 5 { \"database.host\" : \"tutorial-db.datajoint.io\" , \"database.user\" : \"{user}\" , \"database.password\" : \"{password}\" } These settings will be loaded whenever a Python instance is launched from this directory. To configure settings globally, save a similar file as .datajoint_config.json in your home directory. A local config, if present, will take precedent over global settings. Data Pipeline Definition \u00b6 Let's definite a simple data pipeline. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import datajoint as dj schema = dj . Schema ( f \" { dj . config [ 'database.user' ] } _shapes\" ) # (1) @schema # (2) class Rectangle ( dj . Manual ): definition = \"\"\" # (3) shape_id: int --- shape_height: float shape_width: float \"\"\" @schema class Area ( dj . Computed ): definition = \"\"\" -> Rectangle --- shape_area: float \"\"\" def make ( self , key ): rectangle = ( Rectangle & key ) . fetch1 () Area . insert1 ( dict ( shape_id = rectangle [ \"shape_id\" ], shape_area = rectangle [ \"shape_height\" ] * rectangle [ \"shape_width\" ], ) ) This statement creates the database schema {username}_shapes on the server. The @schema decorator for DataJoint classes creates the table on the server. The table is defined by the the definition property. It is a common practice to have a separate Python module for each schema. Therefore, each such module has only one dj.Schema object defined and is usually named schema . The dj.Schema constructor can take a number of optional parameters after the schema name. context - Dictionary for looking up foreign key references. Defaults to None to use local context. connection - Specifies the DataJoint connection object. Defaults to dj.conn() . create_schema - When False , the schema object will not create a schema on the database and will raise an error if one does not already exist. Defaults to True . create_tables - When False , the schema object will not create tables on the database and will raise errors when accessing missing tables. Defaults to True . The @schema decorator uses the class name and the data tier to check whether an appropriate table exists on the database. If a table does not already exist, the decorator creates one on the database using the definition property. The decorator attaches the information about the table to the class, and then returns the class. Diagram \u00b6 Display \u00b6 The diagram displays the relationship of the data model in the data pipeline. This can be done for an entire schema: dj . Diagram ( schema ) Or for individual or sets of tables: dj . Diagram ( schema . Rectangle ) dj . Diagram ( schema . Rectangle ) + dj . Diagram ( schema . Area ) What if I don't see the diagram? Some Python interfaces may require additional draw method. dj . Diagram ( schema ) . draw () Calling the .draw() method is not necessary when working in a Jupyter notebook by entering dj.Diagram(schema) in a notebook cell. The Diagram will automatically render in the notebook by calling its _repr_html_ method. A Diagram displayed without .draw() will be rendered as an SVG, and hovering the mouse over a table will reveal a compact version of the output of the .describe() method. Customize \u00b6 Adding or substracting a number to a diagram object adds nodes downstream or upstream, respectively, in the pipeline. ( dj . Diagram ( schema . Rectangle ) + 1 ) . draw () # (1) Plot all the tables directly downstream from schema.Rectangle ( dj . Diagram ( 'my_schema' ) - 1 + 1 ) . draw () # (1) Plot all tables directly downstream of those directly upstream of this schema. Save \u00b6 The diagram can be saved as either png or svg . dj . Diagram ( schema ) . save ( filename = 'my-diagram' , format = 'png' ) Add data \u00b6 Let's add data for a rectangle: Rectangle . insert1 ( dict ( shape_id = 1 , shape_height = 2 , shape_width = 4 )) Run computation \u00b6 Let's start the computations on our entity: Area . Area . populate ( display_progress = True ) Query \u00b6 Let's inspect the results. Area & \"shape_area >= 8\" shaped_id shape_area 1 8.0", "title": "Getting Started"}, {"location": "getting-started/#getting-started", "text": "", "title": "Getting Started"}, {"location": "getting-started/#installation", "text": "First, please install Python version 3.7 or later. We recommend 3.8. Next, please install DataJoint via one of the following: conda pip + pip + pip + Pre-Requisites Ensure you have conda installed. To add the conda-forge channel: conda config --add channels conda-forge To install: conda install -c conda-forge datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint Pre-Requisites Ensure you have pip installed. Install graphviz pre-requisite for diagram visualization. To install: pip install datajoint", "title": "Installation"}, {"location": "getting-started/#connection", "text": "Note Although you may connect to any MySQL server of your choice, the DataJoint company offers an online tutorial environment. Simply sign up for a free DataJoint account . You will be granted privileges to create schemas that are prefixed as {user}_ . environment variables memory file Before using datajoint , set the following environment variables like so: 1 2 3 DJ_HOST = tutorial-db.datajoint.io DJ_USER ={ user } DJ_PASS ={ password } To set connection settings within Python, perform: 1 2 3 4 5 import datajoint as dj dj . config [ \"database.host\" ] = \"tutorial-db.datajoint.io\" dj . config [ \"database.user\" ] = \" {user} \" dj . config [ \"database.password\" ] = \" {password} \" These configuration settings can be saved either locally or system-wide using one of the following commands: dj . config . save_local () dj . config . save_global () Before using datajoint , create a file named dj_local_conf.json in the current directory like so: 1 2 3 4 5 { \"database.host\" : \"tutorial-db.datajoint.io\" , \"database.user\" : \"{user}\" , \"database.password\" : \"{password}\" } These settings will be loaded whenever a Python instance is launched from this directory. To configure settings globally, save a similar file as .datajoint_config.json in your home directory. A local config, if present, will take precedent over global settings.", "title": "Connection"}, {"location": "getting-started/#data-pipeline-definition", "text": "Let's definite a simple data pipeline. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import datajoint as dj schema = dj . Schema ( f \" { dj . config [ 'database.user' ] } _shapes\" ) # (1) @schema # (2) class Rectangle ( dj . Manual ): definition = \"\"\" # (3) shape_id: int --- shape_height: float shape_width: float \"\"\" @schema class Area ( dj . Computed ): definition = \"\"\" -> Rectangle --- shape_area: float \"\"\" def make ( self , key ): rectangle = ( Rectangle & key ) . fetch1 () Area . insert1 ( dict ( shape_id = rectangle [ \"shape_id\" ], shape_area = rectangle [ \"shape_height\" ] * rectangle [ \"shape_width\" ], ) ) This statement creates the database schema {username}_shapes on the server. The @schema decorator for DataJoint classes creates the table on the server. The table is defined by the the definition property. It is a common practice to have a separate Python module for each schema. Therefore, each such module has only one dj.Schema object defined and is usually named schema . The dj.Schema constructor can take a number of optional parameters after the schema name. context - Dictionary for looking up foreign key references. Defaults to None to use local context. connection - Specifies the DataJoint connection object. Defaults to dj.conn() . create_schema - When False , the schema object will not create a schema on the database and will raise an error if one does not already exist. Defaults to True . create_tables - When False , the schema object will not create tables on the database and will raise errors when accessing missing tables. Defaults to True . The @schema decorator uses the class name and the data tier to check whether an appropriate table exists on the database. If a table does not already exist, the decorator creates one on the database using the definition property. The decorator attaches the information about the table to the class, and then returns the class.", "title": "Data Pipeline Definition"}, {"location": "getting-started/#diagram", "text": "", "title": "Diagram"}, {"location": "getting-started/#display", "text": "The diagram displays the relationship of the data model in the data pipeline. This can be done for an entire schema: dj . Diagram ( schema ) Or for individual or sets of tables: dj . Diagram ( schema . Rectangle ) dj . Diagram ( schema . Rectangle ) + dj . Diagram ( schema . Area ) What if I don't see the diagram? Some Python interfaces may require additional draw method. dj . Diagram ( schema ) . draw () Calling the .draw() method is not necessary when working in a Jupyter notebook by entering dj.Diagram(schema) in a notebook cell. The Diagram will automatically render in the notebook by calling its _repr_html_ method. A Diagram displayed without .draw() will be rendered as an SVG, and hovering the mouse over a table will reveal a compact version of the output of the .describe() method.", "title": "Display"}, {"location": "getting-started/#customize", "text": "Adding or substracting a number to a diagram object adds nodes downstream or upstream, respectively, in the pipeline. ( dj . Diagram ( schema . Rectangle ) + 1 ) . draw () # (1) Plot all the tables directly downstream from schema.Rectangle ( dj . Diagram ( 'my_schema' ) - 1 + 1 ) . draw () # (1) Plot all tables directly downstream of those directly upstream of this schema.", "title": "Customize"}, {"location": "getting-started/#save", "text": "The diagram can be saved as either png or svg . dj . Diagram ( schema ) . save ( filename = 'my-diagram' , format = 'png' )", "title": "Save"}, {"location": "getting-started/#add-data", "text": "Let's add data for a rectangle: Rectangle . insert1 ( dict ( shape_id = 1 , shape_height = 2 , shape_width = 4 ))", "title": "Add data"}, {"location": "getting-started/#run-computation", "text": "Let's start the computations on our entity: Area . Area . populate ( display_progress = True )", "title": "Run computation"}, {"location": "getting-started/#query", "text": "Let's inspect the results. Area & \"shape_area >= 8\" shaped_id shape_area 1 8.0", "title": "Query"}, {"location": "query-lang/common-commands/", "text": "Insert \u00b6 Data entry is as easy as providing the appropriate data structure to a permitted table. Given the following table definition, we can insert data as tuples, dicts, pandas dataframes, or pathlib Path relative paths to local CSV files. mouse_id: int # unique mouse id --- dob: date # mouse date of birth sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown Tuple Dict Pandas CSV mouse . insert1 ( ( 0 , '2017-03-01' , 'M' ) ) # Single entry data = [ ( 1 , '2016-11-19' , 'M' ), ( 2 , '2016-11-20' , 'U' ), ( 5 , '2016-12-25' , 'F' ) ] mouse . insert ( data ) # Multi-entry mouse . insert1 ( dict ( mouse_id = 0 , dob = '2017-03-01' , sex = 'M' ) ) # Single entry data = [ { 'mouse_id' : 1 , 'dob' : '2016-11-19' , 'sex' : 'M' }, { 'mouse_id' : 2 , 'dob' : '2016-11-20' , 'sex' : 'U' }, { 'mouse_id' : 5 , 'dob' : '2016-12-25' , 'sex' : 'F' } ] mouse . insert ( data ) # Multi-entry import pandas as pd data = pd . DataFrame ( [[ 1 , \"2016-11-19\" , \"M\" ], [ 2 , \"2016-11-20\" , \"U\" ], [ 5 , \"2016-12-25\" , \"F\" ]], columns = [ \"mouse_id\" , \"dob\" , \"sex\" ], ) mouse . insert ( data ) Given the following CSV in the current working directory as mice.csv mouse_id,dob,sex 1,2016-11-19,M 2,2016-11-20,U 5,2016-12-25,F We can import as follows: from pathlib import Path mouse . insert ( Path ( './mice.csv' )) Make \u00b6 See the article on make methods Fetch \u00b6 Entire table \u00b6 A fetch command can either retrieve table data as a NumPy recarray or a as a list of dict data = query . fetch () # (1) data = query . fetch ( as_dict = True ) # (2) NumPy recarray List of dict : For very large tables... In some cases, the amount of data returned by fetch can be quite large; it can be useful to use the size_on_disk attribute to determine if running a bare fetch would be wise. Please note that it is only currently possible to query the size of entire tables stored directly in the database at this time. Separate variables \u00b6 name , img = query . fetch1 ( 'mouse_id' , 'dob' ) # when query has exactly one entity name , img = query . fetch ( 'mouse_id' , 'dob' ) # [mouse_id, ...] [dob, ...] Primary key values \u00b6 keydict = tab . fetch1 ( \"KEY\" ) # single key dict when tab has exactly one entity keylist = tab . fetch ( \"KEY\" ) # list of key dictionaries [{}, ...] KEY can also used when returning attribute values as separate variables, such that one of the returned variables contains the entire primary keys. Sorting results \u00b6 To sort the result, use the order_by keyword argument. data = query . fetch ( order_by = 'mouse_id' ) # ascending order data = query . fetch ( order_by = 'mouse_id desc' ) # descending order data = query . fetch ( order_by = ( 'mouse_id' , 'dob' )) # by ID first, dob second data = query . fetch ( order_by = 'KEY' ) # sort by the primary key The order_by argument can be a string specifying the attribute to sort by. By default the sort is in ascending order. Use 'attr desc' to sort in descending order by attribute attr . The value can also be a sequence of strings, in which case, the sort performed on all the attributes jointly in the order specified. The special attribute named 'KEY' represents the primary key attributes in order that they appear in the index. Otherwise, this name can be used as any other argument. If an attribute happens to be a SQL reserved word, it needs to be enclosed in backquotes. For example: data = query . fetch ( order_by = '`select` desc' ) The order_by value is eventually passed to the ORDER BY clause . Limiting results \u00b6 Similar to sorting, the limit and offset arguments can be used to limit the result to a subset of entities. data = query . fetch ( order_by = 'mouse_id' , limit = 10 , offset = 5 ) Note that an offset cannot be used without specifying a limit as well. Usage with Pandas \u00b6 The pandas library is a popular library for data analysis in Python which can easily be used with DataJoint query results. Since the records returned by fetch() are contained within a numpy.recarray , they can be easily converted to pandas.DataFrame objects by passing them into the pandas.DataFrame constructor. For example: import pandas as pd frame = pd . DataFrame ( tab . fetch ()) Calling fetch() with the argument format=\"frame\" returns results as pandas.DataFrame objects indexed by the table's primary key attributes. frame = tab . fetch ( format = \"frame\" ) Returning results as a DataFrame is not possible when fetching a particular subset of attributes or when as_dict is set to True .", "title": "Common Commands"}, {"location": "query-lang/common-commands/#insert", "text": "Data entry is as easy as providing the appropriate data structure to a permitted table. Given the following table definition, we can insert data as tuples, dicts, pandas dataframes, or pathlib Path relative paths to local CSV files. mouse_id: int # unique mouse id --- dob: date # mouse date of birth sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown Tuple Dict Pandas CSV mouse . insert1 ( ( 0 , '2017-03-01' , 'M' ) ) # Single entry data = [ ( 1 , '2016-11-19' , 'M' ), ( 2 , '2016-11-20' , 'U' ), ( 5 , '2016-12-25' , 'F' ) ] mouse . insert ( data ) # Multi-entry mouse . insert1 ( dict ( mouse_id = 0 , dob = '2017-03-01' , sex = 'M' ) ) # Single entry data = [ { 'mouse_id' : 1 , 'dob' : '2016-11-19' , 'sex' : 'M' }, { 'mouse_id' : 2 , 'dob' : '2016-11-20' , 'sex' : 'U' }, { 'mouse_id' : 5 , 'dob' : '2016-12-25' , 'sex' : 'F' } ] mouse . insert ( data ) # Multi-entry import pandas as pd data = pd . DataFrame ( [[ 1 , \"2016-11-19\" , \"M\" ], [ 2 , \"2016-11-20\" , \"U\" ], [ 5 , \"2016-12-25\" , \"F\" ]], columns = [ \"mouse_id\" , \"dob\" , \"sex\" ], ) mouse . insert ( data ) Given the following CSV in the current working directory as mice.csv mouse_id,dob,sex 1,2016-11-19,M 2,2016-11-20,U 5,2016-12-25,F We can import as follows: from pathlib import Path mouse . insert ( Path ( './mice.csv' ))", "title": "Insert"}, {"location": "query-lang/common-commands/#make", "text": "See the article on make methods", "title": "Make"}, {"location": "query-lang/common-commands/#fetch", "text": "", "title": "Fetch"}, {"location": "query-lang/common-commands/#entire-table", "text": "A fetch command can either retrieve table data as a NumPy recarray or a as a list of dict data = query . fetch () # (1) data = query . fetch ( as_dict = True ) # (2) NumPy recarray List of dict : For very large tables... In some cases, the amount of data returned by fetch can be quite large; it can be useful to use the size_on_disk attribute to determine if running a bare fetch would be wise. Please note that it is only currently possible to query the size of entire tables stored directly in the database at this time.", "title": "Entire table"}, {"location": "query-lang/common-commands/#separate-variables", "text": "name , img = query . fetch1 ( 'mouse_id' , 'dob' ) # when query has exactly one entity name , img = query . fetch ( 'mouse_id' , 'dob' ) # [mouse_id, ...] [dob, ...]", "title": "Separate variables"}, {"location": "query-lang/common-commands/#primary-key-values", "text": "keydict = tab . fetch1 ( \"KEY\" ) # single key dict when tab has exactly one entity keylist = tab . fetch ( \"KEY\" ) # list of key dictionaries [{}, ...] KEY can also used when returning attribute values as separate variables, such that one of the returned variables contains the entire primary keys.", "title": "Primary key values"}, {"location": "query-lang/common-commands/#sorting-results", "text": "To sort the result, use the order_by keyword argument. data = query . fetch ( order_by = 'mouse_id' ) # ascending order data = query . fetch ( order_by = 'mouse_id desc' ) # descending order data = query . fetch ( order_by = ( 'mouse_id' , 'dob' )) # by ID first, dob second data = query . fetch ( order_by = 'KEY' ) # sort by the primary key The order_by argument can be a string specifying the attribute to sort by. By default the sort is in ascending order. Use 'attr desc' to sort in descending order by attribute attr . The value can also be a sequence of strings, in which case, the sort performed on all the attributes jointly in the order specified. The special attribute named 'KEY' represents the primary key attributes in order that they appear in the index. Otherwise, this name can be used as any other argument. If an attribute happens to be a SQL reserved word, it needs to be enclosed in backquotes. For example: data = query . fetch ( order_by = '`select` desc' ) The order_by value is eventually passed to the ORDER BY clause .", "title": "Sorting results"}, {"location": "query-lang/common-commands/#limiting-results", "text": "Similar to sorting, the limit and offset arguments can be used to limit the result to a subset of entities. data = query . fetch ( order_by = 'mouse_id' , limit = 10 , offset = 5 ) Note that an offset cannot be used without specifying a limit as well.", "title": "Limiting results"}, {"location": "query-lang/common-commands/#usage-with-pandas", "text": "The pandas library is a popular library for data analysis in Python which can easily be used with DataJoint query results. Since the records returned by fetch() are contained within a numpy.recarray , they can be easily converted to pandas.DataFrame objects by passing them into the pandas.DataFrame constructor. For example: import pandas as pd frame = pd . DataFrame ( tab . fetch ()) Calling fetch() with the argument format=\"frame\" returns results as pandas.DataFrame objects indexed by the table's primary key attributes. frame = tab . fetch ( format = \"frame\" ) Returning results as a DataFrame is not possible when fetching a particular subset of attributes or when as_dict is set to True .", "title": "Usage with Pandas"}, {"location": "query-lang/iteration/", "text": "Iteration \u00b6 The DataJoint model primarily handles data as sets, in the form of tables. However, it can sometimes be useful to access or to perform actions such as visualization upon individual entities sequentially. In DataJoint this is accomplished through iteration. In the simple example below, iteration is used to display the names and values of the attributes of each entity in the simple table or table expression. for entity in table : print ( entity ) This example illustrates the function of the iterator: DataJoint iterates through the whole table expression, returning the entire entity during each step. In this case, each entity will be returned as a dict containing all attributes. At the start of the above loop, DataJoint internally fetches only the primary keys of the entities. Since only the primary keys are needed to distinguish between entities, DataJoint can then iterate over the list of primary keys to execute the loop. At each step of the loop, DataJoint uses a single primary key to fetch an entire entity for use in the iteration, such that print(entity) will print all attributes of each entity. By first fetching only the primary keys and then fetching each entity individually, DataJoint saves memory at the cost of network overhead. This can be particularly useful for tables containing large amounts of data in secondary attributes. The memory savings of the above syntax may not be worth the additional network overhead in all cases, such as for tables with little data stored as secondary attributes. In the example below, DataJoint fetches all of the attributes of each entity in a single call and then iterates over the list of entities stored in memory. for entity in table . fetch ( as_dict = True ): print ( entity )", "title": "Iteration"}, {"location": "query-lang/iteration/#iteration", "text": "The DataJoint model primarily handles data as sets, in the form of tables. However, it can sometimes be useful to access or to perform actions such as visualization upon individual entities sequentially. In DataJoint this is accomplished through iteration. In the simple example below, iteration is used to display the names and values of the attributes of each entity in the simple table or table expression. for entity in table : print ( entity ) This example illustrates the function of the iterator: DataJoint iterates through the whole table expression, returning the entire entity during each step. In this case, each entity will be returned as a dict containing all attributes. At the start of the above loop, DataJoint internally fetches only the primary keys of the entities. Since only the primary keys are needed to distinguish between entities, DataJoint can then iterate over the list of primary keys to execute the loop. At each step of the loop, DataJoint uses a single primary key to fetch an entire entity for use in the iteration, such that print(entity) will print all attributes of each entity. By first fetching only the primary keys and then fetching each entity individually, DataJoint saves memory at the cost of network overhead. This can be particularly useful for tables containing large amounts of data in secondary attributes. The memory savings of the above syntax may not be worth the additional network overhead in all cases, such as for tables with little data stored as secondary attributes. In the example below, DataJoint fetches all of the attributes of each entity in a single call and then iterates over the list of entities stored in memory. for entity in table . fetch ( as_dict = True ): print ( entity )", "title": "Iteration"}, {"location": "query-lang/operators/", "text": "Operators \u00b6 The examples below will use the table definitions in table tiers . Restriction \u00b6 & and - operators permit restriction. By a mapping \u00b6 For a Session table , that has the attribute session_date , we can restrict to sessions from January 1st, 2022: Session & { 'session_date' : \"2022-01-01\" } If there were any typos (e.g., using sess_date instead of session_date ), our query will return all of the entities of Session . By a string \u00b6 Conditions may include arithmetic operations, functions, range tests, etc. Restriction of table A by a string containing an attribute not found in table A produces an error. Session & 'user = \"Alice\"' # (1) Session & 'session_date >= \"2022-01-01\"' # (2) All the sessions performed by Alice All of the sessions on or after January 1st, 2022 By a collection \u00b6 When cond is a collection of conditions, the conditions are applied by logical disjunction (logical OR). Restricting a table by a collection will return all entities that meet any of the conditions in the collection. For example, if we restrict the Session table by a collection containing two conditions, one for user and one for date, the query will return any sessions with a matching user or date. A collection can be a list, a tuple, or a Pandas DataFrame . cond_list = [ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ] # (1) cond_tuple = ( 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ) # (2) import pandas as pd cond_frame = pd . DataFrame ( data = { 'user' : [ 'Alice' ], 'session_date' : [ '2022-01-01' ]}) # (3) Session () & [ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ] A list A tuple A data frame dj.AndList represents logical conjunction(logical AND). Restricting a table by an AndList will return all entities that meet all of the conditions in the list. A & dj.AndList([c1, c2, c3]) is equivalent to A & c1 & c2 & c3 . Student () & dj . AndList ([ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ]) The above will show all the sessions that Alice conducted on the given day. By a Not object \u00b6 The special function dj.Not represents logical negation, such that A & dj.Not (cond) is equivalent to A - cond . By a query \u00b6 Restriction by a query object is a generalization of restriction by a table. The example below creates a query object corresponding to all the users named Alice. The Session table is then restricted by the query object, returning all the sessions performed by Alice. query = User & 'user = \"Alice\"' Session & query Proj \u00b6 Renaming an attribute in python can be done via keyword arguments: table . proj ( new_attr = 'old_attr' ) This can be done in the context of a table definition: @schema class Session ( dj . Manual ): definition = \"\"\" # Experiment Session -> Animal session : smallint # session number for the animal --- session_datetime : datetime # YYYY-MM-DD HH:MM:SS session_start_time : float # seconds relative to session_datetime session_end_time : float # seconds relative to session_datetime -> User.proj(experimenter='username') -> User.proj(supervisor='username') \"\"\" Or to rename multiple values in a table with the following syntax: Table.proj(*existing_attributes,*renamed_attributes) Session . proj ( 'session' , 'session_date' , start = 'session_start_time' , end = 'session_end_time' ) Projection can also be used to to compute new attributes from existing ones. Session . proj ( duration = 'session_end_time-session_start_time' ) & 'duration > 10' Aggr \u00b6 For more complicated calculations, we can use aggregation. Subject . aggr ( Session , n = \"count(*)\" ) # (1) Subject . aggr ( Session , average_start = \"avg(session_start_time)\" ) # (2) Number of sessions per subject. Average session_start_time for each subject Universal set \u00b6 Universal sets offer the complete list of combinations of attributes. # All home cities of students dj . U ( 'laser_wavelength' , 'laser_power' ) & Scan # (1) dj . U ( 'laser_wavelength' , 'laser_power' ) . aggr ( Scan , n = \"count(*)\" ) # (2) dj . U () . aggr ( Session , n = \"max(session)\" ) # (3) All combinations of wavelength and power. Total number of scans for each combination. Largest session number. dj.U() , as shown in the last example above, is often useful for integer IDs. For an example of this process, see the source code for Element Array Electrophysiology's insert_new_params .", "title": "Operators"}, {"location": "query-lang/operators/#operators", "text": "The examples below will use the table definitions in table tiers .", "title": "Operators"}, {"location": "query-lang/operators/#restriction", "text": "& and - operators permit restriction.", "title": "Restriction"}, {"location": "query-lang/operators/#by-a-mapping", "text": "For a Session table , that has the attribute session_date , we can restrict to sessions from January 1st, 2022: Session & { 'session_date' : \"2022-01-01\" } If there were any typos (e.g., using sess_date instead of session_date ), our query will return all of the entities of Session .", "title": "By a mapping"}, {"location": "query-lang/operators/#by-a-string", "text": "Conditions may include arithmetic operations, functions, range tests, etc. Restriction of table A by a string containing an attribute not found in table A produces an error. Session & 'user = \"Alice\"' # (1) Session & 'session_date >= \"2022-01-01\"' # (2) All the sessions performed by Alice All of the sessions on or after January 1st, 2022", "title": "By a string"}, {"location": "query-lang/operators/#by-a-collection", "text": "When cond is a collection of conditions, the conditions are applied by logical disjunction (logical OR). Restricting a table by a collection will return all entities that meet any of the conditions in the collection. For example, if we restrict the Session table by a collection containing two conditions, one for user and one for date, the query will return any sessions with a matching user or date. A collection can be a list, a tuple, or a Pandas DataFrame . cond_list = [ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ] # (1) cond_tuple = ( 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ) # (2) import pandas as pd cond_frame = pd . DataFrame ( data = { 'user' : [ 'Alice' ], 'session_date' : [ '2022-01-01' ]}) # (3) Session () & [ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ] A list A tuple A data frame dj.AndList represents logical conjunction(logical AND). Restricting a table by an AndList will return all entities that meet all of the conditions in the list. A & dj.AndList([c1, c2, c3]) is equivalent to A & c1 & c2 & c3 . Student () & dj . AndList ([ 'user = \"Alice\"' , 'session_date = \"2022-01-01\"' ]) The above will show all the sessions that Alice conducted on the given day.", "title": "By a collection"}, {"location": "query-lang/operators/#by-a-not-object", "text": "The special function dj.Not represents logical negation, such that A & dj.Not (cond) is equivalent to A - cond .", "title": "By a Not object"}, {"location": "query-lang/operators/#by-a-query", "text": "Restriction by a query object is a generalization of restriction by a table. The example below creates a query object corresponding to all the users named Alice. The Session table is then restricted by the query object, returning all the sessions performed by Alice. query = User & 'user = \"Alice\"' Session & query", "title": "By a query"}, {"location": "query-lang/operators/#proj", "text": "Renaming an attribute in python can be done via keyword arguments: table . proj ( new_attr = 'old_attr' ) This can be done in the context of a table definition: @schema class Session ( dj . Manual ): definition = \"\"\" # Experiment Session -> Animal session : smallint # session number for the animal --- session_datetime : datetime # YYYY-MM-DD HH:MM:SS session_start_time : float # seconds relative to session_datetime session_end_time : float # seconds relative to session_datetime -> User.proj(experimenter='username') -> User.proj(supervisor='username') \"\"\" Or to rename multiple values in a table with the following syntax: Table.proj(*existing_attributes,*renamed_attributes) Session . proj ( 'session' , 'session_date' , start = 'session_start_time' , end = 'session_end_time' ) Projection can also be used to to compute new attributes from existing ones. Session . proj ( duration = 'session_end_time-session_start_time' ) & 'duration > 10'", "title": "Proj"}, {"location": "query-lang/operators/#aggr", "text": "For more complicated calculations, we can use aggregation. Subject . aggr ( Session , n = \"count(*)\" ) # (1) Subject . aggr ( Session , average_start = \"avg(session_start_time)\" ) # (2) Number of sessions per subject. Average session_start_time for each subject", "title": "Aggr"}, {"location": "query-lang/operators/#universal-set", "text": "Universal sets offer the complete list of combinations of attributes. # All home cities of students dj . U ( 'laser_wavelength' , 'laser_power' ) & Scan # (1) dj . U ( 'laser_wavelength' , 'laser_power' ) . aggr ( Scan , n = \"count(*)\" ) # (2) dj . U () . aggr ( Session , n = \"max(session)\" ) # (3) All combinations of wavelength and power. Total number of scans for each combination. Largest session number. dj.U() , as shown in the last example above, is often useful for integer IDs. For an example of this process, see the source code for Element Array Electrophysiology's insert_new_params .", "title": "Universal set"}, {"location": "query-lang/query-caching/", "text": "Query Caching \u00b6 Query caching allows avoiding repeated queries to the database by caching the results locally for faster retrieval. To enable queries, set the query cache local path in dj.config , create the directory, and activate the query caching. dj . config [ 'query_cache' ] = os . path . expanduser ( '~/dj_query_cache' ) # (1) # (2) conn = dj . conn () # if queries co-located with tables conn = module . schema . connection # if schema co-located with tables conn = module . table . connection # most flexible conn . set_query_cache ( query_cache = 'main' ) # (3) Set the query cache path Access the active connection object for the tables Activate query caching for a namespace called 'main' The query_cache argument is an arbitrary string serving to differentiate cache states; setting a new value will effectively start a new cache, triggering retrieval of new values once. To turn off query caching, use the following: conn . set_query_cache ( query_cache = None ) ## OR conn . set_query_cache () While query caching is enabled, any insert or delete calls and any transactions are disabled and will raise an error. This ensures that stale data are not used for updating the database in violation of data integrity. To clear and remove the query cache, use the following: conn . purge_query_cache () # Purge the cached queries", "title": "Query Caching"}, {"location": "query-lang/query-caching/#query-caching", "text": "Query caching allows avoiding repeated queries to the database by caching the results locally for faster retrieval. To enable queries, set the query cache local path in dj.config , create the directory, and activate the query caching. dj . config [ 'query_cache' ] = os . path . expanduser ( '~/dj_query_cache' ) # (1) # (2) conn = dj . conn () # if queries co-located with tables conn = module . schema . connection # if schema co-located with tables conn = module . table . connection # most flexible conn . set_query_cache ( query_cache = 'main' ) # (3) Set the query cache path Access the active connection object for the tables Activate query caching for a namespace called 'main' The query_cache argument is an arbitrary string serving to differentiate cache states; setting a new value will effectively start a new cache, triggering retrieval of new values once. To turn off query caching, use the following: conn . set_query_cache ( query_cache = None ) ## OR conn . set_query_cache () While query caching is enabled, any insert or delete calls and any transactions are disabled and will raise an error. This ensures that stale data are not used for updating the database in violation of data integrity. To clear and remove the query cache, use the following: conn . purge_query_cache () # Purge the cached queries", "title": "Query Caching"}, {"location": "reproduce/make-method/", "text": "Make Method \u00b6 Consider the following table definition from the article on table tiers : @schema class FilteredImage ( dj . Computed ): definition = \"\"\" # Filtered image -> Image --- filtered_image : longblob \"\"\" def make ( self , key ): img = ( test . Image & key ) . fetch1 ( 'image' ) key [ 'filtered_image' ] = my_filter ( img ) self . insert1 ( key ) The FilteredImage table can be populated as FilteredImage . populate () The make method receives one argument: the dict key containing the primary key value of an element of key source to be worked on. Optional Arguments \u00b6 The make method also accepts a number of optional arguments that provide more features and allow greater control over the method's behavior. Argument Default Description restrictions A list of restrictions, restricting as (tab.key_source & AndList (restrictions)) - tab.proj() . Here target is the table to be populated, usually tab itself. suppress_errors False If True , encountering an error will cancel the current make call, log the error, and continue to the next make call. Error messages will be logged in the job reservation table (if reserve_jobs is True ) and returned as a list. See also return_exception_objects and reserve_jobs . return_exception_objects False If True , error objects are returned instead of error messages. This applies only when suppress_errors is True . reserve_jobs False If True , reserves job to indicate to other distributed processes. The job reservation table may be access as schema.jobs . Errors are logged in the jobs table. order original The order of execution, either \"original\" , \"reverse\" , or \"random\" . limit None If not None , checks at most this number of keys. max_calls None If not None , populates at most this many keys. Defaults to no limit. display_progress False If True , displays a progress bar. processes 1 Number of processes to use. Set to None to use all cores make_kwargs None Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table. Progress \u00b6 The method table.progress reports how many key_source entries have been populated and how many remain. Two optional parameters allow more advanced use of the method. A parameter of restriction conditions can be provided, specifying which entities to consider. A Boolean parameter display (default is True ) allows disabling the output, such that the numbers of remaining and total entities are returned but not printed.", "title": "Make Method"}, {"location": "reproduce/make-method/#make-method", "text": "Consider the following table definition from the article on table tiers : @schema class FilteredImage ( dj . Computed ): definition = \"\"\" # Filtered image -> Image --- filtered_image : longblob \"\"\" def make ( self , key ): img = ( test . Image & key ) . fetch1 ( 'image' ) key [ 'filtered_image' ] = my_filter ( img ) self . insert1 ( key ) The FilteredImage table can be populated as FilteredImage . populate () The make method receives one argument: the dict key containing the primary key value of an element of key source to be worked on.", "title": "Make Method"}, {"location": "reproduce/make-method/#optional-arguments", "text": "The make method also accepts a number of optional arguments that provide more features and allow greater control over the method's behavior. Argument Default Description restrictions A list of restrictions, restricting as (tab.key_source & AndList (restrictions)) - tab.proj() . Here target is the table to be populated, usually tab itself. suppress_errors False If True , encountering an error will cancel the current make call, log the error, and continue to the next make call. Error messages will be logged in the job reservation table (if reserve_jobs is True ) and returned as a list. See also return_exception_objects and reserve_jobs . return_exception_objects False If True , error objects are returned instead of error messages. This applies only when suppress_errors is True . reserve_jobs False If True , reserves job to indicate to other distributed processes. The job reservation table may be access as schema.jobs . Errors are logged in the jobs table. order original The order of execution, either \"original\" , \"reverse\" , or \"random\" . limit None If not None , checks at most this number of keys. max_calls None If not None , populates at most this many keys. Defaults to no limit. display_progress False If True , displays a progress bar. processes 1 Number of processes to use. Set to None to use all cores make_kwargs None Keyword arguments which do not affect the result of computation to be passed down to each make() call. Computation arguments should be specified within the pipeline e.g. using a dj.Lookup table.", "title": "Optional Arguments"}, {"location": "reproduce/make-method/#progress", "text": "The method table.progress reports how many key_source entries have been populated and how many remain. Two optional parameters allow more advanced use of the method. A parameter of restriction conditions can be provided, specifying which entities to consider. A Boolean parameter display (default is True ) allows disabling the output, such that the numbers of remaining and total entities are returned but not printed.", "title": "Progress"}, {"location": "reproduce/table-tiers/", "text": "Table Tiers \u00b6 To define a DataJoint table in Python: Define a class inheriting from the appropriate DataJoint class: dj.Lookup , dj.Manual , dj.Imported or dj.Computed . Decorate the class with the schema object (see schema ) Define the class property definition to define the table heading. DataJoint for Python is implemented through the use of classes providing access to the actual tables stored on the database. Since only a single table exists on the database for any class, interactions with all instances of the class are equivalent. As such, most methods can be called on the classes themselves rather than on an object, for convenience. Whether calling a DataJoint method on a class or on an instance, the result will only depend on or apply to the corresponding table. All of the basic functionality of DataJoint is built to operate on the classes themselves, even when called on an instance. For example, calling Person.insert(...) (on the class) and Person.insert(...) (on an instance) both have the identical effect of inserting data into the table on the database server. DataJoint does not prevent a user from working with instances, but the workflow is complete without the need for instantiation. It is up to the user whether to implement additional functionality as class methods or methods called on instances. Manual Tables \u00b6 The following code defines two manual tables, Animal and Session : @schema class Animal ( dj . Manual ): definition = \"\"\" # information about animal animal_id : int # animal id assigned by the lab --- -> Species date_of_birth=null : date # YYYY-MM-DD optional sex='' : enum('M', 'F', '') # leave empty if unspecified \"\"\" @schema class Session ( dj . Manual ): definition = \"\"\" # Experiment Session -> Animal session : smallint # session number for the animal --- session_datetime : datetime # YYYY-MM-DD HH:MM:SS session_start_time : float # seconds relative to session_datetime session_end_time : float # seconds relative to session_datetime -> [nullable] User \"\"\" Note that the notation to permit null entries differs for attributes versus foreign key references. Lookup Tables \u00b6 Lookup tables are commonly populated from their contents property. The table below is declared as a lookup table with its contents property provided to generate entities. @schema class User ( dj . Lookup ): definition = \"\"\" # users in the lab username : varchar(20) # user in the lab --- first_name : varchar(20) # user first name last_name : varchar(20) # user last name \"\"\" contents = [ [ 'cajal' , 'Santiago' , 'Cajal' ], [ 'hubel' , 'David' , 'Hubel' ], [ 'wiesel' , 'Torsten' , 'Wiesel' ] ] @schema class ProcessingParamSet ( dj . Lookup ): definition = \"\"\" # Parameter set used for processing of calcium imaging data paramset_idx: smallint --- -> ProcessingMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) (1) params: longblob # dictionary of all applicable parameters \"\"\" This syntax enforces uniqueness of a secondary attribute. Imported and Computed Tables \u00b6 Imported and Computed tables provide make methods to determine how they are populated, either from files or other tables. Imagine that there is a table test.Image that contains 2D grayscale images in its image attribute. We can define the Computed table, test.FilteredImage that filters the image in some way and saves the result in its filtered_image attribute. @schema class FilteredImage ( dj . Computed ): definition = \"\"\" # Filtered image -> Image --- filtered_image : longblob \"\"\" def make ( self , key ): img = ( test . Image & key ) . fetch1 ( 'image' ) key [ 'filtered_image' ] = my_filter ( img ) self . insert1 ( key ) Part Tables \u00b6 The following code defines a Imported table with an associated part table. In Python, the master-part relationship is expressed by making the part a nested class of the master. The part is subclassed from dj.Part and does not need the @schema decorator. @schema class Scan ( dj . Imported ): definition = \"\"\" # Two-photon imaging scan -> Session scan : smallint # scan number within the session --- -> Lens laser_wavelength : decimal(5,1) # um laser_power : decimal(4,1) # mW \"\"\" class ScanField ( dj . Part ): definition = \"\"\" -> master ROI: longblob # Region of interest \"\"\" def make ( self , key ): ... # (1) self . insert1 ( key ) self . ScanField . insert1 ( ROI_information ) This make method is truncated for the sake of brevity. For more detailed examples, please visit Element Calcium Imaging table definitions", "title": "Table Tiers"}, {"location": "reproduce/table-tiers/#table-tiers", "text": "To define a DataJoint table in Python: Define a class inheriting from the appropriate DataJoint class: dj.Lookup , dj.Manual , dj.Imported or dj.Computed . Decorate the class with the schema object (see schema ) Define the class property definition to define the table heading. DataJoint for Python is implemented through the use of classes providing access to the actual tables stored on the database. Since only a single table exists on the database for any class, interactions with all instances of the class are equivalent. As such, most methods can be called on the classes themselves rather than on an object, for convenience. Whether calling a DataJoint method on a class or on an instance, the result will only depend on or apply to the corresponding table. All of the basic functionality of DataJoint is built to operate on the classes themselves, even when called on an instance. For example, calling Person.insert(...) (on the class) and Person.insert(...) (on an instance) both have the identical effect of inserting data into the table on the database server. DataJoint does not prevent a user from working with instances, but the workflow is complete without the need for instantiation. It is up to the user whether to implement additional functionality as class methods or methods called on instances.", "title": "Table Tiers"}, {"location": "reproduce/table-tiers/#manual-tables", "text": "The following code defines two manual tables, Animal and Session : @schema class Animal ( dj . Manual ): definition = \"\"\" # information about animal animal_id : int # animal id assigned by the lab --- -> Species date_of_birth=null : date # YYYY-MM-DD optional sex='' : enum('M', 'F', '') # leave empty if unspecified \"\"\" @schema class Session ( dj . Manual ): definition = \"\"\" # Experiment Session -> Animal session : smallint # session number for the animal --- session_datetime : datetime # YYYY-MM-DD HH:MM:SS session_start_time : float # seconds relative to session_datetime session_end_time : float # seconds relative to session_datetime -> [nullable] User \"\"\" Note that the notation to permit null entries differs for attributes versus foreign key references.", "title": "Manual Tables"}, {"location": "reproduce/table-tiers/#lookup-tables", "text": "Lookup tables are commonly populated from their contents property. The table below is declared as a lookup table with its contents property provided to generate entities. @schema class User ( dj . Lookup ): definition = \"\"\" # users in the lab username : varchar(20) # user in the lab --- first_name : varchar(20) # user first name last_name : varchar(20) # user last name \"\"\" contents = [ [ 'cajal' , 'Santiago' , 'Cajal' ], [ 'hubel' , 'David' , 'Hubel' ], [ 'wiesel' , 'Torsten' , 'Wiesel' ] ] @schema class ProcessingParamSet ( dj . Lookup ): definition = \"\"\" # Parameter set used for processing of calcium imaging data paramset_idx: smallint --- -> ProcessingMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) (1) params: longblob # dictionary of all applicable parameters \"\"\" This syntax enforces uniqueness of a secondary attribute.", "title": "Lookup Tables"}, {"location": "reproduce/table-tiers/#imported-and-computed-tables", "text": "Imported and Computed tables provide make methods to determine how they are populated, either from files or other tables. Imagine that there is a table test.Image that contains 2D grayscale images in its image attribute. We can define the Computed table, test.FilteredImage that filters the image in some way and saves the result in its filtered_image attribute. @schema class FilteredImage ( dj . Computed ): definition = \"\"\" # Filtered image -> Image --- filtered_image : longblob \"\"\" def make ( self , key ): img = ( test . Image & key ) . fetch1 ( 'image' ) key [ 'filtered_image' ] = my_filter ( img ) self . insert1 ( key )", "title": "Imported and Computed Tables"}, {"location": "reproduce/table-tiers/#part-tables", "text": "The following code defines a Imported table with an associated part table. In Python, the master-part relationship is expressed by making the part a nested class of the master. The part is subclassed from dj.Part and does not need the @schema decorator. @schema class Scan ( dj . Imported ): definition = \"\"\" # Two-photon imaging scan -> Session scan : smallint # scan number within the session --- -> Lens laser_wavelength : decimal(5,1) # um laser_power : decimal(4,1) # mW \"\"\" class ScanField ( dj . Part ): definition = \"\"\" -> master ROI: longblob # Region of interest \"\"\" def make ( self , key ): ... # (1) self . insert1 ( key ) self . ScanField . insert1 ( ROI_information ) This make method is truncated for the sake of brevity. For more detailed examples, please visit Element Calcium Imaging table definitions", "title": "Part Tables"}, {"location": "tutorials/json/", "text": "(function (global, factory) { typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() : typeof define === 'function' && define.amd ? define(factory) : (global = global || self, global.ClipboardCopyElement = factory()); }(this, function () { 'use strict'; function createNode(text) { const node = document.createElement('pre'); node.style.width = '1px'; node.style.height = '1px'; node.style.position = 'fixed'; node.style.top = '5px'; node.textContent = text; return node; } function copyNode(node) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(node.textContent); } const selection = getSelection(); if (selection == null) { return Promise.reject(new Error()); } selection.removeAllRanges(); const range = document.createRange(); range.selectNodeContents(node); selection.addRange(range); document.execCommand('copy'); selection.removeAllRanges(); return Promise.resolve(); } function copyText(text) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(text); } const body = document.body; if (!body) { return Promise.reject(new Error()); } const node = createNode(text); body.appendChild(node); copyNode(node); body.removeChild(node); return Promise.resolve(); } function copy(button) { const id = button.getAttribute('for'); const text = button.getAttribute('value'); function trigger() { button.dispatchEvent(new CustomEvent('clipboard-copy', { bubbles: true })); } if (text) { copyText(text).then(trigger); } else if (id) { const root = 'getRootNode' in Element.prototype ? button.getRootNode() : button.ownerDocument; if (!(root instanceof Document || 'ShadowRoot' in window && root instanceof ShadowRoot)) return; const node = root.getElementById(id); if (node) copyTarget(node).then(trigger); } } function copyTarget(content) { if (content instanceof HTMLInputElement || content instanceof HTMLTextAreaElement) { return copyText(content.value); } else if (content instanceof HTMLAnchorElement && content.hasAttribute('href')) { return copyText(content.href); } else { return copyNode(content); } } function clicked(event) { const button = event.currentTarget; if (button instanceof HTMLElement) { copy(button); } } function keydown(event) { if (event.key === ' ' || event.key === 'Enter') { const button = event.currentTarget; if (button instanceof HTMLElement) { event.preventDefault(); copy(button); } } } function focused(event) { event.currentTarget.addEventListener('keydown', keydown); } function blurred(event) { event.currentTarget.removeEventListener('keydown', keydown); } class ClipboardCopyElement extends HTMLElement { constructor() { super(); this.addEventListener('click', clicked); this.addEventListener('focus', focused); this.addEventListener('blur', blurred); } connectedCallback() { if (!this.hasAttribute('tabindex')) { this.setAttribute('tabindex', '0'); } if (!this.hasAttribute('role')) { this.setAttribute('role', 'button'); } } get value() { return this.getAttribute('value') || ''; } set value(text) { this.setAttribute('value', text); } } if (!window.customElements.get('clipboard-copy')) { window.ClipboardCopyElement = ClipboardCopyElement; window.customElements.define('clipboard-copy', ClipboardCopyElement); } return ClipboardCopyElement; })); document.addEventListener('clipboard-copy', function(event) { const notice = event.target.querySelector('.notice') notice.hidden = false setTimeout(function() { notice.hidden = true }, 1000) }) pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight-ipynb .hll { background-color: var(--jp-cell-editor-active-background) } .highlight-ipynb { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight-ipynb .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight-ipynb .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight-ipynb .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight-ipynb .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight-ipynb .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight-ipynb .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight-ipynb .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight-ipynb .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight-ipynb .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight-ipynb .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight-ipynb .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */ .highlight-ipynb .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight-ipynb .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight-ipynb .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight-ipynb .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight-ipynb .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight-ipynb .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight-ipynb .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight-ipynb .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight-ipynb .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight-ipynb .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /* This file is taken from the built JupyterLab theme.css Found on share/nbconvert/templates/lab/static Some changes have been made and marked with CHANGE */ .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-900); --jp-brand-color1: var(--md-blue-700); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-900); --jp-accent-color1: var(--md-green-700); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-900); --jp-warn-color1: var(--md-orange-700); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-900); --jp-error-color1: var(--md-red-700); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-900); --jp-success-color1: var(--md-green-700); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-900); --jp-info-color1: var(--md-cyan-700); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } [data-md-color-scheme=\"slate\"] .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ /* The dark theme shadows need a bit of work, but this will probably also require work on the core layout * colors used in the theme as well. */ --jp-shadow-base-lightness: 32; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-700); --jp-border-color1: var(--md-grey-700); --jp-border-color2: var(--md-grey-800); --jp-border-color3: var(--md-grey-900); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(255, 255, 255, 1); --jp-ui-font-color1: rgba(255, 255, 255, 0.87); --jp-ui-font-color2: rgba(255, 255, 255, 0.54); --jp-ui-font-color3: rgba(255, 255, 255, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(0, 0, 0, 1); --jp-ui-inverse-font-color1: rgba(0, 0, 0, 0.8); --jp-ui-inverse-font-color2: rgba(0, 0, 0, 0.5); --jp-ui-inverse-font-color3: rgba(0, 0, 0, 0.3); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(255, 255, 255, 1); --jp-content-font-color1: rgba(255, 255, 255, 1); --jp-content-font-color2: rgba(255, 255, 255, 0.7); --jp-content-font-color3: rgba(255, 255, 255, 0.5); --jp-content-link-color: var(--md-blue-300); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: #111111; --jp-layout-color1: var(--md-grey-900); --jp-layout-color2: var(--md-grey-800); --jp-layout-color3: var(--md-grey-700); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: white; --jp-inverse-layout-color1: white; --jp-inverse-layout-color2: var(--md-grey-200); --jp-inverse-layout-color3: var(--md-grey-400); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--jp-layout-color1); --jp-cell-editor-border-color: var(--md-grey-700); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 1; --jp-cell-prompt-not-active-font-color: var(--md-grey-300); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: rgba(33, 150, 243, 0.24); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: rgba(244, 67, 54, 0.28); --jp-rendermime-table-row-background: var(--md-grey-900); --jp-rendermime-table-row-hover-background: rgba(3, 169, 244, 0.2); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.6); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color2); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.8); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--jp-layout-color0); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color0); --jp-input-hover-background: var(--jp-layout-color2); --jp-input-background: var(--md-grey-800); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: var(--jp-layout-color2); --jp-editor-selected-focused-background: rgba(33, 150, 243, 0.24); --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: var(--md-green-500); --jp-mirror-editor-atom-color: var(--md-blue-300); --jp-mirror-editor-number-color: var(--md-green-400); --jp-mirror-editor-def-color: var(--md-blue-600); --jp-mirror-editor-variable-color: var(--md-grey-300); --jp-mirror-editor-variable-2-color: var(--md-blue-400); --jp-mirror-editor-variable-3-color: var(--md-green-600); --jp-mirror-editor-punctuation-color: var(--md-blue-400); --jp-mirror-editor-property-color: var(--md-blue-400); --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ff7070; --jp-mirror-editor-string-2-color: var(--md-purple-300); --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: var(--md-green-600); --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: var(--md-green-700); --jp-mirror-editor-attribute-color: var(--md-blue-700); --jp-mirror-editor-header-color: var(--md-blue-500); --jp-mirror-editor-quote-color: var(--md-green-300); --jp-mirror-editor-link-color: var(--md-blue-700); --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: var(--md-grey-400); /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.6; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(255, 225, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* scrollbar related styles. Supports every browser except Edge. */ /* colors based on JetBrain's Darcula theme */ --jp-scrollbar-background-color: #3f4244; --jp-scrollbar-thumb-color: 88, 96, 97; /* need to specify thumb color as an RGB triplet */ --jp-scrollbar-endpad: 3px; /* the minimum gap between the thumb and the ends of a scrollbar */ /* hacks for setting the thumb shape. These do nothing in Firefox */ --jp-scrollbar-thumb-margin: 3.5px; /* the space in between the sides of the thumb and the track */ --jp-scrollbar-thumb-radius: 9px; /* set to a large-ish value for rounded endcaps on the thumb */ /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper{/*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. *//*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */}.jupyter-wrapper [data-jp-theme-scrollbars=true]{scrollbar-color:rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar{scrollbar-color:rgba(var(--jp-scrollbar-thumb-color), 0.5) rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-corner{background:var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-thumb{background:rgb(var(--jp-scrollbar-thumb-color));border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-right:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-bottom:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-corner,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-corner{background-color:rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb{background:rgba(var(--jp-scrollbar-thumb-color), 0.5);border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-right:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-bottom:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{min-height:16px;max-height:16px;min-width:45px;border-top:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{min-width:16px;max-width:16px;min-height:45px;border-left:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar-button{background-color:#f0f0f0;background-position:center center;min-height:15px;max-height:15px;min-width:15px;max-width:15px}.jupyter-wrapper .lm-ScrollBar-button:hover{background-color:#dadada}.jupyter-wrapper .lm-ScrollBar-button.lm-mod-active{background-color:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-track{background:#f0f0f0}.jupyter-wrapper .lm-ScrollBar-thumb{background:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-thumb:hover{background:#bababa}.jupyter-wrapper .lm-ScrollBar-thumb.lm-mod-active{background:#a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-thumb{height:100%;min-width:15px;border-left:1px solid #a0a0a0;border-right:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-thumb{width:100%;min-height:15px;border-top:1px solid #a0a0a0;border-bottom:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-left);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-right);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-up);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-down);background-size:17px}.jupyter-wrapper .p-Widget,.jupyter-wrapper .lm-Widget{box-sizing:border-box;position:relative;overflow:hidden;cursor:default}.jupyter-wrapper .p-Widget.p-mod-hidden,.jupyter-wrapper .lm-Widget.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-CommandPalette,.jupyter-wrapper .lm-CommandPalette{display:flex;flex-direction:column;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-CommandPalette-search,.jupyter-wrapper .lm-CommandPalette-search{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-content,.jupyter-wrapper .lm-CommandPalette-content{flex:1 1 auto;margin:0;padding:0;min-height:0;overflow:auto;list-style-type:none}.jupyter-wrapper .p-CommandPalette-header,.jupyter-wrapper .lm-CommandPalette-header{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-CommandPalette-item,.jupyter-wrapper .lm-CommandPalette-item{display:flex;flex-direction:row}.jupyter-wrapper .p-CommandPalette-itemIcon,.jupyter-wrapper .lm-CommandPalette-itemIcon{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemContent,.jupyter-wrapper .lm-CommandPalette-itemContent{flex:1 1 auto;overflow:hidden}.jupyter-wrapper .p-CommandPalette-itemShortcut,.jupyter-wrapper .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemLabel,.jupyter-wrapper .lm-CommandPalette-itemLabel{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-DockPanel,.jupyter-wrapper .lm-DockPanel{z-index:0}.jupyter-wrapper .p-DockPanel-widget,.jupyter-wrapper .lm-DockPanel-widget{z-index:0}.jupyter-wrapper .p-DockPanel-tabBar,.jupyter-wrapper .lm-DockPanel-tabBar{z-index:1}.jupyter-wrapper .p-DockPanel-handle,.jupyter-wrapper .lm-DockPanel-handle{z-index:2}.jupyter-wrapper .p-DockPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-DockPanel-handle:after,.jupyter-wrapper .lm-DockPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]{cursor:ew-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]{cursor:ns-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-DockPanel-overlay,.jupyter-wrapper .lm-DockPanel-overlay{z-index:3;box-sizing:border-box;pointer-events:none}.jupyter-wrapper .p-DockPanel-overlay.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-overlay.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-Menu,.jupyter-wrapper .lm-Menu{z-index:10000;position:absolute;white-space:nowrap;overflow-x:hidden;overflow-y:auto;outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-Menu-content,.jupyter-wrapper .lm-Menu-content{margin:0;padding:0;display:table;list-style-type:none}.jupyter-wrapper .p-Menu-item,.jupyter-wrapper .lm-Menu-item{display:table-row}.jupyter-wrapper .p-Menu-item.p-mod-hidden,.jupyter-wrapper .p-Menu-item.p-mod-collapsed,.jupyter-wrapper .lm-Menu-item.lm-mod-hidden,.jupyter-wrapper .lm-Menu-item.lm-mod-collapsed{display:none !important}.jupyter-wrapper .p-Menu-itemIcon,.jupyter-wrapper .p-Menu-itemSubmenuIcon,.jupyter-wrapper .lm-Menu-itemIcon,.jupyter-wrapper .lm-Menu-itemSubmenuIcon{display:table-cell;text-align:center}.jupyter-wrapper .p-Menu-itemLabel,.jupyter-wrapper .lm-Menu-itemLabel{display:table-cell;text-align:left}.jupyter-wrapper .p-Menu-itemShortcut,.jupyter-wrapper .lm-Menu-itemShortcut{display:table-cell;text-align:right}.jupyter-wrapper .p-MenuBar,.jupyter-wrapper .lm-MenuBar{outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-MenuBar-content,.jupyter-wrapper .lm-MenuBar-content{margin:0;padding:0;display:flex;flex-direction:row;list-style-type:none}.jupyter-wrapper .p--MenuBar-item,.jupyter-wrapper .lm-MenuBar-item{box-sizing:border-box}.jupyter-wrapper .p-MenuBar-itemIcon,.jupyter-wrapper .p-MenuBar-itemLabel,.jupyter-wrapper .lm-MenuBar-itemIcon,.jupyter-wrapper .lm-MenuBar-itemLabel{display:inline-block}.jupyter-wrapper .p-ScrollBar,.jupyter-wrapper .lm-ScrollBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-ScrollBar[data-orientation=horizontal],.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-ScrollBar[data-orientation=vertical],.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-ScrollBar-button,.jupyter-wrapper .lm-ScrollBar-button{box-sizing:border-box;flex:0 0 auto}.jupyter-wrapper .p-ScrollBar-track,.jupyter-wrapper .lm-ScrollBar-track{box-sizing:border-box;position:relative;overflow:hidden;flex:1 1 auto}.jupyter-wrapper .p-ScrollBar-thumb,.jupyter-wrapper .lm-ScrollBar-thumb{box-sizing:border-box;position:absolute}.jupyter-wrapper .p-SplitPanel-child,.jupyter-wrapper .lm-SplitPanel-child{z-index:0}.jupyter-wrapper .p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel-handle{z-index:1}.jupyter-wrapper .p-SplitPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-SplitPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle{cursor:ew-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle{cursor:ns-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-TabBar,.jupyter-wrapper .lm-TabBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal],.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical],.jupyter-wrapper .lm-TabBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-TabBar-content,.jupyter-wrapper .lm-TabBar-content{margin:0;padding:0;display:flex;flex:1 1 auto;list-style-type:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]>.lm-TabBar-content{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=vertical]>.lm-TabBar-content{flex-direction:column}.jupyter-wrapper .p-TabBar-tab,.jupyter-wrapper .lm-TabBar-tab{display:flex;flex-direction:row;box-sizing:border-box;overflow:hidden}.jupyter-wrapper .p-TabBar-tabIcon,.jupyter-wrapper .p-TabBar-tabCloseIcon,.jupyter-wrapper .lm-TabBar-tabIcon,.jupyter-wrapper .lm-TabBar-tabCloseIcon{flex:0 0 auto}.jupyter-wrapper .p-TabBar-tabLabel,.jupyter-wrapper .lm-TabBar-tabLabel{flex:1 1 auto;overflow:hidden;white-space:nowrap}.jupyter-wrapper .p-TabBar-tab.p-mod-hidden,.jupyter-wrapper .lm-TabBar-tab.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging .lm-TabBar-tab{position:relative}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=horizontal] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=horizontal] .lm-TabBar-tab{left:0;transition:left 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=vertical] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=vertical] .lm-TabBar-tab{top:0;transition:top 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging{transition:none}.jupyter-wrapper .p-TabPanel-tabBar,.jupyter-wrapper .lm-TabPanel-tabBar{z-index:1}.jupyter-wrapper .p-TabPanel-stackedPanel,.jupyter-wrapper .lm-TabPanel-stackedPanel{z-index:0}.jupyter-wrapper ::-moz-selection{background:rgba(125,188,255,.6)}.jupyter-wrapper ::selection{background:rgba(125,188,255,.6)}.jupyter-wrapper .bp3-heading{color:#182026;font-weight:600;margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-dark .bp3-heading{color:#f5f8fa}.jupyter-wrapper h1.bp3-heading,.jupyter-wrapper .bp3-running-text h1{line-height:40px;font-size:36px}.jupyter-wrapper h2.bp3-heading,.jupyter-wrapper .bp3-running-text h2{line-height:32px;font-size:28px}.jupyter-wrapper h3.bp3-heading,.jupyter-wrapper .bp3-running-text h3{line-height:25px;font-size:22px}.jupyter-wrapper h4.bp3-heading,.jupyter-wrapper .bp3-running-text h4{line-height:21px;font-size:18px}.jupyter-wrapper h5.bp3-heading,.jupyter-wrapper .bp3-running-text h5{line-height:19px;font-size:16px}.jupyter-wrapper h6.bp3-heading,.jupyter-wrapper .bp3-running-text h6{line-height:16px;font-size:14px}.jupyter-wrapper .bp3-ui-text{text-transform:none;line-height:1.28581;letter-spacing:0;font-size:14px;font-weight:400}.jupyter-wrapper .bp3-monospace-text{text-transform:none;font-family:monospace}.jupyter-wrapper .bp3-text-muted{color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-text-muted{color:#a7b6c2}.jupyter-wrapper .bp3-text-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-text-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal}.jupyter-wrapper .bp3-running-text{line-height:1.5;font-size:14px}.jupyter-wrapper .bp3-running-text h1{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h1{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h2{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h2{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h3{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h3{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h4{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h4{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h5{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h5{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h6{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h6{color:#f5f8fa}.jupyter-wrapper .bp3-running-text hr{margin:20px 0;border:none;border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text hr{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-running-text p{margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-text-large{font-size:16px}.jupyter-wrapper .bp3-text-small{font-size:12px}.jupyter-wrapper a{text-decoration:none;color:#106ba3}.jupyter-wrapper a:hover{cursor:pointer;text-decoration:underline;color:#106ba3}.jupyter-wrapper a .bp3-icon,.jupyter-wrapper a .bp3-icon-standard,.jupyter-wrapper a .bp3-icon-large{color:inherit}.jupyter-wrapper a code,.jupyter-wrapper .bp3-dark a code{color:inherit}.jupyter-wrapper .bp3-dark a,.jupyter-wrapper .bp3-dark a:hover{color:#48aff0}.jupyter-wrapper .bp3-dark a .bp3-icon,.jupyter-wrapper .bp3-dark a .bp3-icon-standard,.jupyter-wrapper .bp3-dark a .bp3-icon-large,.jupyter-wrapper .bp3-dark a:hover .bp3-icon,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-standard,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-large{color:inherit}.jupyter-wrapper .bp3-running-text code,.jupyter-wrapper .bp3-code{text-transform:none;font-family:monospace;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);background:rgba(255,255,255,.7);padding:2px 5px;color:#5c7080;font-size:smaller}.jupyter-wrapper .bp3-dark .bp3-running-text code,.jupyter-wrapper .bp3-running-text .bp3-dark code,.jupyter-wrapper .bp3-dark .bp3-code{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#a7b6c2}.jupyter-wrapper .bp3-running-text a>code,.jupyter-wrapper a>.bp3-code{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-running-text a>code,.jupyter-wrapper .bp3-running-text .bp3-dark a>code,.jupyter-wrapper .bp3-dark a>.bp3-code{color:inherit}.jupyter-wrapper .bp3-running-text pre,.jupyter-wrapper .bp3-code-block{text-transform:none;font-family:monospace;display:block;margin:10px 0;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);background:rgba(255,255,255,.7);padding:13px 15px 12px;line-height:1.4;color:#182026;font-size:13px;word-break:break-all;word-wrap:break-word}.jupyter-wrapper .bp3-dark .bp3-running-text pre,.jupyter-wrapper .bp3-running-text .bp3-dark pre,.jupyter-wrapper .bp3-dark .bp3-code-block{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-running-text pre>code,.jupyter-wrapper .bp3-code-block>code{-webkit-box-shadow:none;box-shadow:none;background:none;padding:0;color:inherit;font-size:inherit}.jupyter-wrapper .bp3-running-text kbd,.jupyter-wrapper .bp3-key{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background:#fff;min-width:24px;height:24px;padding:3px 6px;vertical-align:middle;line-height:24px;color:#5c7080;font-family:inherit;font-size:12px}.jupyter-wrapper .bp3-running-text kbd .bp3-icon,.jupyter-wrapper .bp3-key .bp3-icon,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-standard,.jupyter-wrapper .bp3-key .bp3-icon-standard,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-large,.jupyter-wrapper .bp3-key .bp3-icon-large{margin-right:5px}.jupyter-wrapper .bp3-dark .bp3-running-text kbd,.jupyter-wrapper .bp3-running-text .bp3-dark kbd,.jupyter-wrapper .bp3-dark .bp3-key{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);background:#394b59;color:#a7b6c2}.jupyter-wrapper .bp3-running-text blockquote,.jupyter-wrapper .bp3-blockquote{margin:0 0 10px;border-left:solid 4px rgba(167,182,194,.5);padding:0 20px}.jupyter-wrapper .bp3-dark .bp3-running-text blockquote,.jupyter-wrapper .bp3-running-text .bp3-dark blockquote,.jupyter-wrapper .bp3-dark .bp3-blockquote{border-color:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-running-text ul,.jupyter-wrapper .bp3-running-text ol,.jupyter-wrapper .bp3-list{margin:10px 0;padding-left:30px}.jupyter-wrapper .bp3-running-text ul li:not(:last-child),.jupyter-wrapper .bp3-running-text ol li:not(:last-child),.jupyter-wrapper .bp3-list li:not(:last-child){margin-bottom:5px}.jupyter-wrapper .bp3-running-text ul ol,.jupyter-wrapper .bp3-running-text ol ol,.jupyter-wrapper .bp3-list ol,.jupyter-wrapper .bp3-running-text ul ul,.jupyter-wrapper .bp3-running-text ol ul,.jupyter-wrapper .bp3-list ul{margin-top:5px}.jupyter-wrapper .bp3-list-unstyled{margin:0;padding:0;list-style:none}.jupyter-wrapper .bp3-list-unstyled li{padding:0}.jupyter-wrapper .bp3-rtl{text-align:right}.jupyter-wrapper .bp3-dark{color:#f5f8fa}.jupyter-wrapper :focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-focus-disabled :focus{outline:none !important}.jupyter-wrapper .bp3-focus-disabled :focus~.bp3-control-indicator{outline:none !important}.jupyter-wrapper .bp3-alert{max-width:400px;padding:20px}.jupyter-wrapper .bp3-alert-body{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-alert-body .bp3-icon{margin-top:0;margin-right:20px;font-size:40px}.jupyter-wrapper .bp3-alert-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;margin-top:10px}.jupyter-wrapper .bp3-alert-footer .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-breadcrumbs{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;cursor:default;height:30px;padding:0;list-style:none}.jupyter-wrapper .bp3-breadcrumbs>li{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-breadcrumbs>li::after{display:block;margin:0 5px;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\");width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs>li:last-of-type::after{display:none}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumb-current,.jupyter-wrapper .bp3-breadcrumbs-collapsed{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumbs-collapsed{color:#5c7080}.jupyter-wrapper .bp3-breadcrumb:hover{text-decoration:none}.jupyter-wrapper .bp3-breadcrumb.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-breadcrumb .bp3-icon{margin-right:5px}.jupyter-wrapper .bp3-breadcrumb-current{color:inherit;font-weight:600}.jupyter-wrapper .bp3-breadcrumb-current .bp3-input{vertical-align:baseline;font-size:inherit;font-weight:inherit}.jupyter-wrapper .bp3-breadcrumbs-collapsed{margin-right:2px;border:none;border-radius:3px;background:#ced9e0;cursor:pointer;padding:1px 5px;vertical-align:text-bottom}.jupyter-wrapper .bp3-breadcrumbs-collapsed::before{display:block;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat;width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs-collapsed:hover{background:#bfccd6;text-decoration:none;color:#182026}.jupyter-wrapper .bp3-dark .bp3-breadcrumb,.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs>li::after{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumb.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-breadcrumb-current{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed:hover{background:rgba(16,22,26,.6);color:#f5f8fa}.jupyter-wrapper .bp3-button{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;min-width:30px;min-height:30px}.jupyter-wrapper .bp3-button>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-button>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-button::before,.jupyter-wrapper .bp3-button>*{margin-right:7px}.jupyter-wrapper .bp3-button:empty::before,.jupyter-wrapper .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button:empty{padding:0 !important}.jupyter-wrapper .bp3-button:disabled,.jupyter-wrapper .bp3-button.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-button.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button.bp3-align-right,.jupyter-wrapper .bp3-align-right .bp3-button{text-align:right}.jupyter-wrapper .bp3-button.bp3-align-left,.jupyter-wrapper .bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active:hover,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-button.bp3-intent-primary{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-success{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0f9960;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0d8050}.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0a6640;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(15,153,96,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-warning{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#d9822b;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#bf7326}.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a66321;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(217,130,43,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-danger{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#db3737;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#c23030}.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a82a2a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(219,55,55,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#fff}.jupyter-wrapper .bp3-button.bp3-large,.jupyter-wrapper .bp3-large .bp3-button{min-width:40px;min-height:40px;padding:5px 15px;font-size:16px}.jupyter-wrapper .bp3-button.bp3-large::before,.jupyter-wrapper .bp3-button.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-button::before,.jupyter-wrapper .bp3-large .bp3-button>*{margin-right:10px}.jupyter-wrapper .bp3-button.bp3-large:empty::before,.jupyter-wrapper .bp3-button.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-button:empty::before,.jupyter-wrapper .bp3-large .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button.bp3-small,.jupyter-wrapper .bp3-small .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-button.bp3-loading{position:relative}.jupyter-wrapper .bp3-button.bp3-loading[class*=bp3-icon-]::before{visibility:hidden}.jupyter-wrapper .bp3-button.bp3-loading .bp3-button-spinner{position:absolute;margin:0}.jupyter-wrapper .bp3-button.bp3-loading>:not(.bp3-button-spinner){visibility:hidden}.jupyter-wrapper .bp3-button[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon,.jupyter-wrapper .bp3-button .bp3-icon-standard,.jupyter-wrapper .bp3-button .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-standard.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-large.bp3-align-right{margin-left:7px}.jupyter-wrapper .bp3-button .bp3-icon:first-child:last-child,.jupyter-wrapper .bp3-button .bp3-spinner+.bp3-icon:last-child{margin:0 -7px}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-])[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-image:none;color:rgba(255,255,255,.3)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-button:disabled::before,.jupyter-wrapper .bp3-button:disabled .bp3-icon,.jupyter-wrapper .bp3-button:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button:disabled .bp3-icon-large,.jupyter-wrapper .bp3-button.bp3-disabled::before,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-large,.jupyter-wrapper .bp3-button[class*=bp3-intent-]::before,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-standard,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-large{color:inherit !important}.jupyter-wrapper .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button.bp3-minimal:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper a.bp3-button{text-align:center;text-decoration:none;-webkit-transition:none;transition:none}.jupyter-wrapper a.bp3-button,.jupyter-wrapper a.bp3-button:hover,.jupyter-wrapper a.bp3-button:active{color:#182026}.jupyter-wrapper a.bp3-button.bp3-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-text{-webkit-box-flex:0;-ms-flex:0 1 auto;flex:0 1 auto}.jupyter-wrapper .bp3-button.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button.bp3-align-right .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-right .bp3-button-text{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.jupyter-wrapper .bp3-button-group .bp3-button{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;z-index:4}.jupyter-wrapper .bp3-button-group .bp3-button:focus{z-index:5}.jupyter-wrapper .bp3-button-group .bp3-button:hover{z-index:6}.jupyter-wrapper .bp3-button-group .bp3-button:active,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-active{z-index:7}.jupyter-wrapper .bp3-button-group .bp3-button:disabled,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]{z-index:9}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:focus{z-index:10}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:hover{z-index:11}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-active{z-index:12}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:first-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:-1px;border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-button-group .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button-group .bp3-button.bp3-fill,.jupyter-wrapper .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;vertical-align:top}.jupyter-wrapper .bp3-button-group.bp3-vertical.bp3-fill{width:unset;height:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical .bp3-button{margin-right:0 !important;width:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:first-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:first-child{border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:last-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-bottom:-1px}.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:1px}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-button:not(:last-child){margin-bottom:1px}.jupyter-wrapper .bp3-callout{line-height:1.5;font-size:14px;position:relative;border-radius:3px;background-color:rgba(138,155,168,.15);width:100%;padding:10px 12px 9px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]{padding-left:40px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout.bp3-callout-icon{padding-left:40px}.jupyter-wrapper .bp3-callout.bp3-callout-icon>.bp3-icon:first-child{position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout .bp3-heading{margin-top:0;margin-bottom:5px;line-height:20px}.jupyter-wrapper .bp3-callout .bp3-heading:last-child{margin-bottom:0}.jupyter-wrapper .bp3-dark .bp3-callout{background-color:rgba(138,155,168,.2)}.jupyter-wrapper .bp3-dark .bp3-callout[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-primary .bp3-heading{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{color:#48aff0}.jupyter-wrapper .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-success .bp3-heading{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{color:#3dcc91}.jupyter-wrapper .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-warning .bp3-heading{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{color:#ffb366}.jupyter-wrapper .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-danger .bp3-heading{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{color:#ff7373}.jupyter-wrapper .bp3-running-text .bp3-callout{margin:20px 0}.jupyter-wrapper .bp3-card{border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#fff;padding:20px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-card.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#30404d}.jupyter-wrapper .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-0.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-1.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-2.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-3.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-4.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);cursor:pointer}.jupyter-wrapper .bp3-card.bp3-interactive:hover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:active{opacity:.9;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);-webkit-transition-duration:0;transition-duration:0}.jupyter-wrapper .bp3-card.bp3-interactive:active.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-collapse{height:0;overflow-y:hidden;-webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body{-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-context-menu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-context-menu-popover-target{position:fixed}.jupyter-wrapper .bp3-divider{margin:5px;border-right:1px solid rgba(16,22,26,.15);border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-divider{border-color:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dialog-container{opacity:1;-webkit-transform:scale(1);transform:scale(1);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;min-height:100%;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter-active>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear-active>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit-active>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:30px 0;border-radius:6px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#ebf1f5;width:500px;padding-bottom:20px;pointer-events:all;-webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text}.jupyter-wrapper .bp3-dialog:focus{outline:0}.jupyter-wrapper .bp3-dialog.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-dialog{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#293742;color:#f5f8fa}.jupyter-wrapper .bp3-dialog-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:6px 6px 0 0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);background:#fff;min-height:40px;padding-right:5px;padding-left:20px}.jupyter-wrapper .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dialog-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-dialog-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-dialog-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-dialog-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4);background:#30404d}.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dialog-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:20px;line-height:18px}.jupyter-wrapper .bp3-dialog-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin:0 20px}.jupyter-wrapper .bp3-dialog-footer-actions{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end}.jupyter-wrapper .bp3-dialog-footer-actions .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-drawer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#fff;padding:0}.jupyter-wrapper .bp3-drawer:focus{outline:0}.jupyter-wrapper .bp3-drawer.bp3-position-top{top:0;right:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{-webkit-transform:translateY(-100%);transform:translateY(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left{top:0;bottom:0;left:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear{-webkit-transform:translateX(-100%);transform:translateX(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{-webkit-transform:translateX(-100%);transform:translateX(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right{top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical){top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-drawer{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-drawer-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border-radius:0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);min-height:40px;padding:5px;padding-left:20px}.jupyter-wrapper .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-drawer-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-drawer-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-drawer-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-drawer-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-drawer-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;overflow:auto;line-height:18px}.jupyter-wrapper .bp3-drawer-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);padding:10px 20px}.jupyter-wrapper .bp3-dark .bp3-drawer-footer{-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.4);box-shadow:inset 0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text{display:inline-block;position:relative;cursor:text;max-width:100%;vertical-align:top;white-space:nowrap}.jupyter-wrapper .bp3-editable-text::before{position:absolute;top:-3px;right:-3px;bottom:-3px;left:-3px;border-radius:3px;content:\"\";-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#137cbd}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#0f9960}.jupyter-wrapper .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#d9822b}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#db3737}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4);box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4);box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4);box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4);box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text-content{display:inherit;position:relative;min-width:inherit;max-width:inherit;vertical-align:top;text-transform:inherit;letter-spacing:inherit;color:inherit;font:inherit;resize:none}.jupyter-wrapper .bp3-editable-text-input{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;width:100%;padding:0;white-space:pre-wrap}.jupyter-wrapper .bp3-editable-text-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:focus{outline:none}.jupyter-wrapper .bp3-editable-text-input::-ms-clear{display:none}.jupyter-wrapper .bp3-editable-text-content{overflow:hidden;padding-right:2px;text-overflow:ellipsis;white-space:pre}.jupyter-wrapper .bp3-editable-text-editing>.bp3-editable-text-content{position:absolute;left:0;visibility:hidden}.jupyter-wrapper .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-editable-text.bp3-multiline{display:block}.jupyter-wrapper .bp3-editable-text.bp3-multiline .bp3-editable-text-content{overflow:auto;white-space:pre-wrap;word-wrap:break-word}.jupyter-wrapper .bp3-control-group{-webkit-transform:translateZ(0);transform:translateZ(0);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-control-group>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select,.jupyter-wrapper .bp3-control-group .bp3-input,.jupyter-wrapper .bp3-control-group .bp3-select{position:relative}.jupyter-wrapper .bp3-control-group .bp3-input{z-index:2;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-input:focus{z-index:14;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-input[readonly],.jupyter-wrapper .bp3-control-group .bp3-input:disabled,.jupyter-wrapper .bp3-control-group .bp3-input.bp3-disabled{z-index:1}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select select,.jupyter-wrapper .bp3-control-group .bp3-select select{-webkit-transform:translateZ(0);transform:translateZ(0);z-index:4;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-button:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select:focus,.jupyter-wrapper .bp3-control-group .bp3-select select:focus{z-index:5}.jupyter-wrapper .bp3-control-group .bp3-button:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select:hover,.jupyter-wrapper .bp3-control-group .bp3-select select:hover{z-index:6}.jupyter-wrapper .bp3-control-group .bp3-button:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select:active,.jupyter-wrapper .bp3-control-group .bp3-select select:active{z-index:7}.jupyter-wrapper .bp3-control-group .bp3-button[readonly],.jupyter-wrapper .bp3-control-group .bp3-button:disabled,.jupyter-wrapper .bp3-control-group .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]{z-index:9}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:focus{z-index:10}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:hover{z-index:11}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:active{z-index:12}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-input-action{z-index:16}.jupyter-wrapper .bp3-control-group .bp3-select::after,.jupyter-wrapper .bp3-control-group .bp3-html-select::after,.jupyter-wrapper .bp3-control-group .bp3-select>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-html-select>.bp3-icon{z-index:17}.jupyter-wrapper .bp3-control-group:not(.bp3-vertical)>*{margin-right:-1px}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>*{margin-right:0}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>.bp3-button+.bp3-button{margin-left:1px}.jupyter-wrapper .bp3-control-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-control-group .bp3-popover-target{border-radius:inherit}.jupyter-wrapper .bp3-control-group>:first-child{border-radius:3px 0 0 3px}.jupyter-wrapper .bp3-control-group>:last-child{margin-right:0;border-radius:0 3px 3px 0}.jupyter-wrapper .bp3-control-group>:only-child{margin-right:0;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input-group .bp3-button{border-radius:3px}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-fill>*:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.jupyter-wrapper .bp3-control-group.bp3-vertical>*{margin-top:-1px}.jupyter-wrapper .bp3-control-group.bp3-vertical>:first-child{margin-top:0;border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-control-group.bp3-vertical>:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-control{display:block;position:relative;margin-bottom:10px;cursor:pointer;text-transform:none}.jupyter-wrapper .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control:not(.bp3-align-right){padding-left:26px}.jupyter-wrapper .bp3-control:not(.bp3-align-right) .bp3-control-indicator{margin-left:-26px}.jupyter-wrapper .bp3-control.bp3-align-right{padding-right:26px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{margin-right:-26px}.jupyter-wrapper .bp3-control.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-control.bp3-inline{display:inline-block;margin-right:20px}.jupyter-wrapper .bp3-control input{position:absolute;top:0;left:0;opacity:0;z-index:-1}.jupyter-wrapper .bp3-control .bp3-control-indicator{display:inline-block;position:relative;margin-top:-3px;margin-right:10px;border:none;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));cursor:pointer;width:1em;height:1em;vertical-align:middle;font-size:16px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-control .bp3-control-indicator::before{display:block;width:1em;height:1em;content:\"\"}.jupyter-wrapper .bp3-control:hover .bp3-control-indicator{background-color:#ebf1f5}.jupyter-wrapper .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background:#d8e1e8}.jupyter-wrapper .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed}.jupyter-wrapper .bp3-control input:focus~.bp3-control-indicator{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{float:right;margin-top:1px;margin-left:10px}.jupyter-wrapper .bp3-control.bp3-large{font-size:16px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right){padding-left:30px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right{padding-right:30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-30px}.jupyter-wrapper .bp3-control.bp3-large .bp3-control-indicator{font-size:20px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-top:0}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control.bp3-checkbox .bp3-control-indicator{border-radius:3px}.jupyter-wrapper .bp3-control.bp3-checkbox input:checked~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-radio .bp3-control-indicator{border-radius:50%}.jupyter-wrapper .bp3-control.bp3-radio input:checked~.bp3-control-indicator::before{background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%)}.jupyter-wrapper .bp3-control.bp3-radio input:checked:disabled~.bp3-control-indicator::before{opacity:.5}.jupyter-wrapper .bp3-control.bp3-radio input:focus~.bp3-control-indicator{-moz-outline-radius:16px}.jupyter-wrapper .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(167,182,194,.5)}.jupyter-wrapper .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(92,112,128,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(206,217,224,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right){padding-left:38px}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{margin-left:-38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right{padding-right:38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{margin-right:-38px}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator{border:none;border-radius:1.75em;-webkit-box-shadow:none !important;box-shadow:none !important;width:auto;min-width:1.75em;-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator::before{position:absolute;left:0;margin:2px;border-radius:50%;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);background:#fff;width:calc(1em - 4px);height:calc(1em - 4px);-webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{left:calc(100% - 1em)}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){padding-left:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right{padding-right:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-45px}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(16,22,26,.7)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(16,22,26,.9)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(57,75,89,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background:#394b59}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-switch-inner-text{text-align:center;font-size:.7em}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{visibility:hidden;margin-right:1.2em;margin-left:.5em;line-height:0}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{visibility:visible;margin-right:.5em;margin-left:1.2em;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:first-child{visibility:visible;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:last-child{visibility:hidden;line-height:0}.jupyter-wrapper .bp3-dark .bp3-control{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-control.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-control .bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0))}.jupyter-wrapper .bp3-dark .bp3-control:hover .bp3-control-indicator{background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background:#202b33}.jupyter-wrapper .bp3-dark .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);cursor:not-allowed}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked~.bp3-control-indicator,.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-file-input{display:inline-block;position:relative;cursor:pointer;height:30px}.jupyter-wrapper .bp3-file-input input{opacity:0;margin:0;min-width:200px}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active:hover,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#182026}.jupyter-wrapper .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#f5f8fa}.jupyter-wrapper .bp3-file-input.bp3-fill{width:100%}.jupyter-wrapper .bp3-file-input.bp3-large,.jupyter-wrapper .bp3-large .bp3-file-input{height:40px}.jupyter-wrapper .bp3-file-input .bp3-file-upload-input-custom-text::after{content:attr(bp3-button-text)}.jupyter-wrapper .bp3-file-upload-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;left:0;padding-right:80px;color:rgba(92,112,128,.6);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-file-upload-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:focus,.jupyter-wrapper .bp3-file-upload-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-file-upload-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;min-width:24px;min-height:24px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;margin:3px;border-radius:3px;width:70px;text-align:center;line-height:24px;content:\"Browse\"}.jupyter-wrapper .bp3-file-upload-input::after:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active:hover,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-file-upload-input:hover::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input:active::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-large .bp3-file-upload-input{height:40px;line-height:40px;font-size:16px;padding-right:95px}.jupyter-wrapper .bp3-large .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-large .bp3-file-upload-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-large .bp3-file-upload-input::after{min-width:30px;min-height:30px;margin:5px;width:85px;line-height:30px}.jupyter-wrapper .bp3-dark .bp3-file-upload-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:hover::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:active::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1)}.jupyter-wrapper .bp3-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0 0 15px}.jupyter-wrapper .bp3-form-group label.bp3-label{margin-bottom:5px}.jupyter-wrapper .bp3-form-group .bp3-control{margin-top:7px}.jupyter-wrapper .bp3-form-group .bp3-form-helper-text{margin-top:5px;color:#5c7080;font-size:12px}.jupyter-wrapper .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#106ba3}.jupyter-wrapper .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#0d8050}.jupyter-wrapper .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#bf7326}.jupyter-wrapper .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#c23030}.jupyter-wrapper .bp3-form-group.bp3-inline{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-form-group.bp3-inline.bp3-large label.bp3-label{margin:0 10px 0 0;line-height:40px}.jupyter-wrapper .bp3-form-group.bp3-inline label.bp3-label{margin:0 10px 0 0;line-height:30px}.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-form-group .bp3-form-helper-text{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-input-group{display:block;position:relative}.jupyter-wrapper .bp3-input-group .bp3-input{position:relative;width:100%}.jupyter-wrapper .bp3-input-group .bp3-input:not(:first-child){padding-left:30px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:last-child){padding-right:30px}.jupyter-wrapper .bp3-input-group .bp3-input-action,.jupyter-wrapper .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-input-group>.bp3-icon{position:absolute;top:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:first-child,.jupyter-wrapper .bp3-input-group>.bp3-button:first-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:first-child{left:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:last-child,.jupyter-wrapper .bp3-input-group>.bp3-button:last-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:last-child{right:0}.jupyter-wrapper .bp3-input-group .bp3-button{min-width:24px;min-height:24px;margin:3px;padding:0 7px}.jupyter-wrapper .bp3-input-group .bp3-button:empty{padding:0}.jupyter-wrapper .bp3-input-group>.bp3-icon{z-index:1;color:#5c7080}.jupyter-wrapper .bp3-input-group>.bp3-icon:empty{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input-action>.bp3-spinner{margin:7px}.jupyter-wrapper .bp3-input-group .bp3-tag{margin:5px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#a7b6c2}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-button{min-width:30px;min-height:30px;margin:5px}.jupyter-wrapper .bp3-input-group.bp3-large>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input-action>.bp3-spinner{margin:12px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:first-child){padding-left:40px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:last-child){padding-right:40px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-button{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-tag{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input-action>.bp3-spinner{margin:4px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:first-child){padding-left:24px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:last-child){padding-right:24px}.jupyter-wrapper .bp3-input-group.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-input-group.bp3-round .bp3-button,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-input,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-tag{border-radius:30px}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#48aff0}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-success>.bp3-icon{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-success>.bp3-icon{color:#3dcc91}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#ffb366}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#ff7373}.jupyter-wrapper .bp3-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none}.jupyter-wrapper .bp3-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:focus,.jupyter-wrapper .bp3-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input[type=search],.jupyter-wrapper .bp3-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-input:disabled,.jupyter-wrapper .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-input.bp3-large{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input.bp3-large[type=search],.jupyter-wrapper .bp3-input.bp3-large.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input.bp3-small{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input.bp3-small[type=search],.jupyter-wrapper .bp3-input.bp3-small.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-dark .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input::-ms-clear{display:none}.jupyter-wrapper textarea.bp3-input{max-width:100%;padding:10px}.jupyter-wrapper textarea.bp3-input,.jupyter-wrapper textarea.bp3-input.bp3-large,.jupyter-wrapper textarea.bp3-input.bp3-small{height:auto;line-height:inherit}.jupyter-wrapper textarea.bp3-input.bp3-small{padding:8px}.jupyter-wrapper .bp3-dark textarea.bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark textarea.bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input:disabled,.jupyter-wrapper .bp3-dark textarea.bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper label.bp3-label{display:block;margin-top:0;margin-bottom:15px}.jupyter-wrapper label.bp3-label .bp3-html-select,.jupyter-wrapper label.bp3-label .bp3-input,.jupyter-wrapper label.bp3-label .bp3-select,.jupyter-wrapper label.bp3-label .bp3-slider,.jupyter-wrapper label.bp3-label .bp3-popover-wrapper{display:block;margin-top:5px;text-transform:none}.jupyter-wrapper label.bp3-label .bp3-button-group{margin-top:5px}.jupyter-wrapper label.bp3-label .bp3-select select,.jupyter-wrapper label.bp3-label .bp3-html-select select{width:100%;vertical-align:top;font-weight:400}.jupyter-wrapper label.bp3-label.bp3-disabled,.jupyter-wrapper label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(92,112,128,.6)}.jupyter-wrapper label.bp3-label.bp3-inline{line-height:30px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-html-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-popover-wrapper{display:inline-block;margin:0 0 0 5px;vertical-align:top}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-button-group{margin:0 0 0 5px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group .bp3-input{margin-left:0}.jupyter-wrapper label.bp3-label.bp3-inline.bp3-large{line-height:40px}.jupyter-wrapper label.bp3-label:not(.bp3-inline) .bp3-popover-target{display:block}.jupyter-wrapper .bp3-dark label.bp3-label{color:#f5f8fa}.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled,.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button{-webkit-box-flex:1;-ms-flex:1 1 14px;flex:1 1 14px;width:30px;min-height:0;padding:0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:first-child{border-radius:0 3px 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:last-child{border-radius:0 0 3px 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:first-child{border-radius:3px 0 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:last-child{border-radius:0 0 0 3px}.jupyter-wrapper .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical>.bp3-button{width:40px}.jupyter-wrapper form{display:block}.jupyter-wrapper .bp3-html-select select,.jupyter-wrapper .bp3-select select{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;border-radius:3px;width:100%;height:30px;padding:0 25px 0 10px;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-html-select select>.bp3-fill,.jupyter-wrapper .bp3-select select>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-html-select select::before,.jupyter-wrapper .bp3-select select::before,.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{margin-right:7px}.jupyter-wrapper .bp3-html-select select:empty::before,.jupyter-wrapper .bp3-select select:empty::before,.jupyter-wrapper .bp3-html-select select>:last-child,.jupyter-wrapper .bp3-select select>:last-child{margin-right:0}.jupyter-wrapper .bp3-html-select select:hover,.jupyter-wrapper .bp3-select select:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-html-select select:active,.jupyter-wrapper .bp3-select select:active,.jupyter-wrapper .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-select select.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled,.jupyter-wrapper .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-select select.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal select{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-large select,.jupyter-wrapper .bp3-select.bp3-large select{height:40px;padding-right:35px;font-size:16px}.jupyter-wrapper .bp3-dark .bp3-html-select select,.jupyter-wrapper .bp3-dark .bp3-select select{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon,.jupyter-wrapper .bp3-select::after{position:absolute;top:7px;right:7px;color:#5c7080;pointer-events:none}.jupyter-wrapper .bp3-html-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-disabled.bp3-select::after{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select,.jupyter-wrapper .bp3-select{display:inline-block;position:relative;vertical-align:middle;letter-spacing:normal}.jupyter-wrapper .bp3-html-select select::-ms-expand,.jupyter-wrapper .bp3-select select::-ms-expand{display:none}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon{color:#5c7080}.jupyter-wrapper .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-select .bp3-icon:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon:hover{color:#f5f8fa}.jupyter-wrapper .bp3-html-select.bp3-large::after,.jupyter-wrapper .bp3-html-select.bp3-large .bp3-icon,.jupyter-wrapper .bp3-select.bp3-large::after,.jupyter-wrapper .bp3-select.bp3-large .bp3-icon{top:12px;right:12px}.jupyter-wrapper .bp3-html-select.bp3-fill,.jupyter-wrapper .bp3-html-select.bp3-fill select,.jupyter-wrapper .bp3-select.bp3-fill,.jupyter-wrapper .bp3-select.bp3-fill select{width:100%}.jupyter-wrapper .bp3-dark .bp3-html-select option,.jupyter-wrapper .bp3-dark .bp3-select option{background-color:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select::after,.jupyter-wrapper .bp3-dark .bp3-select::after{color:#a7b6c2}.jupyter-wrapper .bp3-select::after{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6c6\"}.jupyter-wrapper .bp3-running-text table,.jupyter-wrapper table.bp3-html-table{border-spacing:0;font-size:14px}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th,.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{padding:11px;vertical-align:top;text-align:left}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th{color:#182026;font-weight:600}.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{color:#182026}.jupyter-wrapper .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text table th,.jupyter-wrapper .bp3-running-text .bp3-dark table th,.jupyter-wrapper .bp3-dark table.bp3-html-table th{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table td,.jupyter-wrapper .bp3-running-text .bp3-dark table td,.jupyter-wrapper .bp3-dark table.bp3-html-table td{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child th,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child td,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed th,.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed td,.jupyter-wrapper table.bp3-html-table.bp3-small th,.jupyter-wrapper table.bp3-html-table.bp3-small td{padding-top:6px;padding-bottom:6px}.jupyter-wrapper table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(191,204,214,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(191,204,214,.3);cursor:pointer}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(92,112,128,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(92,112,128,.3);cursor:pointer}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-key-combo{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-key-combo>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-key-combo>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-key-combo::before,.jupyter-wrapper .bp3-key-combo>*{margin-right:5px}.jupyter-wrapper .bp3-key-combo:empty::before,.jupyter-wrapper .bp3-key-combo>:last-child{margin-right:0}.jupyter-wrapper .bp3-hotkey-dialog{top:40px;padding-bottom:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-dialog-body{margin:0;padding:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-hotkey-label{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.jupyter-wrapper .bp3-hotkey-column{margin:auto;max-height:80vh;overflow-y:auto;padding:30px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading{margin-bottom:20px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading:not(:first-child){margin-top:40px}.jupyter-wrapper .bp3-hotkey{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;margin-right:0;margin-left:0}.jupyter-wrapper .bp3-hotkey:not(:last-child){margin-bottom:10px}.jupyter-wrapper .bp3-icon{display:inline-block;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;vertical-align:text-bottom}.jupyter-wrapper .bp3-icon:not(:empty)::before{content:\"\" !important;content:unset !important}.jupyter-wrapper .bp3-icon>svg{display:block}.jupyter-wrapper .bp3-icon>svg:not([fill]){fill:currentColor}.jupyter-wrapper .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-icon-large.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-icon-large.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-icon-large.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-icon-large.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-danger{color:#ff7373}.jupyter-wrapper span.bp3-icon-standard{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon-large{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon:empty{line-height:1;font-family:\"Icons20\";font-size:inherit;font-weight:400;font-style:normal}.jupyter-wrapper span.bp3-icon:empty::before{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-icon-add::before{content:\"\ue63e\"}.jupyter-wrapper .bp3-icon-add-column-left::before{content:\"\ue6f9\"}.jupyter-wrapper .bp3-icon-add-column-right::before{content:\"\ue6fa\"}.jupyter-wrapper .bp3-icon-add-row-bottom::before{content:\"\ue6f8\"}.jupyter-wrapper .bp3-icon-add-row-top::before{content:\"\ue6f7\"}.jupyter-wrapper .bp3-icon-add-to-artifact::before{content:\"\ue67c\"}.jupyter-wrapper .bp3-icon-add-to-folder::before{content:\"\ue6d2\"}.jupyter-wrapper .bp3-icon-airplane::before{content:\"\ue74b\"}.jupyter-wrapper .bp3-icon-align-center::before{content:\"\ue603\"}.jupyter-wrapper .bp3-icon-align-justify::before{content:\"\ue605\"}.jupyter-wrapper .bp3-icon-align-left::before{content:\"\ue602\"}.jupyter-wrapper .bp3-icon-align-right::before{content:\"\ue604\"}.jupyter-wrapper .bp3-icon-alignment-bottom::before{content:\"\ue727\"}.jupyter-wrapper .bp3-icon-alignment-horizontal-center::before{content:\"\ue726\"}.jupyter-wrapper .bp3-icon-alignment-left::before{content:\"\ue722\"}.jupyter-wrapper .bp3-icon-alignment-right::before{content:\"\ue724\"}.jupyter-wrapper .bp3-icon-alignment-top::before{content:\"\ue725\"}.jupyter-wrapper .bp3-icon-alignment-vertical-center::before{content:\"\ue723\"}.jupyter-wrapper .bp3-icon-annotation::before{content:\"\ue6f0\"}.jupyter-wrapper .bp3-icon-application::before{content:\"\ue735\"}.jupyter-wrapper .bp3-icon-applications::before{content:\"\ue621\"}.jupyter-wrapper .bp3-icon-archive::before{content:\"\ue907\"}.jupyter-wrapper .bp3-icon-arrow-bottom-left::before{content:\"\u2199\"}.jupyter-wrapper .bp3-icon-arrow-bottom-right::before{content:\"\u2198\"}.jupyter-wrapper .bp3-icon-arrow-down::before{content:\"\u2193\"}.jupyter-wrapper .bp3-icon-arrow-left::before{content:\"\u2190\"}.jupyter-wrapper .bp3-icon-arrow-right::before{content:\"\u2192\"}.jupyter-wrapper .bp3-icon-arrow-top-left::before{content:\"\u2196\"}.jupyter-wrapper .bp3-icon-arrow-top-right::before{content:\"\u2197\"}.jupyter-wrapper .bp3-icon-arrow-up::before{content:\"\u2191\"}.jupyter-wrapper .bp3-icon-arrows-horizontal::before{content:\"\u2194\"}.jupyter-wrapper .bp3-icon-arrows-vertical::before{content:\"\u2195\"}.jupyter-wrapper .bp3-icon-asterisk::before{content:\"*\"}.jupyter-wrapper .bp3-icon-automatic-updates::before{content:\"\ue65f\"}.jupyter-wrapper .bp3-icon-badge::before{content:\"\ue6e3\"}.jupyter-wrapper .bp3-icon-ban-circle::before{content:\"\ue69d\"}.jupyter-wrapper .bp3-icon-bank-account::before{content:\"\ue76f\"}.jupyter-wrapper .bp3-icon-barcode::before{content:\"\ue676\"}.jupyter-wrapper .bp3-icon-blank::before{content:\"\ue900\"}.jupyter-wrapper .bp3-icon-blocked-person::before{content:\"\ue768\"}.jupyter-wrapper .bp3-icon-bold::before{content:\"\ue606\"}.jupyter-wrapper .bp3-icon-book::before{content:\"\ue6b8\"}.jupyter-wrapper .bp3-icon-bookmark::before{content:\"\ue61a\"}.jupyter-wrapper .bp3-icon-box::before{content:\"\ue6bf\"}.jupyter-wrapper .bp3-icon-briefcase::before{content:\"\ue674\"}.jupyter-wrapper .bp3-icon-bring-data::before{content:\"\ue90a\"}.jupyter-wrapper .bp3-icon-build::before{content:\"\ue72d\"}.jupyter-wrapper .bp3-icon-calculator::before{content:\"\ue70b\"}.jupyter-wrapper .bp3-icon-calendar::before{content:\"\ue62b\"}.jupyter-wrapper .bp3-icon-camera::before{content:\"\ue69e\"}.jupyter-wrapper .bp3-icon-caret-down::before{content:\"\u2304\"}.jupyter-wrapper .bp3-icon-caret-left::before{content:\"\u2329\"}.jupyter-wrapper .bp3-icon-caret-right::before{content:\"\u232a\"}.jupyter-wrapper .bp3-icon-caret-up::before{content:\"\u2303\"}.jupyter-wrapper .bp3-icon-cell-tower::before{content:\"\ue770\"}.jupyter-wrapper .bp3-icon-changes::before{content:\"\ue623\"}.jupyter-wrapper .bp3-icon-chart::before{content:\"\ue67e\"}.jupyter-wrapper .bp3-icon-chat::before{content:\"\ue689\"}.jupyter-wrapper .bp3-icon-chevron-backward::before{content:\"\ue6df\"}.jupyter-wrapper .bp3-icon-chevron-down::before{content:\"\ue697\"}.jupyter-wrapper .bp3-icon-chevron-forward::before{content:\"\ue6e0\"}.jupyter-wrapper .bp3-icon-chevron-left::before{content:\"\ue694\"}.jupyter-wrapper .bp3-icon-chevron-right::before{content:\"\ue695\"}.jupyter-wrapper .bp3-icon-chevron-up::before{content:\"\ue696\"}.jupyter-wrapper .bp3-icon-circle::before{content:\"\ue66a\"}.jupyter-wrapper .bp3-icon-circle-arrow-down::before{content:\"\ue68e\"}.jupyter-wrapper .bp3-icon-circle-arrow-left::before{content:\"\ue68c\"}.jupyter-wrapper .bp3-icon-circle-arrow-right::before{content:\"\ue68b\"}.jupyter-wrapper .bp3-icon-circle-arrow-up::before{content:\"\ue68d\"}.jupyter-wrapper .bp3-icon-citation::before{content:\"\ue61b\"}.jupyter-wrapper .bp3-icon-clean::before{content:\"\ue7c5\"}.jupyter-wrapper .bp3-icon-clipboard::before{content:\"\ue61d\"}.jupyter-wrapper .bp3-icon-cloud::before{content:\"\u2601\"}.jupyter-wrapper .bp3-icon-cloud-download::before{content:\"\ue690\"}.jupyter-wrapper .bp3-icon-cloud-upload::before{content:\"\ue691\"}.jupyter-wrapper .bp3-icon-code::before{content:\"\ue661\"}.jupyter-wrapper .bp3-icon-code-block::before{content:\"\ue6c5\"}.jupyter-wrapper .bp3-icon-cog::before{content:\"\ue645\"}.jupyter-wrapper .bp3-icon-collapse-all::before{content:\"\ue763\"}.jupyter-wrapper .bp3-icon-column-layout::before{content:\"\ue6da\"}.jupyter-wrapper .bp3-icon-comment::before{content:\"\ue68a\"}.jupyter-wrapper .bp3-icon-comparison::before{content:\"\ue637\"}.jupyter-wrapper .bp3-icon-compass::before{content:\"\ue79c\"}.jupyter-wrapper .bp3-icon-compressed::before{content:\"\ue6c0\"}.jupyter-wrapper .bp3-icon-confirm::before{content:\"\ue639\"}.jupyter-wrapper .bp3-icon-console::before{content:\"\ue79b\"}.jupyter-wrapper .bp3-icon-contrast::before{content:\"\ue6cb\"}.jupyter-wrapper .bp3-icon-control::before{content:\"\ue67f\"}.jupyter-wrapper .bp3-icon-credit-card::before{content:\"\ue649\"}.jupyter-wrapper .bp3-icon-cross::before{content:\"\u2717\"}.jupyter-wrapper .bp3-icon-crown::before{content:\"\ue7b4\"}.jupyter-wrapper .bp3-icon-cube::before{content:\"\ue7c8\"}.jupyter-wrapper .bp3-icon-cube-add::before{content:\"\ue7c9\"}.jupyter-wrapper .bp3-icon-cube-remove::before{content:\"\ue7d0\"}.jupyter-wrapper .bp3-icon-curved-range-chart::before{content:\"\ue71b\"}.jupyter-wrapper .bp3-icon-cut::before{content:\"\ue6ef\"}.jupyter-wrapper .bp3-icon-dashboard::before{content:\"\ue751\"}.jupyter-wrapper .bp3-icon-data-lineage::before{content:\"\ue908\"}.jupyter-wrapper .bp3-icon-database::before{content:\"\ue683\"}.jupyter-wrapper .bp3-icon-delete::before{content:\"\ue644\"}.jupyter-wrapper .bp3-icon-delta::before{content:\"\u0394\"}.jupyter-wrapper .bp3-icon-derive-column::before{content:\"\ue739\"}.jupyter-wrapper .bp3-icon-desktop::before{content:\"\ue6af\"}.jupyter-wrapper .bp3-icon-diagram-tree::before{content:\"\ue7b3\"}.jupyter-wrapper .bp3-icon-direction-left::before{content:\"\ue681\"}.jupyter-wrapper .bp3-icon-direction-right::before{content:\"\ue682\"}.jupyter-wrapper .bp3-icon-disable::before{content:\"\ue600\"}.jupyter-wrapper .bp3-icon-document::before{content:\"\ue630\"}.jupyter-wrapper .bp3-icon-document-open::before{content:\"\ue71e\"}.jupyter-wrapper .bp3-icon-document-share::before{content:\"\ue71f\"}.jupyter-wrapper .bp3-icon-dollar::before{content:\"$\"}.jupyter-wrapper .bp3-icon-dot::before{content:\"\u2022\"}.jupyter-wrapper .bp3-icon-double-caret-horizontal::before{content:\"\ue6c7\"}.jupyter-wrapper .bp3-icon-double-caret-vertical::before{content:\"\ue6c6\"}.jupyter-wrapper .bp3-icon-double-chevron-down::before{content:\"\ue703\"}.jupyter-wrapper .bp3-icon-double-chevron-left::before{content:\"\ue6ff\"}.jupyter-wrapper .bp3-icon-double-chevron-right::before{content:\"\ue701\"}.jupyter-wrapper .bp3-icon-double-chevron-up::before{content:\"\ue702\"}.jupyter-wrapper .bp3-icon-doughnut-chart::before{content:\"\ue6ce\"}.jupyter-wrapper .bp3-icon-download::before{content:\"\ue62f\"}.jupyter-wrapper .bp3-icon-drag-handle-horizontal::before{content:\"\ue716\"}.jupyter-wrapper .bp3-icon-drag-handle-vertical::before{content:\"\ue715\"}.jupyter-wrapper .bp3-icon-draw::before{content:\"\ue66b\"}.jupyter-wrapper .bp3-icon-drive-time::before{content:\"\ue615\"}.jupyter-wrapper .bp3-icon-duplicate::before{content:\"\ue69c\"}.jupyter-wrapper .bp3-icon-edit::before{content:\"\u270e\"}.jupyter-wrapper .bp3-icon-eject::before{content:\"\u23cf\"}.jupyter-wrapper .bp3-icon-endorsed::before{content:\"\ue75f\"}.jupyter-wrapper .bp3-icon-envelope::before{content:\"\u2709\"}.jupyter-wrapper .bp3-icon-equals::before{content:\"\ue7d9\"}.jupyter-wrapper .bp3-icon-eraser::before{content:\"\ue773\"}.jupyter-wrapper .bp3-icon-error::before{content:\"\ue648\"}.jupyter-wrapper .bp3-icon-euro::before{content:\"\u20ac\"}.jupyter-wrapper .bp3-icon-exchange::before{content:\"\ue636\"}.jupyter-wrapper .bp3-icon-exclude-row::before{content:\"\ue6ea\"}.jupyter-wrapper .bp3-icon-expand-all::before{content:\"\ue764\"}.jupyter-wrapper .bp3-icon-export::before{content:\"\ue633\"}.jupyter-wrapper .bp3-icon-eye-off::before{content:\"\ue6cc\"}.jupyter-wrapper .bp3-icon-eye-on::before{content:\"\ue75a\"}.jupyter-wrapper .bp3-icon-eye-open::before{content:\"\ue66f\"}.jupyter-wrapper .bp3-icon-fast-backward::before{content:\"\ue6a8\"}.jupyter-wrapper .bp3-icon-fast-forward::before{content:\"\ue6ac\"}.jupyter-wrapper .bp3-icon-feed::before{content:\"\ue656\"}.jupyter-wrapper .bp3-icon-feed-subscribed::before{content:\"\ue78f\"}.jupyter-wrapper .bp3-icon-film::before{content:\"\ue6a1\"}.jupyter-wrapper .bp3-icon-filter::before{content:\"\ue638\"}.jupyter-wrapper .bp3-icon-filter-keep::before{content:\"\ue78c\"}.jupyter-wrapper .bp3-icon-filter-list::before{content:\"\ue6ee\"}.jupyter-wrapper .bp3-icon-filter-open::before{content:\"\ue7d7\"}.jupyter-wrapper .bp3-icon-filter-remove::before{content:\"\ue78d\"}.jupyter-wrapper .bp3-icon-flag::before{content:\"\u2691\"}.jupyter-wrapper .bp3-icon-flame::before{content:\"\ue7a9\"}.jupyter-wrapper .bp3-icon-flash::before{content:\"\ue6b3\"}.jupyter-wrapper .bp3-icon-floppy-disk::before{content:\"\ue6b7\"}.jupyter-wrapper .bp3-icon-flow-branch::before{content:\"\ue7c1\"}.jupyter-wrapper .bp3-icon-flow-end::before{content:\"\ue7c4\"}.jupyter-wrapper .bp3-icon-flow-linear::before{content:\"\ue7c0\"}.jupyter-wrapper .bp3-icon-flow-review::before{content:\"\ue7c2\"}.jupyter-wrapper .bp3-icon-flow-review-branch::before{content:\"\ue7c3\"}.jupyter-wrapper .bp3-icon-flows::before{content:\"\ue659\"}.jupyter-wrapper .bp3-icon-folder-close::before{content:\"\ue652\"}.jupyter-wrapper .bp3-icon-folder-new::before{content:\"\ue7b0\"}.jupyter-wrapper .bp3-icon-folder-open::before{content:\"\ue651\"}.jupyter-wrapper .bp3-icon-folder-shared::before{content:\"\ue653\"}.jupyter-wrapper .bp3-icon-folder-shared-open::before{content:\"\ue670\"}.jupyter-wrapper .bp3-icon-follower::before{content:\"\ue760\"}.jupyter-wrapper .bp3-icon-following::before{content:\"\ue761\"}.jupyter-wrapper .bp3-icon-font::before{content:\"\ue6b4\"}.jupyter-wrapper .bp3-icon-fork::before{content:\"\ue63a\"}.jupyter-wrapper .bp3-icon-form::before{content:\"\ue795\"}.jupyter-wrapper .bp3-icon-full-circle::before{content:\"\ue685\"}.jupyter-wrapper .bp3-icon-full-stacked-chart::before{content:\"\ue75e\"}.jupyter-wrapper .bp3-icon-fullscreen::before{content:\"\ue699\"}.jupyter-wrapper .bp3-icon-function::before{content:\"\ue6e5\"}.jupyter-wrapper .bp3-icon-gantt-chart::before{content:\"\ue6f4\"}.jupyter-wrapper .bp3-icon-geolocation::before{content:\"\ue640\"}.jupyter-wrapper .bp3-icon-geosearch::before{content:\"\ue613\"}.jupyter-wrapper .bp3-icon-git-branch::before{content:\"\ue72a\"}.jupyter-wrapper .bp3-icon-git-commit::before{content:\"\ue72b\"}.jupyter-wrapper .bp3-icon-git-merge::before{content:\"\ue729\"}.jupyter-wrapper .bp3-icon-git-new-branch::before{content:\"\ue749\"}.jupyter-wrapper .bp3-icon-git-pull::before{content:\"\ue728\"}.jupyter-wrapper .bp3-icon-git-push::before{content:\"\ue72c\"}.jupyter-wrapper .bp3-icon-git-repo::before{content:\"\ue748\"}.jupyter-wrapper .bp3-icon-glass::before{content:\"\ue6b1\"}.jupyter-wrapper .bp3-icon-globe::before{content:\"\ue666\"}.jupyter-wrapper .bp3-icon-globe-network::before{content:\"\ue7b5\"}.jupyter-wrapper .bp3-icon-graph::before{content:\"\ue673\"}.jupyter-wrapper .bp3-icon-graph-remove::before{content:\"\ue609\"}.jupyter-wrapper .bp3-icon-greater-than::before{content:\"\ue7e1\"}.jupyter-wrapper .bp3-icon-greater-than-or-equal-to::before{content:\"\ue7e2\"}.jupyter-wrapper .bp3-icon-grid::before{content:\"\ue6d0\"}.jupyter-wrapper .bp3-icon-grid-view::before{content:\"\ue6e4\"}.jupyter-wrapper .bp3-icon-group-objects::before{content:\"\ue60a\"}.jupyter-wrapper .bp3-icon-grouped-bar-chart::before{content:\"\ue75d\"}.jupyter-wrapper .bp3-icon-hand::before{content:\"\ue6de\"}.jupyter-wrapper .bp3-icon-hand-down::before{content:\"\ue6bb\"}.jupyter-wrapper .bp3-icon-hand-left::before{content:\"\ue6bc\"}.jupyter-wrapper .bp3-icon-hand-right::before{content:\"\ue6b9\"}.jupyter-wrapper .bp3-icon-hand-up::before{content:\"\ue6ba\"}.jupyter-wrapper .bp3-icon-header::before{content:\"\ue6b5\"}.jupyter-wrapper .bp3-icon-header-one::before{content:\"\ue793\"}.jupyter-wrapper .bp3-icon-header-two::before{content:\"\ue794\"}.jupyter-wrapper .bp3-icon-headset::before{content:\"\ue6dc\"}.jupyter-wrapper .bp3-icon-heart::before{content:\"\u2665\"}.jupyter-wrapper .bp3-icon-heart-broken::before{content:\"\ue7a2\"}.jupyter-wrapper .bp3-icon-heat-grid::before{content:\"\ue6f3\"}.jupyter-wrapper .bp3-icon-heatmap::before{content:\"\ue614\"}.jupyter-wrapper .bp3-icon-help::before{content:\"?\"}.jupyter-wrapper .bp3-icon-helper-management::before{content:\"\ue66d\"}.jupyter-wrapper .bp3-icon-highlight::before{content:\"\ue6ed\"}.jupyter-wrapper .bp3-icon-history::before{content:\"\ue64a\"}.jupyter-wrapper .bp3-icon-home::before{content:\"\u2302\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart::before{content:\"\ue70c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-asc::before{content:\"\ue75c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-desc::before{content:\"\ue71d\"}.jupyter-wrapper .bp3-icon-horizontal-distribution::before{content:\"\ue720\"}.jupyter-wrapper .bp3-icon-id-number::before{content:\"\ue771\"}.jupyter-wrapper .bp3-icon-image-rotate-left::before{content:\"\ue73a\"}.jupyter-wrapper .bp3-icon-image-rotate-right::before{content:\"\ue73b\"}.jupyter-wrapper .bp3-icon-import::before{content:\"\ue632\"}.jupyter-wrapper .bp3-icon-inbox::before{content:\"\ue629\"}.jupyter-wrapper .bp3-icon-inbox-filtered::before{content:\"\ue7d1\"}.jupyter-wrapper .bp3-icon-inbox-geo::before{content:\"\ue7d2\"}.jupyter-wrapper .bp3-icon-inbox-search::before{content:\"\ue7d3\"}.jupyter-wrapper .bp3-icon-inbox-update::before{content:\"\ue7d4\"}.jupyter-wrapper .bp3-icon-info-sign::before{content:\"\u2139\"}.jupyter-wrapper .bp3-icon-inheritance::before{content:\"\ue7d5\"}.jupyter-wrapper .bp3-icon-inner-join::before{content:\"\ue7a3\"}.jupyter-wrapper .bp3-icon-insert::before{content:\"\ue66c\"}.jupyter-wrapper .bp3-icon-intersection::before{content:\"\ue765\"}.jupyter-wrapper .bp3-icon-ip-address::before{content:\"\ue772\"}.jupyter-wrapper .bp3-icon-issue::before{content:\"\ue774\"}.jupyter-wrapper .bp3-icon-issue-closed::before{content:\"\ue776\"}.jupyter-wrapper .bp3-icon-issue-new::before{content:\"\ue775\"}.jupyter-wrapper .bp3-icon-italic::before{content:\"\ue607\"}.jupyter-wrapper .bp3-icon-join-table::before{content:\"\ue738\"}.jupyter-wrapper .bp3-icon-key::before{content:\"\ue78e\"}.jupyter-wrapper .bp3-icon-key-backspace::before{content:\"\ue707\"}.jupyter-wrapper .bp3-icon-key-command::before{content:\"\ue705\"}.jupyter-wrapper .bp3-icon-key-control::before{content:\"\ue704\"}.jupyter-wrapper .bp3-icon-key-delete::before{content:\"\ue708\"}.jupyter-wrapper .bp3-icon-key-enter::before{content:\"\ue70a\"}.jupyter-wrapper .bp3-icon-key-escape::before{content:\"\ue709\"}.jupyter-wrapper .bp3-icon-key-option::before{content:\"\ue742\"}.jupyter-wrapper .bp3-icon-key-shift::before{content:\"\ue706\"}.jupyter-wrapper .bp3-icon-key-tab::before{content:\"\ue757\"}.jupyter-wrapper .bp3-icon-known-vehicle::before{content:\"\ue73c\"}.jupyter-wrapper .bp3-icon-label::before{content:\"\ue665\"}.jupyter-wrapper .bp3-icon-layer::before{content:\"\ue6cf\"}.jupyter-wrapper .bp3-icon-layers::before{content:\"\ue618\"}.jupyter-wrapper .bp3-icon-layout::before{content:\"\ue60c\"}.jupyter-wrapper .bp3-icon-layout-auto::before{content:\"\ue60d\"}.jupyter-wrapper .bp3-icon-layout-balloon::before{content:\"\ue6d3\"}.jupyter-wrapper .bp3-icon-layout-circle::before{content:\"\ue60e\"}.jupyter-wrapper .bp3-icon-layout-grid::before{content:\"\ue610\"}.jupyter-wrapper .bp3-icon-layout-group-by::before{content:\"\ue611\"}.jupyter-wrapper .bp3-icon-layout-hierarchy::before{content:\"\ue60f\"}.jupyter-wrapper .bp3-icon-layout-linear::before{content:\"\ue6c3\"}.jupyter-wrapper .bp3-icon-layout-skew-grid::before{content:\"\ue612\"}.jupyter-wrapper .bp3-icon-layout-sorted-clusters::before{content:\"\ue6d4\"}.jupyter-wrapper .bp3-icon-learning::before{content:\"\ue904\"}.jupyter-wrapper .bp3-icon-left-join::before{content:\"\ue7a4\"}.jupyter-wrapper .bp3-icon-less-than::before{content:\"\ue7e3\"}.jupyter-wrapper .bp3-icon-less-than-or-equal-to::before{content:\"\ue7e4\"}.jupyter-wrapper .bp3-icon-lifesaver::before{content:\"\ue7c7\"}.jupyter-wrapper .bp3-icon-lightbulb::before{content:\"\ue6b0\"}.jupyter-wrapper .bp3-icon-link::before{content:\"\ue62d\"}.jupyter-wrapper .bp3-icon-list::before{content:\"\u2630\"}.jupyter-wrapper .bp3-icon-list-columns::before{content:\"\ue7b9\"}.jupyter-wrapper .bp3-icon-list-detail-view::before{content:\"\ue743\"}.jupyter-wrapper .bp3-icon-locate::before{content:\"\ue619\"}.jupyter-wrapper .bp3-icon-lock::before{content:\"\ue625\"}.jupyter-wrapper .bp3-icon-log-in::before{content:\"\ue69a\"}.jupyter-wrapper .bp3-icon-log-out::before{content:\"\ue64c\"}.jupyter-wrapper .bp3-icon-manual::before{content:\"\ue6f6\"}.jupyter-wrapper .bp3-icon-manually-entered-data::before{content:\"\ue74a\"}.jupyter-wrapper .bp3-icon-map::before{content:\"\ue662\"}.jupyter-wrapper .bp3-icon-map-create::before{content:\"\ue741\"}.jupyter-wrapper .bp3-icon-map-marker::before{content:\"\ue67d\"}.jupyter-wrapper .bp3-icon-maximize::before{content:\"\ue635\"}.jupyter-wrapper .bp3-icon-media::before{content:\"\ue62c\"}.jupyter-wrapper .bp3-icon-menu::before{content:\"\ue762\"}.jupyter-wrapper .bp3-icon-menu-closed::before{content:\"\ue655\"}.jupyter-wrapper .bp3-icon-menu-open::before{content:\"\ue654\"}.jupyter-wrapper .bp3-icon-merge-columns::before{content:\"\ue74f\"}.jupyter-wrapper .bp3-icon-merge-links::before{content:\"\ue60b\"}.jupyter-wrapper .bp3-icon-minimize::before{content:\"\ue634\"}.jupyter-wrapper .bp3-icon-minus::before{content:\"\u2212\"}.jupyter-wrapper .bp3-icon-mobile-phone::before{content:\"\ue717\"}.jupyter-wrapper .bp3-icon-mobile-video::before{content:\"\ue69f\"}.jupyter-wrapper .bp3-icon-moon::before{content:\"\ue754\"}.jupyter-wrapper .bp3-icon-more::before{content:\"\ue62a\"}.jupyter-wrapper .bp3-icon-mountain::before{content:\"\ue7b1\"}.jupyter-wrapper .bp3-icon-move::before{content:\"\ue693\"}.jupyter-wrapper .bp3-icon-mugshot::before{content:\"\ue6db\"}.jupyter-wrapper .bp3-icon-multi-select::before{content:\"\ue680\"}.jupyter-wrapper .bp3-icon-music::before{content:\"\ue6a6\"}.jupyter-wrapper .bp3-icon-new-drawing::before{content:\"\ue905\"}.jupyter-wrapper .bp3-icon-new-grid-item::before{content:\"\ue747\"}.jupyter-wrapper .bp3-icon-new-layer::before{content:\"\ue902\"}.jupyter-wrapper .bp3-icon-new-layers::before{content:\"\ue903\"}.jupyter-wrapper .bp3-icon-new-link::before{content:\"\ue65c\"}.jupyter-wrapper .bp3-icon-new-object::before{content:\"\ue65d\"}.jupyter-wrapper .bp3-icon-new-person::before{content:\"\ue6e9\"}.jupyter-wrapper .bp3-icon-new-prescription::before{content:\"\ue78b\"}.jupyter-wrapper .bp3-icon-new-text-box::before{content:\"\ue65b\"}.jupyter-wrapper .bp3-icon-ninja::before{content:\"\ue675\"}.jupyter-wrapper .bp3-icon-not-equal-to::before{content:\"\ue7e0\"}.jupyter-wrapper .bp3-icon-notifications::before{content:\"\ue624\"}.jupyter-wrapper .bp3-icon-notifications-updated::before{content:\"\ue7b8\"}.jupyter-wrapper .bp3-icon-numbered-list::before{content:\"\ue746\"}.jupyter-wrapper .bp3-icon-numerical::before{content:\"\ue756\"}.jupyter-wrapper .bp3-icon-office::before{content:\"\ue69b\"}.jupyter-wrapper .bp3-icon-offline::before{content:\"\ue67a\"}.jupyter-wrapper .bp3-icon-oil-field::before{content:\"\ue73f\"}.jupyter-wrapper .bp3-icon-one-column::before{content:\"\ue658\"}.jupyter-wrapper .bp3-icon-outdated::before{content:\"\ue7a8\"}.jupyter-wrapper .bp3-icon-page-layout::before{content:\"\ue660\"}.jupyter-wrapper .bp3-icon-panel-stats::before{content:\"\ue777\"}.jupyter-wrapper .bp3-icon-panel-table::before{content:\"\ue778\"}.jupyter-wrapper .bp3-icon-paperclip::before{content:\"\ue664\"}.jupyter-wrapper .bp3-icon-paragraph::before{content:\"\ue76c\"}.jupyter-wrapper .bp3-icon-path::before{content:\"\ue753\"}.jupyter-wrapper .bp3-icon-path-search::before{content:\"\ue65e\"}.jupyter-wrapper .bp3-icon-pause::before{content:\"\ue6a9\"}.jupyter-wrapper .bp3-icon-people::before{content:\"\ue63d\"}.jupyter-wrapper .bp3-icon-percentage::before{content:\"\ue76a\"}.jupyter-wrapper .bp3-icon-person::before{content:\"\ue63c\"}.jupyter-wrapper .bp3-icon-phone::before{content:\"\u260e\"}.jupyter-wrapper .bp3-icon-pie-chart::before{content:\"\ue684\"}.jupyter-wrapper .bp3-icon-pin::before{content:\"\ue646\"}.jupyter-wrapper .bp3-icon-pivot::before{content:\"\ue6f1\"}.jupyter-wrapper .bp3-icon-pivot-table::before{content:\"\ue6eb\"}.jupyter-wrapper .bp3-icon-play::before{content:\"\ue6ab\"}.jupyter-wrapper .bp3-icon-plus::before{content:\"+\"}.jupyter-wrapper .bp3-icon-polygon-filter::before{content:\"\ue6d1\"}.jupyter-wrapper .bp3-icon-power::before{content:\"\ue6d9\"}.jupyter-wrapper .bp3-icon-predictive-analysis::before{content:\"\ue617\"}.jupyter-wrapper .bp3-icon-prescription::before{content:\"\ue78a\"}.jupyter-wrapper .bp3-icon-presentation::before{content:\"\ue687\"}.jupyter-wrapper .bp3-icon-print::before{content:\"\u2399\"}.jupyter-wrapper .bp3-icon-projects::before{content:\"\ue622\"}.jupyter-wrapper .bp3-icon-properties::before{content:\"\ue631\"}.jupyter-wrapper .bp3-icon-property::before{content:\"\ue65a\"}.jupyter-wrapper .bp3-icon-publish-function::before{content:\"\ue752\"}.jupyter-wrapper .bp3-icon-pulse::before{content:\"\ue6e8\"}.jupyter-wrapper .bp3-icon-random::before{content:\"\ue698\"}.jupyter-wrapper .bp3-icon-record::before{content:\"\ue6ae\"}.jupyter-wrapper .bp3-icon-redo::before{content:\"\ue6c4\"}.jupyter-wrapper .bp3-icon-refresh::before{content:\"\ue643\"}.jupyter-wrapper .bp3-icon-regression-chart::before{content:\"\ue758\"}.jupyter-wrapper .bp3-icon-remove::before{content:\"\ue63f\"}.jupyter-wrapper .bp3-icon-remove-column::before{content:\"\ue755\"}.jupyter-wrapper .bp3-icon-remove-column-left::before{content:\"\ue6fd\"}.jupyter-wrapper .bp3-icon-remove-column-right::before{content:\"\ue6fe\"}.jupyter-wrapper .bp3-icon-remove-row-bottom::before{content:\"\ue6fc\"}.jupyter-wrapper .bp3-icon-remove-row-top::before{content:\"\ue6fb\"}.jupyter-wrapper .bp3-icon-repeat::before{content:\"\ue692\"}.jupyter-wrapper .bp3-icon-reset::before{content:\"\ue7d6\"}.jupyter-wrapper .bp3-icon-resolve::before{content:\"\ue672\"}.jupyter-wrapper .bp3-icon-rig::before{content:\"\ue740\"}.jupyter-wrapper .bp3-icon-right-join::before{content:\"\ue7a5\"}.jupyter-wrapper .bp3-icon-ring::before{content:\"\ue6f2\"}.jupyter-wrapper .bp3-icon-rotate-document::before{content:\"\ue6e1\"}.jupyter-wrapper .bp3-icon-rotate-page::before{content:\"\ue6e2\"}.jupyter-wrapper .bp3-icon-satellite::before{content:\"\ue76b\"}.jupyter-wrapper .bp3-icon-saved::before{content:\"\ue6b6\"}.jupyter-wrapper .bp3-icon-scatter-plot::before{content:\"\ue73e\"}.jupyter-wrapper .bp3-icon-search::before{content:\"\ue64b\"}.jupyter-wrapper .bp3-icon-search-around::before{content:\"\ue608\"}.jupyter-wrapper .bp3-icon-search-template::before{content:\"\ue628\"}.jupyter-wrapper .bp3-icon-search-text::before{content:\"\ue663\"}.jupyter-wrapper .bp3-icon-segmented-control::before{content:\"\ue6ec\"}.jupyter-wrapper .bp3-icon-select::before{content:\"\ue616\"}.jupyter-wrapper .bp3-icon-selection::before{content:\"\u29bf\"}.jupyter-wrapper .bp3-icon-send-to::before{content:\"\ue66e\"}.jupyter-wrapper .bp3-icon-send-to-graph::before{content:\"\ue736\"}.jupyter-wrapper .bp3-icon-send-to-map::before{content:\"\ue737\"}.jupyter-wrapper .bp3-icon-series-add::before{content:\"\ue796\"}.jupyter-wrapper .bp3-icon-series-configuration::before{content:\"\ue79a\"}.jupyter-wrapper .bp3-icon-series-derived::before{content:\"\ue799\"}.jupyter-wrapper .bp3-icon-series-filtered::before{content:\"\ue798\"}.jupyter-wrapper .bp3-icon-series-search::before{content:\"\ue797\"}.jupyter-wrapper .bp3-icon-settings::before{content:\"\ue6a2\"}.jupyter-wrapper .bp3-icon-share::before{content:\"\ue62e\"}.jupyter-wrapper .bp3-icon-shield::before{content:\"\ue7b2\"}.jupyter-wrapper .bp3-icon-shop::before{content:\"\ue6c2\"}.jupyter-wrapper .bp3-icon-shopping-cart::before{content:\"\ue6c1\"}.jupyter-wrapper .bp3-icon-signal-search::before{content:\"\ue909\"}.jupyter-wrapper .bp3-icon-sim-card::before{content:\"\ue718\"}.jupyter-wrapper .bp3-icon-slash::before{content:\"\ue769\"}.jupyter-wrapper .bp3-icon-small-cross::before{content:\"\ue6d7\"}.jupyter-wrapper .bp3-icon-small-minus::before{content:\"\ue70e\"}.jupyter-wrapper .bp3-icon-small-plus::before{content:\"\ue70d\"}.jupyter-wrapper .bp3-icon-small-tick::before{content:\"\ue6d8\"}.jupyter-wrapper .bp3-icon-snowflake::before{content:\"\ue7b6\"}.jupyter-wrapper .bp3-icon-social-media::before{content:\"\ue671\"}.jupyter-wrapper .bp3-icon-sort::before{content:\"\ue64f\"}.jupyter-wrapper .bp3-icon-sort-alphabetical::before{content:\"\ue64d\"}.jupyter-wrapper .bp3-icon-sort-alphabetical-desc::before{content:\"\ue6c8\"}.jupyter-wrapper .bp3-icon-sort-asc::before{content:\"\ue6d5\"}.jupyter-wrapper .bp3-icon-sort-desc::before{content:\"\ue6d6\"}.jupyter-wrapper .bp3-icon-sort-numerical::before{content:\"\ue64e\"}.jupyter-wrapper .bp3-icon-sort-numerical-desc::before{content:\"\ue6c9\"}.jupyter-wrapper .bp3-icon-split-columns::before{content:\"\ue750\"}.jupyter-wrapper .bp3-icon-square::before{content:\"\ue686\"}.jupyter-wrapper .bp3-icon-stacked-chart::before{content:\"\ue6e7\"}.jupyter-wrapper .bp3-icon-star::before{content:\"\u2605\"}.jupyter-wrapper .bp3-icon-star-empty::before{content:\"\u2606\"}.jupyter-wrapper .bp3-icon-step-backward::before{content:\"\ue6a7\"}.jupyter-wrapper .bp3-icon-step-chart::before{content:\"\ue70f\"}.jupyter-wrapper .bp3-icon-step-forward::before{content:\"\ue6ad\"}.jupyter-wrapper .bp3-icon-stop::before{content:\"\ue6aa\"}.jupyter-wrapper .bp3-icon-stopwatch::before{content:\"\ue901\"}.jupyter-wrapper .bp3-icon-strikethrough::before{content:\"\ue7a6\"}.jupyter-wrapper .bp3-icon-style::before{content:\"\ue601\"}.jupyter-wrapper .bp3-icon-swap-horizontal::before{content:\"\ue745\"}.jupyter-wrapper .bp3-icon-swap-vertical::before{content:\"\ue744\"}.jupyter-wrapper .bp3-icon-symbol-circle::before{content:\"\ue72e\"}.jupyter-wrapper .bp3-icon-symbol-cross::before{content:\"\ue731\"}.jupyter-wrapper .bp3-icon-symbol-diamond::before{content:\"\ue730\"}.jupyter-wrapper .bp3-icon-symbol-square::before{content:\"\ue72f\"}.jupyter-wrapper .bp3-icon-symbol-triangle-down::before{content:\"\ue733\"}.jupyter-wrapper .bp3-icon-symbol-triangle-up::before{content:\"\ue732\"}.jupyter-wrapper .bp3-icon-tag::before{content:\"\ue61c\"}.jupyter-wrapper .bp3-icon-take-action::before{content:\"\ue6ca\"}.jupyter-wrapper .bp3-icon-taxi::before{content:\"\ue79e\"}.jupyter-wrapper .bp3-icon-text-highlight::before{content:\"\ue6dd\"}.jupyter-wrapper .bp3-icon-th::before{content:\"\ue667\"}.jupyter-wrapper .bp3-icon-th-derived::before{content:\"\ue669\"}.jupyter-wrapper .bp3-icon-th-disconnect::before{content:\"\ue7d8\"}.jupyter-wrapper .bp3-icon-th-filtered::before{content:\"\ue7c6\"}.jupyter-wrapper .bp3-icon-th-list::before{content:\"\ue668\"}.jupyter-wrapper .bp3-icon-thumbs-down::before{content:\"\ue6be\"}.jupyter-wrapper .bp3-icon-thumbs-up::before{content:\"\ue6bd\"}.jupyter-wrapper .bp3-icon-tick::before{content:\"\u2713\"}.jupyter-wrapper .bp3-icon-tick-circle::before{content:\"\ue779\"}.jupyter-wrapper .bp3-icon-time::before{content:\"\u23f2\"}.jupyter-wrapper .bp3-icon-timeline-area-chart::before{content:\"\ue6cd\"}.jupyter-wrapper .bp3-icon-timeline-bar-chart::before{content:\"\ue620\"}.jupyter-wrapper .bp3-icon-timeline-events::before{content:\"\ue61e\"}.jupyter-wrapper .bp3-icon-timeline-line-chart::before{content:\"\ue61f\"}.jupyter-wrapper .bp3-icon-tint::before{content:\"\ue6b2\"}.jupyter-wrapper .bp3-icon-torch::before{content:\"\ue677\"}.jupyter-wrapper .bp3-icon-tractor::before{content:\"\ue90c\"}.jupyter-wrapper .bp3-icon-train::before{content:\"\ue79f\"}.jupyter-wrapper .bp3-icon-translate::before{content:\"\ue759\"}.jupyter-wrapper .bp3-icon-trash::before{content:\"\ue63b\"}.jupyter-wrapper .bp3-icon-tree::before{content:\"\ue7b7\"}.jupyter-wrapper .bp3-icon-trending-down::before{content:\"\ue71a\"}.jupyter-wrapper .bp3-icon-trending-up::before{content:\"\ue719\"}.jupyter-wrapper .bp3-icon-truck::before{content:\"\ue90b\"}.jupyter-wrapper .bp3-icon-two-columns::before{content:\"\ue657\"}.jupyter-wrapper .bp3-icon-unarchive::before{content:\"\ue906\"}.jupyter-wrapper .bp3-icon-underline::before{content:\"\u2381\"}.jupyter-wrapper .bp3-icon-undo::before{content:\"\u238c\"}.jupyter-wrapper .bp3-icon-ungroup-objects::before{content:\"\ue688\"}.jupyter-wrapper .bp3-icon-unknown-vehicle::before{content:\"\ue73d\"}.jupyter-wrapper .bp3-icon-unlock::before{content:\"\ue626\"}.jupyter-wrapper .bp3-icon-unpin::before{content:\"\ue650\"}.jupyter-wrapper .bp3-icon-unresolve::before{content:\"\ue679\"}.jupyter-wrapper .bp3-icon-updated::before{content:\"\ue7a7\"}.jupyter-wrapper .bp3-icon-upload::before{content:\"\ue68f\"}.jupyter-wrapper .bp3-icon-user::before{content:\"\ue627\"}.jupyter-wrapper .bp3-icon-variable::before{content:\"\ue6f5\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-asc::before{content:\"\ue75b\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-desc::before{content:\"\ue71c\"}.jupyter-wrapper .bp3-icon-vertical-distribution::before{content:\"\ue721\"}.jupyter-wrapper .bp3-icon-video::before{content:\"\ue6a0\"}.jupyter-wrapper .bp3-icon-volume-down::before{content:\"\ue6a4\"}.jupyter-wrapper .bp3-icon-volume-off::before{content:\"\ue6a3\"}.jupyter-wrapper .bp3-icon-volume-up::before{content:\"\ue6a5\"}.jupyter-wrapper .bp3-icon-walk::before{content:\"\ue79d\"}.jupyter-wrapper .bp3-icon-warning-sign::before{content:\"\ue647\"}.jupyter-wrapper .bp3-icon-waterfall-chart::before{content:\"\ue6e6\"}.jupyter-wrapper .bp3-icon-widget::before{content:\"\ue678\"}.jupyter-wrapper .bp3-icon-widget-button::before{content:\"\ue790\"}.jupyter-wrapper .bp3-icon-widget-footer::before{content:\"\ue792\"}.jupyter-wrapper .bp3-icon-widget-header::before{content:\"\ue791\"}.jupyter-wrapper .bp3-icon-wrench::before{content:\"\ue734\"}.jupyter-wrapper .bp3-icon-zoom-in::before{content:\"\ue641\"}.jupyter-wrapper .bp3-icon-zoom-out::before{content:\"\ue642\"}.jupyter-wrapper .bp3-icon-zoom-to-fit::before{content:\"\ue67b\"}.jupyter-wrapper .bp3-submenu>.bp3-popover-wrapper{display:block}.jupyter-wrapper .bp3-submenu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-submenu.bp3-popover{-webkit-box-shadow:none;box-shadow:none;padding:0 5px}.jupyter-wrapper .bp3-submenu.bp3-popover>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover>.bp3-popover-content,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-menu{margin:0;border-radius:3px;background:#fff;min-width:180px;padding:5px;list-style:none;text-align:left;color:#182026}.jupyter-wrapper .bp3-menu-divider{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-divider{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;border-radius:2px;padding:5px 7px;text-decoration:none;line-height:20px;color:inherit;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-menu-item>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>*{margin-right:7px}.jupyter-wrapper .bp3-menu-item:empty::before,.jupyter-wrapper .bp3-menu-item>:last-child{margin-right:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{word-break:break-word}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(167,182,194,.3);cursor:pointer;text-decoration:none}.jupyter-wrapper .bp3-menu-item.bp3-disabled{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(138,155,168,.15);color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{background-color:inherit;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-right:7px}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>.bp3-icon{margin-top:2px;color:#5c7080}.jupyter-wrapper .bp3-menu-item .bp3-menu-item-label{color:#5c7080}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-menu-item:active{background-color:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-menu-item.bp3-disabled{outline:none !important;background-color:inherit !important;cursor:not-allowed !important;color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-large .bp3-menu-item{padding:9px 7px;line-height:22px;font-size:16px}.jupyter-wrapper .bp3-large .bp3-menu-item .bp3-icon{margin-top:3px}.jupyter-wrapper .bp3-large .bp3-menu-item::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-top:1px;margin-right:10px}.jupyter-wrapper button.bp3-menu-item{border:none;background:none;width:100%;text-align:left}.jupyter-wrapper .bp3-menu-header{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15);cursor:default;padding-left:2px}.jupyter-wrapper .bp3-dark .bp3-menu-header{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-header:first-of-type{border-top:none}.jupyter-wrapper .bp3-menu-header>h6{color:#182026;font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;margin:0;padding:10px 7px 0 1px;line-height:17px}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-large .bp3-menu-header>h6{padding-top:15px;padding-bottom:5px;font-size:18px}.jupyter-wrapper .bp3-large .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-dark .bp3-menu{background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item .bp3-menu-item-label{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item:active{background-color:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-divider,.jupyter-wrapper .bp3-dark .bp3-menu-header{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-label .bp3-menu{margin-top:5px}.jupyter-wrapper .bp3-navbar{position:relative;z-index:10;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background-color:#fff;width:100%;height:50px;padding:0 15px}.jupyter-wrapper .bp3-navbar.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-navbar{background-color:#394b59}.jupyter-wrapper .bp3-navbar.bp3-dark{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-navbar{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-navbar.bp3-fixed-top{position:fixed;top:0;right:0;left:0}.jupyter-wrapper .bp3-navbar-heading{margin-right:15px;font-size:16px}.jupyter-wrapper .bp3-navbar-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:50px}.jupyter-wrapper .bp3-navbar-group.bp3-align-left{float:left}.jupyter-wrapper .bp3-navbar-group.bp3-align-right{float:right}.jupyter-wrapper .bp3-navbar-divider{margin:0 10px;border-left:1px solid rgba(16,22,26,.15);height:20px}.jupyter-wrapper .bp3-dark .bp3-navbar-divider{border-left-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-non-ideal-state{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%;text-align:center}.jupyter-wrapper .bp3-non-ideal-state>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-non-ideal-state>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-non-ideal-state::before,.jupyter-wrapper .bp3-non-ideal-state>*{margin-bottom:20px}.jupyter-wrapper .bp3-non-ideal-state:empty::before,.jupyter-wrapper .bp3-non-ideal-state>:last-child{margin-bottom:0}.jupyter-wrapper .bp3-non-ideal-state>*{max-width:400px}.jupyter-wrapper .bp3-non-ideal-state-visual{color:rgba(92,112,128,.6);font-size:60px}.jupyter-wrapper .bp3-dark .bp3-non-ideal-state-visual{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-overflow-list{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;min-width:0}.jupyter-wrapper .bp3-overflow-list-spacer{-ms-flex-negative:1;flex-shrink:1;width:1px}.jupyter-wrapper body.bp3-overlay-open{overflow:hidden}.jupyter-wrapper .bp3-overlay{position:static;top:0;right:0;bottom:0;left:0;z-index:20}.jupyter-wrapper .bp3-overlay:not(.bp3-overlay-open){pointer-events:none}.jupyter-wrapper .bp3-overlay.bp3-overlay-container{position:fixed;overflow:hidden}.jupyter-wrapper .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container{position:fixed;overflow:auto}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-inline{display:inline;overflow:visible}.jupyter-wrapper .bp3-overlay-content{position:fixed;z-index:20}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-content,.jupyter-wrapper .bp3-overlay-scroll-container .bp3-overlay-content{position:absolute}.jupyter-wrapper .bp3-overlay-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;opacity:1;z-index:20;background-color:rgba(16,22,26,.7);overflow:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear{opacity:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter-active,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit{opacity:1}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop:focus{outline:none}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-backdrop{position:absolute}.jupyter-wrapper .bp3-panel-stack{position:relative;overflow:hidden}.jupyter-wrapper .bp3-panel-stack-header{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-negative:0;flex-shrink:0;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:1;-webkit-box-shadow:0 1px rgba(16,22,26,.15);box-shadow:0 1px rgba(16,22,26,.15);height:30px}.jupyter-wrapper .bp3-dark .bp3-panel-stack-header{-webkit-box-shadow:0 1px rgba(255,255,255,.15);box-shadow:0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-panel-stack-header>span{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1;flex:1;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-panel-stack-header .bp3-heading{margin:0 5px}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back{margin-left:5px;padding-left:0;white-space:nowrap}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back .bp3-icon{margin:0 2px}.jupyter-wrapper .bp3-panel-stack-view{position:absolute;top:0;right:0;bottom:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-right:-1px;border-right:1px solid rgba(16,22,26,.15);background-color:#fff;overflow-y:auto}.jupyter-wrapper .bp3-dark .bp3-panel-stack-view{background-color:#30404d}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit-active{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1);display:inline-block;z-index:20;border-radius:3px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow{position:absolute;width:30px;height:30px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{margin:5px;width:20px;height:20px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover{margin-top:-17px;margin-bottom:17px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{bottom:-11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover{margin-left:17px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{left:-11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover{margin-top:17px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{top:-11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover{margin-right:17px;margin-left:-17px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{right:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-popover>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-popover>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{top:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{right:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{left:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{bottom:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-popover .bp3-popover-content{background:#fff;color:inherit}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-fill{fill:#fff}.jupyter-wrapper .bp3-popover-enter>.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover .bp3-popover-content{position:relative;border-radius:3px}.jupyter-wrapper .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{max-width:350px;padding:20px}.jupyter-wrapper .bp3-popover-target+.bp3-overlay .bp3-popover.bp3-popover-content-sizing{width:350px}.jupyter-wrapper .bp3-popover.bp3-minimal{margin:0 !important}.jupyter-wrapper .bp3-popover.bp3-minimal .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-content{background:#30404d;color:inherit}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-fill{fill:#30404d}.jupyter-wrapper .bp3-popover-arrow::before{display:block;position:absolute;-webkit-transform:rotate(45deg);transform:rotate(45deg);border-radius:2px;content:\"\"}.jupyter-wrapper .bp3-tether-pinned .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover-backdrop{background:rgba(255,255,255,0)}.jupyter-wrapper .bp3-transition-container{opacity:1;display:-webkit-box;display:-ms-flexbox;display:flex;z-index:20}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear{opacity:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter-active,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit{opacity:1}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container:focus{outline:none}.jupyter-wrapper .bp3-transition-container.bp3-popover-leave .bp3-popover-content{pointer-events:none}.jupyter-wrapper .bp3-transition-container[data-x-out-of-boundaries]{display:none}.jupyter-wrapper span.bp3-popover-target{display:inline-block}.jupyter-wrapper .bp3-popover-wrapper.bp3-fill{width:100%}.jupyter-wrapper .bp3-portal{position:absolute;top:0;right:0;left:0}@-webkit-keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}@keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}.jupyter-wrapper .bp3-progress-bar{display:block;position:relative;border-radius:40px;background:rgba(92,112,128,.2);width:100%;height:8px;overflow:hidden}.jupyter-wrapper .bp3-progress-bar .bp3-progress-meter{position:absolute;border-radius:40px;background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);background-color:rgba(92,112,128,.8);background-size:30px 30px;width:100%;height:100%;-webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{animation:linear-progress-bar-stripes 300ms linear infinite reverse}.jupyter-wrapper .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{background-image:none}.jupyter-wrapper .bp3-dark .bp3-progress-bar{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-progress-bar .bp3-progress-meter{background-color:#8a9ba8}.jupyter-wrapper .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{background-color:#137cbd}.jupyter-wrapper .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{background-color:#0f9960}.jupyter-wrapper .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{background-color:#d9822b}.jupyter-wrapper .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{background-color:#db3737}@-webkit-keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}@keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}.jupyter-wrapper .bp3-skeleton{border-color:rgba(206,217,224,.2) !important;border-radius:2px;-webkit-box-shadow:none !important;box-shadow:none !important;background:rgba(206,217,224,.2);background-clip:padding-box !important;cursor:default;color:rgba(0,0,0,0) !important;-webkit-animation:1000ms linear infinite alternate skeleton-glow;animation:1000ms linear infinite alternate skeleton-glow;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-skeleton::before,.jupyter-wrapper .bp3-skeleton::after,.jupyter-wrapper .bp3-skeleton *{visibility:hidden !important}.jupyter-wrapper .bp3-slider{width:100%;min-width:150px;height:40px;position:relative;outline:none;cursor:default;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-slider:hover{cursor:pointer}.jupyter-wrapper .bp3-slider:active{cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-slider.bp3-disabled{opacity:.5;cursor:not-allowed}.jupyter-wrapper .bp3-slider.bp3-slider-unlabeled{height:16px}.jupyter-wrapper .bp3-slider-track,.jupyter-wrapper .bp3-slider-progress{top:5px;right:0;left:0;height:6px;position:absolute}.jupyter-wrapper .bp3-slider-track{border-radius:3px;overflow:hidden}.jupyter-wrapper .bp3-slider-progress{background:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-dark .bp3-slider-progress{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-slider-progress.bp3-intent-primary{background-color:#137cbd}.jupyter-wrapper .bp3-slider-progress.bp3-intent-success{background-color:#0f9960}.jupyter-wrapper .bp3-slider-progress.bp3-intent-warning{background-color:#d9822b}.jupyter-wrapper .bp3-slider-progress.bp3-intent-danger{background-color:#db3737}.jupyter-wrapper .bp3-slider-handle{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;position:absolute;top:0;left:0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:pointer;width:16px;height:16px}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-slider-handle:active,.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-slider-handle.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active:hover,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-slider-handle:focus{z-index:1}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5;z-index:2;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:-webkit-grab;cursor:grab}.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-disabled .bp3-slider-handle{-webkit-box-shadow:none;box-shadow:none;background:#bfccd6;pointer-events:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover,.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-slider-handle,.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{background-color:#394b59}.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{background-color:#293742}.jupyter-wrapper .bp3-dark .bp3-disabled .bp3-slider-handle{border-color:#5c7080;-webkit-box-shadow:none;box-shadow:none;background:#5c7080}.jupyter-wrapper .bp3-slider-handle .bp3-slider-label{margin-left:8px;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-disabled .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-slider-handle.bp3-start,.jupyter-wrapper .bp3-slider-handle.bp3-end{width:8px}.jupyter-wrapper .bp3-slider-handle.bp3-start{border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end{margin-left:8px;border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end .bp3-slider-label{margin-left:0}.jupyter-wrapper .bp3-slider-label{-webkit-transform:translate(-50%, 20px);transform:translate(-50%, 20px);display:inline-block;position:absolute;padding:2px 5px;vertical-align:top;line-height:1;font-size:12px}.jupyter-wrapper .bp3-slider.bp3-vertical{width:40px;min-width:40px;height:150px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-track,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:0;bottom:0;left:5px;width:6px;height:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-label{-webkit-transform:translate(20px, 50%);transform:translate(20px, 50%)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{margin-top:-8px;margin-left:0}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{margin-left:0;width:16px;height:8px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{border-top-left-radius:0;border-bottom-right-radius:3px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{-webkit-transform:translate(20px);transform:translate(20px)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{margin-bottom:8px;border-top-left-radius:3px;border-bottom-left-radius:0;border-bottom-right-radius:0}@-webkit-keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.jupyter-wrapper .bp3-spinner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;overflow:visible;vertical-align:middle}.jupyter-wrapper .bp3-spinner svg{display:block}.jupyter-wrapper .bp3-spinner path{fill-opacity:0}.jupyter-wrapper .bp3-spinner .bp3-spinner-head{-webkit-transform-origin:center;transform-origin:center;-webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);stroke:rgba(92,112,128,.8);stroke-linecap:round}.jupyter-wrapper .bp3-spinner .bp3-spinner-track{stroke:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-spinner-animation{-webkit-animation:pt-spinner-animation 500ms linear infinite;animation:pt-spinner-animation 500ms linear infinite}.jupyter-wrapper .bp3-no-spin>.bp3-spinner-animation{-webkit-animation:none;animation:none}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-track{stroke:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-spinner.bp3-intent-primary .bp3-spinner-head{stroke:#137cbd}.jupyter-wrapper .bp3-spinner.bp3-intent-success .bp3-spinner-head{stroke:#0f9960}.jupyter-wrapper .bp3-spinner.bp3-intent-warning .bp3-spinner-head{stroke:#d9822b}.jupyter-wrapper .bp3-spinner.bp3-intent-danger .bp3-spinner-head{stroke:#db3737}.jupyter-wrapper .bp3-tabs.bp3-vertical{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab{border-radius:3px;width:100%;padding:0 10px}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab[aria-selected=true]{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.2)}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{top:0;right:0;bottom:0;left:0;border-radius:3px;background-color:rgba(19,124,189,.2);height:auto}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-panel{margin-top:0;padding-left:20px}.jupyter-wrapper .bp3-tab-list{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:relative;margin:0;border:none;padding:0;list-style:none}.jupyter-wrapper .bp3-tab-list>*:not(:last-child){margin-right:20px}.jupyter-wrapper .bp3-tab{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;cursor:pointer;max-width:100%;vertical-align:top;line-height:30px;color:#182026;font-size:14px}.jupyter-wrapper .bp3-tab a{display:block;text-decoration:none;color:inherit}.jupyter-wrapper .bp3-tab-indicator-wrapper~.bp3-tab{-webkit-box-shadow:none !important;box-shadow:none !important;background-color:rgba(0,0,0,0) !important}.jupyter-wrapper .bp3-tab[aria-disabled=true]{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tab[aria-selected=true]{border-radius:0;-webkit-box-shadow:inset 0 -3px 0 #106ba3;box-shadow:inset 0 -3px 0 #106ba3}.jupyter-wrapper .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-tab:not([aria-disabled=true]):hover{color:#106ba3}.jupyter-wrapper .bp3-tab:focus{-moz-outline-radius:0}.jupyter-wrapper .bp3-large>.bp3-tab{line-height:40px;font-size:16px}.jupyter-wrapper .bp3-tab-panel{margin-top:20px}.jupyter-wrapper .bp3-tab-panel[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-tab-indicator-wrapper{position:absolute;top:0;left:0;-webkit-transform:translateX(0),translateY(0);transform:translateX(0),translateY(0);-webkit-transition:height,width,-webkit-transform;transition:height,width,-webkit-transform;transition:height,transform,width;transition:height,transform,width,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);pointer-events:none}.jupyter-wrapper .bp3-tab-indicator-wrapper .bp3-tab-indicator{position:absolute;right:0;bottom:0;left:0;background-color:#106ba3;height:3px}.jupyter-wrapper .bp3-tab-indicator-wrapper.bp3-no-animation{-webkit-transition:none;transition:none}.jupyter-wrapper .bp3-dark .bp3-tab{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tab[aria-disabled=true]{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true]{-webkit-box-shadow:inset 0 -3px 0 #48aff0;box-shadow:inset 0 -3px 0 #48aff0}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-dark .bp3-tab:not([aria-disabled=true]):hover{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tab-indicator{background-color:#48aff0}.jupyter-wrapper .bp3-flex-expander{-webkit-box-flex:1;-ms-flex:1 1;flex:1 1}.jupyter-wrapper .bp3-tag{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border:none;border-radius:3px;-webkit-box-shadow:none;box-shadow:none;background-color:#5c7080;min-width:20px;max-width:100%;min-height:20px;padding:2px 6px;line-height:16px;color:#f5f8fa;font-size:12px}.jupyter-wrapper .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-interactive:hover{background-color:rgba(92,112,128,.85)}.jupyter-wrapper .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-interactive:active{background-color:rgba(92,112,128,.7)}.jupyter-wrapper .bp3-tag>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag::before,.jupyter-wrapper .bp3-tag>*{margin-right:4px}.jupyter-wrapper .bp3-tag:empty::before,.jupyter-wrapper .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag:focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag.bp3-round{border-radius:30px;padding-right:8px;padding-left:8px}.jupyter-wrapper .bp3-dark .bp3-tag{background-color:#bfccd6;color:#182026}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:hover{background-color:rgba(191,204,214,.85)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:active{background-color:rgba(191,204,214,.7)}.jupyter-wrapper .bp3-dark .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-large{fill:currentColor}.jupyter-wrapper .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-tag .bp3-icon-large{fill:#fff}.jupyter-wrapper .bp3-tag.bp3-large,.jupyter-wrapper .bp3-large .bp3-tag{min-width:30px;min-height:30px;padding:0 10px;line-height:20px;font-size:14px}.jupyter-wrapper .bp3-tag.bp3-large::before,.jupyter-wrapper .bp3-tag.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-tag::before,.jupyter-wrapper .bp3-large .bp3-tag>*{margin-right:7px}.jupyter-wrapper .bp3-tag.bp3-large:empty::before,.jupyter-wrapper .bp3-tag.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-tag:empty::before,.jupyter-wrapper .bp3-large .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag.bp3-large.bp3-round,.jupyter-wrapper .bp3-large .bp3-tag.bp3-round{padding-right:12px;padding-left:12px}.jupyter-wrapper .bp3-tag.bp3-intent-primary{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-success{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-warning{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-danger{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.7)}.jupyter-wrapper .bp3-tag.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-tag.bp3-minimal>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-large{fill:#5c7080}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){background-color:rgba(138,155,168,.2);color:#182026}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(191,204,214,.3)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-])>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-large{fill:#a7b6c2}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{fill:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.25);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{fill:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.25);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{fill:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.25);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{fill:#db3737}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.25);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.45)}.jupyter-wrapper .bp3-tag-remove{display:-webkit-box;display:-ms-flexbox;display:flex;opacity:.5;margin-top:-2px;margin-right:-6px !important;margin-bottom:-2px;border:none;background:none;cursor:pointer;padding:2px;padding-left:0;color:inherit}.jupyter-wrapper .bp3-tag-remove:hover{opacity:.8;background:none;text-decoration:none}.jupyter-wrapper .bp3-tag-remove:active{opacity:1}.jupyter-wrapper .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6d7\"}.jupyter-wrapper .bp3-large .bp3-tag-remove{margin-right:-10px !important;padding:5px;padding-left:0}.jupyter-wrapper .bp3-large .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal}.jupyter-wrapper .bp3-tag-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;cursor:text;height:auto;min-height:30px;padding-right:0;padding-left:5px;line-height:inherit}.jupyter-wrapper .bp3-tag-input>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input>.bp3-tag-input-values{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-icon{margin-top:7px;margin-right:7px;margin-left:2px;color:#5c7080}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-item-align:stretch;align-self:stretch;margin-top:5px;margin-right:7px;min-width:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-right:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:empty::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{padding-left:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-bottom:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag{overflow-wrap:break-word}.jupyter-wrapper .bp3-tag-input .bp3-tag.bp3-active{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:80px;line-height:20px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost:disabled,.jupyter-wrapper .bp3-tag-input .bp3-input-ghost.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-tag-input .bp3-button,.jupyter-wrapper .bp3-tag-input .bp3-spinner{margin:3px;margin-left:0}.jupyter-wrapper .bp3-tag-input .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-tag-input.bp3-large{height:auto;min-height:40px}.jupyter-wrapper .bp3-tag-input.bp3-large::before,.jupyter-wrapper .bp3-tag-input.bp3-large>*{margin-right:10px}.jupyter-wrapper .bp3-tag-input.bp3-large:empty::before,.jupyter-wrapper .bp3-tag-input.bp3-large>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-tag-input-icon{margin-top:10px;margin-left:5px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-input-ghost{line-height:30px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-button{min-width:30px;min-height:30px;padding:5px 10px;margin:5px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-spinner{margin:8px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-tag-input-icon,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-tag-input-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-input-ghost{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;padding:0}.jupyter-wrapper .bp3-input-ghost::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:focus{outline:none !important}.jupyter-wrapper .bp3-toast{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;position:relative !important;margin:20px 0 0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background-color:#fff;min-width:300px;max-width:500px;pointer-events:all}.jupyter-wrapper .bp3-toast.bp3-toast-enter,.jupyter-wrapper .bp3-toast.bp3-toast-appear{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-enter~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit{opacity:1;-webkit-filter:blur(0);filter:blur(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active{opacity:0;-webkit-filter:blur(10px);filter:blur(10px);-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:opacity,filter;transition-property:opacity,filter,-webkit-filter;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:50ms;transition-delay:50ms}.jupyter-wrapper .bp3-toast .bp3-button-group{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;padding-left:0}.jupyter-wrapper .bp3-toast>.bp3-icon{margin:12px;margin-right:0;color:#5c7080}.jupyter-wrapper .bp3-toast.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-toast{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background-color:#394b59}.jupyter-wrapper .bp3-toast.bp3-dark>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-toast>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a:hover{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-]>.bp3-icon{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::before,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button .bp3-icon,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{color:rgba(255,255,255,.7) !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:focus{outline-color:rgba(255,255,255,.5)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:hover{background-color:rgba(255,255,255,.15) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{background-color:rgba(255,255,255,.3) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::after{background:rgba(255,255,255,.3) !important}.jupyter-wrapper .bp3-toast.bp3-intent-primary{background-color:#137cbd;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-success{background-color:#0f9960;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-warning{background-color:#d9822b;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-danger{background-color:#db3737;color:#fff}.jupyter-wrapper .bp3-toast-message{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;padding:11px;word-break:break-word}.jupyter-wrapper .bp3-toast-container{display:-webkit-box !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;right:0;left:0;z-index:40;overflow:hidden;padding:0 20px 20px;pointer-events:none}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-top{top:0;bottom:auto}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-bottom{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;top:auto;bottom:0}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-left{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-right{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active~.bp3-toast{-webkit-transform:translateY(60px);transform:translateY(60px)}.jupyter-wrapper .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow{position:absolute;width:22px;height:22px}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{margin:4px;width:14px;height:14px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip{margin-top:-11px;margin-bottom:11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{bottom:-8px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip{margin-left:11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{left:-8px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip{margin-top:11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{top:-8px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip{margin-right:11px;margin-left:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{right:-8px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-tooltip>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-tooltip>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{top:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{right:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{left:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{bottom:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-fill{fill:#394b59}.jupyter-wrapper .bp3-popover-enter>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear-active>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{padding:10px 12px}.jupyter-wrapper .bp3-tooltip.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-content{background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{fill:#e1e8ed}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-content{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{fill:#137cbd}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-content{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{fill:#0f9960}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-content{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{fill:#d9822b}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-content{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{fill:#db3737}.jupyter-wrapper .bp3-tooltip-indicator{border-bottom:dotted 1px;cursor:help}.jupyter-wrapper .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-tree .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-tree-node-list{margin:0;padding-left:0;list-style:none}.jupyter-wrapper .bp3-tree-root{position:relative;background-color:rgba(0,0,0,0);cursor:default;padding-left:0}.jupyter-wrapper .bp3-tree-node-content-0{padding-left:0px}.jupyter-wrapper .bp3-tree-node-content-1{padding-left:23px}.jupyter-wrapper .bp3-tree-node-content-2{padding-left:46px}.jupyter-wrapper .bp3-tree-node-content-3{padding-left:69px}.jupyter-wrapper .bp3-tree-node-content-4{padding-left:92px}.jupyter-wrapper .bp3-tree-node-content-5{padding-left:115px}.jupyter-wrapper .bp3-tree-node-content-6{padding-left:138px}.jupyter-wrapper .bp3-tree-node-content-7{padding-left:161px}.jupyter-wrapper .bp3-tree-node-content-8{padding-left:184px}.jupyter-wrapper .bp3-tree-node-content-9{padding-left:207px}.jupyter-wrapper .bp3-tree-node-content-10{padding-left:230px}.jupyter-wrapper .bp3-tree-node-content-11{padding-left:253px}.jupyter-wrapper .bp3-tree-node-content-12{padding-left:276px}.jupyter-wrapper .bp3-tree-node-content-13{padding-left:299px}.jupyter-wrapper .bp3-tree-node-content-14{padding-left:322px}.jupyter-wrapper .bp3-tree-node-content-15{padding-left:345px}.jupyter-wrapper .bp3-tree-node-content-16{padding-left:368px}.jupyter-wrapper .bp3-tree-node-content-17{padding-left:391px}.jupyter-wrapper .bp3-tree-node-content-18{padding-left:414px}.jupyter-wrapper .bp3-tree-node-content-19{padding-left:437px}.jupyter-wrapper .bp3-tree-node-content-20{padding-left:460px}.jupyter-wrapper .bp3-tree-node-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:30px;padding-right:5px}.jupyter-wrapper .bp3-tree-node-content:hover{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node-caret-none{min-width:30px}.jupyter-wrapper .bp3-tree-node-caret{color:#5c7080;-webkit-transform:rotate(0deg);transform:rotate(0deg);cursor:pointer;padding:7px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-tree-node-caret:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret:hover{color:#f5f8fa}.jupyter-wrapper .bp3-tree-node-caret.bp3-tree-node-caret-open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tree-node-caret.bp3-icon-standard::before{content:\"\ue695\"}.jupyter-wrapper .bp3-tree-node-icon{position:relative;margin-right:7px}.jupyter-wrapper .bp3-tree-node-label{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-label span{display:inline}.jupyter-wrapper .bp3-tree-node-secondary-label{padding:0 5px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-wrapper,.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-content{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-standard,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-large{color:#fff}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret::before{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret:hover::before{color:#fff}.jupyter-wrapper .bp3-dark .bp3-tree-node-content:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-dark .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-omnibar{-webkit-filter:blur(0);filter:blur(0);opacity:1;top:20vh;left:calc(50% - 250px);z-index:21;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background-color:#fff;width:500px}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter-active,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear-active{-webkit-filter:blur(0);filter:blur(0);opacity:1;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit{-webkit-filter:blur(0);filter:blur(0);opacity:1}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit-active{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar .bp3-input{border-radius:0;background-color:rgba(0,0,0,0)}.jupyter-wrapper .bp3-omnibar .bp3-input,.jupyter-wrapper .bp3-omnibar .bp3-input:focus{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-omnibar .bp3-menu{border-radius:0;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);background-color:rgba(0,0,0,0);max-height:calc(60vh - 40px);overflow:auto}.jupyter-wrapper .bp3-omnibar .bp3-menu:empty{display:none}.jupyter-wrapper .bp3-dark .bp3-omnibar,.jupyter-wrapper .bp3-omnibar.bp3-dark{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-omnibar-overlay .bp3-overlay-backdrop{background-color:rgba(16,22,26,.2)}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper .bp3-multi-select{min-width:150px}.jupyter-wrapper .bp3-multi-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper :root{--jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-build: url(data:image/svg+xml;base64,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);--jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);--jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);--jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);--jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);--jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);--jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter: url(data:image/svg+xml;base64,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);--jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);--jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);--jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);--jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);--jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);--jp-icon-listings-info: url(data:image/svg+xml;base64,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);--jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);--jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);--jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);--jp-icon-react: url(data:image/svg+xml;base64,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);--jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);--jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);--jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-terminal: url(data:image/svg+xml;base64,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);--jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);--jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);--jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K)}.jupyter-wrapper .jp-AddIcon{background-image:var(--jp-icon-add)}.jupyter-wrapper .jp-BugIcon{background-image:var(--jp-icon-bug)}.jupyter-wrapper .jp-BuildIcon{background-image:var(--jp-icon-build)}.jupyter-wrapper .jp-CaretDownEmptyIcon{background-image:var(--jp-icon-caret-down-empty)}.jupyter-wrapper .jp-CaretDownEmptyThinIcon{background-image:var(--jp-icon-caret-down-empty-thin)}.jupyter-wrapper .jp-CaretDownIcon{background-image:var(--jp-icon-caret-down)}.jupyter-wrapper .jp-CaretLeftIcon{background-image:var(--jp-icon-caret-left)}.jupyter-wrapper .jp-CaretRightIcon{background-image:var(--jp-icon-caret-right)}.jupyter-wrapper .jp-CaretUpEmptyThinIcon{background-image:var(--jp-icon-caret-up-empty-thin)}.jupyter-wrapper .jp-CaretUpIcon{background-image:var(--jp-icon-caret-up)}.jupyter-wrapper .jp-CaseSensitiveIcon{background-image:var(--jp-icon-case-sensitive)}.jupyter-wrapper .jp-CheckIcon{background-image:var(--jp-icon-check)}.jupyter-wrapper .jp-CircleEmptyIcon{background-image:var(--jp-icon-circle-empty)}.jupyter-wrapper .jp-CircleIcon{background-image:var(--jp-icon-circle)}.jupyter-wrapper .jp-ClearIcon{background-image:var(--jp-icon-clear)}.jupyter-wrapper .jp-CloseIcon{background-image:var(--jp-icon-close)}.jupyter-wrapper .jp-ConsoleIcon{background-image:var(--jp-icon-console)}.jupyter-wrapper .jp-CopyIcon{background-image:var(--jp-icon-copy)}.jupyter-wrapper .jp-CutIcon{background-image:var(--jp-icon-cut)}.jupyter-wrapper .jp-DownloadIcon{background-image:var(--jp-icon-download)}.jupyter-wrapper .jp-EditIcon{background-image:var(--jp-icon-edit)}.jupyter-wrapper .jp-EllipsesIcon{background-image:var(--jp-icon-ellipses)}.jupyter-wrapper .jp-ExtensionIcon{background-image:var(--jp-icon-extension)}.jupyter-wrapper .jp-FastForwardIcon{background-image:var(--jp-icon-fast-forward)}.jupyter-wrapper .jp-FileIcon{background-image:var(--jp-icon-file)}.jupyter-wrapper .jp-FileUploadIcon{background-image:var(--jp-icon-file-upload)}.jupyter-wrapper .jp-FilterListIcon{background-image:var(--jp-icon-filter-list)}.jupyter-wrapper .jp-FolderIcon{background-image:var(--jp-icon-folder)}.jupyter-wrapper .jp-Html5Icon{background-image:var(--jp-icon-html5)}.jupyter-wrapper .jp-ImageIcon{background-image:var(--jp-icon-image)}.jupyter-wrapper .jp-InspectorIcon{background-image:var(--jp-icon-inspector)}.jupyter-wrapper .jp-JsonIcon{background-image:var(--jp-icon-json)}.jupyter-wrapper .jp-JupyterFaviconIcon{background-image:var(--jp-icon-jupyter-favicon)}.jupyter-wrapper .jp-JupyterIcon{background-image:var(--jp-icon-jupyter)}.jupyter-wrapper .jp-JupyterlabWordmarkIcon{background-image:var(--jp-icon-jupyterlab-wordmark)}.jupyter-wrapper .jp-KernelIcon{background-image:var(--jp-icon-kernel)}.jupyter-wrapper .jp-KeyboardIcon{background-image:var(--jp-icon-keyboard)}.jupyter-wrapper .jp-LauncherIcon{background-image:var(--jp-icon-launcher)}.jupyter-wrapper .jp-LineFormIcon{background-image:var(--jp-icon-line-form)}.jupyter-wrapper .jp-LinkIcon{background-image:var(--jp-icon-link)}.jupyter-wrapper .jp-ListIcon{background-image:var(--jp-icon-list)}.jupyter-wrapper .jp-ListingsInfoIcon{background-image:var(--jp-icon-listings-info)}.jupyter-wrapper .jp-MarkdownIcon{background-image:var(--jp-icon-markdown)}.jupyter-wrapper .jp-NewFolderIcon{background-image:var(--jp-icon-new-folder)}.jupyter-wrapper .jp-NotTrustedIcon{background-image:var(--jp-icon-not-trusted)}.jupyter-wrapper .jp-NotebookIcon{background-image:var(--jp-icon-notebook)}.jupyter-wrapper .jp-PaletteIcon{background-image:var(--jp-icon-palette)}.jupyter-wrapper .jp-PasteIcon{background-image:var(--jp-icon-paste)}.jupyter-wrapper .jp-PythonIcon{background-image:var(--jp-icon-python)}.jupyter-wrapper .jp-RKernelIcon{background-image:var(--jp-icon-r-kernel)}.jupyter-wrapper .jp-ReactIcon{background-image:var(--jp-icon-react)}.jupyter-wrapper .jp-RefreshIcon{background-image:var(--jp-icon-refresh)}.jupyter-wrapper .jp-RegexIcon{background-image:var(--jp-icon-regex)}.jupyter-wrapper .jp-RunIcon{background-image:var(--jp-icon-run)}.jupyter-wrapper .jp-RunningIcon{background-image:var(--jp-icon-running)}.jupyter-wrapper .jp-SaveIcon{background-image:var(--jp-icon-save)}.jupyter-wrapper .jp-SearchIcon{background-image:var(--jp-icon-search)}.jupyter-wrapper .jp-SettingsIcon{background-image:var(--jp-icon-settings)}.jupyter-wrapper .jp-SpreadsheetIcon{background-image:var(--jp-icon-spreadsheet)}.jupyter-wrapper .jp-StopIcon{background-image:var(--jp-icon-stop)}.jupyter-wrapper .jp-TabIcon{background-image:var(--jp-icon-tab)}.jupyter-wrapper .jp-TerminalIcon{background-image:var(--jp-icon-terminal)}.jupyter-wrapper .jp-TextEditorIcon{background-image:var(--jp-icon-text-editor)}.jupyter-wrapper .jp-TrustedIcon{background-image:var(--jp-icon-trusted)}.jupyter-wrapper .jp-UndoIcon{background-image:var(--jp-icon-undo)}.jupyter-wrapper .jp-VegaIcon{background-image:var(--jp-icon-vega)}.jupyter-wrapper .jp-YamlIcon{background-image:var(--jp-icon-yaml)}.jupyter-wrapper :root{--jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==)}.jupyter-wrapper .jp-Icon,.jupyter-wrapper .jp-MaterialIcon{background-position:center;background-repeat:no-repeat;background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-cover{background-position:center;background-repeat:no-repeat;background-size:cover}.jupyter-wrapper .jp-Icon-16{background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-18{background-size:18px;min-width:18px;min-height:18px}.jupyter-wrapper .jp-Icon-20{background-size:20px;min-width:20px;min-height:20px}.jupyter-wrapper .jp-icon0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-accent0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-accent0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-none[fill]{fill:none}.jupyter-wrapper .jp-icon-none[stroke]{stroke:none}.jupyter-wrapper .jp-icon-brand0[fill]{fill:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[fill]{fill:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[fill]{fill:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[fill]{fill:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-brand0[stroke]{stroke:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[stroke]{stroke:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[stroke]{stroke:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[stroke]{stroke:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[stroke]{stroke:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-warn0[fill]{fill:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[fill]{fill:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[fill]{fill:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[fill]{fill:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-warn0[stroke]{stroke:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[stroke]{stroke:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[stroke]{stroke:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[stroke]{stroke:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-contrast0[fill]{fill:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[fill]{fill:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[fill]{fill:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[fill]{fill:var(--jp-icon-contrast-color3)}.jupyter-wrapper .jp-icon-contrast0[stroke]{stroke:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[stroke]{stroke:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[stroke]{stroke:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[stroke]{stroke:var(--jp-icon-contrast-color3)}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:#fff}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper :root{--jp-warn-color0: var(--md-orange-700)}.jupyter-wrapper .jp-DragIcon{margin-right:4px}.jupyter-wrapper .jp-icon-alt .jp-icon0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hoverShow:not(:hover) svg{display:none !important}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[fill]{fill:none}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[stroke]{stroke:none}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper :focus{outline:unset;outline-offset:unset;-moz-outline-radius:unset}.jupyter-wrapper .jp-Button{border-radius:var(--jp-border-radius);padding:0px 12px;font-size:var(--jp-ui-font-size1)}.jupyter-wrapper button.jp-Button.bp3-button.bp3-minimal:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Button.minimal{color:unset !important}.jupyter-wrapper .jp-Button.jp-ToolbarButtonComponent{text-transform:none}.jupyter-wrapper .jp-InputGroup input{box-sizing:border-box;border-radius:0;background-color:rgba(0,0,0,0);color:var(--jp-ui-font-color0);box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .jp-InputGroup input:focus{box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .jp-InputGroup input::placeholder,.jupyter-wrapper input::placeholder{color:var(--jp-ui-font-color3)}.jupyter-wrapper .jp-BPIcon{display:inline-block;vertical-align:middle;margin:auto}.jupyter-wrapper .bp3-icon.jp-BPIcon>svg:not([fill]){fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-InputGroupAction{padding:6px}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select{background-color:initial;border:none;border-radius:0;box-shadow:none;color:var(--jp-ui-font-color0);display:block;font-size:var(--jp-ui-font-size1);height:24px;line-height:14px;padding:0 25px 0 10px;text-align:left;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select:hover,.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select>option{background-color:var(--jp-layout-color2);color:var(--jp-ui-font-color0)}.jupyter-wrapper select{box-sizing:border-box}.jupyter-wrapper .jp-Collapse{display:flex;flex-direction:column;align-items:stretch;border-top:1px solid var(--jp-border-color2);border-bottom:1px solid var(--jp-border-color2)}.jupyter-wrapper .jp-Collapse-header{padding:1px 12px;color:var(--jp-ui-font-color1);background-color:var(--jp-layout-color1);font-size:var(--jp-ui-font-size2)}.jupyter-wrapper .jp-Collapse-header:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Collapse-contents{padding:0px 12px 0px 12px;background-color:var(--jp-layout-color1);color:var(--jp-ui-font-color1);overflow:auto}.jupyter-wrapper :root{--jp-private-commandpalette-search-height: 28px}.jupyter-wrapper .lm-CommandPalette{padding-bottom:0px;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-search{padding:4px;background-color:var(--jp-layout-color1);z-index:2}.jupyter-wrapper .lm-CommandPalette-wrapper{overflow:overlay;padding:0px 9px;background-color:var(--jp-input-active-background);height:30px;box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper{box-shadow:inset 0 0 0 1px var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .lm-CommandPalette-wrapper::after{content:\" \";color:#fff;background-color:var(--jp-brand-color1);position:absolute;top:4px;right:4px;height:30px;width:10px;padding:0px 10px;background-image:var(--jp-icon-search-white);background-size:20px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .lm-CommandPalette-input{background:rgba(0,0,0,0);width:calc(100% - 18px);float:left;border:none;outline:none;font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);line-height:var(--jp-private-commandpalette-search-height)}.jupyter-wrapper .lm-CommandPalette-input::-webkit-input-placeholder,.jupyter-wrapper .lm-CommandPalette-input::-moz-placeholder,.jupyter-wrapper .lm-CommandPalette-input:-ms-input-placeholder{color:var(--jp-ui-font-color3);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-header:first-child{margin-top:0px}.jupyter-wrapper .lm-CommandPalette-header{border-bottom:solid var(--jp-border-width) var(--jp-border-color2);color:var(--jp-ui-font-color1);cursor:pointer;display:flex;font-size:var(--jp-ui-font-size0);font-weight:600;letter-spacing:1px;margin-top:8px;padding:8px 0 8px 12px;text-transform:uppercase}.jupyter-wrapper .lm-CommandPalette-header.lm-mod-active{background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-header>mark{background-color:rgba(0,0,0,0);font-weight:bold;color:var(--jp-ui-font-color1)}.jupyter-wrapper .lm-CommandPalette-item{padding:4px 12px 4px 4px;color:var(--jp-ui-font-color1);font-size:var(--jp-ui-font-size1);font-weight:400;display:flex}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active{background:var(--jp-layout-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled){background:var(--jp-layout-color4)}.jupyter-wrapper .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled){background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-itemContent{overflow:hidden}.jupyter-wrapper .lm-CommandPalette-itemLabel>mark{color:var(--jp-ui-font-color0);background-color:rgba(0,0,0,0);font-weight:bold}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled mark{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemIcon{margin:0 4px 0 0;position:relative;width:16px;top:2px;flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon{opacity:.4}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-itemCaption{display:none}.jupyter-wrapper .lm-CommandPalette-content{background-color:var(--jp-layout-color1)}.jupyter-wrapper .lm-CommandPalette-content:empty:after{content:\"No results\";margin:auto;margin-top:20px;width:100px;display:block;font-size:var(--jp-ui-font-size2);font-family:var(--jp-ui-font-family);font-weight:lighter}.jupyter-wrapper .lm-CommandPalette-emptyMessage{text-align:center;margin-top:24px;line-height:1.32;padding:0px 8px;color:var(--jp-content-font-color3)}.jupyter-wrapper .jp-Dialog{position:absolute;z-index:10000;display:flex;flex-direction:column;align-items:center;justify-content:center;top:0px;left:0px;margin:0;padding:0;width:100%;height:100%;background:var(--jp-dialog-background)}.jupyter-wrapper .jp-Dialog-content{display:flex;flex-direction:column;margin-left:auto;margin-right:auto;background:var(--jp-layout-color1);padding:24px;padding-bottom:12px;min-width:300px;min-height:150px;max-width:1000px;max-height:500px;box-sizing:border-box;box-shadow:var(--jp-elevation-z20);word-wrap:break-word;border-radius:var(--jp-border-radius);font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color1)}.jupyter-wrapper .jp-Dialog-button{overflow:visible}.jupyter-wrapper button.jp-Dialog-button:focus{outline:1px solid var(--jp-brand-color1);outline-offset:4px;-moz-outline-radius:0px}.jupyter-wrapper button.jp-Dialog-button:focus::-moz-focus-inner{border:0}.jupyter-wrapper .jp-Dialog-header{flex:0 0 auto;padding-bottom:12px;font-size:var(--jp-ui-font-size3);font-weight:400;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-body{display:flex;flex-direction:column;flex:1 1 auto;font-size:var(--jp-ui-font-size1);background:var(--jp-layout-color1);overflow:auto}.jupyter-wrapper .jp-Dialog-footer{display:flex;flex-direction:row;justify-content:flex-end;flex:0 0 auto;margin-left:-12px;margin-right:-12px;padding:12px}.jupyter-wrapper .jp-Dialog-title{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .jp-Dialog-body>.jp-select-wrapper{width:100%}.jupyter-wrapper .jp-Dialog-body>button{padding:0px 16px}.jupyter-wrapper .jp-Dialog-body>label{line-height:1.4;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-button.jp-mod-styled:not(:last-child){margin-right:12px}.jupyter-wrapper .jp-HoverBox{position:fixed}.jupyter-wrapper .jp-HoverBox.jp-mod-outofview{display:none}.jupyter-wrapper .jp-IFrame{width:100%;height:100%}.jupyter-wrapper .jp-IFrame>iframe{border:none}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MainAreaWidget>:focus{outline:none}.jupyter-wrapper :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper .jp-Spinner{position:absolute;display:flex;justify-content:center;align-items:center;z-index:10;left:0;top:0;width:100%;height:100%;background:var(--jp-layout-color0);outline:none}.jupyter-wrapper .jp-SpinnerContent{font-size:10px;margin:50px auto;text-indent:-9999em;width:3em;height:3em;border-radius:50%;background:var(--jp-brand-color3);background:linear-gradient(to right, #f37626 10%, rgba(255, 255, 255, 0) 42%);position:relative;animation:load3 1s infinite linear,fadeIn 1s}.jupyter-wrapper .jp-SpinnerContent:before{width:50%;height:50%;background:#f37626;border-radius:100% 0 0 0;position:absolute;top:0;left:0;content:\"\"}.jupyter-wrapper .jp-SpinnerContent:after{background:var(--jp-layout-color0);width:75%;height:75%;border-radius:50%;content:\"\";margin:auto;position:absolute;top:0;left:0;bottom:0;right:0}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}@keyframes load3{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.jupyter-wrapper button.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:none;box-sizing:border-box;text-align:center;line-height:32px;height:32px;padding:0px 12px;letter-spacing:.8px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled{background:var(--jp-input-background);height:28px;box-sizing:border-box;border:var(--jp-border-width) solid var(--jp-border-color1);padding-left:7px;padding-right:7px;font-size:var(--jp-ui-font-size2);color:var(--jp-ui-font-color0);outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled:focus{border:var(--jp-border-width) solid var(--md-blue-500);box-shadow:inset 0 0 4px var(--md-blue-300)}.jupyter-wrapper .jp-select-wrapper{display:flex;position:relative;flex-direction:column;padding:1px;background-color:var(--jp-layout-color1);height:28px;box-sizing:border-box;margin-bottom:12px}.jupyter-wrapper .jp-select-wrapper.jp-mod-focused select.jp-mod-styled{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-input-active-background)}.jupyter-wrapper select.jp-mod-styled:hover{background-color:var(--jp-layout-color1);cursor:pointer;color:var(--jp-ui-font-color0);background-color:var(--jp-input-hover-background);box-shadow:inset 0 0px 1px rgba(0,0,0,.5)}.jupyter-wrapper select.jp-mod-styled{flex:1 1 auto;height:32px;width:100%;font-size:var(--jp-ui-font-size2);background:var(--jp-input-background);color:var(--jp-ui-font-color0);padding:0 25px 0 8px;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper :root{--jp-private-toolbar-height: calc( 28px + var(--jp-border-width) )}.jupyter-wrapper .jp-Toolbar{color:var(--jp-ui-font-color1);flex:0 0 auto;display:flex;flex-direction:row;border-bottom:var(--jp-border-width) solid var(--jp-toolbar-border-color);box-shadow:var(--jp-toolbar-box-shadow);background:var(--jp-toolbar-background);min-height:var(--jp-toolbar-micro-height);padding:2px;z-index:1}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item.jp-Toolbar-spacer{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-Toolbar-item.jp-Toolbar-kernelStatus{display:inline-block;width:32px;background-repeat:no-repeat;background-position:center;background-size:16px}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item{flex:0 0 auto;display:flex;padding-left:1px;padding-right:1px;font-size:var(--jp-ui-font-size1);line-height:var(--jp-private-toolbar-height);height:100%}.jupyter-wrapper div.jp-ToolbarButton{color:rgba(0,0,0,0);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px;margin:0px}.jupyter-wrapper button.jp-ToolbarButtonComponent{background:var(--jp-layout-color1);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px 6px;margin:0px;height:24px;border-radius:var(--jp-border-radius);display:flex;align-items:center;text-align:center;font-size:14px;min-width:unset;min-height:unset}.jupyter-wrapper button.jp-ToolbarButtonComponent:disabled{opacity:.4}.jupyter-wrapper button.jp-ToolbarButtonComponent span{padding:0px;flex:0 0 auto}.jupyter-wrapper button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label{font-size:var(--jp-ui-font-size1);line-height:100%;padding-left:2px;color:var(--jp-ui-font-color1)}.jupyter-wrapper body.p-mod-override-cursor *,.jupyter-wrapper body.lm-mod-override-cursor *{cursor:inherit !important}.jupyter-wrapper .jp-JSONEditor{display:flex;flex-direction:column;width:100%}.jupyter-wrapper .jp-JSONEditor-host{flex:1 1 auto;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;background:var(--jp-layout-color0);min-height:50px;padding:1px}.jupyter-wrapper .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host{border-color:red;outline-color:red}.jupyter-wrapper .jp-JSONEditor-header{display:flex;flex:1 0 auto;padding:0 0 0 12px}.jupyter-wrapper .jp-JSONEditor-header label{flex:0 0 auto}.jupyter-wrapper .jp-JSONEditor-commitButton{height:16px;width:16px;background-size:18px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .jp-JSONEditor-host.jp-mod-focused{background-color:var(--jp-input-active-background);border:1px solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .jp-Editor.jp-mod-dropTarget{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.jupyter-wrapper .CodeMirror-lines{padding:4px 0}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{padding:0 4px}.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{background-color:#fff}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.jupyter-wrapper .CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.jupyter-wrapper .CodeMirror-guttermarker{color:#000}.jupyter-wrapper .CodeMirror-guttermarker-subtle{color:#999}.jupyter-wrapper .CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.jupyter-wrapper .CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.jupyter-wrapper .cm-fat-cursor .CodeMirror-cursor{width:auto;border:0 !important;background:#7e7}.jupyter-wrapper .cm-fat-cursor div.CodeMirror-cursors{z-index:1}.jupyter-wrapper .cm-fat-cursor-mark{background-color:rgba(20,255,20,.5);-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite}.jupyter-wrapper .cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@-webkit-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@keyframes blink{50%{background-color:rgba(0,0,0,0)}}.jupyter-wrapper .cm-tab{display:inline-block;text-decoration:inherit}.jupyter-wrapper .CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:0;overflow:hidden}.jupyter-wrapper .CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.jupyter-wrapper .cm-s-default .cm-header{color:blue}.jupyter-wrapper .cm-s-default .cm-quote{color:#090}.jupyter-wrapper .cm-negative{color:#d44}.jupyter-wrapper .cm-positive{color:#292}.jupyter-wrapper .cm-header,.jupyter-wrapper .cm-strong{font-weight:bold}.jupyter-wrapper .cm-em{font-style:italic}.jupyter-wrapper .cm-link{text-decoration:underline}.jupyter-wrapper .cm-strikethrough{text-decoration:line-through}.jupyter-wrapper .cm-s-default .cm-keyword{color:#708}.jupyter-wrapper .cm-s-default .cm-atom{color:#219}.jupyter-wrapper .cm-s-default .cm-number{color:#164}.jupyter-wrapper .cm-s-default .cm-def{color:blue}.jupyter-wrapper .cm-s-default .cm-variable-2{color:#05a}.jupyter-wrapper .cm-s-default .cm-variable-3,.jupyter-wrapper .cm-s-default .cm-type{color:#085}.jupyter-wrapper .cm-s-default .cm-comment{color:#a50}.jupyter-wrapper .cm-s-default .cm-string{color:#a11}.jupyter-wrapper .cm-s-default .cm-string-2{color:#f50}.jupyter-wrapper .cm-s-default .cm-meta{color:#555}.jupyter-wrapper .cm-s-default .cm-qualifier{color:#555}.jupyter-wrapper .cm-s-default .cm-builtin{color:#30a}.jupyter-wrapper .cm-s-default .cm-bracket{color:#997}.jupyter-wrapper .cm-s-default .cm-tag{color:#170}.jupyter-wrapper .cm-s-default .cm-attribute{color:#00c}.jupyter-wrapper .cm-s-default .cm-hr{color:#999}.jupyter-wrapper .cm-s-default .cm-link{color:#00c}.jupyter-wrapper .cm-s-default .cm-error{color:red}.jupyter-wrapper .cm-invalidchar{color:red}.jupyter-wrapper .CodeMirror-composing{border-bottom:2px solid}.jupyter-wrapper div.CodeMirror span.CodeMirror-matchingbracket{color:#0b0}.jupyter-wrapper div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#a22}.jupyter-wrapper .CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.jupyter-wrapper .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .CodeMirror{position:relative;overflow:hidden;background:#fff}.jupyter-wrapper .CodeMirror-scroll{overflow:scroll !important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:none;position:relative}.jupyter-wrapper .CodeMirror-sizer{position:relative;border-right:30px solid rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-vscrollbar,.jupyter-wrapper .CodeMirror-hscrollbar,.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{position:absolute;z-index:6;display:none}.jupyter-wrapper .CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.jupyter-wrapper .CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.jupyter-wrapper .CodeMirror-scrollbar-filler{right:0;bottom:0}.jupyter-wrapper .CodeMirror-gutter-filler{left:0;bottom:0}.jupyter-wrapper .CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.jupyter-wrapper .CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.jupyter-wrapper .CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:none !important;border:none !important}.jupyter-wrapper .CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.jupyter-wrapper .CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.jupyter-wrapper .CodeMirror-gutter-wrapper ::selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-gutter-wrapper ::-moz-selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-lines{cursor:text;min-height:1px}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:rgba(0,0,0,0);font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line,.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line-like{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.jupyter-wrapper .CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.jupyter-wrapper .CodeMirror-linewidget{position:relative;z-index:2;padding:.1px}.jupyter-wrapper .CodeMirror-rtl pre{direction:rtl}.jupyter-wrapper .CodeMirror-code{outline:none}.jupyter-wrapper .CodeMirror-scroll,.jupyter-wrapper .CodeMirror-sizer,.jupyter-wrapper .CodeMirror-gutter,.jupyter-wrapper .CodeMirror-gutters,.jupyter-wrapper .CodeMirror-linenumber{-moz-box-sizing:content-box;box-sizing:content-box}.jupyter-wrapper .CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.jupyter-wrapper .CodeMirror-cursor{position:absolute;pointer-events:none}.jupyter-wrapper .CodeMirror-measure pre{position:static}.jupyter-wrapper div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}.jupyter-wrapper div.CodeMirror-dragcursors{visibility:visible}.jupyter-wrapper .CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.jupyter-wrapper .CodeMirror-selected{background:#d9d9d9}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.jupyter-wrapper .CodeMirror-crosshair{cursor:crosshair}.jupyter-wrapper .CodeMirror-line::selection,.jupyter-wrapper .CodeMirror-line>span::selection,.jupyter-wrapper .CodeMirror-line>span>span::selection{background:#d7d4f0}.jupyter-wrapper .CodeMirror-line::-moz-selection,.jupyter-wrapper .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.jupyter-wrapper .cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.jupyter-wrapper .cm-force-border{padding-right:.1px}@media print{.jupyter-wrapper .CodeMirror div.CodeMirror-cursors{visibility:hidden}}.jupyter-wrapper .cm-tab-wrap-hack:after{content:\"\"}.jupyter-wrapper span.CodeMirror-selectedtext{background:none}.jupyter-wrapper .CodeMirror-dialog{position:absolute;left:0;right:0;background:inherit;z-index:15;padding:.1em .8em;overflow:hidden;color:inherit}.jupyter-wrapper .CodeMirror-dialog-top{border-bottom:1px solid #eee;top:0}.jupyter-wrapper .CodeMirror-dialog-bottom{border-top:1px solid #eee;bottom:0}.jupyter-wrapper .CodeMirror-dialog input{border:none;outline:none;background:rgba(0,0,0,0);width:20em;color:inherit;font-family:monospace}.jupyter-wrapper .CodeMirror-dialog button{font-size:70%}.jupyter-wrapper .CodeMirror-foldmarker{color:blue;text-shadow:#b9f 1px 1px 2px,#b9f -1px -1px 2px,#b9f 1px -1px 2px,#b9f -1px 1px 2px;font-family:arial;line-height:.3;cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter{width:.7em}.jupyter-wrapper .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter-open:after{content:\"\u25be\"}.jupyter-wrapper .CodeMirror-foldgutter-folded:after{content:\"\u25b8\"}.jupyter-wrapper .cm-s-material.CodeMirror{background-color:#263238;color:#eff}.jupyter-wrapper .cm-s-material .CodeMirror-gutters{background:#263238;color:#546e7a;border:none}.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker,.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker-subtle,.jupyter-wrapper .cm-s-material .CodeMirror-linenumber{color:#546e7a}.jupyter-wrapper .cm-s-material .CodeMirror-cursor{border-left:1px solid #fc0}.jupyter-wrapper .cm-s-material div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material.CodeMirror-focused div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::-moz-selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-activeline-background{background:rgba(0,0,0,.5)}.jupyter-wrapper .cm-s-material .cm-keyword{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-operator{color:#89ddff}.jupyter-wrapper .cm-s-material .cm-variable-2{color:#eff}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#f07178}.jupyter-wrapper .cm-s-material .cm-builtin{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-atom{color:#f78c6c}.jupyter-wrapper .cm-s-material .cm-number{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-def{color:#82aaff}.jupyter-wrapper .cm-s-material .cm-string{color:#c3e88d}.jupyter-wrapper .cm-s-material .cm-string-2{color:#f07178}.jupyter-wrapper .cm-s-material .cm-comment{color:#546e7a}.jupyter-wrapper .cm-s-material .cm-variable{color:#f07178}.jupyter-wrapper .cm-s-material .cm-tag{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-meta{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-attribute{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-property{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-qualifier{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-error{color:#fff;background-color:#ff5370}.jupyter-wrapper .cm-s-material .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-gutters{background:#3f3f3f !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{color:#999}.jupyter-wrapper .cm-s-zenburn .CodeMirror-cursor{border-left:1px solid #fff}.jupyter-wrapper .cm-s-zenburn{background-color:#3f3f3f;color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-builtin{color:#dcdccc;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-comment{color:#7f9f7f}.jupyter-wrapper .cm-s-zenburn span.cm-keyword{color:#f0dfaf;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-atom{color:#bfebbf}.jupyter-wrapper .cm-s-zenburn span.cm-def{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-variable{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-variable-2{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-string{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-string-2{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-number{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-tag{color:#93e0e3}.jupyter-wrapper .cm-s-zenburn span.cm-property{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-attribute{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-qualifier{color:#7cb8bb}.jupyter-wrapper .cm-s-zenburn span.cm-meta{color:#f0dfaf}.jupyter-wrapper .cm-s-zenburn span.cm-header{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.cm-operator{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-matchingbracket{box-sizing:border-box;background:rgba(0,0,0,0);border-bottom:1px solid}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-nonmatchingbracket{border-bottom:1px solid;background:none}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline{background:#000}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline-background{background:#000}.jupyter-wrapper .cm-s-zenburn div.CodeMirror-selected{background:#545454}.jupyter-wrapper .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected{background:#4f4f4f}.jupyter-wrapper .cm-s-abcdef.CodeMirror{background:#0f0f0f;color:#defdef}.jupyter-wrapper .cm-s-abcdef div.CodeMirror-selected{background:#515151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::-moz-selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-gutters{background:#555;border-right:2px solid #314151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker{color:#222}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker-subtle{color:azure}.jupyter-wrapper .cm-s-abcdef .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-abcdef .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-abcdef span.cm-keyword{color:#b8860b;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-atom{color:#77f}.jupyter-wrapper .cm-s-abcdef span.cm-number{color:violet}.jupyter-wrapper .cm-s-abcdef span.cm-def{color:#fffabc}.jupyter-wrapper .cm-s-abcdef span.cm-variable{color:#abcdef}.jupyter-wrapper .cm-s-abcdef span.cm-variable-2{color:#cacbcc}.jupyter-wrapper .cm-s-abcdef span.cm-variable-3,.jupyter-wrapper .cm-s-abcdef span.cm-type{color:#def}.jupyter-wrapper .cm-s-abcdef span.cm-property{color:#fedcba}.jupyter-wrapper .cm-s-abcdef span.cm-operator{color:#ff0}.jupyter-wrapper .cm-s-abcdef span.cm-comment{color:#7a7b7c;font-style:italic}.jupyter-wrapper .cm-s-abcdef span.cm-string{color:#2b4}.jupyter-wrapper .cm-s-abcdef span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-abcdef span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-abcdef span.cm-builtin{color:#30aabc}.jupyter-wrapper .cm-s-abcdef span.cm-bracket{color:#8a8a8a}.jupyter-wrapper .cm-s-abcdef span.cm-tag{color:#fd4}.jupyter-wrapper .cm-s-abcdef span.cm-attribute{color:#df0}.jupyter-wrapper .cm-s-abcdef span.cm-error{color:red}.jupyter-wrapper .cm-s-abcdef span.cm-header{color:#7fffd4;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-link{color:#8a2be2}.jupyter-wrapper .cm-s-abcdef .CodeMirror-activeline-background{background:#314151}.jupyter-wrapper .cm-s-base16-light.CodeMirror{background:#f5f5f5;color:#202020}.jupyter-wrapper .cm-s-base16-light div.CodeMirror-selected{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::-moz-selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-gutters{background:#f5f5f5;border-right:0px}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker-subtle{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-linenumber{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-cursor{border-left:1px solid #505050}.jupyter-wrapper .cm-s-base16-light span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-light span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-property,.jupyter-wrapper .cm-s-base16-light span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-light span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-light span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-light span.cm-bracket{color:#202020}.jupyter-wrapper .cm-s-base16-light span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-error{background:#ac4142;color:#505050}.jupyter-wrapper .cm-s-base16-light .CodeMirror-activeline-background{background:#dddcdc}.jupyter-wrapper .cm-s-base16-light .CodeMirror-matchingbracket{color:#f5f5f5 !important;background-color:#6a9fb5 !important}.jupyter-wrapper .cm-s-base16-dark.CodeMirror{background:#151515;color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark div.CodeMirror-selected{background:#303030}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-gutters{background:#151515;border-right:0px}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker-subtle{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-linenumber{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-cursor{border-left:1px solid #b0b0b0}.jupyter-wrapper .cm-s-base16-dark span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-dark span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-property,.jupyter-wrapper .cm-s-base16-dark span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-dark span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-dark span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-dark span.cm-bracket{color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-error{background:#ac4142;color:#b0b0b0}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-activeline-background{background:#202020}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-dracula.CodeMirror,.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{background-color:#282a36 !important;color:#f8f8f2 !important;border:none}.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{color:#282a36}.jupyter-wrapper .cm-s-dracula .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-dracula .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-dracula .CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula span.cm-comment{color:#6272a4}.jupyter-wrapper .cm-s-dracula span.cm-string,.jupyter-wrapper .cm-s-dracula span.cm-string-2{color:#f1fa8c}.jupyter-wrapper .cm-s-dracula span.cm-number{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-variable{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-2{color:#fff}.jupyter-wrapper .cm-s-dracula span.cm-def{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-operator{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-atom{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-meta{color:#f8f8f2}.jupyter-wrapper .cm-s-dracula span.cm-tag{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-attribute{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-qualifier{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-property{color:#66d9ef}.jupyter-wrapper .cm-s-dracula span.cm-builtin{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-3,.jupyter-wrapper .cm-s-dracula span.cm-type{color:#ffb86c}.jupyter-wrapper .cm-s-dracula .CodeMirror-activeline-background{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch.CodeMirror{background:#322931;color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch div.CodeMirror-selected{background:#433b42 !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-gutters{background:#322931;border-right:0px}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-linenumber{color:#797379}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-cursor{border-left:1px solid #989498 !important}.jupyter-wrapper .cm-s-hopscotch span.cm-comment{color:#b33508}.jupyter-wrapper .cm-s-hopscotch span.cm-atom{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-number{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-property,.jupyter-wrapper .cm-s-hopscotch span.cm-attribute{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-keyword{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-string{color:#fdcc59}.jupyter-wrapper .cm-s-hopscotch span.cm-variable{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-variable-2{color:#1290bf}.jupyter-wrapper .cm-s-hopscotch span.cm-def{color:#fd8b19}.jupyter-wrapper .cm-s-hopscotch span.cm-error{background:#dd464c;color:#989498}.jupyter-wrapper .cm-s-hopscotch span.cm-bracket{color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch span.cm-tag{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-link{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-activeline-background{background:#302020}.jupyter-wrapper .cm-s-mbo.CodeMirror{background:#2c2c2c;color:#ffffec}.jupyter-wrapper .cm-s-mbo div.CodeMirror-selected{background:#716c62}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::-moz-selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-gutters{background:#4e4e4e;border-right:0px}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker{color:#fff}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker-subtle{color:gray}.jupyter-wrapper .cm-s-mbo .CodeMirror-linenumber{color:#dadada}.jupyter-wrapper .cm-s-mbo .CodeMirror-cursor{border-left:1px solid #ffffec}.jupyter-wrapper .cm-s-mbo span.cm-comment{color:#95958a}.jupyter-wrapper .cm-s-mbo span.cm-atom{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-number{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-property,.jupyter-wrapper .cm-s-mbo span.cm-attribute{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-keyword{color:#ffb928}.jupyter-wrapper .cm-s-mbo span.cm-string{color:#ffcf6c}.jupyter-wrapper .cm-s-mbo span.cm-string.cm-property{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable-2{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-def{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-bracket{color:#fffffc;font-weight:bold}.jupyter-wrapper .cm-s-mbo span.cm-tag{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-link{color:#f54b07}.jupyter-wrapper .cm-s-mbo span.cm-error{border-bottom:#636363;color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-qualifier{color:#ffffec}.jupyter-wrapper .cm-s-mbo .CodeMirror-activeline-background{background:#494b41}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingbracket{color:#ffb928 !important}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingtag{background:rgba(255,255,255,.37)}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{color:#999;background-color:#fff}.jupyter-wrapper .cm-s-mdn-like div.CodeMirror-selected{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::-moz-selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-gutters{background:#f8f8f8;border-left:6px solid rgba(0,83,159,.65);color:#333}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-linenumber{color:#aaa;padding-left:8px}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-cursor{border-left:2px solid #222}.jupyter-wrapper .cm-s-mdn-like .cm-keyword{color:#6262ff}.jupyter-wrapper .cm-s-mdn-like .cm-atom{color:#f90}.jupyter-wrapper .cm-s-mdn-like .cm-number{color:#ca7841}.jupyter-wrapper .cm-s-mdn-like .cm-def{color:#8da6ce}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-2,.jupyter-wrapper .cm-s-mdn-like span.cm-tag{color:#690}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-3,.jupyter-wrapper .cm-s-mdn-like span.cm-def,.jupyter-wrapper .cm-s-mdn-like span.cm-type{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-variable{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-property{color:#905}.jupyter-wrapper .cm-s-mdn-like .cm-qualifier{color:#690}.jupyter-wrapper .cm-s-mdn-like .cm-operator{color:#cda869}.jupyter-wrapper .cm-s-mdn-like .cm-comment{color:#777;font-weight:normal}.jupyter-wrapper .cm-s-mdn-like .cm-string{color:#07a;font-style:italic}.jupyter-wrapper .cm-s-mdn-like .cm-string-2{color:#bd6b18}.jupyter-wrapper .cm-s-mdn-like .cm-meta{color:#000}.jupyter-wrapper .cm-s-mdn-like .cm-builtin{color:#9b7536}.jupyter-wrapper .cm-s-mdn-like .cm-tag{color:#997643}.jupyter-wrapper .cm-s-mdn-like .cm-attribute{color:#d6bb6d}.jupyter-wrapper .cm-s-mdn-like .cm-header{color:#ff6400}.jupyter-wrapper .cm-s-mdn-like .cm-hr{color:#aeaeae}.jupyter-wrapper .cm-s-mdn-like .cm-link{color:#ad9361;font-style:italic;text-decoration:none}.jupyter-wrapper .cm-s-mdn-like .cm-error{border-bottom:1px solid red}.jupyter-wrapper div.cm-s-mdn-like .CodeMirror-activeline-background{background:#efefff}.jupyter-wrapper div.cm-s-mdn-like span.CodeMirror-matchingbracket{outline:1px solid gray;color:inherit}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{background-image:url(data:image/png;base64,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)}.jupyter-wrapper .cm-s-seti.CodeMirror{background-color:#151718 !important;color:#cfd2d1 !important;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-gutters{color:#404b53;background-color:#0e1112;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-seti .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-seti.CodeMirror-focused div.CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti span.cm-comment{color:#41535b}.jupyter-wrapper .cm-s-seti span.cm-string,.jupyter-wrapper .cm-s-seti span.cm-string-2{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-number{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-variable{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-variable-2{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-def{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-seti span.cm-operator{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#e6cd69}.jupyter-wrapper .cm-s-seti span.cm-atom{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-meta{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-tag{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-attribute{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-qualifier{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-property{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-variable-3,.jupyter-wrapper .cm-s-seti span.cm-type{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-builtin{color:#9fca56}.jupyter-wrapper .cm-s-seti .CodeMirror-activeline-background{background:#101213}.jupyter-wrapper .cm-s-seti .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .solarized.base03{color:#002b36}.jupyter-wrapper .solarized.base02{color:#073642}.jupyter-wrapper .solarized.base01{color:#586e75}.jupyter-wrapper .solarized.base00{color:#657b83}.jupyter-wrapper .solarized.base0{color:#839496}.jupyter-wrapper .solarized.base1{color:#93a1a1}.jupyter-wrapper .solarized.base2{color:#eee8d5}.jupyter-wrapper .solarized.base3{color:#fdf6e3}.jupyter-wrapper .solarized.solar-yellow{color:#b58900}.jupyter-wrapper .solarized.solar-orange{color:#cb4b16}.jupyter-wrapper .solarized.solar-red{color:#dc322f}.jupyter-wrapper .solarized.solar-magenta{color:#d33682}.jupyter-wrapper .solarized.solar-violet{color:#6c71c4}.jupyter-wrapper .solarized.solar-blue{color:#268bd2}.jupyter-wrapper .solarized.solar-cyan{color:#2aa198}.jupyter-wrapper .solarized.solar-green{color:#859900}.jupyter-wrapper .cm-s-solarized{line-height:1.45em;color-profile:sRGB;rendering-intent:auto}.jupyter-wrapper .cm-s-solarized.cm-s-dark{color:#839496;background-color:#002b36;text-shadow:#002b36 0 1px}.jupyter-wrapper .cm-s-solarized.cm-s-light{background-color:#fdf6e3;color:#657b83;text-shadow:#eee8d5 0 1px}.jupyter-wrapper .cm-s-solarized .CodeMirror-widget{text-shadow:none}.jupyter-wrapper .cm-s-solarized .cm-header{color:#586e75}.jupyter-wrapper .cm-s-solarized .cm-quote{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-keyword{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .cm-atom{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-number{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-def{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-variable{color:#839496}.jupyter-wrapper .cm-s-solarized .cm-variable-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-variable-3,.jupyter-wrapper .cm-s-solarized .cm-type{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-property{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-operator{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-comment{color:#586e75;font-style:italic}.jupyter-wrapper .cm-s-solarized .cm-string{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-string-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-meta{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-qualifier{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-builtin{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-bracket{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-matchingbracket{color:#859900}.jupyter-wrapper .cm-s-solarized .CodeMirror-nonmatchingbracket{color:#dc322f}.jupyter-wrapper .cm-s-solarized .cm-tag{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-attribute{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-hr{color:rgba(0,0,0,0);border-top:1px solid #586e75;display:block}.jupyter-wrapper .cm-s-solarized .cm-link{color:#93a1a1;cursor:pointer}.jupyter-wrapper .cm-s-solarized .cm-special{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-em{color:#999;text-decoration:underline;text-decoration-style:dotted}.jupyter-wrapper .cm-s-solarized .cm-error,.jupyter-wrapper .cm-s-solarized .cm-invalidchar{color:#586e75;border-bottom:1px dotted #dc322f}.jupyter-wrapper .cm-s-solarized.cm-s-dark div.CodeMirror-selected{background:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark.CodeMirror ::selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-light div.CodeMirror-selected{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span>span::selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span>span::-moz-selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.CodeMirror{-moz-box-shadow:inset 7px 0 12px -6px #000;-webkit-box-shadow:inset 7px 0 12px -6px #000;box-shadow:inset 7px 0 12px -6px #000}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutters{border-right:0}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-gutters{background-color:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-linenumber{color:#586e75;text-shadow:#021014 0 -1px}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-gutters{background-color:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-linenumber{color:#839496}.jupyter-wrapper .cm-s-solarized .CodeMirror-linenumber{padding:0 5px}.jupyter-wrapper .cm-s-solarized .CodeMirror-guttermarker-subtle{color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker{color:#ddd}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-guttermarker{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text{color:#586e75}.jupyter-wrapper .cm-s-solarized .CodeMirror-cursor{border-left:1px solid #819090}.jupyter-wrapper .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor{background:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-light .cm-animate-fat-cursor{background-color:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor{background:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor{background-color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background{background:rgba(255,255,255,.06)}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-activeline-background{background:rgba(0,0,0,.06)}.jupyter-wrapper .cm-s-the-matrix.CodeMirror{background:#000;color:lime}.jupyter-wrapper .cm-s-the-matrix div.CodeMirror-selected{background:#2d2d2d}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::-moz-selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-gutters{background:#060;border-right:2px solid lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker{color:lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker-subtle{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-the-matrix span.cm-keyword{color:#008803;font-weight:bold}.jupyter-wrapper .cm-s-the-matrix span.cm-atom{color:#3ff}.jupyter-wrapper .cm-s-the-matrix span.cm-number{color:#ffb94f}.jupyter-wrapper .cm-s-the-matrix span.cm-def{color:#99c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable{color:#f6c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-2{color:#c6f}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-3,.jupyter-wrapper .cm-s-the-matrix span.cm-type{color:#96f}.jupyter-wrapper .cm-s-the-matrix span.cm-property{color:#62ffa0}.jupyter-wrapper .cm-s-the-matrix span.cm-operator{color:#999}.jupyter-wrapper .cm-s-the-matrix span.cm-comment{color:#ccc}.jupyter-wrapper .cm-s-the-matrix span.cm-string{color:#39c}.jupyter-wrapper .cm-s-the-matrix span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-the-matrix span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-builtin{color:#30a}.jupyter-wrapper .cm-s-the-matrix span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-the-matrix span.cm-tag{color:#ffbd40}.jupyter-wrapper .cm-s-the-matrix span.cm-attribute{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-error{color:red}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-activeline-background{background:#040}.jupyter-wrapper .cm-s-xq-light span.cm-keyword{line-height:1em;font-weight:bold;color:#5a5cad}.jupyter-wrapper .cm-s-xq-light span.cm-atom{color:#6c8cd5}.jupyter-wrapper .cm-s-xq-light span.cm-number{color:#164}.jupyter-wrapper .cm-s-xq-light span.cm-def{text-decoration:underline}.jupyter-wrapper .cm-s-xq-light span.cm-variable{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-2{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-3,.jupyter-wrapper .cm-s-xq-light span.cm-type{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-comment{color:#0080ff;font-style:italic}.jupyter-wrapper .cm-s-xq-light span.cm-string{color:red}.jupyter-wrapper .cm-s-xq-light span.cm-meta{color:#ff0}.jupyter-wrapper .cm-s-xq-light span.cm-qualifier{color:gray}.jupyter-wrapper .cm-s-xq-light span.cm-builtin{color:#7ea656}.jupyter-wrapper .cm-s-xq-light span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-xq-light span.cm-tag{color:#3f7f7f}.jupyter-wrapper .cm-s-xq-light span.cm-attribute{color:#7f007f}.jupyter-wrapper .cm-s-xq-light span.cm-error{color:red}.jupyter-wrapper .cm-s-xq-light .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .cm-s-xq-light .CodeMirror-matchingbracket{outline:1px solid gray;color:#000 !important;background:#ff0}.jupyter-wrapper .CodeMirror{line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);border:0;border-radius:0;height:auto}.jupyter-wrapper .CodeMirror pre{padding:0 var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-dialog{background-color:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .CodeMirror-lines{padding:var(--jp-code-padding) 0}.jupyter-wrapper .CodeMirror-linenumber{padding:0 8px}.jupyter-wrapper .jp-CodeMirrorEditor-static{margin:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor,.jupyter-wrapper .jp-CodeMirrorEditor-static{cursor:text}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}@media screen and (min-width: 2138px)and (max-width: 4319px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color)}}@media screen and (min-width: 4320px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color)}}.jupyter-wrapper .CodeMirror.jp-mod-readOnly .CodeMirror-cursor{display:none}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid var(--jp-border-color2);background-color:var(--jp-layout-color0)}.jupyter-wrapper .jp-CollaboratorCursor{border-left:5px solid rgba(0,0,0,0);border-right:5px solid rgba(0,0,0,0);border-top:none;border-bottom:3px solid;background-clip:content-box;margin-left:-5px;margin-right:-5px}.jupyter-wrapper .CodeMirror-selectedtext.cm-searching{background-color:var(--jp-search-selected-match-background-color) !important;color:var(--jp-search-selected-match-color) !important}.jupyter-wrapper .cm-searching{background-color:var(--jp-search-unselected-match-background-color) !important;color:var(--jp-search-unselected-match-color) !important}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background-color:var(--jp-editor-selected-focused-background)}.jupyter-wrapper .CodeMirror-selected{background-color:var(--jp-editor-selected-background)}.jupyter-wrapper .jp-CollaboratorCursor-hover{position:absolute;z-index:1;transform:translateX(-50%);color:#fff;border-radius:3px;padding-left:4px;padding-right:4px;padding-top:1px;padding-bottom:1px;text-align:center;font-size:var(--jp-ui-font-size1);white-space:nowrap}.jupyter-wrapper .jp-CodeMirror-ruler{border-left:1px dashed var(--jp-border-color2)}.jupyter-wrapper .CodeMirror.cm-s-jupyter{background:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .jp-CodeConsole .CodeMirror.cm-s-jupyter,.jupyter-wrapper .jp-Notebook .CodeMirror.cm-s-jupyter{background:rgba(0,0,0,0)}.jupyter-wrapper .cm-s-jupyter .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}.jupyter-wrapper .cm-s-jupyter span.cm-keyword{color:var(--jp-mirror-editor-keyword-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-atom{color:var(--jp-mirror-editor-atom-color)}.jupyter-wrapper .cm-s-jupyter span.cm-number{color:var(--jp-mirror-editor-number-color)}.jupyter-wrapper .cm-s-jupyter span.cm-def{color:var(--jp-mirror-editor-def-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable{color:var(--jp-mirror-editor-variable-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-2{color:var(--jp-mirror-editor-variable-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-3{color:var(--jp-mirror-editor-variable-3-color)}.jupyter-wrapper .cm-s-jupyter span.cm-punctuation{color:var(--jp-mirror-editor-punctuation-color)}.jupyter-wrapper .cm-s-jupyter span.cm-property{color:var(--jp-mirror-editor-property-color)}.jupyter-wrapper .cm-s-jupyter span.cm-operator{color:var(--jp-mirror-editor-operator-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-comment{color:var(--jp-mirror-editor-comment-color);font-style:italic}.jupyter-wrapper .cm-s-jupyter span.cm-string{color:var(--jp-mirror-editor-string-color)}.jupyter-wrapper .cm-s-jupyter span.cm-string-2{color:var(--jp-mirror-editor-string-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-meta{color:var(--jp-mirror-editor-meta-color)}.jupyter-wrapper .cm-s-jupyter span.cm-qualifier{color:var(--jp-mirror-editor-qualifier-color)}.jupyter-wrapper .cm-s-jupyter span.cm-builtin{color:var(--jp-mirror-editor-builtin-color)}.jupyter-wrapper .cm-s-jupyter span.cm-bracket{color:var(--jp-mirror-editor-bracket-color)}.jupyter-wrapper .cm-s-jupyter span.cm-tag{color:var(--jp-mirror-editor-tag-color)}.jupyter-wrapper .cm-s-jupyter span.cm-attribute{color:var(--jp-mirror-editor-attribute-color)}.jupyter-wrapper .cm-s-jupyter span.cm-header{color:var(--jp-mirror-editor-header-color)}.jupyter-wrapper .cm-s-jupyter span.cm-quote{color:var(--jp-mirror-editor-quote-color)}.jupyter-wrapper .cm-s-jupyter span.cm-link{color:var(--jp-mirror-editor-link-color)}.jupyter-wrapper .cm-s-jupyter span.cm-error{color:var(--jp-mirror-editor-error-color)}.jupyter-wrapper .cm-s-jupyter span.cm-hr{color:#999}.jupyter-wrapper .cm-s-jupyter span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}.jupyter-wrapper .cm-s-jupyter .CodeMirror-activeline-background,.jupyter-wrapper .cm-s-jupyter .CodeMirror-gutter{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-RenderedLatex{color:var(--jp-content-font-color1);font-size:var(--jp-content-font-size1);line-height:var(--jp-content-line-height)}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedLatex{padding:var(--jp-code-padding);text-align:left}.jupyter-wrapper .jp-MimeDocument{outline:none}.jupyter-wrapper :root{--jp-private-filebrowser-button-height: 28px;--jp-private-filebrowser-button-width: 48px}.jupyter-wrapper .jp-FileBrowser{display:flex;flex-direction:column;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{border-bottom:none;height:auto;margin:var(--jp-toolbar-header-margin);box-shadow:none}.jupyter-wrapper .jp-BreadCrumbs{flex:0 0 auto;margin:4px 12px}.jupyter-wrapper .jp-BreadCrumbs-item{margin:0px 2px;padding:0px 2px;border-radius:var(--jp-border-radius);cursor:pointer}.jupyter-wrapper .jp-BreadCrumbs-item:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-BreadCrumbs-item:first-child{margin-left:0px}.jupyter-wrapper .jp-BreadCrumbs-item.jp-mod-dropTarget{background-color:var(--jp-brand-color2);opacity:.7}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{padding:0px}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{justify-content:space-evenly}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item{flex:1}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent{width:100%}.jupyter-wrapper .jp-DirListing{flex:1 1 auto;display:flex;flex-direction:column;outline:0}.jupyter-wrapper .jp-DirListing-header{flex:0 0 auto;display:flex;flex-direction:row;overflow:hidden;border-top:var(--jp-border-width) solid var(--jp-border-color2);border-bottom:var(--jp-border-width) solid var(--jp-border-color1);box-shadow:var(--jp-toolbar-box-shadow);z-index:2}.jupyter-wrapper .jp-DirListing-headerItem{padding:4px 12px 2px 12px;font-weight:500}.jupyter-wrapper .jp-DirListing-headerItem:hover{background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-name{flex:1 0 84px}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-modified{flex:0 0 112px;border-left:var(--jp-border-width) solid var(--jp-border-color2);text-align:right}.jupyter-wrapper .jp-DirListing-narrow .jp-id-modified,.jupyter-wrapper .jp-DirListing-narrow .jp-DirListing-itemModified{display:none}.jupyter-wrapper .jp-DirListing-headerItem.jp-mod-selected{font-weight:600}.jupyter-wrapper .jp-DirListing-content{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-DirListing.jp-mod-native-drop .jp-DirListing-content{outline:5px dashed rgba(128,128,128,.5);outline-offset:-10px;cursor:copy}.jupyter-wrapper .jp-DirListing-item{display:flex;flex-direction:row;padding:4px 12px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected{color:#fff;background:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-dropTarget{background:var(--jp-brand-color3)}.jupyter-wrapper .jp-DirListing-item:hover:not(.jp-mod-selected){background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-itemIcon{flex:0 0 20px;margin-right:4px}.jupyter-wrapper .jp-DirListing-itemText{flex:1 0 64px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;user-select:none}.jupyter-wrapper .jp-DirListing-itemModified{flex:0 0 125px;text-align:right}.jupyter-wrapper .jp-DirListing-editor{flex:1 0 64px;outline:none;border:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before{color:#32cd32;content:\"\u25cf\";font-size:8px;position:absolute;left:-8px}.jupyter-wrapper .jp-DirListing-item.lm-mod-drag-image,.jupyter-wrapper .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image{font-size:var(--jp-ui-font-size1);padding-left:4px;margin-left:4px;width:160px;background-color:var(--jp-ui-inverse-font-color2);box-shadow:var(--jp-elevation-z2);border-radius:0px;color:var(--jp-ui-font-color1);transform:translateX(-40%) translateY(-58%)}.jupyter-wrapper .jp-DirListing-deadSpace{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-Document{min-width:120px;min-height:120px;outline:none}.jupyter-wrapper .jp-FileDialog.jp-mod-conflict input{color:red}.jupyter-wrapper .jp-FileDialog .jp-new-name-title{margin-top:12px}.jupyter-wrapper .jp-OutputArea{overflow-y:auto}.jupyter-wrapper .jp-OutputArea-child{display:flex;flex-direction:row}.jupyter-wrapper .jp-OutputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-outprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-OutputArea-output{height:auto;overflow:auto;user-select:text;-moz-user-select:text;-webkit-user-select:text;-ms-user-select:text}.jupyter-wrapper .jp-OutputArea-child .jp-OutputArea-output{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-OutputArea-output.jp-mod-isolated{width:100%;display:block}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-OutputArea-output pre{border:none;margin:0px;padding:0px;overflow-x:auto;overflow-y:auto;word-break:break-all;word-wrap:break-word;white-space:pre-wrap}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedHTMLCommon table{margin-left:0;margin-right:0}.jupyter-wrapper .jp-OutputArea-output dl,.jupyter-wrapper .jp-OutputArea-output dt,.jupyter-wrapper .jp-OutputArea-output dd{display:block}.jupyter-wrapper .jp-OutputArea-output dl{width:100%;overflow:hidden;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dt{font-weight:bold;float:left;width:20%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dd{float:left;width:80%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt{display:none}.jupyter-wrapper .jp-OutputArea-output.jp-OutputArea-executeResult{margin-left:0px;flex:1 1 auto}.jupyter-wrapper .jp-OutputArea-executeResult.jp-RenderedText{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-OutputArea-stdin{line-height:var(--jp-code-line-height);padding-top:var(--jp-code-padding);display:flex}.jupyter-wrapper .jp-Stdin-prompt{color:var(--jp-content-font-color0);padding-right:var(--jp-code-padding);vertical-align:baseline;flex:0 0 auto}.jupyter-wrapper .jp-Stdin-input{font-family:var(--jp-code-font-family);font-size:inherit;color:inherit;background-color:inherit;width:42%;min-width:200px;vertical-align:baseline;padding:0em .25em;margin:0em .25em;flex:0 0 70%}.jupyter-wrapper .jp-Stdin-input:focus{box-shadow:none}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea{height:100%;display:block}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea-output:only-child{height:100%}.jupyter-wrapper .jp-Collapser{flex:0 0 var(--jp-cell-collapser-width);padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0);border-radius:var(--jp-border-radius);opacity:1}.jupyter-wrapper .jp-Collapser-child{display:block;width:100%;box-sizing:border-box;position:absolute;top:0px;bottom:0px}.jupyter-wrapper .jp-CellHeader,.jupyter-wrapper .jp-CellFooter{height:0px;width:100%;padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-InputArea{display:flex;flex-direction:row}.jupyter-wrapper .jp-InputArea-editor{flex:1 1 auto}.jupyter-wrapper .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);border-radius:0px;background:var(--jp-cell-editor-background)}.jupyter-wrapper .jp-InputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-inprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);opacity:var(--jp-cell-prompt-opacity);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-Placeholder{display:flex;flex-direction:row;flex:1 1 auto}.jupyter-wrapper .jp-Placeholder-prompt{box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content{flex:1 1 auto;border:none;background:rgba(0,0,0,0);height:20px;box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon{width:32px;height:16px;border:1px solid rgba(0,0,0,0);border-radius:var(--jp-border-radius)}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon:hover{border:1px solid var(--jp-border-color1);box-shadow:0px 0px 2px 0px rgba(0,0,0,.25);background-color:var(--jp-layout-color0)}.jupyter-wrapper :root{--jp-private-cell-scrolling-output-offset: 5px}.jupyter-wrapper .jp-Cell{padding:var(--jp-cell-padding);margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Cell-inputWrapper,.jupyter-wrapper .jp-Cell-outputWrapper{display:flex;flex-direction:row;padding:0px;margin:0px;overflow:visible}.jupyter-wrapper .jp-Cell-inputArea,.jupyter-wrapper .jp-Cell-outputArea{flex:1 1 auto}.jupyter-wrapper .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser{border:none !important;background:rgba(0,0,0,0) !important}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser{min-height:var(--jp-cell-collapser-min-height)}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper{margin-top:5px}.jupyter-wrapper .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea{overflow-y:auto;max-height:200px;box-shadow:inset 0 0 6px 2px rgba(0,0,0,.3);margin-left:var(--jp-private-cell-scrolling-output-offset)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt{flex:0 0 calc(var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset))}.jupyter-wrapper .jp-MarkdownOutput{flex:1 1 auto;margin-top:0;margin-bottom:0;padding-left:var(--jp-code-padding)}.jupyter-wrapper .jp-MarkdownOutput.jp-RenderedHTMLCommon{overflow:auto}.jupyter-wrapper .jp-NotebookPanel-toolbar{padding:2px}.jupyter-wrapper .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused{border:none;box-shadow:none}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown select{height:24px;font-size:var(--jp-ui-font-size1);line-height:14px;border-radius:0;display:block}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown span{top:5px !important}.jupyter-wrapper :root{--jp-private-notebook-dragImage-width: 304px;--jp-private-notebook-dragImage-height: 36px;--jp-private-notebook-selected-color: var(--md-blue-400);--jp-private-notebook-active-color: var(--md-green-400)}.jupyter-wrapper .jp-NotebookPanel{display:block;height:100%}.jupyter-wrapper .jp-NotebookPanel.jp-Document{min-width:240px;min-height:120px}.jupyter-wrapper .jp-Notebook{padding:var(--jp-notebook-padding);outline:none;overflow:auto;background:var(--jp-layout-color0)}.jupyter-wrapper .jp-Notebook.jp-mod-scrollPastEnd::after{display:block;content:\"\";min-height:var(--jp-notebook-scroll-padding)}.jupyter-wrapper .jp-Notebook .jp-Cell{overflow:visible}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-InputPrompt{cursor:move}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser{background:var(--jp-brand-color1)}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-Collapser:hover{box-shadow:var(--jp-elevation-z2);background:var(--jp-brand-color1);opacity:var(--jp-cell-collapser-not-active-hover-opacity)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover{background:var(--jp-brand-color0);opacity:1}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected{background:var(--jp-notebook-multiselected-color)}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected){background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-cell-editor-active-background)}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropSource{opacity:.5}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropTarget,.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget{border-top-color:var(--jp-private-notebook-selected-color);border-top-style:solid;border-top-width:2px}.jupyter-wrapper .jp-dragImage{display:flex;flex-direction:row;width:var(--jp-private-notebook-dragImage-width);height:var(--jp-private-notebook-dragImage-height);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background);overflow:visible}.jupyter-wrapper .jp-dragImage-singlePrompt{box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-dragImage .jp-dragImage-content{flex:1 1 auto;z-index:2;font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);line-height:var(--jp-code-line-height);padding:var(--jp-code-padding);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background-color);color:var(--jp-content-font-color3);text-align:left;margin:4px 4px 4px 0px}.jupyter-wrapper .jp-dragImage .jp-dragImage-prompt{flex:0 0 auto;min-width:36px;color:var(--jp-cell-inprompt-font-color);padding:var(--jp-code-padding);padding-left:12px;font-family:var(--jp-cell-prompt-font-family);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:1.9;font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0)}.jupyter-wrapper .jp-dragImage-multipleBack{z-index:-1;position:absolute;height:32px;width:300px;top:8px;left:8px;background:var(--jp-layout-color2);border:var(--jp-border-width) solid var(--jp-input-border-color);box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-NotebookTools{display:block;min-width:var(--jp-sidebar-min-width);color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1);overflow:auto}.jupyter-wrapper .jp-NotebookTools-tool{padding:0px 12px 0 12px}.jupyter-wrapper .jp-ActiveCellTool{padding:12px;background-color:var(--jp-layout-color1);border-top:none !important}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-prompt{flex:0 0 auto;padding-left:0px}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor{flex:1 1 auto;background:var(--jp-cell-editor-background);border-color:var(--jp-cell-editor-border-color)}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror{background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MetadataEditorTool{flex-direction:column;padding:12px 0px 12px 0px}.jupyter-wrapper .jp-RankedPanel>:not(:first-child){margin-top:12px}.jupyter-wrapper .jp-KeySelector select.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:var(--jp-border-width) solid var(--jp-border-color1)}.jupyter-wrapper .jp-KeySelector label,.jupyter-wrapper .jp-MetadataEditorTool label{line-height:1.4}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook{--jp-content-font-size1: var(--jp-content-presentation-font-size1);--jp-code-font-size: var(--jp-code-presentation-font-size)}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt{flex:0 0 110px}.jupyter-wrapper .md-typeset__scrollwrap{margin:0}.jupyter-wrapper .jp-MarkdownOutput{padding:0}.jupyter-wrapper h1 .anchor-link,.jupyter-wrapper h2 .anchor-link,.jupyter-wrapper h3 .anchor-link,.jupyter-wrapper h4 .anchor-link,.jupyter-wrapper h5 .anchor-link,.jupyter-wrapper h6 .anchor-link{display:none;margin-left:.5rem;color:var(--md-default-fg-color--lighter)}.jupyter-wrapper h1 .anchor-link:hover,.jupyter-wrapper h2 .anchor-link:hover,.jupyter-wrapper h3 .anchor-link:hover,.jupyter-wrapper h4 .anchor-link:hover,.jupyter-wrapper h5 .anchor-link:hover,.jupyter-wrapper h6 .anchor-link:hover{text-decoration:none;color:var(--md-accent-fg-color)}.jupyter-wrapper h1:hover .anchor-link,.jupyter-wrapper h2:hover .anchor-link,.jupyter-wrapper h3:hover .anchor-link,.jupyter-wrapper h4:hover .anchor-link,.jupyter-wrapper h5:hover .anchor-link,.jupyter-wrapper h6:hover .anchor-link{display:inline-block}.jupyter-wrapper .jp-InputArea{width:100%}.jupyter-wrapper .jp-Cell-inputArea{width:100%}.jupyter-wrapper .jp-RenderedHTMLCommon{width:100%}.jupyter-wrapper .jp-Cell-inputWrapper .jp-InputPrompt{display:none}.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt{display:block}.jupyter-wrapper .highlight pre{overflow:auto}.jupyter-wrapper .celltoolbar{border:none;background:#eee;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;box-pack:end;justify-content:flex-start;display:-webkit-flex}.jupyter-wrapper .celltoolbar .tags_button_container{display:flex}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container{display:flex;flex-direction:row;flex-grow:1;overflow:hidden;position:relative}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;box-shadow:none;width:inherit;font-size:11px;font-family:\"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace;height:22px;display:inline-block}.jupyter-wrapper .jp-InputArea-editor{width:1px}.jupyter-wrapper .jp-InputPrompt{overflow:unset}.jupyter-wrapper .jp-OutputPrompt{overflow:unset}.jupyter-wrapper .jp-RenderedText{font-size:var(--jp-code-font-size)}.jupyter-wrapper .highlight-ipynb{overflow:auto}.jupyter-wrapper .highlight-ipynb pre{margin:0;padding:5px 10px}.jupyter-wrapper table{width:max-content}.jupyter-wrapper table.dataframe{margin-left:auto;margin-right:auto;border:none;border-collapse:collapse;border-spacing:0;color:#000;font-size:12px;table-layout:fixed}.jupyter-wrapper table.dataframe thead{border-bottom:1px solid #000;vertical-align:bottom}.jupyter-wrapper table.dataframe tr,.jupyter-wrapper table.dataframe th,.jupyter-wrapper table.dataframe td{text-align:right;vertical-align:middle;padding:.5em .5em;line-height:normal;white-space:normal;max-width:none;border:none}.jupyter-wrapper table.dataframe th{font-weight:bold}.jupyter-wrapper table.dataframe tbody tr:nth-child(odd){background:#f5f5f5}.jupyter-wrapper table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}.jupyter-wrapper *+table{margin-top:1em}.jupyter-wrapper .jp-InputArea-editor{position:relative}.jupyter-wrapper .zeroclipboard-container{position:absolute;top:-3px;right:0;z-index:1000}.jupyter-wrapper .zeroclipboard-container clipboard-copy{-webkit-appearance:button;-moz-appearance:button;padding:7px 5px;font:11px system-ui,sans-serif;display:inline-block;cursor:default}.jupyter-wrapper .zeroclipboard-container .clipboard-copy-icon{padding:4px 4px 2px;color:#57606a;vertical-align:text-bottom}.jupyter-wrapper .clipboard-copy-txt{display:none}[data-md-color-scheme=slate] .clipboard-copy-icon{color:#fff !important}[data-md-color-scheme=slate] table.dataframe{color:#e9ebfc}[data-md-color-scheme=slate] table.dataframe thead{border-bottom:1px solid rgba(233,235,252,.12)}[data-md-color-scheme=slate] table.dataframe tbody tr:nth-child(odd){background:#222}[data-md-color-scheme=slate] table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}table{width:max-content} /*# sourceMappingURL=mkdocs-jupyter.css.map*/ init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Using the json type \u00b6 \u26a0\ufe0f Note the following before using the json type Supported only for MySQL >= 8.0 when JSON_VALUE introduced. Equivalent Percona is fully-compatible. MariaDB is not supported since JSON_VALUE does not allow type specification like MySQL's. Not yet supported in DataJoint MATLAB First you will need to install and connect to a DataJoint data pipeline . Now let's start by importing the datajoint client. In [1]: Copied! import datajoint as dj import datajoint as dj Table Definition \u00b6 For this exercise, let's imagine we work for an awesome company that is organizing a fun RC car race across various teams in the company. Let's see which team has the fastest car! \ud83c\udfce\ufe0f This establishes 2 important entities: a Team and a Car . Normally we'd map this to their own dedicated table, however, let's assume that Team is well-structured but Car is less structured then we'd prefer. In other words, the structure for what makes up a car is varing too much between entries (perhaps because users of the pipeline haven't agreed yet on the definition? \ud83e\udd37). This would make it a good use-case to keep Team as a table but make Car actually a json type defined within the Team table. Let's begin. In [2]: Copied! schema = dj . Schema ( f \" { dj . config [ 'database.user' ] } _json\" ) schema = dj.Schema(f\"{dj.config['database.user']}_json\") [2023-02-12 00:14:33,027][INFO]: Connecting root@fakeservices.datajoint.io:3306 [2023-02-12 00:14:33,039][INFO]: Connected root@fakeservices.datajoint.io:3306 In [3]: Copied! @schema class Team ( dj . Lookup ): definition = \"\"\" # A team within a company name: varchar(40) # team name --- car=null: json # A car belonging to a team (null to allow registering first but specifying car later) unique index(car.length:decimal(4, 1)) # Add an index if this key is frequently accessed \"\"\" @schema class Team(dj.Lookup): definition = \"\"\" # A team within a company name: varchar(40) # team name --- car=null: json # A car belonging to a team (null to allow registering first but specifying car later) unique index(car.length:decimal(4, 1)) # Add an index if this key is frequently accessed \"\"\" Insert \u00b6 Let's suppose that engineering is first up to register their car. In [4]: Copied! Team . insert1 ( { \"name\" : \"engineering\" , \"car\" : { \"name\" : \"Rever\" , \"length\" : 20.5 , \"inspected\" : True , \"tire_pressure\" : [ 32 , 31 , 33 , 34 ], \"headlights\" : [ { \"side\" : \"left\" , \"hyper_white\" : None , }, { \"side\" : \"right\" , \"hyper_white\" : None , }, ], }, } ) Team.insert1( { \"name\": \"engineering\", \"car\": { \"name\": \"Rever\", \"length\": 20.5, \"inspected\": True, \"tire_pressure\": [32, 31, 33, 34], \"headlights\": [ { \"side\": \"left\", \"hyper_white\": None, }, { \"side\": \"right\", \"hyper_white\": None, }, ], }, } ) Next, business and marketing teams are up and register their cars. A few points to notice below: The person signing up on behalf of marketing does not know the specifics of the car during registration but another team member will be updating this soon before the race. Notice how the business and engineering teams appear to specify the same property but refer to it as safety_inspected and inspected respectfully. In [5]: Copied! Team . insert ( [ { \"name\" : \"marketing\" , \"car\" : None , }, { \"name\" : \"business\" , \"car\" : { \"name\" : \"Chaching\" , \"length\" : 100 , \"safety_inspected\" : False , \"tire_pressure\" : [ 34 , 30 , 27 , 32 ], \"headlights\" : [ { \"side\" : \"left\" , \"hyper_white\" : True , }, { \"side\" : \"right\" , \"hyper_white\" : True , }, ], }, }, ] ) Team.insert( [ { \"name\": \"marketing\", \"car\": None, }, { \"name\": \"business\", \"car\": { \"name\": \"Chaching\", \"length\": 100, \"safety_inspected\": False, \"tire_pressure\": [34, 30, 27, 32], \"headlights\": [ { \"side\": \"left\", \"hyper_white\": True, }, { \"side\": \"right\", \"hyper_white\": True, }, ], }, }, ] ) We can preview the table data much like normal but notice how the value of car behaves like other BLOB-like attributes. In [6]: Copied! Team () Team() Out[6]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) marketing =BLOB= engineering =BLOB= business =BLOB= Total: 3 Restriction \u00b6 Now let's see what kinds of queries we can form to demostrate how we can query this pipeline. In [7]: Copied! # Which team has a `car` equal to 100 inches long? Team & { 'car.length' : 100 } # Which team has a `car` equal to 100 inches long? Team & {'car.length': 100} Out[7]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) business =BLOB= Total: 1 In [8]: Copied! # Which team has a `car` less than 50 inches long? Team & \"car->>'$.length' < 50\" # Which team has a `car` less than 50 inches long? Team & \"car->>'$.length' < 50\" Out[8]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= Total: 1 In [9]: Copied! # Any team that has had their car inspected? Team & [{ 'car.inspected:unsigned' : True }, { 'car.safety_inspected:unsigned' : True }] # Any team that has had their car inspected? Team & [{'car.inspected:unsigned': True}, {'car.safety_inspected:unsigned': True}] Out[9]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= Total: 1 In [10]: Copied! # Which teams do not have hyper white lights for their first head light? Team & { \"car.headlights[0].hyper_white\" : None } # Which teams do not have hyper white lights for their first head light? Team & {\"car.headlights[0].hyper_white\": None} Out[10]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= marketing =BLOB= Total: 2 Notice that the previous query will satisfy the None check if it experiences any of the following scenarious: if entire record missing ( marketing satisfies this) JSON key is missing JSON value is set to JSON null ( engineering satisfies this) Projection \u00b6 Projections can be quite useful with the json type since we can extract out just what we need. This allows greater query flexibility but more importantly, for us to be able to fetch only what is pertinent. In [11]: Copied! # Only interested in the car names and the length but let the type be inferred q_untyped = Team . proj ( car_name = 'car.name' , car_length = \"car.length\" , ) q_untyped # Only interested in the car names and the length but let the type be inferred q_untyped = Team.proj( car_name='car.name', car_length=\"car.length\", ) q_untyped Out[11]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } name team name car_name calculated attribute car_length calculated attribute business Chaching 100 engineering Rever 20.5 marketing None None Total: 3 In [12]: Copied! q_untyped . fetch ( as_dict = True ) q_untyped.fetch(as_dict=True) Out[12]: [{'name': 'business', 'car_name': 'Chaching', 'car_length': '100'}, {'name': 'engineering', 'car_name': 'Rever', 'car_length': '20.5'}, {'name': 'marketing', 'car_name': None, 'car_length': None}] In [13]: Copied! # Nevermind, I'll specify the type explicitly q_typed = Team . proj ( car_name = 'car.name' , car_length = \"car.length:float\" , ) q_typed # Nevermind, I'll specify the type explicitly q_typed = Team.proj( car_name='car.name', car_length=\"car.length:float\", ) q_typed Out[13]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } name team name car_name calculated attribute car_length calculated attribute business Chaching 100.0 engineering Rever 20.5 marketing None None Total: 3 In [14]: Copied! q_typed . fetch ( as_dict = True ) q_typed.fetch(as_dict=True) Out[14]: [{'name': 'business', 'car_name': 'Chaching', 'car_length': 100.0}, {'name': 'engineering', 'car_name': 'Rever', 'car_length': 20.5}, {'name': 'marketing', 'car_name': None, 'car_length': None}] Describe \u00b6 Lastly, the .describe() function on the Team table can help us generate the table's definition. This is useful if we are connected directly to the pipeline without the original source. In [16]: Copied! rebuilt_definition = Team . describe () print ( rebuilt_definition ) rebuilt_definition = Team.describe() print(rebuilt_definition) # A team within a company name : varchar(40) # team name --- car=null : json # A car belonging to a team (null to allow registering first but specifying car later) UNIQUE INDEX ((json_value(`car`, _utf8mb4'$.length' returning decimal(4, 1)))) Cleanup \u00b6 Finally, let's clean up what we created in this tutorial. In [17]: Copied! schema . drop () schema.drop() In [ ]: Copied!", "title": "Using the json type"}, {"location": "tutorials/json/#using-the-json-type", "text": "\u26a0\ufe0f Note the following before using the json type Supported only for MySQL >= 8.0 when JSON_VALUE introduced. Equivalent Percona is fully-compatible. MariaDB is not supported since JSON_VALUE does not allow type specification like MySQL's. Not yet supported in DataJoint MATLAB First you will need to install and connect to a DataJoint data pipeline . Now let's start by importing the datajoint client. In [1]: Copied! import datajoint as dj import datajoint as dj", "title": "Using the json type"}, {"location": "tutorials/json/#table-definition", "text": "For this exercise, let's imagine we work for an awesome company that is organizing a fun RC car race across various teams in the company. Let's see which team has the fastest car! \ud83c\udfce\ufe0f This establishes 2 important entities: a Team and a Car . Normally we'd map this to their own dedicated table, however, let's assume that Team is well-structured but Car is less structured then we'd prefer. In other words, the structure for what makes up a car is varing too much between entries (perhaps because users of the pipeline haven't agreed yet on the definition? \ud83e\udd37). This would make it a good use-case to keep Team as a table but make Car actually a json type defined within the Team table. Let's begin. In [2]: Copied! schema = dj . Schema ( f \" { dj . config [ 'database.user' ] } _json\" ) schema = dj.Schema(f\"{dj.config['database.user']}_json\") [2023-02-12 00:14:33,027][INFO]: Connecting root@fakeservices.datajoint.io:3306 [2023-02-12 00:14:33,039][INFO]: Connected root@fakeservices.datajoint.io:3306 In [3]: Copied! @schema class Team ( dj . Lookup ): definition = \"\"\" # A team within a company name: varchar(40) # team name --- car=null: json # A car belonging to a team (null to allow registering first but specifying car later) unique index(car.length:decimal(4, 1)) # Add an index if this key is frequently accessed \"\"\" @schema class Team(dj.Lookup): definition = \"\"\" # A team within a company name: varchar(40) # team name --- car=null: json # A car belonging to a team (null to allow registering first but specifying car later) unique index(car.length:decimal(4, 1)) # Add an index if this key is frequently accessed \"\"\"", "title": "Table Definition"}, {"location": "tutorials/json/#insert", "text": "Let's suppose that engineering is first up to register their car. In [4]: Copied! Team . insert1 ( { \"name\" : \"engineering\" , \"car\" : { \"name\" : \"Rever\" , \"length\" : 20.5 , \"inspected\" : True , \"tire_pressure\" : [ 32 , 31 , 33 , 34 ], \"headlights\" : [ { \"side\" : \"left\" , \"hyper_white\" : None , }, { \"side\" : \"right\" , \"hyper_white\" : None , }, ], }, } ) Team.insert1( { \"name\": \"engineering\", \"car\": { \"name\": \"Rever\", \"length\": 20.5, \"inspected\": True, \"tire_pressure\": [32, 31, 33, 34], \"headlights\": [ { \"side\": \"left\", \"hyper_white\": None, }, { \"side\": \"right\", \"hyper_white\": None, }, ], }, } ) Next, business and marketing teams are up and register their cars. A few points to notice below: The person signing up on behalf of marketing does not know the specifics of the car during registration but another team member will be updating this soon before the race. Notice how the business and engineering teams appear to specify the same property but refer to it as safety_inspected and inspected respectfully. In [5]: Copied! Team . insert ( [ { \"name\" : \"marketing\" , \"car\" : None , }, { \"name\" : \"business\" , \"car\" : { \"name\" : \"Chaching\" , \"length\" : 100 , \"safety_inspected\" : False , \"tire_pressure\" : [ 34 , 30 , 27 , 32 ], \"headlights\" : [ { \"side\" : \"left\" , \"hyper_white\" : True , }, { \"side\" : \"right\" , \"hyper_white\" : True , }, ], }, }, ] ) Team.insert( [ { \"name\": \"marketing\", \"car\": None, }, { \"name\": \"business\", \"car\": { \"name\": \"Chaching\", \"length\": 100, \"safety_inspected\": False, \"tire_pressure\": [34, 30, 27, 32], \"headlights\": [ { \"side\": \"left\", \"hyper_white\": True, }, { \"side\": \"right\", \"hyper_white\": True, }, ], }, }, ] ) We can preview the table data much like normal but notice how the value of car behaves like other BLOB-like attributes. In [6]: Copied! Team () Team() Out[6]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) marketing =BLOB= engineering =BLOB= business =BLOB= Total: 3", "title": "Insert"}, {"location": "tutorials/json/#restriction", "text": "Now let's see what kinds of queries we can form to demostrate how we can query this pipeline. In [7]: Copied! # Which team has a `car` equal to 100 inches long? Team & { 'car.length' : 100 } # Which team has a `car` equal to 100 inches long? Team & {'car.length': 100} Out[7]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) business =BLOB= Total: 1 In [8]: Copied! # Which team has a `car` less than 50 inches long? Team & \"car->>'$.length' < 50\" # Which team has a `car` less than 50 inches long? Team & \"car->>'$.length' < 50\" Out[8]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= Total: 1 In [9]: Copied! # Any team that has had their car inspected? Team & [{ 'car.inspected:unsigned' : True }, { 'car.safety_inspected:unsigned' : True }] # Any team that has had their car inspected? Team & [{'car.inspected:unsigned': True}, {'car.safety_inspected:unsigned': True}] Out[9]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= Total: 1 In [10]: Copied! # Which teams do not have hyper white lights for their first head light? Team & { \"car.headlights[0].hyper_white\" : None } # Which teams do not have hyper white lights for their first head light? Team & {\"car.headlights[0].hyper_white\": None} Out[10]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } A team within a company name team name car A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= marketing =BLOB= Total: 2 Notice that the previous query will satisfy the None check if it experiences any of the following scenarious: if entire record missing ( marketing satisfies this) JSON key is missing JSON value is set to JSON null ( engineering satisfies this)", "title": "Restriction"}, {"location": "tutorials/json/#projection", "text": "Projections can be quite useful with the json type since we can extract out just what we need. This allows greater query flexibility but more importantly, for us to be able to fetch only what is pertinent. In [11]: Copied! # Only interested in the car names and the length but let the type be inferred q_untyped = Team . proj ( car_name = 'car.name' , car_length = \"car.length\" , ) q_untyped # Only interested in the car names and the length but let the type be inferred q_untyped = Team.proj( car_name='car.name', car_length=\"car.length\", ) q_untyped Out[11]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } name team name car_name calculated attribute car_length calculated attribute business Chaching 100 engineering Rever 20.5 marketing None None Total: 3 In [12]: Copied! q_untyped . fetch ( as_dict = True ) q_untyped.fetch(as_dict=True) Out[12]: [{'name': 'business', 'car_name': 'Chaching', 'car_length': '100'}, {'name': 'engineering', 'car_name': 'Rever', 'car_length': '20.5'}, {'name': 'marketing', 'car_name': None, 'car_length': None}] In [13]: Copied! # Nevermind, I'll specify the type explicitly q_typed = Team . proj ( car_name = 'car.name' , car_length = \"car.length:float\" , ) q_typed # Nevermind, I'll specify the type explicitly q_typed = Team.proj( car_name='car.name', car_length=\"car.length:float\", ) q_typed Out[13]: .Table{ border-collapse:collapse; } .Table th{ background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid; font-weight: normal; font-family: monospace; font-size: 100%; } .Table td{ padding:4px; border:#f0e0e0 1px solid; font-size:100%; } .Table tr:nth-child(odd){ background: #ffffff; } .Table tr:nth-child(even){ background: #f3f1ff; } /* Tooltip container */ .djtooltip { } /* Tooltip text */ .djtooltip .djtooltiptext { visibility: hidden; width: 120px; background-color: black; color: #fff; text-align: center; padding: 5px 0; border-radius: 6px; /* Position the tooltip text - see examples below! */ position: absolute; z-index: 1; } #primary { font-weight: bold; color: black; } #nonprimary { font-weight: normal; color: white; } /* Show the tooltip text when you mouse over the tooltip container */ .djtooltip:hover .djtooltiptext { visibility: visible; } name team name car_name calculated attribute car_length calculated attribute business Chaching 100.0 engineering Rever 20.5 marketing None None Total: 3 In [14]: Copied! q_typed . fetch ( as_dict = True ) q_typed.fetch(as_dict=True) Out[14]: [{'name': 'business', 'car_name': 'Chaching', 'car_length': 100.0}, {'name': 'engineering', 'car_name': 'Rever', 'car_length': 20.5}, {'name': 'marketing', 'car_name': None, 'car_length': None}]", "title": "Projection"}, {"location": "tutorials/json/#describe", "text": "Lastly, the .describe() function on the Team table can help us generate the table's definition. This is useful if we are connected directly to the pipeline without the original source. In [16]: Copied! rebuilt_definition = Team . describe () print ( rebuilt_definition ) rebuilt_definition = Team.describe() print(rebuilt_definition) # A team within a company name : varchar(40) # team name --- car=null : json # A car belonging to a team (null to allow registering first but specifying car later) UNIQUE INDEX ((json_value(`car`, _utf8mb4'$.length' returning decimal(4, 1))))", "title": "Describe"}, {"location": "tutorials/json/#cleanup", "text": "Finally, let's clean up what we created in this tutorial. In [17]: Copied! schema . drop () schema.drop() In [ ]: Copied!", "title": "Cleanup"}]}