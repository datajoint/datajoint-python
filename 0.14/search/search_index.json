{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Welcome to the DataJoint API for Python!", "text": "<p>The DataJoint API for Python is a framework for scientific workflow management based on relational principles. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, computing, and querying data.</p> <p>DataJoint was initially developed in 2009 by Dimitri Yatsenko in Andreas Tolias' Lab at Baylor College of Medicine for the distributed processing and management of large volumes of data streaming from regular experiments. Starting in 2011, DataJoint has been available as an open-source project adopted by other labs and improved through contributions from several developers. Presently, the primary developer of DataJoint open-source software is the company DataJoint.</p>"}, {"location": "#data-pipeline-example", "title": "Data Pipeline Example", "text": "<p>Yatsenko et al., bioRxiv 2021</p>"}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Install from PyPI<pre><code>pip install datajoint\n</code></pre> </li> </ul> <ul> <li>Detailed Getting Started Guide</li> </ul> <ul> <li>Interactive Tutorials on GitHub Codespaces</li> </ul> <ul> <li>DataJoint Elements - Catalog of example pipelines for neuroscience experiments</li> </ul> <ul> <li> <p>Contribute</p> <ul> <li>Development Environment</li> </ul> <ul> <li>Guidelines</li> </ul> </li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": ""}, {"location": "changelog/#release-notes", "title": "Release notes", "text": ""}, {"location": "changelog/#0141-jun-02-2023", "title": "0.14.1 -- Jun 02, 2023", "text": "<ul> <li>Fixed - Fix altering a part table that uses the \"master\" keyword - PR #991</li> <li>Fixed - <code>.ipynb</code> output in tutorials is not visible in dark mode (#1078) PR #1080</li> <li>Fixed - preview table font for darkmode PR #1089</li> <li>Changed - Readme to update links and include example pipeline image</li> <li>Changed - Docs to add landing page and update navigation</li> <li>Changed - <code>.data</code> method to <code>.stream</code> in the <code>get()</code> method for S3 (external) objects PR #1085</li> <li>Fixed - Docs to rename <code>create_virtual_module</code> to <code>VirtualModule</code></li> <li>Added - Skeleton from <code>datajoint-company/datajoint-docs</code> repository for docs migration</li> <li>Added - Initial <code>pytest</code> for <code>test_connection</code></li> </ul>"}, {"location": "changelog/#0140-feb-13-2023", "title": "0.14.0 -- Feb 13, 2023", "text": "<ul> <li>Added - <code>json</code> data type (#245) PR #1051</li> <li>Fixed - Activating a schema requires all tables to exist even if <code>create_tables=False</code> PR #1058</li> <li>Changed - Populate call with <code>reserve_jobs=True</code> to exclude <code>error</code> and <code>ignore</code> keys - PR #1062</li> <li>Added - Support for inserting data with CSV files - PR #1067</li> <li>Changed - Switch testing image from <code>pydev</code> to <code>djtest</code> PR #1012</li> <li>Added - DevContainer development environment compatible with GH Codespaces PR 1071</li> <li>Fixed - Convert lingering prints by replacing with logs PR #1073</li> <li>Changed - <code>table.progress()</code> defaults to no stdout PR #1073</li> <li>Changed - <code>table.describe()</code> defaults to no stdout PR #1073</li> <li>Deprecated - <code>table._update()</code> PR #1073</li> <li>Deprecated - old-style foreign key syntax PR #1073</li> <li>Deprecated - <code>dj.migrate_dj011_external_blob_storage_to_dj012()</code> PR #1073</li> <li>Added - Method to set job keys to \"ignore\" status - PR #1068</li> </ul>"}, {"location": "changelog/#0138-sep-21-2022", "title": "0.13.8 -- Sep 21, 2022", "text": "<ul> <li>Added - New documentation structure based on markdown PR #1052</li> <li>Fixed - Fix queries with backslashes (#999) PR #1052</li> </ul>"}, {"location": "changelog/#0137-jul-13-2022", "title": "0.13.7 -- Jul 13, 2022", "text": "<ul> <li>Fixed - Fix networkx incompatable change by version pinning to 2.6.3 (#1035) PR #1036</li> <li>Added - Support for serializing numpy datetime64 types (#1022) PR #1036</li> <li>Changed - Add traceback to default logging PR #1036</li> </ul>"}, {"location": "changelog/#0136-jun-13-2022", "title": "0.13.6 -- Jun 13, 2022", "text": "<ul> <li>Added - Config option to set threshold for when to stop using checksums for filepath stores. PR #1025</li> <li>Added - Unified package level logger for package (#667) PR #1031</li> <li>Changed - Swap various datajoint messages, warnings, etc. to use the new logger. (#667) PR #1031</li> <li>Fixed - Fix query caching deleting non-datajoint files PR #1027</li> <li>Changed - Minimum Python version for Datajoint-Python is now 3.7 PR #1027</li> </ul>"}, {"location": "changelog/#0135-may-19-2022", "title": "0.13.5 -- May 19, 2022", "text": "<ul> <li>Changed - Import ABC from collections.abc for Python 3.10 compatibility</li> <li>Fixed - Fix multiprocessing value error (#1013) PR #1026</li> </ul>"}, {"location": "changelog/#0134-mar-28-2022", "title": "0.13.4 -- Mar, 28 2022", "text": "<ul> <li>Added - Allow reading blobs produced by legacy 32-bit compiled mYm library for matlab. PR #995</li> <li>Fixed - Add missing <code>jobs</code> argument for multiprocessing PR #997</li> <li>Added - Test for multiprocessing PR #1008</li> <li>Fixed - Fix external store key name doesn't allow '-' (#1005) PR #1006</li> <li>Added - Adopted black formatting into code base PR #998</li> </ul>"}, {"location": "changelog/#0133-feb-9-2022", "title": "0.13.3 -- Feb 9, 2022", "text": "<ul> <li>Fixed - Fix error in listing ancestors, descendants with part tables.</li> <li>Fixed - Fix Python 3.10 compatibility (#983) PR #972</li> <li>Fixed - Allow renaming non-conforming attributes in proj (#982) PR #972</li> <li>Added - Expose proxy feature for S3 external stores (#961) PR #962</li> <li>Added - implement multiprocessing in populate (#695) PR #704, #969</li> <li>Fixed - Dependencies not properly loaded on populate. (#902) PR #919</li> <li>Fixed - Replace use of numpy aliases of built-in types with built-in type. (#938) PR #939</li> <li>Fixed - Deletes and drops must include the master of each part. (#151, #374) PR #957</li> <li>Fixed - <code>ExternalTable.delete</code> should not remove row on error (#953) PR #956</li> <li>Fixed - Fix error handling of remove_object function in <code>s3.py</code> (#952) PR #955</li> <li>Fixed - Fix regression issue with <code>DISTINCT</code> clause and <code>GROUP_BY</code> (#914) PR #963</li> <li>Fixed - Fix sql code generation to comply with sql mode <code>ONLY_FULL_GROUP_BY</code> (#916) PR #965</li> <li>Fixed - Fix count for left-joined <code>QueryExpressions</code> (#951) PR #966</li> <li>Fixed - Fix assertion error when performing a union into a join (#930) PR #967</li> <li>Changed <code>~jobs.error_stack</code> from blob to mediumblob to allow error stacks &gt;64kB in jobs (#984) PR #986</li> <li>Fixed - Fix error when performing a union on multiple tables (#926) PR #964</li> <li>Added - Allow optional keyword arguments for <code>make()</code> in <code>populate()</code> PR #971</li> </ul>"}, {"location": "changelog/#0132-may-7-2021", "title": "0.13.2 -- May 7, 2021", "text": "<ul> <li>Changed <code>setuptools_certificate</code> dependency to new name <code>otumat</code></li> <li>Fixed - Explicit calls to <code>dj.Connection</code> throw error due to missing <code>host_input</code> (#895) PR #907</li> <li>Fixed - Correct count of deleted items. (#897) PR #912</li> </ul>"}, {"location": "changelog/#0131-apr-16-2021", "title": "0.13.1 -- Apr 16, 2021", "text": "<ul> <li>Added <code>None</code> as an alias for <code>IS NULL</code> comparison in <code>dict</code> restrictions (#824) PR #893</li> <li>Changed - Drop support for MySQL 5.6 since it has reached EOL PR #893</li> <li>Fixed - <code>schema.list_tables()</code> is not topologically sorted (#838) PR #893</li> <li>Fixed - Diagram part tables do not show proper class name (#882) PR #893</li> <li>Fixed - Error in complex restrictions (#892) PR #893</li> <li>Fixed - WHERE and GROUP BY clases are dropped on joins with aggregation (#898, #899) PR #893</li> </ul>"}, {"location": "changelog/#0130-mar-24-2021", "title": "0.13.0 -- Mar 24, 2021", "text": "<ul> <li>Re-implement query transpilation into SQL, fixing issues (#386, #449, #450, #484, #558). PR #754</li> <li>Re-implement cascading deletes for better performance. PR #839</li> <li>Add support for deferred schema activation to allow for greater modularity. (#834) PR #839</li> <li>Add query caching mechanism for offline development (#550) PR #839</li> <li>Add table method <code>.update1</code> to update a row in the table with new values (#867) PR #763, #889</li> <li>Python datatypes are now enabled by default in blobs (#761). PR #859</li> <li>Added permissive join and restriction operators <code>@</code> and <code>^</code> (#785) PR #754</li> <li>Support DataJoint datatype and connection plugins (#715, #729) PR 730, #735</li> <li>Add <code>dj.key_hash</code> alias to <code>dj.hash.key_hash</code> (#804) PR #862</li> <li>Default enable_python_native_blobs to True</li> <li>Bugfix - Regression error on joins with same attribute name (#857) PR #878</li> <li>Bugfix - Error when <code>fetch1('KEY')</code> when <code>dj.config['fetch_format']='frame'</code> set (#876) PR #880, #878</li> <li>Bugfix - Error when cascading deletes in tables with many, complex keys (#883, #886) PR #839</li> <li>Add deprecation warning for <code>_update</code>. PR #889</li> <li>Add <code>purge_query_cache</code> utility. PR #889</li> <li>Add tests for query caching and permissive join and restriction. PR #889</li> <li>Drop support for Python 3.5 (#829) PR #861</li> </ul>"}, {"location": "changelog/#0129-mar-12-2021", "title": "0.12.9 -- Mar 12, 2021", "text": "<ul> <li>Fix bug with fetch1 with <code>dj.config['fetch_format']=\"frame\"</code>. (#876) PR #880</li> </ul>"}, {"location": "changelog/#0128-jan-12-2021", "title": "0.12.8 -- Jan 12, 2021", "text": "<ul> <li>table.children, .parents, .descendents, and ancestors can return queryable objects. PR #833</li> <li>Load dependencies before querying dependencies. (#179) PR #833</li> <li>Fix display of part tables in <code>schema.save</code>. (#821) PR #833</li> <li>Add <code>schema.list_tables</code>. (#838) PR #844</li> <li>Fix minio new version regression. PR #847</li> <li>Add more S3 logging for debugging. (#831) PR #832</li> <li>Convert testing framework from TravisCI to GitHub Actions (#841) PR #840</li> </ul>"}, {"location": "changelog/#0127-oct-27-2020", "title": "0.12.7 -- Oct 27, 2020", "text": "<ul> <li>Fix case sensitivity issues to adapt to MySQL 8+. PR #819</li> <li>Fix pymysql regression bug (#814) PR #816</li> <li>Adapted attribute types now have dtype=object in all recarray results. PR #811</li> </ul>"}, {"location": "changelog/#0126-may-15-2020", "title": "0.12.6 -- May 15, 2020", "text": "<ul> <li>Add <code>order_by</code> to <code>dj.kill</code> (#668, #779) PR #775, #783</li> <li>Add explicit S3 bucket and file storage location existence checks (#748) PR #781</li> <li>Modify <code>_update</code> to allow nullable updates for strings/date (#664) PR #760</li> <li>Avoid logging events on auxiliary tables (#737) PR #753</li> <li>Add <code>kill_quick</code> and expand display to include host (#740) PR #741</li> <li>Bugfix - pandas insert fails due to additional <code>index</code> field (#666) PR #776</li> <li>Bugfix - <code>delete_external_files=True</code> does not remove from S3 (#686) PR #781</li> <li>Bugfix - pandas fetch throws error when <code>fetch_format='frame'</code> PR #774</li> </ul>"}, {"location": "changelog/#0125-feb-24-2020", "title": "0.12.5 -- Feb 24, 2020", "text": "<ul> <li>Rename module <code>dj.schema</code> into <code>dj.schemas</code>. <code>dj.schema</code> remains an alias for class <code>dj.Schema</code>. (#731) PR #732</li> <li><code>dj.create_virtual_module</code> is now called <code>dj.VirtualModule</code> (#731) PR #732</li> <li>Bugfix - SSL <code>KeyError</code> on failed connection (#716) PR #725</li> <li>Bugfix - Unable to run unit tests using nosetests (#723) PR #724</li> <li>Bugfix - <code>suppress_errors</code> does not suppress loss of connection error (#720) PR #721</li> </ul>"}, {"location": "changelog/#0124-jan-14-2020", "title": "0.12.4 -- Jan 14, 2020", "text": "<ul> <li>Support for simple scalar datatypes in blobs (#690) PR #709</li> <li>Add support for the <code>serial</code> data type in declarations: alias for <code>bigint unsigned auto_increment</code> PR #713</li> <li>Improve the log table to avoid primary key collisions PR #713</li> <li>Improve documentation in README PR #713</li> </ul>"}, {"location": "changelog/#0123-nov-22-2019", "title": "0.12.3 -- Nov 22, 2019", "text": "<ul> <li>Bugfix - networkx 2.4 causes error in diagrams (#675) PR #705</li> <li>Bugfix - include table definition in doc string and help (#698, #699) PR #706</li> <li>Bugfix - job reservation fails when native python datatype support is disabled (#701) PR #702</li> </ul>"}, {"location": "changelog/#0122-nov-11-2019", "title": "0.12.2 -- Nov 11, 2019", "text": "<ul> <li>Bugfix - Convoluted error thrown if there is a reference to a non-existent table attribute (#691) PR #696</li> <li>Bugfix - Insert into external does not trim leading slash if defined in <code>dj.config['stores']['&lt;store&gt;']['location']</code> (#692) PR #693</li> </ul>"}, {"location": "changelog/#0121-nov-2-2019", "title": "0.12.1 -- Nov 2, 2019", "text": "<ul> <li>Bugfix - AttributeAdapter converts into a string (#684) PR #688</li> </ul>"}, {"location": "changelog/#0120-oct-31-2019", "title": "0.12.0 -- Oct 31, 2019", "text": "<ul> <li>Dropped support for Python 3.4</li> <li>Support secure connections with TLS (aka SSL) PR #620</li> <li>Convert numpy array from python object to appropriate data type if all elements are of the same type (#587) PR #608</li> <li>Remove expression requirement to have additional attributes (#604) PR #604</li> <li>Support for filepath datatype (#481) PR #603, #659</li> <li>Support file attachment datatype (#480, #592, #637) PR #659</li> <li>Fetch return a dict array when specifying <code>as_dict=True</code> for specified attributes. (#595) PR #593</li> <li>Support of ellipsis in <code>proj</code>: <code>query_expression.proj(.., '-movie')</code> (#499) PR #578</li> <li>Expand support of blob serialization (#572, #520, #427, #392, #244, #594) PR #577</li> <li>Support for alter (#110) PR #573</li> <li>Support for <code>conda install datajoint</code> via <code>conda-forge</code> channel (#293)</li> <li><code>dj.conn()</code> accepts a <code>port</code> keyword argument (#563) PR #571</li> <li>Support for UUID datatype (#562) PR #567</li> <li><code>query_expr.fetch(\"KEY\", as_dict=False)</code> returns results as <code>np.recarray</code>(#414) PR #574</li> <li><code>dj.ERD</code> is now called <code>dj.Diagram</code> (#255, #546) PR #565</li> <li><code>dj.Diagram</code> underlines \"distinguished\" classes (#378) PR #557</li> <li>Accept alias for supported MySQL datatypes (#544) PR #545</li> <li>Support for pandas in <code>fetch</code> (#459, #537) PR #534</li> <li>Support for ordering by \"KEY\" in <code>fetch</code> (#541) PR #534</li> <li>Add config to enable python native blobs PR #672, #676</li> <li>Add secure option for external storage (#663) PR #674, #676</li> <li>Add blob migration utility from DJ011 to DJ012 PR #673</li> <li>Improved external storage - a migration script needed from version 0.11 (#467, #475, #480, #497) PR #532</li> <li>Increase default display rows (#523) PR #526</li> <li>Bugfixes (#521, #205, #279, #477, #570, #581, #597, #596, #618, #633, #643, #644, #647, #648, #650, #656)</li> <li>Minor improvements (#538)</li> </ul>"}, {"location": "changelog/#0113-jul-26-2019", "title": "0.11.3 -- Jul 26, 2019", "text": "<ul> <li>Fix incompatibility with pyparsing 2.4.1 (#629) PR #631</li> </ul>"}, {"location": "changelog/#0112-jul-25-2019", "title": "0.11.2 -- Jul 25, 2019", "text": "<ul> <li>Fix #628 - incompatibility with pyparsing 2.4.1</li> </ul>"}, {"location": "changelog/#0111-nov-15-2018", "title": "0.11.1 -- Nov 15, 2018", "text": "<ul> <li>Fix ordering of attributes in proj (#483, #516)</li> <li>Prohibit direct insert into auto-populated tables (#511)</li> </ul>"}, {"location": "changelog/#0110-oct-25-2018", "title": "0.11.0 -- Oct 25, 2018", "text": "<ul> <li>Full support of dependencies with renamed attributes using projection syntax (#300, #345, #436, #506, #507)</li> <li>Rename internal class and module names to comply with terminology in documentation (#494, #500)</li> <li>Full support of secondary indexes (#498, 500)</li> <li>ERD no longer shows numbers in nodes corresponding to derived dependencies (#478, #500)</li> <li>Full support of unique and nullable dependencies (#254, #301, #493, #495, #500)</li> <li>Improve memory management in <code>populate</code> (#461, #486)</li> <li>Fix query errors and redundancies (#456, #463, #482)</li> </ul>"}, {"location": "changelog/#0101-aug-28-2018", "title": "0.10.1 -- Aug 28, 2018", "text": "<ul> <li>Fix ERD Tooltip message (#431)</li> <li>Networkx 2.0 support (#443)</li> <li>Fix insert from query with skip_duplicates=True (#451)</li> <li>Sped up queries (#458)</li> <li>Bugfix in restriction of the form (A &amp; B) * B (#463)</li> <li>Improved error messages (#466)</li> </ul>"}, {"location": "changelog/#0100-jan-10-2018", "title": "0.10.0 -- Jan 10, 2018", "text": "<ul> <li>Deletes are more efficient (#424)</li> <li>ERD shows table definition on tooltip hover in Jupyter (#422)</li> <li>S3 external storage</li> <li>Garbage collection for external sorage</li> <li>Most operators and methods of tables can be invoked as class methods rather than instance methods (#407)</li> <li>The schema decorator object no longer requires locals() to specify the context</li> <li>Compatibility with pymysql 0.8.0+</li> <li>More efficient loading of dependencies (#403)</li> </ul>"}, {"location": "changelog/#090-nov-17-2017", "title": "0.9.0 -- Nov 17, 2017", "text": "<ul> <li>Made graphviz installation optional</li> <li>Implement file-based external storage</li> <li>Implement union operator +</li> <li>Implement file-based external storage</li> </ul>"}, {"location": "changelog/#080-jul-26-2017", "title": "0.8.0 -- Jul 26, 2017", "text": "<p>Documentation and tutorials available at https://docs.datajoint.io and https://tutorials.datajoint.io</p> <ul> <li>improved the ERD graphics and features using the graphviz libraries (#207, #333)</li> <li>improved password handling logic (#322, #321)</li> <li>the use of the <code>contents</code> property to populate tables now only works in <code>dj.Lookup</code> classes (#310).</li> <li>allow suppressing the display of size of query results through the <code>show_tuple_count</code> configuration option (#309)</li> <li>implemented renamed foreign keys to spec (#333)</li> <li>added the <code>limit</code> keyword argument to populate (#329)</li> <li>reduced the number of displayed messages (#308)</li> <li>added <code>size_on_disk</code> property for dj.Schema() objects (#323)</li> <li>job keys are entered in the jobs table (#316, #243)</li> <li>simplified the <code>fetch</code> and <code>fetch1</code> syntax, deprecating the <code>fetch[...]</code> syntax (#319)</li> <li>the jobs tables now store the connection ids to allow identifying abandoned jobs (#288, #317)</li> </ul>"}, {"location": "changelog/#050-298-mar-8-2017", "title": "0.5.0 (#298) -- Mar 8, 2017", "text": "<ul> <li>All fetched integers are now 64-bit long and all fetched floats are double precision.</li> <li>Added <code>dj.create_virtual_module</code></li> </ul>"}, {"location": "changelog/#0410-286-feb-6-2017", "title": "0.4.10 (#286) -- Feb 6, 2017", "text": "<ul> <li>Removed Vagrant and Readthedocs support</li> <li>Explicit saving of configuration (issue #284)</li> </ul>"}, {"location": "changelog/#049-285-feb-2-2017", "title": "0.4.9 (#285) -- Feb 2, 2017", "text": "<ul> <li>Fixed setup.py for pip install</li> </ul>"}, {"location": "changelog/#047-281-jan-24-2017", "title": "0.4.7 (#281) -- Jan 24, 2017", "text": "<ul> <li>Fixed issues related to order of attributes in projection.</li> </ul>"}, {"location": "changelog/#046-277-dec-22-2016", "title": "0.4.6 (#277) -- Dec 22, 2016", "text": "<ul> <li>Proper handling of interruptions during populate</li> </ul>"}, {"location": "changelog/#045-274-dec-20-2016", "title": "0.4.5 (#274) -- Dec 20, 2016", "text": "<ul> <li>Populate reports how many keys remain to be populated at the start.</li> </ul>"}, {"location": "changelog/#043-271-dec-6-2016", "title": "0.4.3 (#271) -- Dec 6, 2016", "text": "<ul> <li>Fixed aggregation issues (#270)</li> <li>datajoint no longer attempts to connect to server at import time</li> <li>dropped support of view (reversed #257)</li> <li>more elegant handling of insufficient privileges (#268)</li> </ul>"}, {"location": "changelog/#042-267-dec-6-2016", "title": "0.4.2 (#267) -- Dec 6, 2016", "text": "<ul> <li>improved table appearance in Jupyter</li> </ul>"}, {"location": "changelog/#041-266-oct-28-2016", "title": "0.4.1 (#266) -- Oct 28, 2016", "text": "<ul> <li>bugfix for very long error messages</li> </ul>"}, {"location": "changelog/#039-sep-27-2016", "title": "0.3.9 -- Sep 27, 2016", "text": "<ul> <li>Added support for datatype <code>YEAR</code></li> <li>Fixed issues with <code>dj.U</code> and the <code>aggr</code> operator (#246, #247)</li> </ul>"}, {"location": "changelog/#038-aug-2-2016", "title": "0.3.8 -- Aug 2, 2016", "text": "<ul> <li>added the <code>_update</code> method in <code>base_relation</code>. It allows updating values in existing tuples.</li> <li>bugfix in reading values of type double. Previously it was cast as float32.</li> </ul>"}, {"location": "changelog/#037-jul-31-2016", "title": "0.3.7 -- Jul 31, 2016", "text": "<ul> <li>added parameter <code>ignore_extra_fields</code> in <code>insert</code></li> <li><code>insert(..., skip_duplicates=True)</code> now relies on <code>SELECT IGNORE</code>. Previously it explicitly checked if tuple already exists.</li> <li>table previews now include blob attributes displaying the string"}, {"location": "changelog/#036-jul-30-2016", "title": "0.3.6 -- Jul 30, 2016", "text": "<ul> <li>bugfix in <code>schema.spawn_missing_classes</code>. Previously, spawned part classes would not show in ERDs.</li> <li>dj.key now causes fetch to return as a list of dicts. Previously it was a recarray.</li> </ul>"}, {"location": "changelog/#035", "title": "0.3.5", "text": "<ul> <li><code>dj.set_password()</code> now asks for user confirmation before changing the password.</li> <li>fixed issue #228</li> </ul>"}, {"location": "changelog/#034", "title": "0.3.4", "text": "<ul> <li>Added method the <code>ERD.add_parts</code> method, which adds the part tables of all tables currently in the ERD.</li> <li><code>ERD() + arg</code> and <code>ERD() - arg</code> can now accept table classes as arg.</li> </ul>"}, {"location": "changelog/#033", "title": "0.3.3", "text": "<ul> <li>Suppressed warnings (redirected them to logging). Previoiusly, scipy would throw warnings in ERD, for example.</li> <li>Added ERD.from_sequence as a shortcut to combining the ERDs of multiple sources</li> <li>ERD() no longer text the context argument.</li> <li>ERD.draw() now takes an optional context argument. By default uses the caller's locals.</li> </ul>"}, {"location": "changelog/#032", "title": "0.3.2", "text": "<ul> <li>Fixed issue #223: <code>insert</code> can insert relations without fetching.</li> <li>ERD() now takes the <code>context</code> argument, which specifies in which context to look for classes. The default is taken from the argument (schema or table).</li> <li>ERD.draw() no longer has the <code>prefix</code> argument: class names are shown as found in the context.</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the DataJoint API for Python, please cite the following manuscript and Research Resource Identifier (RRID):</p> <ul> <li>Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P, Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: managing big scientific data using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. doi: https://doi.org/10.1101/031658</li> </ul> <ul> <li>DataJoint API for Python - RRID:SCR_014543 - Version <code>Enter version here</code></li> </ul>"}, {"location": "develop/", "title": "Develop", "text": "<p>Included with the codebase is the recommended development environment configured using DevContainer.</p>"}, {"location": "develop/#launch-environment", "title": "Launch Environment", "text": "<p>Here are some options that provide a great developer experience:</p> <ul> <li>Cloud-based IDE: (recommended)<ul> <li>Launch using GitHub Codespaces using the option <code>Create codespace on master</code> in the codebase repository on your fork.</li> <li>Build time for a 2-Core codespace is ~6m. This is done infrequently and cached for convenience.</li> <li>Start time for a 2-Core codespace is ~2m. This will pull the built codespace from cache when you need it.</li> <li>Tip: GitHub auto names the codespace but you can rename the codespace so that it is easier to identify later.</li> </ul> </li> <li>Local IDE:<ul> <li>Ensure you have Git</li> <li>Ensure you have Docker</li> <li>Ensure you have VSCode</li> <li>Install the Dev Containers extension</li> <li><code>git clone</code> the codebase repository and open it in VSCode</li> <li>Use the <code>Dev Containers extension</code> to <code>Reopen in Container</code> (More info in the <code>Getting started</code> included with the extension)</li> </ul> </li> </ul> <p>You will know your environment has finished loading once you see a terminal open related to <code>Running postStartCommand</code> with a final message: <code>Done</code>.</p>"}, {"location": "develop/#features", "title": "Features", "text": "<p>Once you've successfully launched the development environment, you'll be able to take advantage of our developer tooling to help improve productivity and quality.</p>"}, {"location": "develop/#syntax-tests", "title": "Syntax Tests", "text": "<p>The following will verify that there are no syntax errors.</p> <pre><code>flake8 datajoint --count --select=E9,F63,F7,F82 --show-source --statistics\n</code></pre>"}, {"location": "develop/#integration-tests", "title": "Integration Tests", "text": "<p>The following will verify there are no regression errors by running our test suite of unit and integration tests.</p> <ul> <li>Entire test suite:   <pre><code>nosetests -vw tests_old\n</code></pre> <p>Note: We are in the process of upgrading to <code>pytest</code> tests. To run those, use: <pre><code>pytest -sv --cov-report term-missing --cov=datajoint tests\n</code></pre></p> </li> </ul> <ul> <li>A single functional test:   <pre><code>nosetests -vs --tests=tests_old.test_external_class:test_insert_and_fetch\n</code></pre> <p>Note: We are in the process of upgrading to <code>pytest</code> tests. To run those, use: <pre><code>pytest -sv tests/test_connection.py::test_dj_conn\n</code></pre></p> </li> <li>A single class test:   <pre><code>nosetests -vs --tests=tests_old.test_fetch:TestFetch.test_getattribute_for_fetch1\n</code></pre></li> </ul>"}, {"location": "develop/#style-tests", "title": "Style Tests", "text": "<p>The following will verify that there are no code styling errors.</p> <pre><code>flake8 --ignore=E203,E722,W503 datajoint --count --max-complexity=62 --max-line-length=127 --statistics\n</code></pre> <p>The following will ensure the codebase has been formatted with black.</p> <pre><code>black datajoint --check -v\n</code></pre> <p>The following will ensure the test suite has been formatted with black.</p> <pre><code>black tests --check -v\n</code></pre>"}, {"location": "develop/#jupyter", "title": "Jupyter", "text": "<p>Jupyter notebooks are supported in this environment. This means that when you <code>import datajoint</code>, it will use the current state of the source.</p> <p>Be sure to see the reference documenation if you are new to running Jupyter notebooks w/ VSCode.</p>"}, {"location": "develop/#debugger", "title": "Debugger", "text": "<p>VSCode Debugger is a powerful tool that can really accelerate fixes.</p> <p>Try it as follows:</p> <ul> <li>Create a python script of your choice</li> <li><code>import datajoint</code> (This will use the current state of the source)</li> <li>Add breakpoints by adding red dots next to line numbers</li> <li>Select the <code>Run and Debug</code> tab</li> <li>Start by clicking the button <code>Run and Debug</code></li> </ul>"}, {"location": "develop/#mysql-cli", "title": "MySQL CLI", "text": "<p>It is often useful in development to connect to DataJoint's relational database backend directly using the MySQL CLI.</p> <p>Connect as follows to the database running within your developer environment:</p> <pre><code>mysql -hfakeservices.datajoint.io -uroot -psimple\n</code></pre>"}, {"location": "develop/#documentation", "title": "Documentation", "text": "<p>Our documentation is built using MkDocs Material. The easiest way to improve the documentation is by using the <code>docs/docker-compose.yaml</code> environment. The source can be modified in <code>docs/src</code> using markdown.</p> <p>The docs environment can be run using 3 modes:</p> <ul> <li>LIVE: (recommended) This serves the docs locally. It supports live reloading on saves to <code>docs/src</code> files but does not support the docs version dropdown. Useful to see changes live.   <pre><code>MODE=\"LIVE\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build\n</code></pre></li> <li>QA: This serves the docs locally. It supports the docs version dropdown but does not support live reloading. Useful as a final check.   <pre><code>MODE=\"QA\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build\n</code></pre></li> <li>BUILD: This compiles the docs. Most useful for the docs deployment automation. Other modes are more useful to new contributors.   <pre><code>MODE=\"BUILD\" PACKAGE=datajoint UPSTREAM_REPO=https://github.com/datajoint/datajoint-python.git HOST_UID=$(id -u) docker compose -f docs/docker-compose.yaml up --build\n</code></pre></li> </ul> <p>When the docs are served locally, use the VSCode <code>PORTS</code> tab (next to <code>TERMINAL</code>) to manage access to the forwarded ports. Docs are served on port <code>8080</code>.</p>"}, {"location": "existing-pipelines/", "title": "Existing Pipelines", "text": "<p>This section describes how to work with database schemas without access to the original code that generated the schema. These situations often arise when the database is created by another user who has not shared the generating code yet or when the database schema is created from a programming language other than Python.</p>"}, {"location": "existing-pipelines/#loading-classes", "title": "Loading Classes", "text": "<p>Typically, a DataJoint schema is created as a dedicated Python module. This module defines a schema object that is used to link classes declared in the module to tables in the database schema. With the module installed, you can simply import it to interact with its tables:</p> <pre><code>import datajoint as dj\nfrom element_calcium_imaging import scan # (1)\n</code></pre> <ol> <li>This and other DataJoint Elements are  installable via <code>pip</code> or downloadable via their respective GitHub repositories.</li> </ol> <p>To visualize an unfamiliar schema, see commands for generating diagrams.</p>"}, {"location": "existing-pipelines/#spawning-missing-classes", "title": "Spawning Missing Classes", "text": "<p>Now, imagine we do not have access to the  Python definition of Scan, or we're unsure if the version on our server matches the definition available. We can use the <code>dj.list_schemas</code> function to list the available database schemas.</p> <pre><code>import datajoint as dj\ndj.conn() # (1)\ndj.list_schemas() # (2)\ndj.Schema('schema_name').list_tables() # (3)\n</code></pre> <ol> <li>Establish a connection to the server.</li> <li>List the available schemas on the server.</li> <li>List the tables for a given schema from the previous step. These will appear in their raw database form, with underscores instead of camelcase and special characters for Part tables.</li> </ol> <p>Just as with a new schema, we can create a schema object to connect to the chosen database schema. If the schema already exists, <code>dj.Schema</code> is initialized as usual.</p> <p>If a diagram will shows a mixture of class names and database table names, the <code>spawn_missing_classes</code> method will spawn classes into the local namespace for any tables missing their classes. This will allow us to interact with all tables as if they were declared in the current namespace.</p> <pre><code>schema.spawn_missing_classes()\n</code></pre>"}, {"location": "existing-pipelines/#virtual-modules", "title": "Virtual Modules", "text": "<p>While <code>spawn_missing_classes</code> creates the new classes in the local namespace, it is often more convenient to import a schema with its Python module, equivalent to the Python command. We can mimmick this import without having access to the schema using the <code>VirtualModule</code> class object:</p> <pre><code>import datajoint as dj\nsubject = dj.VirtualModule(module_name='subject', schema_name='db_subject')\n</code></pre> <p>Now, <code>subject</code> behaves as an imported module complete with the schema object and all the table classes.</p> <p>The class object <code>VirtualModule</code> of the <code>dj.Schema</code> class provides access to virtual modules. It creates a python module with the given name from the name of a schema on the server, automatically adds classes to it corresponding to the tables in the schema.</p> <p>The function can take several parameters:</p> <ul> <li><code>module_name</code>: displayed module name.</li> </ul> <ul> <li><code>schema_name</code>: name of the database in MySQL.</li> </ul> <p><code>create_schema</code>: if <code>True</code>, create the schema on the database server if it does not  already exist; if <code>False</code> (default), raise an error when the schema is not found.</p> <ul> <li><code>create_tables</code>: if <code>True</code>, <code>module.schema</code> can be used as the decorator for declaring   new classes; if <code>False</code>, such use will raise an error stating that the module is   intend only to work with existing tables.</li> </ul> <p>The function returns the Python module containing classes from the schema object with all the table classes already declared inside it.</p> <p><code>create_schema=False</code> may be useful if we want to make sure that the schema already  exists.  If none exists, <code>create_schema=True</code> will create an empty schema.</p> <pre><code>dj.VirtualModule('what', 'nonexistent')\n</code></pre> <p>Returns</p> <pre><code>DataJointError: Database named `nonexistent` was not defined. Set argument create_schema=True to create it.\n</code></pre> <p><code>create_tables=False</code> prevents the use of the schema object of the virtual module for creating new tables in the existing schema. This is a precautionary measure since virtual modules are often used for completed schemas. <code>create_tables=True</code> will new tables to the existing schema. A more common approach in this scenario would be to create a new schema object and to use the <code>spawn_missing_classes</code> function to make the classes available.</p> <p>However, you if do decide to create new tables in an existing tables using the virtual module, you may do so by using the schema object from the module as the decorator for declaring new tables:</p> <pre><code>uni = dj.VirtualModule('university.py', 'dimitri_university', create_tables=True)\n</code></pre> <pre><code>@uni.schema\nclass Example(dj.Manual):\n    definition = \"\"\"\n    -&gt; uni.Student\n    ---\n    example : varchar(255)\n    \"\"\"\n</code></pre> <pre><code>dj.Diagram(uni)\n</code></pre>"}, {"location": "faq/", "title": "Frequently Asked Questions", "text": ""}, {"location": "faq/#how-do-i-use-datajoint-with-a-gui", "title": "How do I use DataJoint with a GUI?", "text": "<ol> <li> <p>The DataJoint Works platform is set up as a fully managed service to host and execute data pipelines.</p> </li> <li> <p>LabBook is an open source project for data entry.</p> </li> </ol>"}, {"location": "faq/#does-datajoint-support-other-programming-languages", "title": "Does DataJoint support other programming languages?", "text": "<p>DataJoint Python and [Matlab] (https://datajoint.com/docs/core/datajoint-matlab/) APIs are both actively supported. Previous projects implemented some DataJoint features in Julia and Rust. DataJoint's data model and data representation are largely language independent, which means that any language with a DataJoint client can work with a data pipeline defined in any other language. DataJoint clients for other programming languages will be implemented based on demand. All languages must comply to the same data model and computation approach as defined in DataJoint: a simpler relational data model.</p>"}, {"location": "faq/#can-i-use-datajoint-with-my-current-database", "title": "Can I use DataJoint with my current database?", "text": "<p>Researchers use many different tools to keep records, from simple formalized file heirarchies to complete software packages for colony management and standard file types like NWB. Existing projects have built interfaces with many such tools, such as PyRAT. The only requirement for interface is that tool has an open API. Contact Support@DataJoint.com with inquiries. The DataJoint team will consider development requests based on community demand.</p>"}, {"location": "faq/#is-datajoint-an-orm", "title": "Is DataJoint an ORM?", "text": "<p>Programmers are familiar with object-relational mappings (ORM) in various programming languages. Python in particular has several popular ORMs such as SQLAlchemy and Django ORM. The purpose of ORMs is to allow representations and manipulations of objects from the host programming language as data in a relational database. ORMs allow making objects persistent between program executions by creating a bridge (i.e., mapping) between the object model used by the host language and the relational model allowed by the database. The result is always a compromise, usually toward the object model. ORMs usually forgo key concepts, features, and capabilities of the relational model for the sake of convenient programming constructs in the language.</p> <p>In contrast, DataJoint implements a data model that is a refinement of the relational data model without compromising its core principles of data representation and queries. DataJoint supports data integrity (entity integrity, referential integrity, and group integrity) and provides a fully capable relational query language. DataJoint remains absolutely data-centric, with the primary focus on the structure and integrity of the data pipeline. Other ORMs are more application-centric, primarily focusing on the application design while the database plays a secondary role supporting the application with object persistence and sharing.</p>"}, {"location": "faq/#what-is-the-difference-between-datajoint-and-alyx", "title": "What is the difference between DataJoint and Alyx?", "text": "<p>Alyx is an experiment management database application developed in Kenneth Harris' lab at UCL.</p> <p>Alyx is an application with a fixed pipeline design with a nice graphical user interface. In contrast, DataJoint is a general-purpose library for designing and building data processing pipelines.</p> <p>Alyx is geared towards ease of data entry and tracking for a specific workflow (e.g. mouse colony information and some pre-specified experiments) and data types. DataJoint could be used as a more general purposes tool to design, implement, and execute processing on such workflows/pipelines from scratch, and DataJoint focuses on flexibility, data integrity, and ease of data analysis. The purposes are partly overlapping and complementary. The International Brain Lab project is developing a bridge from Alyx to DataJoint, hosted as an open-source project. It implements a DataJoint schema that replicates the major features of the Alyx application and a synchronization script from an existing Alyx database to its DataJoint counterpart.</p>"}, {"location": "api/datajoint/__init__/", "title": "__init__.py", "text": "<p>DataJoint for Python is a framework for building data piplines using MySQL databases to represent pipeline structure and bulk storage systems for large objects. DataJoint is built on the foundation of the relational data model and prescribes a consistent method for organizing, populating, and querying data.</p> <p>The DataJoint data model is described in https://arxiv.org/abs/1807.11104</p> <p>DataJoint is free software under the LGPL License. In addition, we request that any use of DataJoint leading to a publication be acknowledged in the publication.</p> <p>Please cite:</p> <p>- http://biorxiv.org/content/early/2015/11/14/031658   - http://dx.doi.org/10.1101/031658</p>"}, {"location": "api/datajoint/__init__/#datajoint.AttributeAdapter", "title": "<code>AttributeAdapter</code>", "text": "<p>Base class for adapter objects for user-defined attribute types.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>class AttributeAdapter:\n\"\"\"\n    Base class for adapter objects for user-defined attribute types.\n    \"\"\"\n\n    @property\n    def attribute_type(self):\n\"\"\"\n        :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def get(self, value):\n\"\"\"\n        convert value retrieved from the the attribute in a table into the adapted type\n\n        :param value: value from the database\n\n        :return: object of the adapted type\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def put(self, obj):\n\"\"\"\n        convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n        :param obj: an object of the adapted type\n        :return: value to store in the database\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "title": "<code>attribute_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"</p>"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.get", "title": "<code>get(value)</code>", "text": "<p>convert value retrieved from the the attribute in a table into the adapted type</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value from the database</p> required <p>Returns:</p> Type Description <p>object of the adapted type</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get(self, value):\n\"\"\"\n    convert value retrieved from the the attribute in a table into the adapted type\n\n    :param value: value from the database\n\n    :return: object of the adapted type\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.attribute_adapter.AttributeAdapter.put", "title": "<code>put(obj)</code>", "text": "<p>convert an object of the adapted type into a value that DataJoint can store in a table attribute</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>an object of the adapted type</p> required <p>Returns:</p> Type Description <p>value to store in the database</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def put(self, obj):\n\"\"\"\n    convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n    :param obj: an object of the adapted type\n    :return: value to store in the database\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n\"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.key", "title": "<code>key</code>", "text": "<p>object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key</p> Source code in <code>datajoint/fetch.py</code> <pre><code>class key:\n\"\"\"\n    object that allows requesting the primary key as an argument in expression.fetch()\n    The string \"KEY\" can be used instead of the class key\n    \"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.kill", "title": "<code>kill(restriction=None, connection=None, order_by=None)</code>", "text": "<p>view and kill database connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()</p> <code>None</code> <code>order_by</code> <p>order by a single attribute or the list of attributes. defaults to 'id'.  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill(restriction=None, connection=None, order_by=None):\n\"\"\"\n    view and kill database connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n    :param order_by: order by a single attribute or the list of attributes. defaults to 'id'.\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\".\n        dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes\n    \"\"\"\n\n    if connection is None:\n        connection = conn()\n\n    if order_by is not None and not isinstance(order_by, str):\n        order_by = \",\".join(order_by)\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n        + (\" ORDER BY %s\" % (order_by or \"id\"))\n    )\n\n    while True:\n        print(\"  ID USER         HOST          STATE         TIME    INFO\")\n        print(\"+--+ +----------+ +-----------+ +-----------+ +-----+\")\n        cur = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in connection.query(query, as_dict=True)\n        )\n        for process in cur:\n            try:\n                print(\n                    \"{id:&gt;4d} {user:&lt;12s} {host:&lt;12s} {state:&lt;12s} {time:&gt;7d}  {info}\".format(\n                        **process\n                    )\n                )\n            except TypeError:\n                print(process)\n        response = input('process to kill or \"q\" to quit &gt; ')\n        if response == \"q\":\n            break\n        if response:\n            try:\n                pid = int(response)\n            except ValueError:\n                pass  # ignore non-numeric input\n            else:\n                try:\n                    connection.query(\"kill %d\" % pid)\n                except pymysql.err.InternalError:\n                    logger.warn(\"Process not found\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Schema", "title": "<code>Schema</code>", "text": "<p>A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace <code>context</code> in which other UserTable classes are defined.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class Schema:\n\"\"\"\n    A schema object is a decorator for UserTable classes that binds them to their database.\n    It also specifies the namespace `context` in which other UserTable classes are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_name=None,\n        context=None,\n        *,\n        connection=None,\n        create_schema=True,\n        create_tables=True,\n        add_objects=None,\n    ):\n\"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        If the schema_name is omitted, then schema.activate(..) must be called later\n        to associate with the database.\n\n        :param schema_name: the database schema to associate.\n        :param context: dictionary for looking up foreign key references, leave None to use local context.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: When False, do not create the schema and raise an error if missing.\n        :param create_tables: When False, do not create tables and raise errors when accessing missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context in which table classes\n        are declared.\n        \"\"\"\n        self._log = None\n        self.connection = connection\n        self.database = None\n        self.context = context\n        self.create_schema = create_schema\n        self.create_tables = create_tables\n        self._jobs = None\n        self.external = ExternalMapping(self)\n        self.add_objects = add_objects\n        self.declare_list = []\n        if schema_name:\n            self.activate(schema_name)\n\n    def is_activated(self):\n        return self.database is not None\n\n    def activate(\n        self,\n        schema_name=None,\n        *,\n        connection=None,\n        create_schema=None,\n        create_tables=None,\n        add_objects=None,\n    ):\n\"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        :param schema_name: the database schema to associate.\n            schema_name=None is used to assert that the schema has already been activated.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: If False, do not create the schema and raise an error if missing.\n        :param create_tables: If False, do not create tables and raise errors when attempting\n            to access missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context\n            in which table classes are declared.\n        \"\"\"\n        if schema_name is None:\n            if self.exists:\n                return\n            raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n        if self.database is not None and self.exists:\n            if self.database == schema_name:  # already activated\n                return\n            raise DataJointError(\n                \"The schema is already activated for schema {db}.\".format(\n                    db=self.database\n                )\n            )\n        if connection is not None:\n            self.connection = connection\n        if self.connection is None:\n            self.connection = conn()\n        self.database = schema_name\n        if create_schema is not None:\n            self.create_schema = create_schema\n        if create_tables is not None:\n            self.create_tables = create_tables\n        if add_objects:\n            self.add_objects = add_objects\n        if not self.exists:\n            if not self.create_schema or not self.database:\n                raise DataJointError(\n                    \"Database `{name}` has not yet been declared. \"\n                    \"Set argument create_schema=True to create it.\".format(\n                        name=schema_name\n                    )\n                )\n            # create database\n            logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n            try:\n                self.connection.query(\n                    \"CREATE DATABASE `{name}`\".format(name=schema_name)\n                )\n            except AccessError:\n                raise DataJointError(\n                    \"Schema `{name}` does not exist and could not be created. \"\n                    \"Check permissions.\".format(name=schema_name)\n                )\n            else:\n                self.log(\"created\")\n        self.connection.register(self)\n\n        # decorate all tables already decorated\n        for cls, context in self.declare_list:\n            if self.add_objects:\n                context = dict(context, **self.add_objects)\n            self._decorate_master(cls, context)\n\n    def _assert_exists(self, message=None):\n        if not self.exists:\n            raise DataJointError(\n                message\n                or \"Schema `{db}` has not been created.\".format(db=self.database)\n            )\n\n    def __call__(self, cls, *, context=None):\n\"\"\"\n        Binds the supplied class to a schema. This is intended to be used as a decorator.\n\n        :param cls: class to decorate.\n        :param context: supplied when called from spawn_missing_classes\n        \"\"\"\n        context = context or self.context or inspect.currentframe().f_back.f_locals\n        if issubclass(cls, Part):\n            raise DataJointError(\n                \"The schema decorator should not be applied to Part tables.\"\n            )\n        if self.is_activated():\n            self._decorate_master(cls, context)\n        else:\n            self.declare_list.append((cls, context))\n        return cls\n\n    def _decorate_master(self, cls, context):\n\"\"\"\n\n        :param cls: the master class to process\n        :param context: the class' declaration context\n        \"\"\"\n        self._decorate_table(\n            cls, context=dict(context, self=cls, **{cls.__name__: cls})\n        )\n        # Process part tables\n        for part in ordered_dir(cls):\n            if part[0].isupper():\n                part = getattr(cls, part)\n                if inspect.isclass(part) and issubclass(part, Part):\n                    part._master = cls\n                    # allow addressing master by name or keyword 'master'\n                    self._decorate_table(\n                        part,\n                        context=dict(\n                            context, master=cls, self=part, **{cls.__name__: cls}\n                        ),\n                    )\n\n    def _decorate_table(self, table_class, context, assert_declared=False):\n\"\"\"\n        assign schema properties to the table class and declare the table\n        \"\"\"\n        table_class.database = self.database\n        table_class._connection = self.connection\n        table_class._heading = Heading(\n            table_info=dict(\n                conn=self.connection,\n                database=self.database,\n                table_name=table_class.table_name,\n                context=context,\n            )\n        )\n        table_class._support = [table_class.full_table_name]\n        table_class.declaration_context = context\n\n        # instantiate the class, declare the table if not already\n        instance = table_class()\n        is_declared = instance.is_declared\n        if not is_declared and not assert_declared and self.create_tables:\n            instance.declare(context)\n            self.connection.dependencies.clear()\n        is_declared = is_declared or instance.is_declared\n\n        # add table definition to the doc string\n        if isinstance(table_class.definition, str):\n            table_class.__doc__ = (\n                (table_class.__doc__ or \"\")\n                + \"\\nTable definition:\\n\\n\"\n                + table_class.definition\n            )\n\n        # fill values in Lookup tables from their contents property\n        if (\n            isinstance(instance, Lookup)\n            and hasattr(instance, \"contents\")\n            and is_declared\n        ):\n            contents = list(instance.contents)\n            if len(contents) &gt; len(instance):\n                if instance.heading.has_autoincrement:\n                    warnings.warn(\n                        (\n                            \"Contents has changed but cannot be inserted because \"\n                            \"{table} has autoincrement.\"\n                        ).format(table=instance.__class__.__name__)\n                    )\n                else:\n                    instance.insert(contents, skip_duplicates=True)\n\n    @property\n    def log(self):\n        self._assert_exists()\n        if self._log is None:\n            self._log = Log(self.connection, self.database)\n        return self._log\n\n    def __repr__(self):\n        return \"Schema `{name}`\\n\".format(name=self.database)\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of the entire schema in bytes\n        \"\"\"\n        self._assert_exists()\n        return int(\n            self.connection.query(\n\"\"\"\n            SELECT SUM(data_length + index_length)\n            FROM information_schema.tables WHERE table_schema='{db}'\n            \"\"\".format(\n                    db=self.database\n                )\n            ).fetchone()[0]\n        )\n\n    def spawn_missing_classes(self, context=None):\n\"\"\"\n        Creates the appropriate python user table classes from tables in the schema and places them\n        in the context.\n\n        :param context: alternative context to place the missing classes into, e.g. locals()\n        \"\"\"\n        self._assert_exists()\n        if context is None:\n            if self.context is not None:\n                context = self.context\n            else:\n                # if context is missing, use the calling namespace\n                frame = inspect.currentframe().f_back\n                context = frame.f_locals\n                del frame\n        tables = [\n            row[0]\n            for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n            if lookup_class_name(\n                \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n            )\n            is None\n        ]\n        master_classes = (Lookup, Manual, Imported, Computed)\n        part_tables = []\n        for table_name in tables:\n            class_name = to_camel_case(table_name)\n            if class_name not in context:\n                try:\n                    cls = next(\n                        cls\n                        for cls in master_classes\n                        if re.fullmatch(cls.tier_regexp, table_name)\n                    )\n                except StopIteration:\n                    if re.fullmatch(Part.tier_regexp, table_name):\n                        part_tables.append(table_name)\n                else:\n                    # declare and decorate master table classes\n                    context[class_name] = self(\n                        type(class_name, (cls,), dict()), context=context\n                    )\n\n        # attach parts to masters\n        for table_name in part_tables:\n            groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n            class_name = to_camel_case(groups[\"part\"])\n            try:\n                master_class = context[to_camel_case(groups[\"master\"])]\n            except KeyError:\n                raise DataJointError(\n                    \"The table %s does not follow DataJoint naming conventions\"\n                    % table_name\n                )\n            part_class = type(class_name, (Part,), dict(definition=...))\n            part_class._master = master_class\n            self._decorate_table(part_class, context=context, assert_declared=True)\n            setattr(master_class, class_name, part_class)\n\n    def drop(self, force=False):\n\"\"\"\n        Drop the associated schema if it exists\n        \"\"\"\n        if not self.exists:\n            logger.info(\n                \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                    database=self.database\n                )\n            )\n        elif (\n            not config[\"safemode\"]\n            or force\n            or user_choice(\n                \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n            )\n            == \"yes\"\n        ):\n            logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n            try:\n                self.connection.query(\n                    \"DROP DATABASE `{database}`\".format(database=self.database)\n                )\n                logger.debug(\n                    \"Schema `{database}` was dropped successfully.\".format(\n                        database=self.database\n                    )\n                )\n            except AccessError:\n                raise AccessError(\n                    \"An attempt to drop schema `{database}` \"\n                    \"has failed. Check permissions.\".format(database=self.database)\n                )\n\n    @property\n    def exists(self):\n\"\"\"\n        :return: true if the associated schema exists on the server\n        \"\"\"\n        if self.database is None:\n            raise DataJointError(\"Schema must be activated first.\")\n        return bool(\n            self.connection.query(\n                \"SELECT schema_name \"\n                \"FROM information_schema.schemata \"\n                \"WHERE schema_name = '{database}'\".format(database=self.database)\n            ).rowcount\n        )\n\n    @property\n    def jobs(self):\n\"\"\"\n        schema.jobs provides a view of the job reservation table for the schema\n\n        :return: jobs table\n        \"\"\"\n        self._assert_exists()\n        if self._jobs is None:\n            self._jobs = JobTable(self.connection, self.database)\n        return self._jobs\n\n    @property\n    def code(self):\n        self._assert_exists()\n        return self.save()\n\n    def save(self, python_filename=None):\n\"\"\"\n        Generate the code for a module that recreates the schema.\n        This method is in preparation for a future release and is not officially supported.\n\n        :return: a string containing the body of a complete Python module defining this schema.\n        \"\"\"\n        self._assert_exists()\n        module_count = itertools.count()\n        # add virtual modules for referenced modules with names vmod0, vmod1, ...\n        module_lookup = collections.defaultdict(\n            lambda: \"vmod\" + str(next(module_count))\n        )\n        db = self.database\n\n        def make_class_definition(table):\n            tier = _get_tier(table).__name__\n            class_name = table.split(\".\")[1].strip(\"`\")\n            indent = \"\"\n            if tier == \"Part\":\n                class_name = class_name.split(\"__\")[-1]\n                indent += \"    \"\n            class_name = to_camel_case(class_name)\n\n            def replace(s):\n                d, tabs = s.group(1), s.group(2)\n                return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                    to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n                )\n\n            return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n                \"{indent}class {class_name}(dj.{tier}):\\n\"\n                '{indent}    definition = \"\"\"\\n'\n                '{indent}    {defi}\"\"\"'\n            ).format(\n                class_name=class_name,\n                indent=indent,\n                tier=tier,\n                defi=re.sub(\n                    r\"`([^`]+)`.`([^`]+)`\",\n                    replace,\n                    FreeTable(self.connection, table).describe(),\n                ).replace(\"\\n\", \"\\n    \" + indent),\n            )\n\n        diagram = Diagram(self)\n        body = \"\\n\\n\".join(\n            make_class_definition(table) for table in diagram.topological_sort()\n        )\n        python_code = \"\\n\\n\".join(\n            (\n                '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n                \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n                \"\\n\".join(\n                    \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                        module=v, schema_name=k\n                    )\n                    for k, v in module_lookup.items()\n                ),\n                body,\n            )\n        )\n        if python_filename is None:\n            return python_code\n        with open(python_filename, \"wt\") as f:\n            f.write(python_code)\n\n    def list_tables(self):\n\"\"\"\n        Return a list of all tables in the schema except tables with ~ in first character such\n        as ~logs and ~job\n\n        :return: A list of table names from the database schema.\n        \"\"\"\n        return [\n            t\n            for d, t in (\n                full_t.replace(\"`\", \"\").split(\".\")\n                for full_t in Diagram(self).topological_sort()\n            )\n            if d == self.database\n        ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.activate", "title": "<code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)</code>", "text": "<p>Associate database schema <code>schema_name</code>. If the schema does not exist, attempt to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <p>the database schema to associate. schema_name=None is used to assert that the schema has already been activated.</p> <code>None</code> <code>connection</code> <p>Connection object. Defaults to datajoint.conn().</p> <code>None</code> <code>create_schema</code> <p>If False, do not create the schema and raise an error if missing.</p> <code>None</code> <code>create_tables</code> <p>If False, do not create tables and raise errors when attempting to access missing tables.</p> <code>None</code> <code>add_objects</code> <p>a mapping with additional objects to make available to the context in which table classes are declared.</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def activate(\n    self,\n    schema_name=None,\n    *,\n    connection=None,\n    create_schema=None,\n    create_tables=None,\n    add_objects=None,\n):\n\"\"\"\n    Associate database schema `schema_name`. If the schema does not exist, attempt to\n    create it on the server.\n\n    :param schema_name: the database schema to associate.\n        schema_name=None is used to assert that the schema has already been activated.\n    :param connection: Connection object. Defaults to datajoint.conn().\n    :param create_schema: If False, do not create the schema and raise an error if missing.\n    :param create_tables: If False, do not create tables and raise errors when attempting\n        to access missing tables.\n    :param add_objects: a mapping with additional objects to make available to the context\n        in which table classes are declared.\n    \"\"\"\n    if schema_name is None:\n        if self.exists:\n            return\n        raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n    if self.database is not None and self.exists:\n        if self.database == schema_name:  # already activated\n            return\n        raise DataJointError(\n            \"The schema is already activated for schema {db}.\".format(\n                db=self.database\n            )\n        )\n    if connection is not None:\n        self.connection = connection\n    if self.connection is None:\n        self.connection = conn()\n    self.database = schema_name\n    if create_schema is not None:\n        self.create_schema = create_schema\n    if create_tables is not None:\n        self.create_tables = create_tables\n    if add_objects:\n        self.add_objects = add_objects\n    if not self.exists:\n        if not self.create_schema or not self.database:\n            raise DataJointError(\n                \"Database `{name}` has not yet been declared. \"\n                \"Set argument create_schema=True to create it.\".format(\n                    name=schema_name\n                )\n            )\n        # create database\n        logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n        try:\n            self.connection.query(\n                \"CREATE DATABASE `{name}`\".format(name=schema_name)\n            )\n        except AccessError:\n            raise DataJointError(\n                \"Schema `{name}` does not exist and could not be created. \"\n                \"Check permissions.\".format(name=schema_name)\n            )\n        else:\n            self.log(\"created\")\n    self.connection.register(self)\n\n    # decorate all tables already decorated\n    for cls, context in self.declare_list:\n        if self.add_objects:\n            context = dict(context, **self.add_objects)\n        self._decorate_master(cls, context)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of the entire schema in bytes</p>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.spawn_missing_classes", "title": "<code>spawn_missing_classes(context=None)</code>", "text": "<p>Creates the appropriate python user table classes from tables in the schema and places them in the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>alternative context to place the missing classes into, e.g. locals()</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def spawn_missing_classes(self, context=None):\n\"\"\"\n    Creates the appropriate python user table classes from tables in the schema and places them\n    in the context.\n\n    :param context: alternative context to place the missing classes into, e.g. locals()\n    \"\"\"\n    self._assert_exists()\n    if context is None:\n        if self.context is not None:\n            context = self.context\n        else:\n            # if context is missing, use the calling namespace\n            frame = inspect.currentframe().f_back\n            context = frame.f_locals\n            del frame\n    tables = [\n        row[0]\n        for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n        if lookup_class_name(\n            \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n        )\n        is None\n    ]\n    master_classes = (Lookup, Manual, Imported, Computed)\n    part_tables = []\n    for table_name in tables:\n        class_name = to_camel_case(table_name)\n        if class_name not in context:\n            try:\n                cls = next(\n                    cls\n                    for cls in master_classes\n                    if re.fullmatch(cls.tier_regexp, table_name)\n                )\n            except StopIteration:\n                if re.fullmatch(Part.tier_regexp, table_name):\n                    part_tables.append(table_name)\n            else:\n                # declare and decorate master table classes\n                context[class_name] = self(\n                    type(class_name, (cls,), dict()), context=context\n                )\n\n    # attach parts to masters\n    for table_name in part_tables:\n        groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n        class_name = to_camel_case(groups[\"part\"])\n        try:\n            master_class = context[to_camel_case(groups[\"master\"])]\n        except KeyError:\n            raise DataJointError(\n                \"The table %s does not follow DataJoint naming conventions\"\n                % table_name\n            )\n        part_class = type(class_name, (Part,), dict(definition=...))\n        part_class._master = master_class\n        self._decorate_table(part_class, context=context, assert_declared=True)\n        setattr(master_class, class_name, part_class)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.drop", "title": "<code>drop(force=False)</code>", "text": "<p>Drop the associated schema if it exists</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    Drop the associated schema if it exists\n    \"\"\"\n    if not self.exists:\n        logger.info(\n            \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                database=self.database\n            )\n        )\n    elif (\n        not config[\"safemode\"]\n        or force\n        or user_choice(\n            \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n        )\n        == \"yes\"\n    ):\n        logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n        try:\n            self.connection.query(\n                \"DROP DATABASE `{database}`\".format(database=self.database)\n            )\n            logger.debug(\n                \"Schema `{database}` was dropped successfully.\".format(\n                    database=self.database\n                )\n            )\n        except AccessError:\n            raise AccessError(\n                \"An attempt to drop schema `{database}` \"\n                \"has failed. Check permissions.\".format(database=self.database)\n            )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.exists", "title": "<code>exists</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>true if the associated schema exists on the server</p>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.jobs", "title": "<code>jobs</code>  <code>property</code>", "text": "<p>schema.jobs provides a view of the job reservation table for the schema</p> <p>Returns:</p> Type Description <p>jobs table</p>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.save", "title": "<code>save(python_filename=None)</code>", "text": "<p>Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported.</p> <p>Returns:</p> Type Description <p>a string containing the body of a complete Python module defining this schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def save(self, python_filename=None):\n\"\"\"\n    Generate the code for a module that recreates the schema.\n    This method is in preparation for a future release and is not officially supported.\n\n    :return: a string containing the body of a complete Python module defining this schema.\n    \"\"\"\n    self._assert_exists()\n    module_count = itertools.count()\n    # add virtual modules for referenced modules with names vmod0, vmod1, ...\n    module_lookup = collections.defaultdict(\n        lambda: \"vmod\" + str(next(module_count))\n    )\n    db = self.database\n\n    def make_class_definition(table):\n        tier = _get_tier(table).__name__\n        class_name = table.split(\".\")[1].strip(\"`\")\n        indent = \"\"\n        if tier == \"Part\":\n            class_name = class_name.split(\"__\")[-1]\n            indent += \"    \"\n        class_name = to_camel_case(class_name)\n\n        def replace(s):\n            d, tabs = s.group(1), s.group(2)\n            return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n            )\n\n        return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n            \"{indent}class {class_name}(dj.{tier}):\\n\"\n            '{indent}    definition = \"\"\"\\n'\n            '{indent}    {defi}\"\"\"'\n        ).format(\n            class_name=class_name,\n            indent=indent,\n            tier=tier,\n            defi=re.sub(\n                r\"`([^`]+)`.`([^`]+)`\",\n                replace,\n                FreeTable(self.connection, table).describe(),\n            ).replace(\"\\n\", \"\\n    \" + indent),\n        )\n\n    diagram = Diagram(self)\n    body = \"\\n\\n\".join(\n        make_class_definition(table) for table in diagram.topological_sort()\n    )\n    python_code = \"\\n\\n\".join(\n        (\n            '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n            \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n            \"\\n\".join(\n                \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                    module=v, schema_name=k\n                )\n                for k, v in module_lookup.items()\n            ),\n            body,\n        )\n    )\n    if python_filename is None:\n        return python_code\n    with open(python_filename, \"wt\") as f:\n        f.write(python_code)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.schemas.Schema.list_tables", "title": "<code>list_tables()</code>", "text": "<p>Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job</p> <p>Returns:</p> Type Description <p>A list of table names from the database schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_tables(self):\n\"\"\"\n    Return a list of all tables in the schema except tables with ~ in first character such\n    as ~logs and ~job\n\n    :return: A list of table names from the database schema.\n    \"\"\"\n    return [\n        t\n        for d, t in (\n            full_t.replace(\"`\", \"\").split(\".\")\n            for full_t in Diagram(self).topological_sort()\n        )\n        if d == self.database\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.AndList", "title": "<code>AndList</code>", "text": "<p>         Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n\"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/__init__/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Not", "title": "<code>Not</code>", "text": "<p>invert restriction</p> Source code in <code>datajoint/condition.py</code> <pre><code>class Not:\n\"\"\"invert restriction\"\"\"\n\n    def __init__(self, restriction):\n        self.restriction = restriction\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Diagram", "title": "<code>Diagram</code>", "text": "<p>         Bases: <code>nx.DiGraph</code></p> <p>Entity relationship diagram.</p> <p>Usage:</p> <p>diag = Diagram(source)</p> <p>source can be a base table object, a base table class, a schema, or a module that has a schema.</p> <p>diag.draw()</p> <p>draws the diagram using pyplot</p> <p>diag1 + diag2  - combines the two diagrams. diag + n   - expands n levels of successors diag - n   - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table</p> <p>Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed</p> Source code in <code>datajoint/diagram.py</code> <pre><code>class Diagram(nx.DiGraph):\n\"\"\"\n    Entity relationship diagram.\n\n    Usage:\n\n    &gt;&gt;&gt;  diag = Diagram(source)\n\n    source can be a base table object, a base table class, a schema, or a module that has a schema.\n\n    &gt;&gt;&gt; diag.draw()\n\n    draws the diagram using pyplot\n\n    diag1 + diag2  - combines the two diagrams.\n    diag + n   - expands n levels of successors\n    diag - n   - expands n levels of predecessors\n    Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table\n\n    Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth.\n    Only those tables that are loaded in the connection object are displayed\n    \"\"\"\n\n    def __init__(self, source, context=None):\n        if isinstance(source, Diagram):\n            # copy constructor\n            self.nodes_to_show = set(source.nodes_to_show)\n            self.context = source.context\n            super().__init__(source)\n            return\n\n        # get the caller's context\n        if context is None:\n            frame = inspect.currentframe().f_back\n            self.context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        else:\n            self.context = context\n\n        # find connection in the source\n        try:\n            connection = source.connection\n        except AttributeError:\n            try:\n                connection = source.schema.connection\n            except AttributeError:\n                raise DataJointError(\n                    \"Could not find database connection in %s\" % repr(source[0])\n                )\n\n        # initialize graph from dependencies\n        connection.dependencies.load()\n        super().__init__(connection.dependencies)\n\n        # Enumerate nodes from all the items in the list\n        self.nodes_to_show = set()\n        try:\n            self.nodes_to_show.add(source.full_table_name)\n        except AttributeError:\n            try:\n                database = source.database\n            except AttributeError:\n                try:\n                    database = source.schema.database\n                except AttributeError:\n                    raise DataJointError(\n                        \"Cannot plot Diagram for %s\" % repr(source)\n                    )\n            for node in self:\n                if node.startswith(\"`%s`\" % database):\n                    self.nodes_to_show.add(node)\n\n    @classmethod\n    def from_sequence(cls, sequence):\n\"\"\"\n        The join Diagram for all objects in sequence\n\n        :param sequence: a sequence (e.g. list, tuple)\n        :return: Diagram(arg1) + ... + Diagram(argn)\n        \"\"\"\n        return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n\n    def add_parts(self):\n\"\"\"\n        Adds to the diagram the part tables of tables already included in the diagram\n        :return:\n        \"\"\"\n\n        def is_part(part, master):\n\"\"\"\n            :param part:  `database`.`table_name`\n            :param master:   `database`.`table_name`\n            :return: True if part is part of master.\n            \"\"\"\n            part = [s.strip(\"`\") for s in part.split(\".\")]\n            master = [s.strip(\"`\") for s in master.split(\".\")]\n            return (\n                master[0] == part[0]\n                and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n            )\n\n        self = Diagram(self)  # copy\n        self.nodes_to_show.update(\n            n\n            for n in self.nodes()\n            if any(is_part(n, m) for m in self.nodes_to_show)\n        )\n        return self\n\n    def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n        return unite_master_parts(\n            list(\n                nx.algorithms.dag.topological_sort(\n                    nx.DiGraph(self).subgraph(self.nodes_to_show)\n                )\n            )\n        )\n\n    def __add__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Union of the diagrams when arg is another Diagram\n                 or an expansion downstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.add(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    new = nx.algorithms.boundary.node_boundary(\n                        self, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            self, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __sub__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Difference of the diagrams when arg is another Diagram or\n                 an expansion upstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.difference_update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.remove(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    graph = nx.DiGraph(self).reverse()\n                    new = nx.algorithms.boundary.node_boundary(\n                        graph, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            graph, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __mul__(self, arg):\n\"\"\"\n        Intersection of two diagrams\n        :param arg: another Diagram\n        :return: a new Diagram comprising nodes that are present in both operands.\n        \"\"\"\n        self = Diagram(self)  # copy\n        self.nodes_to_show.intersection_update(arg.nodes_to_show)\n        return self\n\n    def _make_graph(self):\n\"\"\"\n        Make the self.graph - a graph object ready for drawing\n        \"\"\"\n        # mark \"distinguished\" tables, i.e. those that introduce new primary key\n        # attributes\n        for name in self.nodes_to_show:\n            foreign_attributes = set(\n                attr\n                for p in self.in_edges(name, data=True)\n                for attr in p[2][\"attr_map\"]\n                if p[2][\"primary\"]\n            )\n            self.nodes[name][\"distinguished\"] = (\n                \"primary_key\" in self.nodes[name]\n                and foreign_attributes &lt; self.nodes[name][\"primary_key\"]\n            )\n        # include aliased nodes that are sandwiched between two displayed nodes\n        gaps = set(\n            nx.algorithms.boundary.node_boundary(self, self.nodes_to_show)\n        ).intersection(\n            nx.algorithms.boundary.node_boundary(\n                nx.DiGraph(self).reverse(), self.nodes_to_show\n            )\n        )\n        nodes = self.nodes_to_show.union(a for a in gaps if a.isdigit)\n        # construct subgraph and rename nodes to class names\n        graph = nx.DiGraph(nx.DiGraph(self).subgraph(nodes))\n        nx.set_node_attributes(\n            graph, name=\"node_type\", values={n: _get_tier(n) for n in graph}\n        )\n        # relabel nodes to class names\n        mapping = {\n            node: lookup_class_name(node, self.context) or node\n            for node in graph.nodes()\n        }\n        new_names = [mapping.values()]\n        if len(new_names) &gt; len(set(new_names)):\n            raise DataJointError(\n                \"Some classes have identical names. The Diagram cannot be plotted.\"\n            )\n        nx.relabel_nodes(graph, mapping, copy=False)\n        return graph\n\n    def make_dot(self):\n        graph = self._make_graph()\n        graph.nodes()\n\n        scale = 1.2  # scaling factor for fonts and boxes\n        label_props = {  # http://matplotlib.org/examples/color/named_colors.html\n            None: dict(\n                shape=\"circle\",\n                color=\"#FFFF0040\",\n                fontcolor=\"yellow\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            _AliasNode: dict(\n                shape=\"circle\",\n                color=\"#FF880080\",\n                fontcolor=\"#FF880080\",\n                fontsize=round(scale * 0),\n                size=0.05 * scale,\n                fixed=True,\n            ),\n            Manual: dict(\n                shape=\"box\",\n                color=\"#00FF0030\",\n                fontcolor=\"darkgreen\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Lookup: dict(\n                shape=\"plaintext\",\n                color=\"#00000020\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Computed: dict(\n                shape=\"ellipse\",\n                color=\"#FF000020\",\n                fontcolor=\"#7F0000A0\",\n                fontsize=round(scale * 10),\n                size=0.3 * scale,\n                fixed=True,\n            ),\n            Imported: dict(\n                shape=\"ellipse\",\n                color=\"#00007F40\",\n                fontcolor=\"#00007FA0\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Part: dict(\n                shape=\"plaintext\",\n                color=\"#0000000\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.1 * scale,\n                fixed=False,\n            ),\n        }\n        node_props = {\n            node: label_props[d[\"node_type\"]]\n            for node, d in dict(graph.nodes(data=True)).items()\n        }\n\n        dot = nx.drawing.nx_pydot.to_pydot(graph)\n        for node in dot.get_nodes():\n            node.set_shape(\"circle\")\n            name = node.get_name().strip('\"')\n            props = node_props[name]\n            node.set_fontsize(props[\"fontsize\"])\n            node.set_fontcolor(props[\"fontcolor\"])\n            node.set_shape(props[\"shape\"])\n            node.set_fontname(\"arial\")\n            node.set_fixedsize(\"shape\" if props[\"fixed\"] else False)\n            node.set_width(props[\"size\"])\n            node.set_height(props[\"size\"])\n            if name.split(\".\")[0] in self.context:\n                cls = eval(name, self.context)\n                assert issubclass(cls, Table)\n                description = cls().describe(context=self.context).split(\"\\n\")\n                description = (\n                    \"-\" * 30\n                    if q.startswith(\"---\")\n                    else q.replace(\"-&gt;\", \"&amp;#8594;\")\n                    if \"-&gt;\" in q\n                    else q.split(\":\")[0]\n                    for q in description\n                    if not q.startswith(\"#\")\n                )\n                node.set_tooltip(\"&amp;#13;\".join(description))\n            node.set_label(\n                \"&lt;&lt;u&gt;\" + name + \"&lt;/u&gt;&gt;\"\n                if node.get(\"distinguished\") == \"True\"\n                else name\n            )\n            node.set_color(props[\"color\"])\n            node.set_style(\"filled\")\n\n        for edge in dot.get_edges():\n            # see https://graphviz.org/doc/info/attrs.html\n            src = edge.get_source().strip('\"')\n            dest = edge.get_destination().strip('\"')\n            props = graph.get_edge_data(src, dest)\n            edge.set_color(\"#00000040\")\n            edge.set_style(\"solid\" if props[\"primary\"] else \"dashed\")\n            master_part = graph.nodes[dest][\n                \"node_type\"\n            ] is Part and dest.startswith(src + \".\")\n            edge.set_weight(3 if master_part else 1)\n            edge.set_arrowhead(\"none\")\n            edge.set_penwidth(0.75 if props[\"multi\"] else 2)\n\n        return dot\n\n    def make_svg(self):\n        from IPython.display import SVG\n\n        return SVG(self.make_dot().create_svg())\n\n    def make_png(self):\n        return io.BytesIO(self.make_dot().create_png())\n\n    def make_image(self):\n        if plot_active:\n            return plt.imread(self.make_png())\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def _repr_svg_(self):\n        return self.make_svg()._repr_svg_()\n\n    def draw(self):\n        if plot_active:\n            plt.imshow(self.make_image())\n            plt.gca().axis(\"off\")\n            plt.show()\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def save(self, filename, format=None):\n        if format is None:\n            if filename.lower().endswith(\".png\"):\n                format = \"png\"\n            elif filename.lower().endswith(\".svg\"):\n                format = \"svg\"\n        if format.lower() == \"png\":\n            with open(filename, \"wb\") as f:\n                f.write(self.make_png().getbuffer().tobytes())\n        elif format.lower() == \"svg\":\n            with open(filename, \"w\") as f:\n                f.write(self.make_svg().data)\n        else:\n            raise DataJointError(\"Unsupported file format\")\n\n    @staticmethod\n    def _layout(graph, **kwargs):\n        return pydot_layout(graph, prog=\"dot\", **kwargs)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.from_sequence", "title": "<code>from_sequence(sequence)</code>  <code>classmethod</code>", "text": "<p>The join Diagram for all objects in sequence</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <p>a sequence (e.g. list, tuple)</p> required <p>Returns:</p> Type Description <p>Diagram(arg1) + ... + Diagram(argn)</p> Source code in <code>datajoint/diagram.py</code> <pre><code>@classmethod\ndef from_sequence(cls, sequence):\n\"\"\"\n    The join Diagram for all objects in sequence\n\n    :param sequence: a sequence (e.g. list, tuple)\n    :return: Diagram(arg1) + ... + Diagram(argn)\n    \"\"\"\n    return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.add_parts", "title": "<code>add_parts()</code>", "text": "<p>Adds to the diagram the part tables of tables already included in the diagram</p> <p>Returns:</p> Type Description Source code in <code>datajoint/diagram.py</code> <pre><code>def add_parts(self):\n\"\"\"\n    Adds to the diagram the part tables of tables already included in the diagram\n    :return:\n    \"\"\"\n\n    def is_part(part, master):\n\"\"\"\n        :param part:  `database`.`table_name`\n        :param master:   `database`.`table_name`\n        :return: True if part is part of master.\n        \"\"\"\n        part = [s.strip(\"`\") for s in part.split(\".\")]\n        master = [s.strip(\"`\") for s in master.split(\".\")]\n        return (\n            master[0] == part[0]\n            and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n        )\n\n    self = Diagram(self)  # copy\n    self.nodes_to_show.update(\n        n\n        for n in self.nodes()\n        if any(is_part(n, m) for m in self.nodes_to_show)\n    )\n    return self\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.diagram.Diagram.topological_sort", "title": "<code>topological_sort()</code>", "text": "<p>Returns:</p> Type Description <p>list of nodes in topological order</p> Source code in <code>datajoint/diagram.py</code> <pre><code>def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n    return unite_master_parts(\n        list(\n            nx.algorithms.dag.topological_sort(\n                nx.DiGraph(self).subgraph(self.nodes_to_show)\n            )\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.MatCell", "title": "<code>MatCell</code>", "text": "<p>         Bases: <code>np.ndarray</code></p> <p>a numpy ndarray representing a Matlab cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatCell(np.ndarray):\n\"\"\"a numpy ndarray representing a Matlab cell array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.MatStruct", "title": "<code>MatStruct</code>", "text": "<p>         Bases: <code>np.recarray</code></p> <p>numpy.recarray representing a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatStruct(np.recarray):\n\"\"\"numpy.recarray representing a Matlab struct array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n\"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS prefered, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Manual", "title": "<code>Manual</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Lookup", "title": "<code>Lookup</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Imported", "title": "<code>Imported</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Connection", "title": "<code>Connection</code>", "text": "<p>A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys).</p> <p>Most of the parameters below should be set in the local configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>host name, may include port number as hostname:port, in which case it overrides the value in port</p> required <code>user</code> <p>user name</p> required <code>password</code> <p>password</p> required <code>port</code> <p>port number</p> <code>None</code> <code>init_fun</code> <p>connection initialization function (SQL)</p> <code>None</code> <code>use_tls</code> <p>TLS encryption option</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>class Connection:\n\"\"\"\n    A dj.Connection object manages a connection to a database server.\n    It also catalogues modules, schemas, tables, and their dependencies (foreign keys).\n\n    Most of the parameters below should be set in the local configuration file.\n\n    :param host: host name, may include port number as hostname:port, in which case it overrides the value in port\n    :param user: user name\n    :param password: password\n    :param port: port number\n    :param init_fun: connection initialization function (SQL)\n    :param use_tls: TLS encryption option\n    \"\"\"\n\n    def __init__(self, host, user, password, port=None, init_fun=None, use_tls=None):\n        host_input, host = (host, get_host_hook(host))\n        if \":\" in host:\n            # the port in the hostname overrides the port argument\n            host, port = host.split(\":\")\n            port = int(port)\n        elif port is None:\n            port = config[\"database.port\"]\n        self.conn_info = dict(host=host, port=port, user=user, passwd=password)\n        if use_tls is not False:\n            self.conn_info[\"ssl\"] = (\n                use_tls if isinstance(use_tls, dict) else {\"ssl\": {}}\n            )\n        self.conn_info[\"ssl_input\"] = use_tls\n        self.conn_info[\"host_input\"] = host_input\n        self.init_fun = init_fun\n        logger.info(\"Connecting {user}@{host}:{port}\".format(**self.conn_info))\n        self._conn = None\n        self._query_cache = None\n        connect_host_hook(self)\n        if self.is_connected:\n            logger.info(\"Connected {user}@{host}:{port}\".format(**self.conn_info))\n            self.connection_id = self.query(\"SELECT connection_id()\").fetchone()[0]\n        else:\n            raise errors.LostConnectionError(\"Connection failed.\")\n        self._in_transaction = False\n        self.schemas = dict()\n        self.dependencies = Dependencies(self)\n\n    def __eq__(self, other):\n        return self.conn_info == other.conn_info\n\n    def __repr__(self):\n        connected = \"connected\" if self.is_connected else \"disconnected\"\n        return \"DataJoint connection ({connected}) {user}@{host}:{port}\".format(\n            connected=connected, **self.conn_info\n        )\n\n    def connect(self):\n\"\"\"Connect to the database server.\"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n            try:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if k not in [\"ssl_input\", \"host_input\"]\n                    },\n                )\n            except client.err.InternalError:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if not (\n                            k in [\"ssl_input\", \"host_input\"]\n                            or k == \"ssl\"\n                            and self.conn_info[\"ssl_input\"] is None\n                        )\n                    },\n                )\n        self._conn.autocommit(True)\n\n    def set_query_cache(self, query_cache=None):\n\"\"\"\n        When query_cache is not None, the connection switches into the query caching mode, which entails:\n        1. Only SELECT queries are allowed.\n        2. The results of queries are cached under the path indicated by dj.config['query_cache']\n        3. query_cache is a string that differentiates different cache states.\n\n        :param query_cache: a string to initialize the hash for query results\n        \"\"\"\n        self._query_cache = query_cache\n\n    def purge_query_cache(self):\n\"\"\"Purges all query cache.\"\"\"\n        if (\n            isinstance(config.get(cache_key), str)\n            and pathlib.Path(config[cache_key]).is_dir()\n        ):\n            for path in pathlib.Path(config[cache_key]).iterdir():\n                if not path.is_dir():\n                    path.unlink()\n\n    def close(self):\n        self._conn.close()\n\n    def register(self, schema):\n        self.schemas[schema.database] = schema\n        self.dependencies.clear()\n\n    def ping(self):\n\"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n        self._conn.ping(reconnect=False)\n\n    @property\n    def is_connected(self):\n\"\"\"Return true if the object is connected to the database server.\"\"\"\n        try:\n            self.ping()\n        except:\n            return False\n        return True\n\n    @staticmethod\n    def _execute_query(cursor, query, args, suppress_warnings):\n        try:\n            with warnings.catch_warnings():\n                if suppress_warnings:\n                    # suppress all warnings arising from underlying SQL library\n                    warnings.simplefilter(\"ignore\")\n                cursor.execute(query, args)\n        except client.err.Error as err:\n            raise translate_query_error(err, query)\n\n    def query(\n        self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n    ):\n\"\"\"\n        Execute the specified query and return the tuple generator (cursor).\n\n        :param query: SQL query\n        :param args: additional arguments for the client.cursor\n        :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                        query results as dictionary.\n        :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n        :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n        \"\"\"\n        # check cache first:\n        use_query_cache = bool(self._query_cache)\n        if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n            raise errors.DataJointError(\n                \"Only SELECT queries are allowed when query caching is on.\"\n            )\n        if use_query_cache:\n            if not config[cache_key]:\n                raise errors.DataJointError(\n                    f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n                )\n            hash_ = uuid_from_buffer(\n                (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n                + pack(args)\n            )\n            cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n            try:\n                buffer = cache_path.read_bytes()\n            except FileNotFoundError:\n                pass  # proceed to query the database\n            else:\n                return EmulatedCursor(unpack(buffer))\n\n        if reconnect is None:\n            reconnect = config[\"database.reconnect\"]\n        logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n        cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n        cursor = self._conn.cursor(cursor=cursor_class)\n        try:\n            self._execute_query(cursor, query, args, suppress_warnings)\n        except errors.LostConnectionError:\n            if not reconnect:\n                raise\n            logger.warning(\"MySQL server has gone away. Reconnecting to the server.\")\n            connect_host_hook(self)\n            if self._in_transaction:\n                self.cancel_transaction()\n                raise errors.LostConnectionError(\n                    \"Connection was lost during a transaction.\"\n                )\n            logger.debug(\"Re-executing\")\n            cursor = self._conn.cursor(cursor=cursor_class)\n            self._execute_query(cursor, query, args, suppress_warnings)\n\n        if use_query_cache:\n            data = cursor.fetchall()\n            cache_path.write_bytes(pack(data))\n            return EmulatedCursor(data)\n\n        return cursor\n\n    def get_user(self):\n\"\"\"\n        :return: the user name and host name provided by the client to the server.\n        \"\"\"\n        return self.query(\"SELECT user()\").fetchone()[0]\n\n    # ---------- transaction processing\n    @property\n    def in_transaction(self):\n\"\"\"\n        :return: True if there is an open transaction.\n        \"\"\"\n        self._in_transaction = self._in_transaction and self.is_connected\n        return self._in_transaction\n\n    def start_transaction(self):\n\"\"\"\n        Starts a transaction error.\n        \"\"\"\n        if self.in_transaction:\n            raise errors.DataJointError(\"Nested connections are not supported.\")\n        self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n        self._in_transaction = True\n        logger.debug(\"Transaction started\")\n\n    def cancel_transaction(self):\n\"\"\"\n        Cancels the current transaction and rolls back all changes made during the transaction.\n        \"\"\"\n        self.query(\"ROLLBACK\")\n        self._in_transaction = False\n        logger.debug(\"Transaction cancelled. Rolling back ...\")\n\n    def commit_transaction(self):\n\"\"\"\n        Commit all changes made during the transaction and close it.\n\n        \"\"\"\n        self.query(\"COMMIT\")\n        self._in_transaction = False\n        logger.debug(\"Transaction committed and closed.\")\n\n    # -------- context manager for transactions\n    @property\n    @contextmanager\n    def transaction(self):\n\"\"\"\n        Context manager for transactions. Opens an transaction and closes it after the with statement.\n        If an error is caught during the transaction, the commits are automatically rolled back.\n        All errors are raised again.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.conn().transaction as conn:\n        &gt;&gt;&gt;     # transaction is open here\n        \"\"\"\n        try:\n            self.start_transaction()\n            yield self\n        except:\n            self.cancel_transaction()\n            raise\n        else:\n            self.commit_transaction()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.connect", "title": "<code>connect()</code>", "text": "<p>Connect to the database server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def connect(self):\n\"\"\"Connect to the database server.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n        try:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if k not in [\"ssl_input\", \"host_input\"]\n                },\n            )\n        except client.err.InternalError:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if not (\n                        k in [\"ssl_input\", \"host_input\"]\n                        or k == \"ssl\"\n                        and self.conn_info[\"ssl_input\"] is None\n                    )\n                },\n            )\n    self._conn.autocommit(True)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.set_query_cache", "title": "<code>set_query_cache(query_cache=None)</code>", "text": "<p>When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states.</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <p>a string to initialize the hash for query results</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def set_query_cache(self, query_cache=None):\n\"\"\"\n    When query_cache is not None, the connection switches into the query caching mode, which entails:\n    1. Only SELECT queries are allowed.\n    2. The results of queries are cached under the path indicated by dj.config['query_cache']\n    3. query_cache is a string that differentiates different cache states.\n\n    :param query_cache: a string to initialize the hash for query results\n    \"\"\"\n    self._query_cache = query_cache\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.purge_query_cache", "title": "<code>purge_query_cache()</code>", "text": "<p>Purges all query cache.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def purge_query_cache(self):\n\"\"\"Purges all query cache.\"\"\"\n    if (\n        isinstance(config.get(cache_key), str)\n        and pathlib.Path(config[cache_key]).is_dir()\n    ):\n        for path in pathlib.Path(config[cache_key]).iterdir():\n            if not path.is_dir():\n                path.unlink()\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.ping", "title": "<code>ping()</code>", "text": "<p>Ping the connection or raises an exception if the connection is closed.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def ping(self):\n\"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n    self._conn.ping(reconnect=False)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.is_connected", "title": "<code>is_connected</code>  <code>property</code>", "text": "<p>Return true if the object is connected to the database server.</p>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.query", "title": "<code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)</code>", "text": "<p>Execute the specified query and return the tuple generator (cursor).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>SQL query</p> required <code>args</code> <p>additional arguments for the client.cursor</p> <code>()</code> <code>as_dict</code> <p>If as_dict is set to True, the returned cursor objects returns query results as dictionary.</p> <code>False</code> <code>suppress_warnings</code> <p>If True, suppress all warnings arising from underlying query library</p> <code>True</code> <code>reconnect</code> <p>when None, get from config, when True, attempt to reconnect if disconnected</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def query(\n    self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n):\n\"\"\"\n    Execute the specified query and return the tuple generator (cursor).\n\n    :param query: SQL query\n    :param args: additional arguments for the client.cursor\n    :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                    query results as dictionary.\n    :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n    :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n    \"\"\"\n    # check cache first:\n    use_query_cache = bool(self._query_cache)\n    if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n        raise errors.DataJointError(\n            \"Only SELECT queries are allowed when query caching is on.\"\n        )\n    if use_query_cache:\n        if not config[cache_key]:\n            raise errors.DataJointError(\n                f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n            )\n        hash_ = uuid_from_buffer(\n            (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n            + pack(args)\n        )\n        cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n        try:\n            buffer = cache_path.read_bytes()\n        except FileNotFoundError:\n            pass  # proceed to query the database\n        else:\n            return EmulatedCursor(unpack(buffer))\n\n    if reconnect is None:\n        reconnect = config[\"database.reconnect\"]\n    logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n    cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n    cursor = self._conn.cursor(cursor=cursor_class)\n    try:\n        self._execute_query(cursor, query, args, suppress_warnings)\n    except errors.LostConnectionError:\n        if not reconnect:\n            raise\n        logger.warning(\"MySQL server has gone away. Reconnecting to the server.\")\n        connect_host_hook(self)\n        if self._in_transaction:\n            self.cancel_transaction()\n            raise errors.LostConnectionError(\n                \"Connection was lost during a transaction.\"\n            )\n        logger.debug(\"Re-executing\")\n        cursor = self._conn.cursor(cursor=cursor_class)\n        self._execute_query(cursor, query, args, suppress_warnings)\n\n    if use_query_cache:\n        data = cursor.fetchall()\n        cache_path.write_bytes(pack(data))\n        return EmulatedCursor(data)\n\n    return cursor\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.get_user", "title": "<code>get_user()</code>", "text": "<p>Returns:</p> Type Description <p>the user name and host name provided by the client to the server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def get_user(self):\n\"\"\"\n    :return: the user name and host name provided by the client to the server.\n    \"\"\"\n    return self.query(\"SELECT user()\").fetchone()[0]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.in_transaction", "title": "<code>in_transaction</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True if there is an open transaction.</p>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.start_transaction", "title": "<code>start_transaction()</code>", "text": "<p>Starts a transaction error.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def start_transaction(self):\n\"\"\"\n    Starts a transaction error.\n    \"\"\"\n    if self.in_transaction:\n        raise errors.DataJointError(\"Nested connections are not supported.\")\n    self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n    self._in_transaction = True\n    logger.debug(\"Transaction started\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.cancel_transaction", "title": "<code>cancel_transaction()</code>", "text": "<p>Cancels the current transaction and rolls back all changes made during the transaction.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def cancel_transaction(self):\n\"\"\"\n    Cancels the current transaction and rolls back all changes made during the transaction.\n    \"\"\"\n    self.query(\"ROLLBACK\")\n    self._in_transaction = False\n    logger.debug(\"Transaction cancelled. Rolling back ...\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.commit_transaction", "title": "<code>commit_transaction()</code>", "text": "<p>Commit all changes made during the transaction and close it.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def commit_transaction(self):\n\"\"\"\n    Commit all changes made during the transaction and close it.\n\n    \"\"\"\n    self.query(\"COMMIT\")\n    self._in_transaction = False\n    logger.debug(\"Transaction committed and closed.\")\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.connection.Connection.transaction", "title": "<code>transaction</code>  <code>property</code>", "text": "<p>Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again.</p> <p>Example:</p> <p>import datajoint as dj with dj.conn().transaction as conn:     # transaction is open here</p>"}, {"location": "api/datajoint/__init__/#datajoint.Computed", "title": "<code>Computed</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.Part", "title": "<code>Part</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.user_tables.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.VirtualModule", "title": "<code>VirtualModule</code>", "text": "<p>         Bases: <code>types.ModuleType</code></p> <p>A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class VirtualModule(types.ModuleType):\n\"\"\"\n    A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database.\n    It declares the schema objects and a class for each table.\n    \"\"\"\n\n    def __init__(\n        self,\n        module_name,\n        schema_name,\n        *,\n        create_schema=False,\n        create_tables=False,\n        connection=None,\n        add_objects=None,\n    ):\n\"\"\"\n        Creates a python module with the given name from the name of a schema on the server and\n        automatically adds classes to it corresponding to the tables in the schema.\n\n        :param module_name: displayed module name\n        :param schema_name: name of the database in mysql\n        :param create_schema: if True, create the schema on the database server\n        :param create_tables: if True, module.schema can be used as the decorator for declaring new\n        :param connection: a dj.Connection object to pass into the schema\n        :param add_objects: additional objects to add to the module\n        :return: the python module containing classes from the schema object and the table classes\n        \"\"\"\n        super(VirtualModule, self).__init__(name=module_name)\n        _schema = Schema(\n            schema_name,\n            create_schema=create_schema,\n            create_tables=create_tables,\n            connection=connection,\n        )\n        if add_objects:\n            self.__dict__.update(add_objects)\n        self.__dict__[\"schema\"] = _schema\n        _schema.spawn_missing_classes(context=self.__dict__)\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.list_schemas", "title": "<code>list_schemas(connection=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>connection</code> <p>a dj.Connection object</p> <code>None</code> <p>Returns:</p> Type Description <p>list of all accessible schemas on the server</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_schemas(connection=None):\n\"\"\"\n\n    :param connection: a dj.Connection object\n    :return: list of all accessible schemas on the server\n    \"\"\"\n    return [\n        r[0]\n        for r in (connection or conn()).query(\n            \"SELECT schema_name \"\n            \"FROM information_schema.schemata \"\n            'WHERE schema_name &lt;&gt; \"information_schema\"'\n        )\n    ]\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.U", "title": "<code>U</code>", "text": "<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expressio <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class U:\n\"\"\"\n    dj.U objects are the universal sets representing all possible values of their attributes.\n    dj.U objects cannot be queried on their own but are useful for forming some queries.\n    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.\n    The universal set is the set of all possible combinations of values of the attributes.\n    Without any attributes, dj.U() represents the set with one element that has no attributes.\n\n    Restriction:\n\n    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.\n\n    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:\n\n    &gt;&gt;&gt; dj.U('contrast', 'brightness') &amp; stimulus\n\n    Aggregation:\n\n    In aggregation, dj.U is used for summary calculation over an entire set:\n\n    The following expression yields one element with one attribute `s` containing the total number of elements in\n    query expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(*)')\n\n    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in\n    query expressio `expr`.\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(distinct attr)')\n    &gt;&gt;&gt; dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')\n\n    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`\n    over entire result set of expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, s='sum(attr)')\n\n    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of\n    their occurrences in the result set of query expression `expr`.\n\n    &gt;&gt;&gt; dj.U(attr1,attr2).aggr(expr, n='count(*)')\n\n    Joins:\n\n    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result\n    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on\n    non-primary key attributes.\n    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw\n    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first\n    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.\n    \"\"\"\n\n    def __init__(self, *primary_key):\n        self._primary_key = primary_key\n\n    @property\n    def primary_key(self):\n        return self._primary_key\n\n    def __and__(self, other):\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be restricted with a QueryExpression.\")\n        result = copy.copy(other)\n        result._distinct = True\n        result._heading = result.heading.set_primary_key(self.primary_key)\n        result = result.proj()\n        return result\n\n    def join(self, other, left=False):\n\"\"\"\n        Joining U with a query expression has the effect of promoting the attributes of U to\n        the primary key of the other query expression.\n\n        :param other: the other query expression to join with.\n        :param left: ignored. dj.U always acts as if left=False\n        :return: a copy of the other query expression with the primary key extended.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found\"\n                % next(k for k in self.primary_key if k not in other.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        result = copy.copy(other)\n        result._heading = result.heading.set_primary_key(\n            other.primary_key\n            + [k for k in self.primary_key if k not in other.primary_key]\n        )\n        return result\n\n    def __mul__(self, other):\n\"\"\"shorthand for join\"\"\"\n        return self.join(other)\n\n    def aggr(self, group, **named_attributes):\n\"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if named_attributes.get(\"keep_all_rows\", False):\n            raise DataJointError(\n                \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n            )\n        return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n            **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.join", "title": "<code>join(other, left=False)</code>", "text": "<p>Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>the other query expression to join with.</p> required <code>left</code> <p>ignored. dj.U always acts as if left=False</p> <code>False</code> <p>Returns:</p> Type Description <p>a copy of the other query expression with the primary key extended.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, left=False):\n\"\"\"\n    Joining U with a query expression has the effect of promoting the attributes of U to\n    the primary key of the other query expression.\n\n    :param other: the other query expression to join with.\n    :param left: ignored. dj.U always acts as if left=False\n    :return: a copy of the other query expression with the primary key extended.\n    \"\"\"\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate if a class\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found\"\n            % next(k for k in self.primary_key if k not in other.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    result = copy.copy(other)\n    result._heading = result.heading.set_primary_key(\n        other.primary_key\n        + [k for k in self.primary_key if k not in other.primary_key]\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.expression.U.aggr", "title": "<code>aggr(group, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, **named_attributes):\n\"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if named_attributes.get(\"keep_all_rows\", False):\n        raise DataJointError(\n            \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n        )\n    return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n        **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/__init__/#datajoint.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n\"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/admin/", "title": "admin.py", "text": ""}, {"location": "api/datajoint/admin/#datajoint.admin.user_choice", "title": "<code>user_choice(prompt, choices=('yes', 'no'), default=None)</code>", "text": "<p>Prompts the user for confirmation.  The default value, if any, is capitalized.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>Information to display to the user.</p> required <code>choices</code> <p>an iterable of possible choices.</p> <code>('yes', 'no')</code> <code>default</code> <p>default choice</p> <code>None</code> <p>Returns:</p> Type Description <p>the user's choice</p> Source code in <code>datajoint/utils.py</code> <pre><code>def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n\"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = \", \".join(\n        (choice.title() if choice == default else choice for choice in choices)\n    )\n    response = None\n    while response not in choices:\n        response = input(prompt + \" [\" + choice_list + \"]: \")\n        response = response.lower() if response else default\n    return response\n</code></pre>"}, {"location": "api/datajoint/admin/#datajoint.admin.kill", "title": "<code>kill(restriction=None, connection=None, order_by=None)</code>", "text": "<p>view and kill database connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()</p> <code>None</code> <code>order_by</code> <p>order by a single attribute or the list of attributes. defaults to 'id'.  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\". dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill(restriction=None, connection=None, order_by=None):\n\"\"\"\n    view and kill database connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n    :param order_by: order by a single attribute or the list of attributes. defaults to 'id'.\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') lists only connections from hosts containing \"compute\".\n        dj.kill('TIME &gt; 600') lists only connections in their current state for more than 10 minutes\n    \"\"\"\n\n    if connection is None:\n        connection = conn()\n\n    if order_by is not None and not isinstance(order_by, str):\n        order_by = \",\".join(order_by)\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n        + (\" ORDER BY %s\" % (order_by or \"id\"))\n    )\n\n    while True:\n        print(\"  ID USER         HOST          STATE         TIME    INFO\")\n        print(\"+--+ +----------+ +-----------+ +-----------+ +-----+\")\n        cur = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in connection.query(query, as_dict=True)\n        )\n        for process in cur:\n            try:\n                print(\n                    \"{id:&gt;4d} {user:&lt;12s} {host:&lt;12s} {state:&lt;12s} {time:&gt;7d}  {info}\".format(\n                        **process\n                    )\n                )\n            except TypeError:\n                print(process)\n        response = input('process to kill or \"q\" to quit &gt; ')\n        if response == \"q\":\n            break\n        if response:\n            try:\n                pid = int(response)\n            except ValueError:\n                pass  # ignore non-numeric input\n            else:\n                try:\n                    connection.query(\"kill %d\" % pid)\n                except pymysql.err.InternalError:\n                    logger.warn(\"Process not found\")\n</code></pre>"}, {"location": "api/datajoint/admin/#datajoint.admin.kill_quick", "title": "<code>kill_quick(restriction=None, connection=None)</code>", "text": "<p>Kill database connections without prompting. Returns number of terminated connections.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>restriction to be applied to processlist</p> <code>None</code> <code>connection</code> <p>a datajoint.Connection object. Default calls datajoint.conn()  Restrictions are specified as strings and can involve any of the attributes of information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.  Examples: dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\".</p> <code>None</code> Source code in <code>datajoint/admin.py</code> <pre><code>def kill_quick(restriction=None, connection=None):\n\"\"\"\n    Kill database connections without prompting. Returns number of terminated connections.\n\n    :param restriction: restriction to be applied to processlist\n    :param connection: a datajoint.Connection object. Default calls datajoint.conn()\n\n    Restrictions are specified as strings and can involve any of the attributes of\n    information_schema.processlist: ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO.\n\n    Examples:\n        dj.kill('HOST LIKE \"%compute%\"') terminates connections from hosts containing \"compute\".\n    \"\"\"\n    if connection is None:\n        connection = conn()\n\n    query = (\n        \"SELECT * FROM information_schema.processlist WHERE id &lt;&gt; CONNECTION_ID()\"\n        + (\"\" if restriction is None else \" AND (%s)\" % restriction)\n    )\n\n    cur = (\n        {k.lower(): v for k, v in elem.items()}\n        for elem in connection.query(query, as_dict=True)\n    )\n    nkill = 0\n    for process in cur:\n        connection.query(\"kill %d\" % process[\"id\"])\n        nkill += 1\n    return nkill\n</code></pre>"}, {"location": "api/datajoint/admin/#datajoint.admin.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n\"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS prefered, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/", "title": "attribute_adapter.py", "text": ""}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter", "title": "<code>AttributeAdapter</code>", "text": "<p>Base class for adapter objects for user-defined attribute types.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>class AttributeAdapter:\n\"\"\"\n    Base class for adapter objects for user-defined attribute types.\n    \"\"\"\n\n    @property\n    def attribute_type(self):\n\"\"\"\n        :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def get(self, value):\n\"\"\"\n        convert value retrieved from the the attribute in a table into the adapted type\n\n        :param value: value from the database\n\n        :return: object of the adapted type\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def put(self, obj):\n\"\"\"\n        convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n        :param obj: an object of the adapted type\n        :return: value to store in the database\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "title": "<code>attribute_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"</p>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.get", "title": "<code>get(value)</code>", "text": "<p>convert value retrieved from the the attribute in a table into the adapted type</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value from the database</p> required <p>Returns:</p> Type Description <p>object of the adapted type</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get(self, value):\n\"\"\"\n    convert value retrieved from the the attribute in a table into the adapted type\n\n    :param value: value from the database\n\n    :return: object of the adapted type\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.AttributeAdapter.put", "title": "<code>put(obj)</code>", "text": "<p>convert an object of the adapted type into a value that DataJoint can store in a table attribute</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>an object of the adapted type</p> required <p>Returns:</p> Type Description <p>value to store in the database</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def put(self, obj):\n\"\"\"\n    convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n    :param obj: an object of the adapted type\n    :return: value to store in the database\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/attribute_adapter/#datajoint.attribute_adapter.get_adapter", "title": "<code>get_adapter(context, adapter_name)</code>", "text": "<p>Extract the AttributeAdapter object by its name from the context and validate.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get_adapter(context, adapter_name):\n\"\"\"\n    Extract the AttributeAdapter object by its name from the context and validate.\n    \"\"\"\n    if not _support_adapted_types():\n        raise DataJointError(\"Support for Adapted Attribute types is disabled.\")\n    adapter_name = adapter_name.lstrip(\"&lt;\").rstrip(\"&gt;\")\n    try:\n        adapter = (\n            context[adapter_name]\n            if adapter_name in context\n            else type_plugins[adapter_name][\"object\"].load()\n        )\n    except KeyError:\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' is not defined.\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter, AttributeAdapter):\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' must be an instance of datajoint.AttributeAdapter\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter.attribute_type, str) or not re.match(\n        r\"^\\w\", adapter.attribute_type\n    ):\n        raise DataJointError(\n            \"Invalid attribute type {type} in attribute adapter '{adapter_name}'\".format(\n                type=adapter.attribute_type, adapter_name=adapter_name\n            )\n        )\n    return adapter\n</code></pre>"}, {"location": "api/datajoint/autopopulate/", "title": "autopopulate.py", "text": "<p>This module defines class dj.AutoPopulate</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n\"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.QueryExpression", "title": "<code>QueryExpression</code>", "text": "<p>QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union.</p> <p>A QueryExpression object has a support, a restriction (an AndList), and heading. Property <code>heading</code> (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj.</p> <p>Property <code>support</code> is the list of table names or other QueryExpressions to be joined.</p> <p>The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute.</p> <p>Application of operators does not always lead to the creation of a subquery. A subquery is generated when:     1. A restriction is applied on any computed or renamed attributes     2. A projection is applied remapping remapped attributes     3. Subclasses: Join, Aggregation, and Union have additional specific rules.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class QueryExpression:\n\"\"\"\n    QueryExpression implements query operators to derive new entity set from its input.\n    A QueryExpression object generates a SELECT statement in SQL.\n    QueryExpression operators are restrict, join, proj, aggr, and union.\n\n    A QueryExpression object has a support, a restriction (an AndList), and heading.\n    Property `heading` (type dj.Heading) contains information about the attributes.\n    It is loaded from the database and updated by proj.\n\n    Property `support` is the list of table names or other QueryExpressions to be joined.\n\n    The restriction is applied first without having access to the attributes generated by the projection.\n    Then projection is applied by selecting modifying the heading attribute.\n\n    Application of operators does not always lead to the creation of a subquery.\n    A subquery is generated when:\n        1. A restriction is applied on any computed or renamed attributes\n        2. A projection is applied remapping remapped attributes\n        3. Subclasses: Join, Aggregation, and Union have additional specific rules.\n    \"\"\"\n\n    _restriction = None\n    _restriction_attributes = None\n    _left = []  # list of booleans True for left joins, False for inner joins\n    _original_heading = None  # heading before projections\n\n    # subclasses or instantiators must provide values\n    _connection = None\n    _heading = None\n    _support = None\n\n    # If the query will be using distinct\n    _distinct = False\n\n    @property\n    def connection(self):\n\"\"\"a dj.Connection object\"\"\"\n        assert self._connection is not None\n        return self._connection\n\n    @property\n    def support(self):\n\"\"\"A list of table names or subqueries to from the FROM clause\"\"\"\n        assert self._support is not None\n        return self._support\n\n    @property\n    def heading(self):\n\"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\"\n        return self._heading\n\n    @property\n    def original_heading(self):\n\"\"\"a dj.Heading object reflecting the attributes before projection\"\"\"\n        return self._original_heading or self.heading\n\n    @property\n    def restriction(self):\n\"\"\"a AndList object of restrictions applied to input to produce the result\"\"\"\n        if self._restriction is None:\n            self._restriction = AndList()\n        return self._restriction\n\n    @property\n    def restriction_attributes(self):\n\"\"\"the set of attribute names invoked in the WHERE clause\"\"\"\n        if self._restriction_attributes is None:\n            self._restriction_attributes = set()\n        return self._restriction_attributes\n\n    @property\n    def primary_key(self):\n        return self.heading.primary_key\n\n    _subquery_alias_count = count()  # count for alias names used in the FROM clause\n\n    def from_clause(self):\n        support = (\n            \"(\" + src.make_sql() + \") as `$%x`\" % next(self._subquery_alias_count)\n            if isinstance(src, QueryExpression)\n            else src\n            for src in self.support\n        )\n        clause = next(support)\n        for s, left in zip(support, self._left):\n            clause += \" NATURAL{left} JOIN {clause}\".format(\n                left=\" LEFT\" if left else \"\", clause=s\n            )\n        return clause\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self.restriction\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self.restriction)\n        )\n\n    def make_sql(self, fields=None):\n\"\"\"\n        Make the SQL SELECT statement.\n\n        :param fields: used to explicitly set the select attributes\n        \"\"\"\n        return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n            distinct=\"DISTINCT \" if self._distinct else \"\",\n            fields=self.heading.as_sql(fields or self.heading.names),\n            from_=self.from_clause(),\n            where=self.where_clause(),\n        )\n\n    # --------- query operators -----------\n    def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = [self]\n        result._heading = self.heading.make_subquery_heading()\n        return result\n\n    def restrict(self, restriction):\n\"\"\"\n        Produces a new expression with the new restriction applied.\n        rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n        rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n        The primary key of the result is unaffected.\n        Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n        Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n        (logical disjunction of conditions)\n        Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n        The expressions in each row equivalent:\n\n        rel &amp; True                          rel\n        rel &amp; False                         the empty entity set\n        rel &amp; 'TRUE'                        rel\n        rel &amp; 'FALSE'                       the empty entity set\n        rel - cond                          rel &amp; Not(cond)\n        rel - 'TRUE'                        rel &amp; False\n        rel - 'FALSE'                       rel\n        rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n        rel &amp; AndList()                     rel\n        rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n        rel &amp; []                            rel &amp; False\n        rel &amp; None                          rel &amp; False\n        rel &amp; any_empty_entity_set          rel &amp; False\n        rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n        rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n        rel - AndList()                     rel &amp; False\n        rel - []                            rel\n        rel - None                          rel\n        rel - any_empty_entity_set          rel\n\n        When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n        one element in arg (hence arg is treated as an OrList).\n        Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n        Two elements match when their common attributes have equal values or when they have no common attributes.\n        All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n        QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n        ultimately call restrict()\n\n        :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n        string, or an AndList.\n        \"\"\"\n        attributes = set()\n        new_condition = make_condition(self, restriction, attributes)\n        if new_condition is True:\n            return self  # restriction has no effect, return the same object\n        # check that all attributes in condition are present in the query\n        try:\n            raise DataJointError(\n                \"Attribute `%s` is not found in query.\"\n                % next(attr for attr in attributes if attr not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        # If the new condition uses any new attributes, a subquery is required.\n        # However, Aggregation's HAVING statement works fine with aliased attributes.\n        need_subquery = isinstance(self, Union) or (\n            not isinstance(self, Aggregation) and self.heading.new_attributes\n        )\n        if need_subquery:\n            result = self.make_subquery()\n        else:\n            result = copy.copy(self)\n            result._restriction = AndList(\n                self.restriction\n            )  # copy to preserve the original\n        result.restriction.append(new_condition)\n        result.restriction_attributes.update(attributes)\n        return result\n\n    def restrict_in_place(self, restriction):\n        self.__dict__.update(self.restrict(restriction).__dict__)\n\n    def __and__(self, restriction):\n\"\"\"\n        Restriction operator e.g. ``q1 &amp; q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(restriction)\n\n    def __xor__(self, restriction):\n\"\"\"\n        Permissive restriction operator ignoring compatibility check  e.g. ``q1 ^ q2``.\n        \"\"\"\n        if inspect.isclass(restriction) and issubclass(restriction, QueryExpression):\n            restriction = restriction()\n        if isinstance(restriction, Not):\n            return self.restrict(Not(PromiscuousOperand(restriction.restriction)))\n        return self.restrict(PromiscuousOperand(restriction))\n\n    def __sub__(self, restriction):\n\"\"\"\n        Inverted restriction e.g. ``q1 - q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(Not(restriction))\n\n    def __neg__(self):\n\"\"\"\n        Convert between restriction and inverted restriction e.g. ``-q1``.\n        :return: target restriction\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        if isinstance(self, Not):\n            return self.restriction\n        return Not(self)\n\n    def __mul__(self, other):\n\"\"\"\n        join of query expressions `self` and `other` e.g. ``q1 * q2``.\n        \"\"\"\n        return self.join(other)\n\n    def __matmul__(self, other):\n\"\"\"\n        Permissive join of query expressions `self` and `other` ignoring compatibility check\n            e.g. ``q1 @ q2``.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        return self.join(other, semantic_check=False)\n\n    def join(self, other, semantic_check=True, left=False):\n\"\"\"\n        create the joined QueryExpression.\n        a * b  is short for A.join(B)\n        a @ b  is short for A.join(B, semantic_check=False)\n        Additionally, left=True will retain the rows of self, effectively performing a left join.\n        \"\"\"\n        # trigger subqueries if joining on renamed attributes\n        if isinstance(other, U):\n            return other * self\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"The argument of join must be a QueryExpression\")\n        if semantic_check:\n            assert_join_compatibility(self, other)\n        join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n        # needs subquery if self's FROM clause has common attributes with other's FROM clause\n        need_subquery1 = need_subquery2 = bool(\n            (set(self.original_heading.names) &amp; set(other.original_heading.names))\n            - join_attributes\n        )\n        # need subquery if any of the join attributes are derived\n        need_subquery1 = (\n            need_subquery1\n            or isinstance(self, Aggregation)\n            or any(n in self.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        need_subquery2 = (\n            need_subquery2\n            or isinstance(other, Aggregation)\n            or any(n in other.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        if need_subquery1:\n            self = self.make_subquery()\n        if need_subquery2:\n            other = other.make_subquery()\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = self.support + other.support\n        result._left = self._left + [left] + other._left\n        result._heading = self.heading.join(other.heading)\n        result._restriction = AndList(self.restriction)\n        result._restriction.append(other.restriction)\n        result._original_heading = self.original_heading.join(other.original_heading)\n        assert len(result.support) == len(result._left) + 1\n        return result\n\n    def __add__(self, other):\n\"\"\"union e.g. ``q1 + q2``.\"\"\"\n        return Union.create(self, other)\n\n    def proj(self, *attributes, **named_attributes):\n\"\"\"\n        Projection operator.\n\n        :param attributes:  attributes to be included in the result. (The primary key is already included).\n        :param named_attributes: new attributes computed or renamed from existing attributes.\n        :return: the projected expression.\n        Primary key attributes cannot be excluded but may be renamed.\n        If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n        Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n        Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n        self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n        self.proj() -- include only primary key\n        self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n        self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n        self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n        self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n        self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n        from other attributes available before the projection.\n        Each attribute name can only be used once.\n        \"\"\"\n        named_attributes = {\n            k: translate_attribute(v)[1] for k, v in named_attributes.items()\n        }\n        # new attributes in parentheses are included again with the new name without removing original\n        duplication_pattern = re.compile(\n            rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n        )\n        # attributes without parentheses renamed\n        rename_pattern = re.compile(\n            rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n        )\n        replicate_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        rename_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        compute_map = {\n            k: v\n            for k, v in named_attributes.items()\n            if not duplication_pattern.match(v) and not rename_pattern.match(v)\n        }\n        attributes = set(attributes)\n        # include primary key\n        attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n        # include all secondary attributes with Ellipsis\n        if Ellipsis in attributes:\n            attributes.discard(Ellipsis)\n            attributes.update(\n                (\n                    a\n                    for a in self.heading.secondary_attributes\n                    if a not in attributes and a not in rename_map.values()\n                )\n            )\n        try:\n            raise DataJointError(\n                \"%s is not a valid data type for an attribute in .proj\"\n                % next(a for a in attributes if not isinstance(a, str))\n            )\n        except StopIteration:\n            pass  # normal case\n        # remove excluded attributes, specified as `-attr'\n        excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n        attributes.difference_update(excluded)\n        excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n        attributes.difference_update(excluded)\n        try:\n            raise DataJointError(\n                \"Cannot exclude primary key attribute %s\",\n                next(a for a in excluded if a in self.primary_key),\n            )\n        except StopIteration:\n            pass  # all ok\n        # check that all attributes exist in heading\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(a for a in attributes if a not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that all mentioned names are present in heading\n        mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n        try:\n            raise DataJointError(\n                \"Attribute '%s' not found.\"\n                % next(a for a in mentions if not self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that newly created attributes do not clash with any other selected attributes\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in rename_map\n                    if a in attributes.union(compute_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in compute_map\n                    if a in attributes.union(rename_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in replicate_map\n                    if a in attributes.union(rename_map).union(compute_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # need a subquery if the projection remaps any remapped attributes\n        used = set(q for v in compute_map.values() for q in extract_column_names(v))\n        used.update(rename_map.values())\n        used.update(replicate_map.values())\n        used.intersection_update(self.heading.names)\n        need_subquery = isinstance(self, Union) or any(\n            self.heading[name].attribute_expression is not None for name in used\n        )\n        if not need_subquery and self.restriction:\n            # need a subquery if the restriction applies to attributes that have been renamed\n            need_subquery = any(\n                name in self.restriction_attributes\n                for name in self.heading.new_attributes\n            )\n\n        result = self.make_subquery() if need_subquery else copy.copy(self)\n        result._original_heading = result.original_heading\n        result._heading = result.heading.select(\n            attributes,\n            rename_map=dict(**rename_map, **replicate_map),\n            compute_map=compute_map,\n        )\n        return result\n\n    def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if Ellipsis in attributes:\n            # expand ellipsis to include only attributes from the left table\n            attributes = set(attributes)\n            attributes.discard(Ellipsis)\n            attributes.update(self.heading.secondary_attributes)\n        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n            *attributes, **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n\n    # ---------- Fetch operators --------------------\n    @property\n    def fetch1(self):\n        return Fetch1(self)\n\n    @property\n    def fetch(self):\n        return Fetch(self)\n\n    def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the first few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n\n    def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the last few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n\n    def __len__(self):\n\"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\"\n        return self.connection.query(\n            \"SELECT {select_} FROM {from_}{where}\".format(\n                select_=(\n                    \"count(*)\"\n                    if any(self._left)\n                    else \"count(DISTINCT {fields})\".format(\n                        fields=self.heading.as_sql(\n                            self.primary_key, include_aliases=False\n                        )\n                    )\n                ),\n                from_=self.from_clause(),\n                where=self.where_clause(),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n\"\"\"\n        :return: True if the result is not empty. Equivalent to len(self) &gt; 0 but often\n            faster e.g. ``bool(q1)``.\n        \"\"\"\n        return bool(\n            self.connection.query(\n                \"SELECT EXISTS(SELECT 1 FROM {from_}{where})\".format(\n                    from_=self.from_clause(), where=self.where_clause()\n                )\n            ).fetchone()[0]\n        )\n\n    def __contains__(self, item):\n\"\"\"\n        returns True if the restriction in item matches any entries in self\n            e.g. ``restriction in q1``.\n\n        :param item: any restriction\n        (item in query_expression) is equivalent to bool(query_expression &amp; item) but may be\n        executed more efficiently.\n        \"\"\"\n        return bool(self &amp; item)  # May be optimized e.g. using an EXISTS query\n\n    def __iter__(self):\n\"\"\"\n        returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``.\n\n        :param self: iterator-compatible QueryExpression object\n        \"\"\"\n        self._iter_only_key = all(v.in_key for v in self.heading.attributes.values())\n        self._iter_keys = self.fetch(\"KEY\")\n        return self\n\n    def __next__(self):\n\"\"\"\n        returns the next record on an iterator-compatible QueryExpression object\n            e.g. ``next(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: dict\n        \"\"\"\n        try:\n            key = self._iter_keys.pop(0)\n        except AttributeError:\n            # self._iter_keys is missing because __iter__ has not been called.\n            raise TypeError(\n                \"A QueryExpression object is not an iterator. \"\n                \"Use iter(obj) to create an iterator.\"\n            )\n        except IndexError:\n            raise StopIteration\n        else:\n            if self._iter_only_key:\n                return key\n            else:\n                try:\n                    return (self &amp; key).fetch1()\n                except DataJointError:\n                    # The data may have been deleted since the moment the keys were fetched\n                    # -- move on to next entry.\n                    return next(self)\n\n    def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n        See expression.fetch() for input description.\n        :return: query cursor\n        \"\"\"\n        if offset and limit is None:\n            raise DataJointError(\"limit is required when offset is set\")\n        sql = self.make_sql()\n        if order_by is not None:\n            sql += \" ORDER BY \" + \", \".join(order_by)\n        if limit is not None:\n            sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n        logger.debug(sql)\n        return self.connection.query(sql, as_dict=as_dict)\n\n    def __repr__(self):\n\"\"\"\n        returns the string representation of a QueryExpression object e.g. ``str(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: str\n        \"\"\"\n        return (\n            super().__repr__()\n            if config[\"loglevel\"].lower() == \"debug\"\n            else self.preview()\n        )\n\n    def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n        return preview(self, limit, width)\n\n    def _repr_html_(self):\n\"\"\":return: HTML to display table in Jupyter notebook.\"\"\"\n        return repr_html(self)\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.connection", "title": "<code>connection</code>  <code>property</code>", "text": "<p>a dj.Connection object</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.support", "title": "<code>support</code>  <code>property</code>", "text": "<p>A list of table names or subqueries to from the FROM clause</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.heading", "title": "<code>heading</code>  <code>property</code>", "text": "<p>a dj.Heading object, reflects the effects of the projection operator .proj</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.original_heading", "title": "<code>original_heading</code>  <code>property</code>", "text": "<p>a dj.Heading object reflecting the attributes before projection</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.restriction", "title": "<code>restriction</code>  <code>property</code>", "text": "<p>a AndList object of restrictions applied to input to produce the result</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.restriction_attributes", "title": "<code>restriction_attributes</code>  <code>property</code>", "text": "<p>the set of attribute names invoked in the WHERE clause</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.make_sql", "title": "<code>make_sql(fields=None)</code>", "text": "<p>Make the SQL SELECT statement.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>used to explicitly set the select attributes</p> <code>None</code> Source code in <code>datajoint/expression.py</code> <pre><code>def make_sql(self, fields=None):\n\"\"\"\n    Make the SQL SELECT statement.\n\n    :param fields: used to explicitly set the select attributes\n    \"\"\"\n    return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n        distinct=\"DISTINCT \" if self._distinct else \"\",\n        fields=self.heading.as_sql(fields or self.heading.names),\n        from_=self.from_clause(),\n        where=self.where_clause(),\n    )\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.make_subquery", "title": "<code>make_subquery()</code>", "text": "<p>create a new SELECT statement where self is the FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = [self]\n    result._heading = self.heading.make_subquery_heading()\n    return result\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.restrict", "title": "<code>restrict(restriction)</code>", "text": "<p>Produces a new expression with the new restriction applied. rel.restrict(restriction)  is equivalent to  rel &amp; restriction. rel.restrict(Not(restriction))  is equivalent to  rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class.</p> <p>The expressions in each row equivalent:</p> <p>rel &amp; True                          rel rel &amp; False                         the empty entity set rel &amp; 'TRUE'                        rel rel &amp; 'FALSE'                       the empty entity set rel - cond                          rel &amp; Not(cond) rel - 'TRUE'                        rel &amp; False rel - 'FALSE'                       rel rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2 rel &amp; AndList()                     rel rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2)) rel &amp; []                            rel &amp; False rel &amp; None                          rel &amp; False rel &amp; any_empty_entity_set          rel &amp; False rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)] rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2) rel - AndList()                     rel &amp; False rel - []                            rel rel - None                          rel rel - any_empty_entity_set          rel</p> <p>When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.</p> <p>QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict()</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList.</p> required Source code in <code>datajoint/expression.py</code> <pre><code>def restrict(self, restriction):\n\"\"\"\n    Produces a new expression with the new restriction applied.\n    rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n    rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n    The primary key of the result is unaffected.\n    Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n    Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n    (logical disjunction of conditions)\n    Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n    The expressions in each row equivalent:\n\n    rel &amp; True                          rel\n    rel &amp; False                         the empty entity set\n    rel &amp; 'TRUE'                        rel\n    rel &amp; 'FALSE'                       the empty entity set\n    rel - cond                          rel &amp; Not(cond)\n    rel - 'TRUE'                        rel &amp; False\n    rel - 'FALSE'                       rel\n    rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n    rel &amp; AndList()                     rel\n    rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n    rel &amp; []                            rel &amp; False\n    rel &amp; None                          rel &amp; False\n    rel &amp; any_empty_entity_set          rel &amp; False\n    rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n    rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n    rel - AndList()                     rel &amp; False\n    rel - []                            rel\n    rel - None                          rel\n    rel - any_empty_entity_set          rel\n\n    When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n    one element in arg (hence arg is treated as an OrList).\n    Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n    Two elements match when their common attributes have equal values or when they have no common attributes.\n    All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n    QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n    ultimately call restrict()\n\n    :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n    string, or an AndList.\n    \"\"\"\n    attributes = set()\n    new_condition = make_condition(self, restriction, attributes)\n    if new_condition is True:\n        return self  # restriction has no effect, return the same object\n    # check that all attributes in condition are present in the query\n    try:\n        raise DataJointError(\n            \"Attribute `%s` is not found in query.\"\n            % next(attr for attr in attributes if attr not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    # If the new condition uses any new attributes, a subquery is required.\n    # However, Aggregation's HAVING statement works fine with aliased attributes.\n    need_subquery = isinstance(self, Union) or (\n        not isinstance(self, Aggregation) and self.heading.new_attributes\n    )\n    if need_subquery:\n        result = self.make_subquery()\n    else:\n        result = copy.copy(self)\n        result._restriction = AndList(\n            self.restriction\n        )  # copy to preserve the original\n    result.restriction.append(new_condition)\n    result.restriction_attributes.update(attributes)\n    return result\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.join", "title": "<code>join(other, semantic_check=True, left=False)</code>", "text": "<p>create the joined QueryExpression. a * b  is short for A.join(B) a @ b  is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, semantic_check=True, left=False):\n\"\"\"\n    create the joined QueryExpression.\n    a * b  is short for A.join(B)\n    a @ b  is short for A.join(B, semantic_check=False)\n    Additionally, left=True will retain the rows of self, effectively performing a left join.\n    \"\"\"\n    # trigger subqueries if joining on renamed attributes\n    if isinstance(other, U):\n        return other * self\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"The argument of join must be a QueryExpression\")\n    if semantic_check:\n        assert_join_compatibility(self, other)\n    join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n    # needs subquery if self's FROM clause has common attributes with other's FROM clause\n    need_subquery1 = need_subquery2 = bool(\n        (set(self.original_heading.names) &amp; set(other.original_heading.names))\n        - join_attributes\n    )\n    # need subquery if any of the join attributes are derived\n    need_subquery1 = (\n        need_subquery1\n        or isinstance(self, Aggregation)\n        or any(n in self.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    need_subquery2 = (\n        need_subquery2\n        or isinstance(other, Aggregation)\n        or any(n in other.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    if need_subquery1:\n        self = self.make_subquery()\n    if need_subquery2:\n        other = other.make_subquery()\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = self.support + other.support\n    result._left = self._left + [left] + other._left\n    result._heading = self.heading.join(other.heading)\n    result._restriction = AndList(self.restriction)\n    result._restriction.append(other.restriction)\n    result._original_heading = self.original_heading.join(other.original_heading)\n    assert len(result.support) == len(result._left) + 1\n    return result\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.proj", "title": "<code>proj(*attributes, **named_attributes)</code>", "text": "<p>Projection operator.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <p>attributes to be included in the result. (The primary key is already included).</p> <code>()</code> <code>named_attributes</code> <p>new attributes computed or renamed from existing attributes.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def proj(self, *attributes, **named_attributes):\n\"\"\"\n    Projection operator.\n\n    :param attributes:  attributes to be included in the result. (The primary key is already included).\n    :param named_attributes: new attributes computed or renamed from existing attributes.\n    :return: the projected expression.\n    Primary key attributes cannot be excluded but may be renamed.\n    If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n    Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n    Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n    self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n    self.proj() -- include only primary key\n    self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n    self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n    self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n    self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n    self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n    from other attributes available before the projection.\n    Each attribute name can only be used once.\n    \"\"\"\n    named_attributes = {\n        k: translate_attribute(v)[1] for k, v in named_attributes.items()\n    }\n    # new attributes in parentheses are included again with the new name without removing original\n    duplication_pattern = re.compile(\n        rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n    )\n    # attributes without parentheses renamed\n    rename_pattern = re.compile(\n        rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n    )\n    replicate_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    rename_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    compute_map = {\n        k: v\n        for k, v in named_attributes.items()\n        if not duplication_pattern.match(v) and not rename_pattern.match(v)\n    }\n    attributes = set(attributes)\n    # include primary key\n    attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n    # include all secondary attributes with Ellipsis\n    if Ellipsis in attributes:\n        attributes.discard(Ellipsis)\n        attributes.update(\n            (\n                a\n                for a in self.heading.secondary_attributes\n                if a not in attributes and a not in rename_map.values()\n            )\n        )\n    try:\n        raise DataJointError(\n            \"%s is not a valid data type for an attribute in .proj\"\n            % next(a for a in attributes if not isinstance(a, str))\n        )\n    except StopIteration:\n        pass  # normal case\n    # remove excluded attributes, specified as `-attr'\n    excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n    attributes.difference_update(excluded)\n    excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n    attributes.difference_update(excluded)\n    try:\n        raise DataJointError(\n            \"Cannot exclude primary key attribute %s\",\n            next(a for a in excluded if a in self.primary_key),\n        )\n    except StopIteration:\n        pass  # all ok\n    # check that all attributes exist in heading\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(a for a in attributes if a not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that all mentioned names are present in heading\n    mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n    try:\n        raise DataJointError(\n            \"Attribute '%s' not found.\"\n            % next(a for a in mentions if not self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that newly created attributes do not clash with any other selected attributes\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in rename_map\n                if a in attributes.union(compute_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in compute_map\n                if a in attributes.union(rename_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in replicate_map\n                if a in attributes.union(rename_map).union(compute_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # need a subquery if the projection remaps any remapped attributes\n    used = set(q for v in compute_map.values() for q in extract_column_names(v))\n    used.update(rename_map.values())\n    used.update(replicate_map.values())\n    used.intersection_update(self.heading.names)\n    need_subquery = isinstance(self, Union) or any(\n        self.heading[name].attribute_expression is not None for name in used\n    )\n    if not need_subquery and self.restriction:\n        # need a subquery if the restriction applies to attributes that have been renamed\n        need_subquery = any(\n            name in self.restriction_attributes\n            for name in self.heading.new_attributes\n        )\n\n    result = self.make_subquery() if need_subquery else copy.copy(self)\n    result._original_heading = result.original_heading\n    result._heading = result.heading.select(\n        attributes,\n        rename_map=dict(**rename_map, **replicate_map),\n        compute_map=compute_map,\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.aggr", "title": "<code>aggr(group, *attributes, keep_all_rows=False, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>keep_all_rows</code> <p>True=keep all the rows from self. False=keep only rows that match entries in group.</p> <code>False</code> <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if Ellipsis in attributes:\n        # expand ellipsis to include only attributes from the left table\n        attributes = set(attributes)\n        attributes.discard(Ellipsis)\n        attributes.update(self.heading.secondary_attributes)\n    return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n        *attributes, **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.head", "title": "<code>head(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25)</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the first few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.tail", "title": "<code>tail(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the last few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.cursor", "title": "<code>cursor(offset=0, limit=None, order_by=None, as_dict=False)</code>", "text": "<p>See expression.fetch() for input description.</p> <p>Returns:</p> Type Description <p>query cursor</p> Source code in <code>datajoint/expression.py</code> <pre><code>def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n    See expression.fetch() for input description.\n    :return: query cursor\n    \"\"\"\n    if offset and limit is None:\n        raise DataJointError(\"limit is required when offset is set\")\n    sql = self.make_sql()\n    if order_by is not None:\n        sql += \" ORDER BY \" + \", \".join(order_by)\n    if limit is not None:\n        sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n    logger.debug(sql)\n    return self.connection.query(sql, as_dict=as_dict)\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.expression.QueryExpression.preview", "title": "<code>preview(limit=None, width=None)</code>", "text": "<p>Returns:</p> Type Description <p>a string of preview of the contents of the query.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n    return preview(self, limit, width)\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AndList", "title": "<code>AndList</code>", "text": "<p>         Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n\"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.LostConnectionError", "title": "<code>LostConnectionError</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Loss of server connection</p> Source code in <code>datajoint/errors.py</code> <pre><code>class LostConnectionError(DataJointError):\n\"\"\"\n    Loss of server connection\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate", "title": "<code>AutoPopulate</code>", "text": "<p>AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property <code>key_source</code>, and must define the callback method <code>make</code>.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>class AutoPopulate:\n\"\"\"\n    AutoPopulate is a mixin class that adds the method populate() to a Table class.\n    Auto-populated tables must inherit from both Table and AutoPopulate,\n    must define the property `key_source`, and must define the callback method `make`.\n    \"\"\"\n\n    _key_source = None\n    _allow_insert = False\n\n    @property\n    def key_source(self):\n\"\"\"\n        :return: the query expression that yields primary key values to be passed,\n        sequentially, to the ``make`` method when populate() is called.\n        The default value is the join of the parent tables references from the primary key.\n        Subclasses may override they key_source to change the scope or the granularity\n        of the make calls.\n        \"\"\"\n\n        def _rename_attributes(table, props):\n            return (\n                table.proj(\n                    **{\n                        attr: ref\n                        for attr, ref in props[\"attr_map\"].items()\n                        if attr != ref\n                    }\n                )\n                if props[\"aliased\"]\n                else table.proj()\n            )\n\n        if self._key_source is None:\n            parents = self.target.parents(\n                primary=True, as_objects=True, foreign_key_info=True\n            )\n            if not parents:\n                raise DataJointError(\n                    \"A table must have dependencies \"\n                    \"from its primary key for auto-populate to work\"\n                )\n            self._key_source = _rename_attributes(*parents[0])\n            for q in parents[1:]:\n                self._key_source *= _rename_attributes(*q)\n        return self._key_source\n\n    def make(self, key):\n\"\"\"\n        Derived classes must implement method `make` that fetches data from tables\n        above them in the dependency hierarchy, restricting by the given key,\n        computes secondary attributes, and inserts the new tuples into self.\n        \"\"\"\n        raise NotImplementedError(\n            \"Subclasses of AutoPopulate must implement the method `make`\"\n        )\n\n    @property\n    def target(self):\n\"\"\"\n        :return: table to be populated.\n        In the typical case, dj.AutoPopulate is mixed into a dj.Table class by\n        inheritance and the target is self.\n        \"\"\"\n        return self\n\n    def _job_key(self, key):\n\"\"\"\n        :param key:  they key returned for the job from the key source\n        :return: the dict to use to generate the job reservation hash\n        This method allows subclasses to control the job reservation granularity.\n        \"\"\"\n        return key\n\n    def _jobs_to_do(self, restrictions):\n\"\"\"\n        :return: the query yeilding the keys to be computed (derived from self.key_source)\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"Cannot call populate on a restricted table. \"\n                \"Instead, pass conditions to populate() as arguments.\"\n            )\n        todo = self.key_source\n\n        # key_source is a QueryExpression subclass -- trigger instantiation\n        if inspect.isclass(todo) and issubclass(todo, QueryExpression):\n            todo = todo()\n\n        if not isinstance(todo, QueryExpression):\n            raise DataJointError(\"Invalid key_source value\")\n\n        try:\n            # check if target lacks any attributes from the primary key of key_source\n            raise DataJointError(\n                \"The populate target lacks attribute %s \"\n                \"from the primary key of key_source\"\n                % next(\n                    name\n                    for name in todo.heading.primary_key\n                    if name not in self.target.heading\n                )\n            )\n        except StopIteration:\n            pass\n        return (todo &amp; AndList(restrictions)).proj()\n\n    def populate(\n        self,\n        *restrictions,\n        suppress_errors=False,\n        return_exception_objects=False,\n        reserve_jobs=False,\n        order=\"original\",\n        limit=None,\n        max_calls=None,\n        display_progress=False,\n        processes=1,\n        make_kwargs=None,\n    ):\n\"\"\"\n        ``table.populate()`` calls ``table.make(key)`` for every primary key in\n        ``self.key_source`` for which there is not already a tuple in table.\n\n        :param restrictions: a list of restrictions each restrict\n            (table.key_source - target.proj())\n        :param suppress_errors: if True, do not terminate execution.\n        :param return_exception_objects: return error objects instead of just error messages\n        :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n        :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n        :param limit: if not None, check at most this many keys\n        :param max_calls: if not None, populate at most this many keys\n        :param display_progress: if True, report progress_bar\n        :param processes: number of processes to use. Set to None to use all cores\n        :param make_kwargs: Keyword arguments which do not affect the result of computation\n            to be passed down to each ``make()`` call. Computation arguments should be\n            specified within the pipeline e.g. using a `dj.Lookup` table.\n        :type make_kwargs: dict, optional\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n        valid_order = [\"original\", \"reverse\", \"random\"]\n        if order not in valid_order:\n            raise DataJointError(\n                \"The order argument must be one of %s\" % str(valid_order)\n            )\n        jobs = (\n            self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n        )\n\n        # define and set up signal handler for SIGTERM:\n        if reserve_jobs:\n\n            def handler(signum, frame):\n                logger.info(\"Populate terminated by SIGTERM\")\n                raise SystemExit(\"SIGTERM received\")\n\n            old_handler = signal.signal(signal.SIGTERM, handler)\n\n        keys = (self._jobs_to_do(restrictions) - self.target).fetch(\"KEY\", limit=limit)\n\n        # exclude \"error\" or \"ignore\" jobs\n        if reserve_jobs:\n            exclude_key_hashes = (\n                jobs\n                &amp; {\"table_name\": self.target.table_name}\n                &amp; 'status in (\"error\", \"ignore\")'\n            ).fetch(\"key_hash\")\n            keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n        if order == \"reverse\":\n            keys.reverse()\n        elif order == \"random\":\n            random.shuffle(keys)\n\n        logger.debug(\"Found %d keys to populate\" % len(keys))\n\n        keys = keys[:max_calls]\n        nkeys = len(keys)\n        if not nkeys:\n            return\n\n        processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n        error_list = []\n        populate_kwargs = dict(\n            suppress_errors=suppress_errors,\n            return_exception_objects=return_exception_objects,\n            make_kwargs=make_kwargs,\n        )\n\n        if processes == 1:\n            for key in (\n                tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n            ):\n                error = self._populate1(key, jobs, **populate_kwargs)\n                if error is not None:\n                    error_list.append(error)\n        else:\n            # spawn multiple processes\n            self.connection.close()  # disconnect parent process from MySQL server\n            del self.connection._conn.ctx  # SSLContext is not pickleable\n            with mp.Pool(\n                processes, _initialize_populate, (self, jobs, populate_kwargs)\n            ) as pool, (\n                tqdm(desc=\"Processes: \", total=nkeys)\n                if display_progress\n                else contextlib.nullcontext()\n            ) as progress_bar:\n                for error in pool.imap(_call_populate1, keys, chunksize=1):\n                    if error is not None:\n                        error_list.append(error)\n                    if display_progress:\n                        progress_bar.update()\n            self.connection.connect()  # reconnect parent process to MySQL server\n\n        # restore original signal handler:\n        if reserve_jobs:\n            signal.signal(signal.SIGTERM, old_handler)\n\n        if suppress_errors:\n            return error_list\n\n    def _populate1(\n        self, key, jobs, suppress_errors, return_exception_objects, make_kwargs=None\n    ):\n\"\"\"\n        populates table for one source key, calling self.make inside a transaction.\n        :param jobs: the jobs table or None if not reserve_jobs\n        :param key: dict specifying job to populate\n        :param suppress_errors: bool if errors should be suppressed and returned\n        :param return_exception_objects: if True, errors must be returned as objects\n        :return: (key, error) when suppress_errors=True, otherwise None\n        \"\"\"\n        make = self._make_tuples if hasattr(self, \"_make_tuples\") else self.make\n\n        if jobs is None or jobs.reserve(self.target.table_name, self._job_key(key)):\n            self.connection.start_transaction()\n            if key in self.target:  # already populated\n                self.connection.cancel_transaction()\n                if jobs is not None:\n                    jobs.complete(self.target.table_name, self._job_key(key))\n            else:\n                logger.debug(f\"Making {key} -&gt; {self.target.full_table_name}\")\n                self.__class__._allow_insert = True\n                try:\n                    make(dict(key), **(make_kwargs or {}))\n                except (KeyboardInterrupt, SystemExit, Exception) as error:\n                    try:\n                        self.connection.cancel_transaction()\n                    except LostConnectionError:\n                        pass\n                    error_message = \"{exception}{msg}\".format(\n                        exception=error.__class__.__name__,\n                        msg=\": \" + str(error) if str(error) else \"\",\n                    )\n                    logger.debug(\n                        f\"Error making {key} -&gt; {self.target.full_table_name} - {error_message}\"\n                    )\n                    if jobs is not None:\n                        # show error name and error message (if any)\n                        jobs.error(\n                            self.target.table_name,\n                            self._job_key(key),\n                            error_message=error_message,\n                            error_stack=traceback.format_exc(),\n                        )\n                    if not suppress_errors or isinstance(error, SystemExit):\n                        raise\n                    else:\n                        logger.error(error)\n                        return key, error if return_exception_objects else error_message\n                else:\n                    self.connection.commit_transaction()\n                    logger.debug(\n                        f\"Success making {key} -&gt; {self.target.full_table_name}\"\n                    )\n                    if jobs is not None:\n                        jobs.complete(self.target.table_name, self._job_key(key))\n                finally:\n                    self.__class__._allow_insert = False\n\n    def progress(self, *restrictions, display=False):\n\"\"\"\n        Report the progress of populating the table.\n        :return: (remaining, total) -- numbers of tuples to be populated\n        \"\"\"\n        todo = self._jobs_to_do(restrictions)\n        total = len(todo)\n        remaining = len(todo - self.target)\n        if display:\n            logger.info(\n                \"%-20s\" % self.__class__.__name__\n                + \" Completed %d of %d (%2.1f%%)   %s\"\n                % (\n                    total - remaining,\n                    total,\n                    100 - 100 * remaining / (total + 1e-12),\n                    datetime.datetime.strftime(\n                        datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                    ),\n                ),\n            )\n        return remaining, total\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>the query expression that yields primary key values to be passed, sequentially, to the <code>make</code> method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls.</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.make", "title": "<code>make(key)</code>", "text": "<p>Derived classes must implement method <code>make</code> that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def make(self, key):\n\"\"\"\n    Derived classes must implement method `make` that fetches data from tables\n    above them in the dependency hierarchy, restricting by the given key,\n    computes secondary attributes, and inserts the new tuples into self.\n    \"\"\"\n    raise NotImplementedError(\n        \"Subclasses of AutoPopulate must implement the method `make`\"\n    )\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.target", "title": "<code>target</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self.</p>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.populate", "title": "<code>populate(*restrictions, suppress_errors=False, return_exception_objects=False, reserve_jobs=False, order='original', limit=None, max_calls=None, display_progress=False, processes=1, make_kwargs=None)</code>", "text": "<p><code>table.populate()</code> calls <code>table.make(key)</code> for every primary key in <code>self.key_source</code> for which there is not already a tuple in table.</p> <p>Parameters:</p> Name Type Description Default <code>restrictions</code> <p>a list of restrictions each restrict (table.key_source - target.proj())</p> <code>()</code> <code>suppress_errors</code> <p>if True, do not terminate execution.</p> <code>False</code> <code>return_exception_objects</code> <p>return error objects instead of just error messages</p> <code>False</code> <code>reserve_jobs</code> <p>if True, reserve jobs to populate in asynchronous fashion</p> <code>False</code> <code>order</code> <p>\"original\"|\"reverse\"|\"random\"  - the order of execution</p> <code>'original'</code> <code>limit</code> <p>if not None, check at most this many keys</p> <code>None</code> <code>max_calls</code> <p>if not None, populate at most this many keys</p> <code>None</code> <code>display_progress</code> <p>if True, report progress_bar</p> <code>False</code> <code>processes</code> <p>number of processes to use. Set to None to use all cores</p> <code>1</code> <code>make_kwargs</code> <code>dict, optional</code> <p>Keyword arguments which do not affect the result of computation to be passed down to each <code>make()</code> call. Computation arguments should be specified within the pipeline e.g. using a <code>dj.Lookup</code> table.</p> <code>None</code> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def populate(\n    self,\n    *restrictions,\n    suppress_errors=False,\n    return_exception_objects=False,\n    reserve_jobs=False,\n    order=\"original\",\n    limit=None,\n    max_calls=None,\n    display_progress=False,\n    processes=1,\n    make_kwargs=None,\n):\n\"\"\"\n    ``table.populate()`` calls ``table.make(key)`` for every primary key in\n    ``self.key_source`` for which there is not already a tuple in table.\n\n    :param restrictions: a list of restrictions each restrict\n        (table.key_source - target.proj())\n    :param suppress_errors: if True, do not terminate execution.\n    :param return_exception_objects: return error objects instead of just error messages\n    :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n    :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n    :param limit: if not None, check at most this many keys\n    :param max_calls: if not None, populate at most this many keys\n    :param display_progress: if True, report progress_bar\n    :param processes: number of processes to use. Set to None to use all cores\n    :param make_kwargs: Keyword arguments which do not affect the result of computation\n        to be passed down to each ``make()`` call. Computation arguments should be\n        specified within the pipeline e.g. using a `dj.Lookup` table.\n    :type make_kwargs: dict, optional\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n    valid_order = [\"original\", \"reverse\", \"random\"]\n    if order not in valid_order:\n        raise DataJointError(\n            \"The order argument must be one of %s\" % str(valid_order)\n        )\n    jobs = (\n        self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n    )\n\n    # define and set up signal handler for SIGTERM:\n    if reserve_jobs:\n\n        def handler(signum, frame):\n            logger.info(\"Populate terminated by SIGTERM\")\n            raise SystemExit(\"SIGTERM received\")\n\n        old_handler = signal.signal(signal.SIGTERM, handler)\n\n    keys = (self._jobs_to_do(restrictions) - self.target).fetch(\"KEY\", limit=limit)\n\n    # exclude \"error\" or \"ignore\" jobs\n    if reserve_jobs:\n        exclude_key_hashes = (\n            jobs\n            &amp; {\"table_name\": self.target.table_name}\n            &amp; 'status in (\"error\", \"ignore\")'\n        ).fetch(\"key_hash\")\n        keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n    if order == \"reverse\":\n        keys.reverse()\n    elif order == \"random\":\n        random.shuffle(keys)\n\n    logger.debug(\"Found %d keys to populate\" % len(keys))\n\n    keys = keys[:max_calls]\n    nkeys = len(keys)\n    if not nkeys:\n        return\n\n    processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n    error_list = []\n    populate_kwargs = dict(\n        suppress_errors=suppress_errors,\n        return_exception_objects=return_exception_objects,\n        make_kwargs=make_kwargs,\n    )\n\n    if processes == 1:\n        for key in (\n            tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n        ):\n            error = self._populate1(key, jobs, **populate_kwargs)\n            if error is not None:\n                error_list.append(error)\n    else:\n        # spawn multiple processes\n        self.connection.close()  # disconnect parent process from MySQL server\n        del self.connection._conn.ctx  # SSLContext is not pickleable\n        with mp.Pool(\n            processes, _initialize_populate, (self, jobs, populate_kwargs)\n        ) as pool, (\n            tqdm(desc=\"Processes: \", total=nkeys)\n            if display_progress\n            else contextlib.nullcontext()\n        ) as progress_bar:\n            for error in pool.imap(_call_populate1, keys, chunksize=1):\n                if error is not None:\n                    error_list.append(error)\n                if display_progress:\n                    progress_bar.update()\n        self.connection.connect()  # reconnect parent process to MySQL server\n\n    # restore original signal handler:\n    if reserve_jobs:\n        signal.signal(signal.SIGTERM, old_handler)\n\n    if suppress_errors:\n        return error_list\n</code></pre>"}, {"location": "api/datajoint/autopopulate/#datajoint.autopopulate.AutoPopulate.progress", "title": "<code>progress(*restrictions, display=False)</code>", "text": "<p>Report the progress of populating the table.</p> <p>Returns:</p> Type Description <p>(remaining, total) -- numbers of tuples to be populated</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def progress(self, *restrictions, display=False):\n\"\"\"\n    Report the progress of populating the table.\n    :return: (remaining, total) -- numbers of tuples to be populated\n    \"\"\"\n    todo = self._jobs_to_do(restrictions)\n    total = len(todo)\n    remaining = len(todo - self.target)\n    if display:\n        logger.info(\n            \"%-20s\" % self.__class__.__name__\n            + \" Completed %d of %d (%2.1f%%)   %s\"\n            % (\n                total - remaining,\n                total,\n                100 - 100 * remaining / (total + 1e-12),\n                datetime.datetime.strftime(\n                    datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                ),\n            ),\n        )\n    return remaining, total\n</code></pre>"}, {"location": "api/datajoint/blob/", "title": "blob.py", "text": "<p>(De)serialization methods for basic datatypes and numpy.ndarrays with provisions for mutual compatibility with Matlab-based serialization implemented by mYm.</p>"}, {"location": "api/datajoint/blob/#datajoint.blob.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.MatCell", "title": "<code>MatCell</code>", "text": "<p>         Bases: <code>np.ndarray</code></p> <p>a numpy ndarray representing a Matlab cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatCell(np.ndarray):\n\"\"\"a numpy ndarray representing a Matlab cell array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.MatStruct", "title": "<code>MatStruct</code>", "text": "<p>         Bases: <code>np.recarray</code></p> <p>numpy.recarray representing a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>class MatStruct(np.recarray):\n\"\"\"numpy.recarray representing a Matlab struct array\"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob", "title": "<code>Blob</code>", "text": "Source code in <code>datajoint/blob.py</code> <pre><code>class Blob:\n    def __init__(self, squeeze=False):\n        self._squeeze = squeeze\n        self._blob = None\n        self._pos = 0\n        self.protocol = None\n\n    def set_dj0(self):\n        if not config.get(\"enable_python_native_blobs\"):\n            raise DataJointError(\n\"\"\"v0.12+ python native blobs disabled.\n                See also: https://github.com/datajoint/datajoint-python#python-native-blobs\"\"\"\n            )\n\n        self.protocol = b\"dj0\\0\"  # when using new blob features\n\n    def squeeze(self, array, convert_to_scalar=True):\n\"\"\"\n        Simplify the input array - squeeze out all singleton dimensions.\n        If convert_to_scalar, then convert zero-dimensional arrays to scalars\n        \"\"\"\n        if not self._squeeze:\n            return array\n        array = array.squeeze()\n        return array.item() if array.ndim == 0 and convert_to_scalar else array\n\n    def unpack(self, blob):\n        self._blob = blob\n        try:\n            # decompress\n            prefix = next(\n                p for p in compression if self._blob[self._pos :].startswith(p)\n            )\n        except StopIteration:\n            pass  # assume uncompressed but could be unrecognized compression\n        else:\n            self._pos += len(prefix)\n            blob_size = self.read_value()\n            blob = compression[prefix](self._blob[self._pos :])\n            assert len(blob) == blob_size\n            self._blob = blob\n            self._pos = 0\n        blob_format = self.read_zero_terminated_string()\n        if blob_format in (\"mYm\", \"dj0\"):\n            return self.read_blob(n_bytes=len(self._blob) - self._pos)\n\n    def read_blob(self, n_bytes=None):\n        start = self._pos\n        data_structure_code = chr(self.read_value(\"uint8\"))\n        try:\n            call = {\n                # MATLAB-compatible, inherited from original mYm\n                \"A\": self.read_array,  # matlab-compatible numeric arrays and scalars with ndim==0\n                \"P\": self.read_sparse_array,  # matlab sparse array -- not supported yet\n                \"S\": self.read_struct,  # matlab struct array\n                \"C\": self.read_cell_array,  # matlab cell array\n                # basic data types\n                \"\\xFF\": self.read_none,  # None\n                \"\\x01\": self.read_tuple,  # a Sequence (e.g. tuple)\n                \"\\x02\": self.read_list,  # a MutableSequence (e.g. list)\n                \"\\x03\": self.read_set,  # a Set\n                \"\\x04\": self.read_dict,  # a Mapping (e.g. dict)\n                \"\\x05\": self.read_string,  # a UTF8-encoded string\n                \"\\x06\": self.read_bytes,  # a ByteString\n                \"\\x0a\": self.read_int,  # unbounded scalar int\n                \"\\x0b\": self.read_bool,  # scalar boolean\n                \"\\x0c\": self.read_complex,  # scalar 128-bit complex number\n                \"\\x0d\": self.read_float,  # scalar 64-bit float\n                \"F\": self.read_recarray,  # numpy array with fields, including recarrays\n                \"d\": self.read_decimal,  # a decimal\n                \"t\": self.read_datetime,  # date, time, or datetime\n                \"u\": self.read_uuid,  # UUID\n            }[data_structure_code]\n        except KeyError:\n            raise DataJointError(\n                'Unknown data structure code \"%s\". Upgrade datajoint.'\n                % data_structure_code\n            )\n        v = call()\n        if n_bytes is not None and self._pos - start != n_bytes:\n            raise DataJointError(\"Blob length check failed! Invalid blob\")\n        return v\n\n    def pack_blob(self, obj):\n        # original mYm-based serialization from datajoint-matlab\n        if isinstance(obj, MatCell):\n            return self.pack_cell_array(obj)\n        if isinstance(obj, MatStruct):\n            return self.pack_struct(obj)\n        if isinstance(obj, np.ndarray) and obj.dtype.fields is None:\n            return self.pack_array(obj)\n\n        # blob types in the expanded dj0 blob format\n        self.set_dj0()\n        if not isinstance(obj, (np.ndarray, np.number)):\n            # python built-in data types\n            if isinstance(obj, bool):\n                return self.pack_bool(obj)\n            if isinstance(obj, int):\n                return self.pack_int(obj)\n            if isinstance(obj, complex):\n                return self.pack_complex(obj)\n            if isinstance(obj, float):\n                return self.pack_float(obj)\n        if isinstance(obj, np.ndarray) and obj.dtype.fields:\n            return self.pack_recarray(np.array(obj))\n        if isinstance(obj, (np.number, np.datetime64)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (bool, np.bool_)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (float, int, complex)):\n            return self.pack_array(np.array(obj))\n        if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n            return self.pack_datetime(obj)\n        if isinstance(obj, Decimal):\n            return self.pack_decimal(obj)\n        if isinstance(obj, uuid.UUID):\n            return self.pack_uuid(obj)\n        if isinstance(obj, collections.abc.Mapping):\n            return self.pack_dict(obj)\n        if isinstance(obj, str):\n            return self.pack_string(obj)\n        if isinstance(obj, collections.abc.ByteString):\n            return self.pack_bytes(obj)\n        if isinstance(obj, collections.abc.MutableSequence):\n            return self.pack_list(obj)\n        if isinstance(obj, collections.abc.Sequence):\n            return self.pack_tuple(obj)\n        if isinstance(obj, collections.abc.Set):\n            return self.pack_set(obj)\n        if obj is None:\n            return self.pack_none()\n        raise DataJointError(\n            \"Packing object of type %s currently not supported!\" % type(obj)\n        )\n\n    def read_array(self):\n        n_dims = int(self.read_value())\n        shape = self.read_value(count=n_dims)\n        n_elem = np.prod(shape, dtype=int)\n        dtype_id, is_complex = self.read_value(\"uint32\", 2)\n\n        # Get dtype from type id\n        dtype = deserialize_lookup[dtype_id][\"dtype\"]\n\n        # Check if name is void\n        if deserialize_lookup[dtype_id][\"scalar_type\"] == \"VOID\":\n            data = np.array(\n                list(self.read_blob(self.read_value()) for _ in range(n_elem)),\n                dtype=np.dtype(\"O\"),\n            )\n        # Check if name is char\n        elif deserialize_lookup[dtype_id][\"scalar_type\"] == \"CHAR\":\n            # compensate for MATLAB packing of char arrays\n            data = self.read_value(dtype, count=2 * n_elem)\n            data = data[::2].astype(\"U1\")\n            if n_dims == 2 and shape[0] == 1 or n_dims == 1:\n                compact = data.squeeze()\n                data = (\n                    compact\n                    if compact.shape == ()\n                    else np.array(\"\".join(data.squeeze()))\n                )\n                shape = (1,)\n        else:\n            data = self.read_value(dtype, count=n_elem)\n            if is_complex:\n                data = data + 1j * self.read_value(dtype, count=n_elem)\n        return self.squeeze(data.reshape(shape, order=\"F\"))\n\n    def pack_array(self, array):\n\"\"\"\n        Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.\n        \"\"\"\n        if \"datetime64\" in array.dtype.name:\n            self.set_dj0()\n        blob = (\n            b\"A\"\n            + np.uint64(array.ndim).tobytes()\n            + np.array(array.shape, dtype=np.uint64).tobytes()\n        )\n        is_complex = np.iscomplexobj(array)\n        if is_complex:\n            array, imaginary = np.real(array), np.imag(array)\n        try:\n            type_id = serialize_lookup[array.dtype][\"type_id\"]\n        except KeyError:\n            # U is for unicode string\n            if array.dtype.char == \"U\":\n                type_id = serialize_lookup[np.dtype(\"O\")][\"type_id\"]\n            else:\n                raise DataJointError(f\"Type {array.dtype} is ambiguous or unknown\")\n\n        blob += np.array([type_id, is_complex], dtype=np.uint32).tobytes()\n        if (\n            array.dtype.char == \"U\"\n            or serialize_lookup[array.dtype][\"scalar_type\"] == \"VOID\"\n        ):\n            blob += b\"\".join(\n                len_u64(it) + it\n                for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n            )\n            self.set_dj0()  # not supported by original mym\n        elif serialize_lookup[array.dtype][\"scalar_type\"] == \"CHAR\":\n            blob += (\n                array.view(np.uint8).astype(np.uint16).tobytes()\n            )  # convert to 16-bit chars for MATLAB\n        else:  # numeric arrays\n            if array.ndim == 0:  # not supported by original mym\n                self.set_dj0()\n            blob += array.tobytes(order=\"F\")\n            if is_complex:\n                blob += imaginary.tobytes(order=\"F\")\n        return blob\n\n    def read_recarray(self):\n\"\"\"\n        Serialize an np.ndarray with fields, including recarrays\n        \"\"\"\n        n_fields = self.read_value(\"uint32\")\n        if not n_fields:\n            return np.array(None)  # empty array\n        field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n        arrays = [self.read_blob() for _ in range(n_fields)]\n        rec = np.empty(\n            arrays[0].shape,\n            np.dtype([(f, t.dtype) for f, t in zip(field_names, arrays)]),\n        )\n        for f, t in zip(field_names, arrays):\n            rec[f] = t\n        return rec.view(np.recarray)\n\n    def pack_recarray(self, array):\n\"\"\"Serialize a Matlab struct array\"\"\"\n        return (\n            b\"F\"\n            + len_u32(array.dtype)\n            + \"\\0\".join(array.dtype.names).encode()  # number of fields\n            + b\"\\0\"\n            + b\"\".join(  # field names\n                self.pack_recarray(array[f])\n                if array[f].dtype.fields\n                else self.pack_array(array[f])\n                for f in array.dtype.names\n            )\n        )\n\n    def read_sparse_array(self):\n        raise DataJointError(\n            \"datajoint-python does not yet support sparse arrays. Issue (#590)\"\n        )\n\n    def read_int(self):\n        return int.from_bytes(\n            self.read_binary(self.read_value(\"uint16\")), byteorder=\"little\", signed=True\n        )\n\n    @staticmethod\n    def pack_int(v):\n        n_bytes = v.bit_length() // 8 + 1\n        assert 0 &lt; n_bytes &lt;= 0xFFFF, \"Integers are limited to 65535 bytes\"\n        return (\n            b\"\\x0a\"\n            + np.uint16(n_bytes).tobytes()\n            + v.to_bytes(n_bytes, byteorder=\"little\", signed=True)\n        )\n\n    def read_bool(self):\n        return bool(self.read_value(\"bool\"))\n\n    @staticmethod\n    def pack_bool(v):\n        return b\"\\x0b\" + np.array(v, dtype=\"bool\").tobytes()\n\n    def read_complex(self):\n        return complex(self.read_value(\"complex128\"))\n\n    @staticmethod\n    def pack_complex(v):\n        return b\"\\x0c\" + np.array(v, dtype=\"complex128\").tobytes()\n\n    def read_float(self):\n        return float(self.read_value(\"float64\"))\n\n    @staticmethod\n    def pack_float(v):\n        return b\"\\x0d\" + np.array(v, dtype=\"float64\").tobytes()\n\n    def read_decimal(self):\n        return Decimal(self.read_string())\n\n    @staticmethod\n    def pack_decimal(d):\n        s = str(d)\n        return b\"d\" + len_u64(s) + s.encode()\n\n    def read_string(self):\n        return self.read_binary(self.read_value()).decode()\n\n    @staticmethod\n    def pack_string(s):\n        blob = s.encode()\n        return b\"\\5\" + len_u64(blob) + blob\n\n    def read_bytes(self):\n        return self.read_binary(self.read_value())\n\n    @staticmethod\n    def pack_bytes(s):\n        return b\"\\6\" + len_u64(s) + s\n\n    def read_none(self):\n        pass\n\n    @staticmethod\n    def pack_none():\n        return b\"\\xFF\"\n\n    def read_tuple(self):\n        return tuple(\n            self.read_blob(self.read_value()) for _ in range(self.read_value())\n        )\n\n    def pack_tuple(self, t):\n        return (\n            b\"\\1\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_list(self):\n        return list(self.read_blob(self.read_value()) for _ in range(self.read_value()))\n\n    def pack_list(self, t):\n        return (\n            b\"\\2\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_set(self):\n        return set(self.read_blob(self.read_value()) for _ in range(self.read_value()))\n\n    def pack_set(self, t):\n        return (\n            b\"\\3\"\n            + len_u64(t)\n            + b\"\".join(len_u64(it) + it for it in (self.pack_blob(i) for i in t))\n        )\n\n    def read_dict(self):\n        return dict(\n            (self.read_blob(self.read_value()), self.read_blob(self.read_value()))\n            for _ in range(self.read_value())\n        )\n\n    def pack_dict(self, d):\n        return (\n            b\"\\4\"\n            + len_u64(d)\n            + b\"\".join(\n                b\"\".join((len_u64(it) + it) for it in packed)\n                for packed in (map(self.pack_blob, pair) for pair in d.items())\n            )\n        )\n\n    def read_struct(self):\n\"\"\"deserialize matlab stuct\"\"\"\n        n_dims = self.read_value()\n        shape = self.read_value(count=n_dims)\n        n_elem = np.prod(shape, dtype=int)\n        n_fields = self.read_value(\"uint32\")\n        if not n_fields:\n            return np.array(None)  # empty array\n        field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n        raw_data = [\n            tuple(\n                self.read_blob(n_bytes=int(self.read_value())) for _ in range(n_fields)\n            )\n            for __ in range(n_elem)\n        ]\n        data = np.array(raw_data, dtype=list(zip(field_names, repeat(object))))\n        return self.squeeze(\n            data.reshape(shape, order=\"F\"), convert_to_scalar=False\n        ).view(MatStruct)\n\n    def pack_struct(self, array):\n\"\"\"Serialize a Matlab struct array\"\"\"\n        return (\n            b\"S\"\n            + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n            + len_u32(array.dtype.names)  # dimensionality\n            + \"\\0\".join(array.dtype.names).encode()  # number of fields\n            + b\"\\0\"\n            + b\"\".join(  # field names\n                len_u64(it) + it\n                for it in (\n                    self.pack_blob(e) for rec in array.flatten(order=\"F\") for e in rec\n                )\n            )\n        )  # values\n\n    def read_cell_array(self):\n\"\"\"deserialize MATLAB cell array\"\"\"\n        n_dims = self.read_value()\n        shape = self.read_value(count=n_dims)\n        n_elem = int(np.prod(shape))\n        result = [self.read_blob(n_bytes=self.read_value()) for _ in range(n_elem)]\n        return (\n            self.squeeze(\n                np.array(result).reshape(shape, order=\"F\"), convert_to_scalar=False\n            )\n        ).view(MatCell)\n\n    def pack_cell_array(self, array):\n        return (\n            b\"C\"\n            + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n            + b\"\".join(\n                len_u64(it) + it\n                for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n            )\n        )\n\n    def read_datetime(self):\n\"\"\"deserialize datetime.date, .time, or .datetime\"\"\"\n        date, time = self.read_value(\"int32\"), self.read_value(\"int64\")\n        date = (\n            datetime.date(year=date // 10000, month=(date // 100) % 100, day=date % 100)\n            if date &gt;= 0\n            else None\n        )\n        time = (\n            datetime.time(\n                hour=(time // 10000000000) % 100,\n                minute=(time // 100000000) % 100,\n                second=(time // 1000000) % 100,\n                microsecond=time % 1000000,\n            )\n            if time &gt;= 0\n            else None\n        )\n        return time and date and datetime.datetime.combine(date, time) or time or date\n\n    @staticmethod\n    def pack_datetime(d):\n        if isinstance(d, datetime.datetime):\n            date, time = d.date(), d.time()\n        elif isinstance(d, datetime.date):\n            date, time = d, None\n        else:\n            date, time = None, d\n        return b\"t\" + (\n            np.int32(\n                -1 if date is None else (date.year * 100 + date.month) * 100 + date.day\n            ).tobytes()\n            + np.int64(\n                -1\n                if time is None\n                else ((time.hour * 100 + time.minute) * 100 + time.second) * 1000000\n                + time.microsecond\n            ).tobytes()\n        )\n\n    def read_uuid(self):\n        q = self.read_binary(16)\n        return uuid.UUID(bytes=q)\n\n    @staticmethod\n    def pack_uuid(obj):\n        return b\"u\" + obj.bytes\n\n    def read_zero_terminated_string(self):\n        target = self._blob.find(b\"\\0\", self._pos)\n        data = self._blob[self._pos : target].decode()\n        self._pos = target + 1\n        return data\n\n    def read_value(self, dtype=None, count=1):\n        if dtype is None:\n            dtype = \"uint32\" if use_32bit_dims else \"uint64\"\n        data = np.frombuffer(self._blob, dtype=dtype, count=count, offset=self._pos)\n        self._pos += data.dtype.itemsize * data.size\n        return data[0] if count == 1 else data\n\n    def read_binary(self, size):\n        self._pos += int(size)\n        return self._blob[self._pos - int(size) : self._pos]\n\n    def pack(self, obj, compress):\n        self.protocol = b\"mYm\\0\"  # will be replaced with dj0 if new features are used\n        blob = self.pack_blob(\n            obj\n        )  # this may reset the protocol and must precede protocol evaluation\n        blob = self.protocol + blob\n        if compress and len(blob) &gt; 1000:\n            compressed = b\"ZL123\\0\" + len_u64(blob) + zlib.compress(blob)\n            if len(compressed) &lt; len(blob):\n                blob = compressed\n        return blob\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.squeeze", "title": "<code>squeeze(array, convert_to_scalar=True)</code>", "text": "<p>Simplify the input array - squeeze out all singleton dimensions. If convert_to_scalar, then convert zero-dimensional arrays to scalars</p> Source code in <code>datajoint/blob.py</code> <pre><code>def squeeze(self, array, convert_to_scalar=True):\n\"\"\"\n    Simplify the input array - squeeze out all singleton dimensions.\n    If convert_to_scalar, then convert zero-dimensional arrays to scalars\n    \"\"\"\n    if not self._squeeze:\n        return array\n    array = array.squeeze()\n    return array.item() if array.ndim == 0 and convert_to_scalar else array\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_array", "title": "<code>pack_array(array)</code>", "text": "<p>Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_array(self, array):\n\"\"\"\n    Serialize an np.ndarray into bytes.  Scalars are encoded with ndim=0.\n    \"\"\"\n    if \"datetime64\" in array.dtype.name:\n        self.set_dj0()\n    blob = (\n        b\"A\"\n        + np.uint64(array.ndim).tobytes()\n        + np.array(array.shape, dtype=np.uint64).tobytes()\n    )\n    is_complex = np.iscomplexobj(array)\n    if is_complex:\n        array, imaginary = np.real(array), np.imag(array)\n    try:\n        type_id = serialize_lookup[array.dtype][\"type_id\"]\n    except KeyError:\n        # U is for unicode string\n        if array.dtype.char == \"U\":\n            type_id = serialize_lookup[np.dtype(\"O\")][\"type_id\"]\n        else:\n            raise DataJointError(f\"Type {array.dtype} is ambiguous or unknown\")\n\n    blob += np.array([type_id, is_complex], dtype=np.uint32).tobytes()\n    if (\n        array.dtype.char == \"U\"\n        or serialize_lookup[array.dtype][\"scalar_type\"] == \"VOID\"\n    ):\n        blob += b\"\".join(\n            len_u64(it) + it\n            for it in (self.pack_blob(e) for e in array.flatten(order=\"F\"))\n        )\n        self.set_dj0()  # not supported by original mym\n    elif serialize_lookup[array.dtype][\"scalar_type\"] == \"CHAR\":\n        blob += (\n            array.view(np.uint8).astype(np.uint16).tobytes()\n        )  # convert to 16-bit chars for MATLAB\n    else:  # numeric arrays\n        if array.ndim == 0:  # not supported by original mym\n            self.set_dj0()\n        blob += array.tobytes(order=\"F\")\n        if is_complex:\n            blob += imaginary.tobytes(order=\"F\")\n    return blob\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_recarray", "title": "<code>read_recarray()</code>", "text": "<p>Serialize an np.ndarray with fields, including recarrays</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_recarray(self):\n\"\"\"\n    Serialize an np.ndarray with fields, including recarrays\n    \"\"\"\n    n_fields = self.read_value(\"uint32\")\n    if not n_fields:\n        return np.array(None)  # empty array\n    field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n    arrays = [self.read_blob() for _ in range(n_fields)]\n    rec = np.empty(\n        arrays[0].shape,\n        np.dtype([(f, t.dtype) for f, t in zip(field_names, arrays)]),\n    )\n    for f, t in zip(field_names, arrays):\n        rec[f] = t\n    return rec.view(np.recarray)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_recarray", "title": "<code>pack_recarray(array)</code>", "text": "<p>Serialize a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_recarray(self, array):\n\"\"\"Serialize a Matlab struct array\"\"\"\n    return (\n        b\"F\"\n        + len_u32(array.dtype)\n        + \"\\0\".join(array.dtype.names).encode()  # number of fields\n        + b\"\\0\"\n        + b\"\".join(  # field names\n            self.pack_recarray(array[f])\n            if array[f].dtype.fields\n            else self.pack_array(array[f])\n            for f in array.dtype.names\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_struct", "title": "<code>read_struct()</code>", "text": "<p>deserialize matlab stuct</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_struct(self):\n\"\"\"deserialize matlab stuct\"\"\"\n    n_dims = self.read_value()\n    shape = self.read_value(count=n_dims)\n    n_elem = np.prod(shape, dtype=int)\n    n_fields = self.read_value(\"uint32\")\n    if not n_fields:\n        return np.array(None)  # empty array\n    field_names = [self.read_zero_terminated_string() for _ in range(n_fields)]\n    raw_data = [\n        tuple(\n            self.read_blob(n_bytes=int(self.read_value())) for _ in range(n_fields)\n        )\n        for __ in range(n_elem)\n    ]\n    data = np.array(raw_data, dtype=list(zip(field_names, repeat(object))))\n    return self.squeeze(\n        data.reshape(shape, order=\"F\"), convert_to_scalar=False\n    ).view(MatStruct)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.pack_struct", "title": "<code>pack_struct(array)</code>", "text": "<p>Serialize a Matlab struct array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def pack_struct(self, array):\n\"\"\"Serialize a Matlab struct array\"\"\"\n    return (\n        b\"S\"\n        + np.array((array.ndim,) + array.shape, dtype=np.uint64).tobytes()\n        + len_u32(array.dtype.names)  # dimensionality\n        + \"\\0\".join(array.dtype.names).encode()  # number of fields\n        + b\"\\0\"\n        + b\"\".join(  # field names\n            len_u64(it) + it\n            for it in (\n                self.pack_blob(e) for rec in array.flatten(order=\"F\") for e in rec\n            )\n        )\n    )  # values\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_cell_array", "title": "<code>read_cell_array()</code>", "text": "<p>deserialize MATLAB cell array</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_cell_array(self):\n\"\"\"deserialize MATLAB cell array\"\"\"\n    n_dims = self.read_value()\n    shape = self.read_value(count=n_dims)\n    n_elem = int(np.prod(shape))\n    result = [self.read_blob(n_bytes=self.read_value()) for _ in range(n_elem)]\n    return (\n        self.squeeze(\n            np.array(result).reshape(shape, order=\"F\"), convert_to_scalar=False\n        )\n    ).view(MatCell)\n</code></pre>"}, {"location": "api/datajoint/blob/#datajoint.blob.Blob.read_datetime", "title": "<code>read_datetime()</code>", "text": "<p>deserialize datetime.date, .time, or .datetime</p> Source code in <code>datajoint/blob.py</code> <pre><code>def read_datetime(self):\n\"\"\"deserialize datetime.date, .time, or .datetime\"\"\"\n    date, time = self.read_value(\"int32\"), self.read_value(\"int64\")\n    date = (\n        datetime.date(year=date // 10000, month=(date // 100) % 100, day=date % 100)\n        if date &gt;= 0\n        else None\n    )\n    time = (\n        datetime.time(\n            hour=(time // 10000000000) % 100,\n            minute=(time // 100000000) % 100,\n            second=(time // 1000000) % 100,\n            microsecond=time % 1000000,\n        )\n        if time &gt;= 0\n        else None\n    )\n    return time and date and datetime.datetime.combine(date, time) or time or date\n</code></pre>"}, {"location": "api/datajoint/condition/", "title": "condition.py", "text": "<p>methods for generating SQL WHERE clauses from datajoint restriction conditions</p>"}, {"location": "api/datajoint/condition/#datajoint.condition.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.PromiscuousOperand", "title": "<code>PromiscuousOperand</code>", "text": "<p>A container for an operand to ignore join compatibility</p> Source code in <code>datajoint/condition.py</code> <pre><code>class PromiscuousOperand:\n\"\"\"\n    A container for an operand to ignore join compatibility\n    \"\"\"\n\n    def __init__(self, operand):\n        self.operand = operand\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.AndList", "title": "<code>AndList</code>", "text": "<p>         Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n\"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.Not", "title": "<code>Not</code>", "text": "<p>invert restriction</p> Source code in <code>datajoint/condition.py</code> <pre><code>class Not:\n\"\"\"invert restriction\"\"\"\n\n    def __init__(self, restriction):\n        self.restriction = restriction\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.assert_join_compatibility", "title": "<code>assert_join_compatibility(expr1, expr2)</code>", "text": "<p>Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible.</p> <p>Parameters:</p> Name Type Description Default <code>expr1</code> <p>A QueryExpression object</p> required <code>expr2</code> <p>A QueryExpression object</p> required Source code in <code>datajoint/condition.py</code> <pre><code>def assert_join_compatibility(expr1, expr2):\n\"\"\"\n    Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible,\n    the matching attributes in the two expressions must be in the primary key of one or the\n    other expression.\n    Raises an exception if not compatible.\n\n    :param expr1: A QueryExpression object\n    :param expr2: A QueryExpression object\n    \"\"\"\n    from .expression import QueryExpression, U\n\n    for rel in (expr1, expr2):\n        if not isinstance(rel, (U, QueryExpression)):\n            raise DataJointError(\n                \"Object %r is not a QueryExpression and cannot be joined.\" % rel\n            )\n    if not isinstance(expr1, U) and not isinstance(\n        expr2, U\n    ):  # dj.U is always compatible\n        try:\n            raise DataJointError(\n                \"Cannot join query expressions on dependent attribute `%s`\"\n                % next(\n                    r\n                    for r in set(expr1.heading.secondary_attributes).intersection(\n                        expr2.heading.secondary_attributes\n                    )\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.make_condition", "title": "<code>make_condition(query_expression, condition, columns)</code>", "text": "<p>Translate the input condition into the equivalent SQL condition (a string)</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <p>a dj.QueryExpression object to apply condition</p> required <code>condition</code> <p>any valid restriction object.</p> required <code>columns</code> <p>a set passed by reference to collect all column names used in the condition.</p> required <p>Returns:</p> Type Description <p>an SQL condition string or a boolean value.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def make_condition(query_expression, condition, columns):\n\"\"\"\n    Translate the input condition into the equivalent SQL condition (a string)\n\n    :param query_expression: a dj.QueryExpression object to apply condition\n    :param condition: any valid restriction object.\n    :param columns: a set passed by reference to collect all column names used in the\n        condition.\n    :return: an SQL condition string or a boolean value.\n    \"\"\"\n    from .expression import QueryExpression, Aggregation, U\n\n    def prep_value(k, v):\n\"\"\"prepare SQL condition\"\"\"\n        key_match, k = translate_attribute(k)\n        if key_match[\"path\"] is None:\n            k = f\"`{k}`\"\n        if (\n            query_expression.heading[key_match[\"attr\"]].json\n            and key_match[\"path\"] is not None\n            and isinstance(v, dict)\n        ):\n            return f\"{k}='{json.dumps(v)}'\"\n        if v is None:\n            return f\"{k} IS NULL\"\n        if query_expression.heading[key_match[\"attr\"]].uuid:\n            if not isinstance(v, uuid.UUID):\n                try:\n                    v = uuid.UUID(v)\n                except (AttributeError, ValueError):\n                    raise DataJointError(\n                        \"Badly formed UUID {v} in restriction by `{k}`\".format(k=k, v=v)\n                    )\n            return f\"{k}=X'{v.bytes.hex()}'\"\n        if isinstance(\n            v,\n            (\n                datetime.date,\n                datetime.datetime,\n                datetime.time,\n                decimal.Decimal,\n                list,\n            ),\n        ):\n            return f'{k}=\"{v}\"'\n        if isinstance(v, str):\n            v = v.replace(\"%\", \"%%\").replace(\"\\\\\", \"\\\\\\\\\")\n            return f'{k}=\"{v}\"'\n        return f\"{k}={v}\"\n\n    def combine_conditions(negate, conditions):\n        return f\"{'NOT ' if negate else ''} ({')AND('.join(conditions)})\"\n\n    negate = False\n    while isinstance(condition, Not):\n        negate = not negate\n        condition = condition.restriction\n\n    # restrict by string\n    if isinstance(condition, str):\n        columns.update(extract_column_names(condition))\n        return combine_conditions(\n            negate, conditions=[condition.strip().replace(\"%\", \"%%\")]\n        )  # escape %, see issue #376\n\n    # restrict by AndList\n    if isinstance(condition, AndList):\n        # omit all conditions that evaluate to True\n        items = [\n            item\n            for item in (\n                make_condition(query_expression, cond, columns) for cond in condition\n            )\n            if item is not True\n        ]\n        if any(item is False for item in items):\n            return negate  # if any item is False, the whole thing is False\n        if not items:\n            return not negate  # and empty AndList is True\n        return combine_conditions(negate, conditions=items)\n\n    # restriction by dj.U evaluates to True\n    if isinstance(condition, U):\n        return not negate\n\n    # restrict by boolean\n    if isinstance(condition, bool):\n        return negate != condition\n\n    # restrict by a mapping/dict -- convert to an AndList of string equality conditions\n    if isinstance(condition, collections.abc.Mapping):\n        common_attributes = set(c.split(\".\", 1)[0] for c in condition).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluates to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[\n                prep_value(k, v)\n                for k, v in condition.items()\n                if k.split(\".\", 1)[0] in common_attributes  # handle json indexing\n            ],\n        )\n\n    # restrict by a numpy record -- convert to an AndList of string equality conditions\n    if isinstance(condition, numpy.void):\n        common_attributes = set(condition.dtype.fields).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluate to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[prep_value(k, condition[k]) for k in common_attributes],\n        )\n\n    # restrict by a QueryExpression subclass -- trigger instantiation and move on\n    if inspect.isclass(condition) and issubclass(condition, QueryExpression):\n        condition = condition()\n\n    # restrict by another expression (aka semijoin and antijoin)\n    check_compatibility = True\n    if isinstance(condition, PromiscuousOperand):\n        condition = condition.operand\n        check_compatibility = False\n\n    if isinstance(condition, QueryExpression):\n        if check_compatibility:\n            assert_join_compatibility(query_expression, condition)\n        common_attributes = [\n            q for q in condition.heading.names if q in query_expression.heading.names\n        ]\n        columns.update(common_attributes)\n        if isinstance(condition, Aggregation):\n            condition = condition.make_subquery()\n        return (\n            # without common attributes, any non-empty set matches everything\n            (not negate if condition else negate)\n            if not common_attributes\n            else \"({fields}) {not_}in ({subquery})\".format(\n                fields=\"`\" + \"`,`\".join(common_attributes) + \"`\",\n                not_=\"not \" if negate else \"\",\n                subquery=condition.make_sql(common_attributes),\n            )\n        )\n\n    # restrict by pandas.DataFrames\n    if isinstance(condition, pandas.DataFrame):\n        condition = condition.to_records()  # convert to numpy.recarray and move on\n\n    # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList\n    try:\n        or_list = [make_condition(query_expression, q, columns) for q in condition]\n    except TypeError:\n        raise DataJointError(\"Invalid restriction type %r\" % condition)\n    else:\n        or_list = [\n            item for item in or_list if item is not False\n        ]  # ignore False conditions\n        if any(item is True for item in or_list):  # if any item is True, entirely True\n            return not negate\n        return (\n            f\"{'NOT ' if negate else ''} ({' OR '.join(or_list)})\"\n            if or_list\n            else negate\n        )\n</code></pre>"}, {"location": "api/datajoint/condition/#datajoint.condition.extract_column_names", "title": "<code>extract_column_names(sql_expression)</code>", "text": "<p>extract all presumed column names from an sql expression such as the WHERE clause, for example.</p> <p>Parameters:</p> Name Type Description Default <code>sql_expression</code> <p>a string containing an SQL expression</p> required <p>Returns:</p> Type Description <p>set of extracted column names This may be MySQL-specific for now.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def extract_column_names(sql_expression):\n\"\"\"\n    extract all presumed column names from an sql expression such as the WHERE clause,\n    for example.\n\n    :param sql_expression: a string containing an SQL expression\n    :return: set of extracted column names\n    This may be MySQL-specific for now.\n    \"\"\"\n    assert isinstance(sql_expression, str)\n    result = set()\n    s = sql_expression  # for terseness\n    # remove escaped quotes\n    s = re.sub(r\"(\\\\\\\")|(\\\\\\')\", \"\", s)\n    # remove quoted text\n    s = re.sub(r\"'[^']*'\", \"\", s)\n    s = re.sub(r'\"[^\"]*\"', \"\", s)\n    # find all tokens in back quotes and remove them\n    result.update(re.findall(r\"`([a-z][a-z_0-9]*)`\", s))\n    s = re.sub(r\"`[a-z][a-z_0-9]*`\", \"\", s)\n    # remove space before parentheses\n    s = re.sub(r\"\\s*\\(\", \"(\", s)\n    # remove tokens followed by ( since they must be functions\n    s = re.sub(r\"(\\b[a-z][a-z_0-9]*)\\(\", \"(\", s)\n    remaining_tokens = set(re.findall(r\"\\b[a-z][a-z_0-9]*\\b\", s))\n    # update result removing reserved words\n    result.update(\n        remaining_tokens\n        - {\n            \"is\",\n            \"in\",\n            \"between\",\n            \"like\",\n            \"and\",\n            \"or\",\n            \"null\",\n            \"not\",\n            \"interval\",\n            \"second\",\n            \"minute\",\n            \"hour\",\n            \"day\",\n            \"month\",\n            \"week\",\n            \"year\",\n        }\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/connection/", "title": "connection.py", "text": "<p>This module contains the Connection class that manages the connection to the database, and the <code>conn</code> function that provides access to a persistent connection in datajoint.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.Dependencies", "title": "<code>Dependencies</code>", "text": "<p>         Bases: <code>nx.DiGraph</code></p> <p>The graph of dependencies (foreign keys) between loaded tables.</p> <p>Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>class Dependencies(nx.DiGraph):\n\"\"\"\n    The graph of dependencies (foreign keys) between loaded tables.\n\n    Note: the 'connection' argument should normally be supplied;\n    Empty use is permitted to facilitate use of networkx algorithms which\n    internally create objects with the expectation of empty constructors.\n    See also: https://github.com/datajoint/datajoint-python/pull/443\n    \"\"\"\n\n    def __init__(self, connection=None):\n        self._conn = connection\n        self._node_alias_count = itertools.count()\n        self._loaded = False\n        super().__init__(self)\n\n    def clear(self):\n        self._loaded = False\n        super().clear()\n\n    def load(self, force=True):\n\"\"\"\n        Load dependencies for all loaded schemas.\n        This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n        \"\"\"\n        # reload from scratch to prevent duplication of renamed edges\n        if self._loaded and not force:\n            return\n\n        self.clear()\n\n        # load primary key info\n        keys = self._conn.query(\n\"\"\"\n                SELECT\n                    concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n                FROM information_schema.key_column_usage\n                WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n                \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            )\n        )\n        pks = defaultdict(set)\n        for key in keys:\n            pks[key[0]].add(key[1])\n\n        # add nodes to the graph\n        for n, pk in pks.items():\n            self.add_node(n, primary_key=pk)\n\n        # load foreign keys\n        keys = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in self._conn.query(\n\"\"\"\n        SELECT constraint_name,\n            concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n            concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n            column_name, referenced_column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n            referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n        \"\"\".format(\n                    schemas=\"','\".join(self._conn.schemas)\n                ),\n                as_dict=True,\n            )\n        )\n        fks = defaultdict(lambda: dict(attr_map=dict()))\n        for key in keys:\n            d = fks[\n                (\n                    key[\"constraint_name\"],\n                    key[\"referencing_table\"],\n                    key[\"referenced_table\"],\n                )\n            ]\n            d[\"referencing_table\"] = key[\"referencing_table\"]\n            d[\"referenced_table\"] = key[\"referenced_table\"]\n            d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n        # add edges to the graph\n        for fk in fks.values():\n            props = dict(\n                primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n                attr_map=fk[\"attr_map\"],\n                aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n                multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n            )\n            if not props[\"aliased\"]:\n                self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n            else:\n                # for aliased dependencies, add an extra node in the format '1', '2', etc\n                alias_node = \"%d\" % next(self._node_alias_count)\n                self.add_node(alias_node)\n                self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n                self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n        if not nx.is_directed_acyclic_graph(self):\n            raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n        self._loaded = True\n\n    def parents(self, table_name, primary=None):\n\"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referenced by the foreign keys of table\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[0]: p[2]\n            for p in self.in_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def children(self, table_name, primary=None):\n\"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referencing the table through foreign keys\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[1]: p[2]\n            for p in self.out_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def descendants(self, full_table_name):\n\"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.algorithms.dag.descendants(self, full_table_name))\n        return unite_master_parts(\n            [full_table_name] + list(nx.algorithms.dag.topological_sort(nodes))\n        )\n\n    def ancestors(self, full_table_name):\n\"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.algorithms.dag.ancestors(self, full_table_name))\n        return list(\n            reversed(\n                unite_master_parts(\n                    list(nx.algorithms.dag.topological_sort(nodes)) + [full_table_name]\n                )\n            )\n        )\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.dependencies.Dependencies.load", "title": "<code>load(force=True)</code>", "text": "<p>Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def load(self, force=True):\n\"\"\"\n    Load dependencies for all loaded schemas.\n    This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n    \"\"\"\n    # reload from scratch to prevent duplication of renamed edges\n    if self._loaded and not force:\n        return\n\n    self.clear()\n\n    # load primary key info\n    keys = self._conn.query(\n\"\"\"\n            SELECT\n                concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n            FROM information_schema.key_column_usage\n            WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n            \"\"\".format(\n            schemas=\"','\".join(self._conn.schemas)\n        )\n    )\n    pks = defaultdict(set)\n    for key in keys:\n        pks[key[0]].add(key[1])\n\n    # add nodes to the graph\n    for n, pk in pks.items():\n        self.add_node(n, primary_key=pk)\n\n    # load foreign keys\n    keys = (\n        {k.lower(): v for k, v in elem.items()}\n        for elem in self._conn.query(\n\"\"\"\n    SELECT constraint_name,\n        concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n        concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n        column_name, referenced_column_name\n    FROM information_schema.key_column_usage\n    WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n        referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n    \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            ),\n            as_dict=True,\n        )\n    )\n    fks = defaultdict(lambda: dict(attr_map=dict()))\n    for key in keys:\n        d = fks[\n            (\n                key[\"constraint_name\"],\n                key[\"referencing_table\"],\n                key[\"referenced_table\"],\n            )\n        ]\n        d[\"referencing_table\"] = key[\"referencing_table\"]\n        d[\"referenced_table\"] = key[\"referenced_table\"]\n        d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n    # add edges to the graph\n    for fk in fks.values():\n        props = dict(\n            primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n            attr_map=fk[\"attr_map\"],\n            aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n            multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n        )\n        if not props[\"aliased\"]:\n            self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n        else:\n            # for aliased dependencies, add an extra node in the format '1', '2', etc\n            alias_node = \"%d\" % next(self._node_alias_count)\n            self.add_node(alias_node)\n            self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n            self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n    if not nx.is_directed_acyclic_graph(self):\n        raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n    self._loaded = True\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.dependencies.Dependencies.parents", "title": "<code>parents(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referenced by the foreign keys of table</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def parents(self, table_name, primary=None):\n\"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referenced by the foreign keys of table\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[0]: p[2]\n        for p in self.in_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.dependencies.Dependencies.children", "title": "<code>children(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referencing the table through foreign keys</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def children(self, table_name, primary=None):\n\"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referencing the table through foreign keys\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[1]: p[2]\n        for p in self.out_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.dependencies.Dependencies.descendants", "title": "<code>descendants(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def descendants(self, full_table_name):\n\"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.algorithms.dag.descendants(self, full_table_name))\n    return unite_master_parts(\n        [full_table_name] + list(nx.algorithms.dag.topological_sort(nodes))\n    )\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.dependencies.Dependencies.ancestors", "title": "<code>ancestors(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def ancestors(self, full_table_name):\n\"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.algorithms.dag.ancestors(self, full_table_name))\n    return list(\n        reversed(\n            unite_master_parts(\n                list(nx.algorithms.dag.topological_sort(nodes)) + [full_table_name]\n            )\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.translate_query_error", "title": "<code>translate_query_error(client_error, query)</code>", "text": "<p>Take client error and original query and return the corresponding DataJoint exception.</p> <p>Parameters:</p> Name Type Description Default <code>client_error</code> <p>the exception raised by the client interface</p> required <code>query</code> <p>sql query with placeholders</p> required <p>Returns:</p> Type Description <p>an instance of the corresponding subclass of datajoint.errors.DataJointError</p> Source code in <code>datajoint/connection.py</code> <pre><code>def translate_query_error(client_error, query):\n\"\"\"\n    Take client error and original query and return the corresponding DataJoint exception.\n\n    :param client_error: the exception raised by the client interface\n    :param query: sql query with placeholders\n    :return: an instance of the corresponding subclass of datajoint.errors.DataJointError\n    \"\"\"\n    logger.debug(\"type: {}, args: {}\".format(type(client_error), client_error.args))\n\n    err, *args = client_error.args\n\n    # Loss of connection errors\n    if err in (0, \"(0, '')\"):\n        return errors.LostConnectionError(\n            \"Server connection lost due to an interface error.\", *args\n        )\n    if err == 2006:\n        return errors.LostConnectionError(\"Connection timed out\", *args)\n    if err == 2013:\n        return errors.LostConnectionError(\"Server connection lost\", *args)\n    # Access errors\n    if err in (1044, 1142):\n        return errors.AccessError(\"Insufficient privileges.\", args[0], query)\n    # Integrity errors\n    if err == 1062:\n        return errors.DuplicateError(*args)\n    if err == 1451:\n        return errors.IntegrityError(*args)\n    if err == 1452:\n        return errors.IntegrityError(*args)\n    # Syntax errors\n    if err == 1064:\n        return errors.QuerySyntaxError(args[0], query)\n    # Existence errors\n    if err == 1146:\n        return errors.MissingTableError(args[0], query)\n    if err == 1364:\n        return errors.MissingAttributeError(*args)\n    if err == 1054:\n        return errors.UnknownAttributeError(*args)\n    # all the other errors are re-raised in original form\n    return client_error\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n\"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS prefered, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.EmulatedCursor", "title": "<code>EmulatedCursor</code>", "text": "<p>acts like a cursor</p> Source code in <code>datajoint/connection.py</code> <pre><code>class EmulatedCursor:\n\"\"\"acts like a cursor\"\"\"\n\n    def __init__(self, data):\n        self._data = data\n        self._iter = iter(self._data)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._iter)\n\n    def fetchall(self):\n        return self._data\n\n    def fetchone(self):\n        return next(self._iter)\n\n    @property\n    def rowcount(self):\n        return len(self._data)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection", "title": "<code>Connection</code>", "text": "<p>A dj.Connection object manages a connection to a database server. It also catalogues modules, schemas, tables, and their dependencies (foreign keys).</p> <p>Most of the parameters below should be set in the local configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>host name, may include port number as hostname:port, in which case it overrides the value in port</p> required <code>user</code> <p>user name</p> required <code>password</code> <p>password</p> required <code>port</code> <p>port number</p> <code>None</code> <code>init_fun</code> <p>connection initialization function (SQL)</p> <code>None</code> <code>use_tls</code> <p>TLS encryption option</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>class Connection:\n\"\"\"\n    A dj.Connection object manages a connection to a database server.\n    It also catalogues modules, schemas, tables, and their dependencies (foreign keys).\n\n    Most of the parameters below should be set in the local configuration file.\n\n    :param host: host name, may include port number as hostname:port, in which case it overrides the value in port\n    :param user: user name\n    :param password: password\n    :param port: port number\n    :param init_fun: connection initialization function (SQL)\n    :param use_tls: TLS encryption option\n    \"\"\"\n\n    def __init__(self, host, user, password, port=None, init_fun=None, use_tls=None):\n        host_input, host = (host, get_host_hook(host))\n        if \":\" in host:\n            # the port in the hostname overrides the port argument\n            host, port = host.split(\":\")\n            port = int(port)\n        elif port is None:\n            port = config[\"database.port\"]\n        self.conn_info = dict(host=host, port=port, user=user, passwd=password)\n        if use_tls is not False:\n            self.conn_info[\"ssl\"] = (\n                use_tls if isinstance(use_tls, dict) else {\"ssl\": {}}\n            )\n        self.conn_info[\"ssl_input\"] = use_tls\n        self.conn_info[\"host_input\"] = host_input\n        self.init_fun = init_fun\n        logger.info(\"Connecting {user}@{host}:{port}\".format(**self.conn_info))\n        self._conn = None\n        self._query_cache = None\n        connect_host_hook(self)\n        if self.is_connected:\n            logger.info(\"Connected {user}@{host}:{port}\".format(**self.conn_info))\n            self.connection_id = self.query(\"SELECT connection_id()\").fetchone()[0]\n        else:\n            raise errors.LostConnectionError(\"Connection failed.\")\n        self._in_transaction = False\n        self.schemas = dict()\n        self.dependencies = Dependencies(self)\n\n    def __eq__(self, other):\n        return self.conn_info == other.conn_info\n\n    def __repr__(self):\n        connected = \"connected\" if self.is_connected else \"disconnected\"\n        return \"DataJoint connection ({connected}) {user}@{host}:{port}\".format(\n            connected=connected, **self.conn_info\n        )\n\n    def connect(self):\n\"\"\"Connect to the database server.\"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n            try:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if k not in [\"ssl_input\", \"host_input\"]\n                    },\n                )\n            except client.err.InternalError:\n                self._conn = client.connect(\n                    init_command=self.init_fun,\n                    sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                    \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                    charset=config[\"connection.charset\"],\n                    **{\n                        k: v\n                        for k, v in self.conn_info.items()\n                        if not (\n                            k in [\"ssl_input\", \"host_input\"]\n                            or k == \"ssl\"\n                            and self.conn_info[\"ssl_input\"] is None\n                        )\n                    },\n                )\n        self._conn.autocommit(True)\n\n    def set_query_cache(self, query_cache=None):\n\"\"\"\n        When query_cache is not None, the connection switches into the query caching mode, which entails:\n        1. Only SELECT queries are allowed.\n        2. The results of queries are cached under the path indicated by dj.config['query_cache']\n        3. query_cache is a string that differentiates different cache states.\n\n        :param query_cache: a string to initialize the hash for query results\n        \"\"\"\n        self._query_cache = query_cache\n\n    def purge_query_cache(self):\n\"\"\"Purges all query cache.\"\"\"\n        if (\n            isinstance(config.get(cache_key), str)\n            and pathlib.Path(config[cache_key]).is_dir()\n        ):\n            for path in pathlib.Path(config[cache_key]).iterdir():\n                if not path.is_dir():\n                    path.unlink()\n\n    def close(self):\n        self._conn.close()\n\n    def register(self, schema):\n        self.schemas[schema.database] = schema\n        self.dependencies.clear()\n\n    def ping(self):\n\"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n        self._conn.ping(reconnect=False)\n\n    @property\n    def is_connected(self):\n\"\"\"Return true if the object is connected to the database server.\"\"\"\n        try:\n            self.ping()\n        except:\n            return False\n        return True\n\n    @staticmethod\n    def _execute_query(cursor, query, args, suppress_warnings):\n        try:\n            with warnings.catch_warnings():\n                if suppress_warnings:\n                    # suppress all warnings arising from underlying SQL library\n                    warnings.simplefilter(\"ignore\")\n                cursor.execute(query, args)\n        except client.err.Error as err:\n            raise translate_query_error(err, query)\n\n    def query(\n        self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n    ):\n\"\"\"\n        Execute the specified query and return the tuple generator (cursor).\n\n        :param query: SQL query\n        :param args: additional arguments for the client.cursor\n        :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                        query results as dictionary.\n        :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n        :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n        \"\"\"\n        # check cache first:\n        use_query_cache = bool(self._query_cache)\n        if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n            raise errors.DataJointError(\n                \"Only SELECT queries are allowed when query caching is on.\"\n            )\n        if use_query_cache:\n            if not config[cache_key]:\n                raise errors.DataJointError(\n                    f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n                )\n            hash_ = uuid_from_buffer(\n                (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n                + pack(args)\n            )\n            cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n            try:\n                buffer = cache_path.read_bytes()\n            except FileNotFoundError:\n                pass  # proceed to query the database\n            else:\n                return EmulatedCursor(unpack(buffer))\n\n        if reconnect is None:\n            reconnect = config[\"database.reconnect\"]\n        logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n        cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n        cursor = self._conn.cursor(cursor=cursor_class)\n        try:\n            self._execute_query(cursor, query, args, suppress_warnings)\n        except errors.LostConnectionError:\n            if not reconnect:\n                raise\n            logger.warning(\"MySQL server has gone away. Reconnecting to the server.\")\n            connect_host_hook(self)\n            if self._in_transaction:\n                self.cancel_transaction()\n                raise errors.LostConnectionError(\n                    \"Connection was lost during a transaction.\"\n                )\n            logger.debug(\"Re-executing\")\n            cursor = self._conn.cursor(cursor=cursor_class)\n            self._execute_query(cursor, query, args, suppress_warnings)\n\n        if use_query_cache:\n            data = cursor.fetchall()\n            cache_path.write_bytes(pack(data))\n            return EmulatedCursor(data)\n\n        return cursor\n\n    def get_user(self):\n\"\"\"\n        :return: the user name and host name provided by the client to the server.\n        \"\"\"\n        return self.query(\"SELECT user()\").fetchone()[0]\n\n    # ---------- transaction processing\n    @property\n    def in_transaction(self):\n\"\"\"\n        :return: True if there is an open transaction.\n        \"\"\"\n        self._in_transaction = self._in_transaction and self.is_connected\n        return self._in_transaction\n\n    def start_transaction(self):\n\"\"\"\n        Starts a transaction error.\n        \"\"\"\n        if self.in_transaction:\n            raise errors.DataJointError(\"Nested connections are not supported.\")\n        self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n        self._in_transaction = True\n        logger.debug(\"Transaction started\")\n\n    def cancel_transaction(self):\n\"\"\"\n        Cancels the current transaction and rolls back all changes made during the transaction.\n        \"\"\"\n        self.query(\"ROLLBACK\")\n        self._in_transaction = False\n        logger.debug(\"Transaction cancelled. Rolling back ...\")\n\n    def commit_transaction(self):\n\"\"\"\n        Commit all changes made during the transaction and close it.\n\n        \"\"\"\n        self.query(\"COMMIT\")\n        self._in_transaction = False\n        logger.debug(\"Transaction committed and closed.\")\n\n    # -------- context manager for transactions\n    @property\n    @contextmanager\n    def transaction(self):\n\"\"\"\n        Context manager for transactions. Opens an transaction and closes it after the with statement.\n        If an error is caught during the transaction, the commits are automatically rolled back.\n        All errors are raised again.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.conn().transaction as conn:\n        &gt;&gt;&gt;     # transaction is open here\n        \"\"\"\n        try:\n            self.start_transaction()\n            yield self\n        except:\n            self.cancel_transaction()\n            raise\n        else:\n            self.commit_transaction()\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.connect", "title": "<code>connect()</code>", "text": "<p>Connect to the database server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def connect(self):\n\"\"\"Connect to the database server.\"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \".*deprecated.*\")\n        try:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if k not in [\"ssl_input\", \"host_input\"]\n                },\n            )\n        except client.err.InternalError:\n            self._conn = client.connect(\n                init_command=self.init_fun,\n                sql_mode=\"NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,\"\n                \"STRICT_ALL_TABLES,NO_ENGINE_SUBSTITUTION,ONLY_FULL_GROUP_BY\",\n                charset=config[\"connection.charset\"],\n                **{\n                    k: v\n                    for k, v in self.conn_info.items()\n                    if not (\n                        k in [\"ssl_input\", \"host_input\"]\n                        or k == \"ssl\"\n                        and self.conn_info[\"ssl_input\"] is None\n                    )\n                },\n            )\n    self._conn.autocommit(True)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.set_query_cache", "title": "<code>set_query_cache(query_cache=None)</code>", "text": "<p>When query_cache is not None, the connection switches into the query caching mode, which entails: 1. Only SELECT queries are allowed. 2. The results of queries are cached under the path indicated by dj.config['query_cache'] 3. query_cache is a string that differentiates different cache states.</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <p>a string to initialize the hash for query results</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def set_query_cache(self, query_cache=None):\n\"\"\"\n    When query_cache is not None, the connection switches into the query caching mode, which entails:\n    1. Only SELECT queries are allowed.\n    2. The results of queries are cached under the path indicated by dj.config['query_cache']\n    3. query_cache is a string that differentiates different cache states.\n\n    :param query_cache: a string to initialize the hash for query results\n    \"\"\"\n    self._query_cache = query_cache\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.purge_query_cache", "title": "<code>purge_query_cache()</code>", "text": "<p>Purges all query cache.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def purge_query_cache(self):\n\"\"\"Purges all query cache.\"\"\"\n    if (\n        isinstance(config.get(cache_key), str)\n        and pathlib.Path(config[cache_key]).is_dir()\n    ):\n        for path in pathlib.Path(config[cache_key]).iterdir():\n            if not path.is_dir():\n                path.unlink()\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.ping", "title": "<code>ping()</code>", "text": "<p>Ping the connection or raises an exception if the connection is closed.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def ping(self):\n\"\"\"Ping the connection or raises an exception if the connection is closed.\"\"\"\n    self._conn.ping(reconnect=False)\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.is_connected", "title": "<code>is_connected</code>  <code>property</code>", "text": "<p>Return true if the object is connected to the database server.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.query", "title": "<code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)</code>", "text": "<p>Execute the specified query and return the tuple generator (cursor).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>SQL query</p> required <code>args</code> <p>additional arguments for the client.cursor</p> <code>()</code> <code>as_dict</code> <p>If as_dict is set to True, the returned cursor objects returns query results as dictionary.</p> <code>False</code> <code>suppress_warnings</code> <p>If True, suppress all warnings arising from underlying query library</p> <code>True</code> <code>reconnect</code> <p>when None, get from config, when True, attempt to reconnect if disconnected</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def query(\n    self, query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None\n):\n\"\"\"\n    Execute the specified query and return the tuple generator (cursor).\n\n    :param query: SQL query\n    :param args: additional arguments for the client.cursor\n    :param as_dict: If as_dict is set to True, the returned cursor objects returns\n                    query results as dictionary.\n    :param suppress_warnings: If True, suppress all warnings arising from underlying query library\n    :param reconnect: when None, get from config, when True, attempt to reconnect if disconnected\n    \"\"\"\n    # check cache first:\n    use_query_cache = bool(self._query_cache)\n    if use_query_cache and not re.match(r\"\\s*(SELECT|SHOW)\", query):\n        raise errors.DataJointError(\n            \"Only SELECT queries are allowed when query caching is on.\"\n        )\n    if use_query_cache:\n        if not config[cache_key]:\n            raise errors.DataJointError(\n                f\"Provide filepath dj.config['{cache_key}'] when using query caching.\"\n            )\n        hash_ = uuid_from_buffer(\n            (str(self._query_cache) + re.sub(r\"`\\$\\w+`\", \"\", query)).encode()\n            + pack(args)\n        )\n        cache_path = pathlib.Path(config[cache_key]) / str(hash_)\n        try:\n            buffer = cache_path.read_bytes()\n        except FileNotFoundError:\n            pass  # proceed to query the database\n        else:\n            return EmulatedCursor(unpack(buffer))\n\n    if reconnect is None:\n        reconnect = config[\"database.reconnect\"]\n    logger.debug(\"Executing SQL:\" + query[:query_log_max_length])\n    cursor_class = client.cursors.DictCursor if as_dict else client.cursors.Cursor\n    cursor = self._conn.cursor(cursor=cursor_class)\n    try:\n        self._execute_query(cursor, query, args, suppress_warnings)\n    except errors.LostConnectionError:\n        if not reconnect:\n            raise\n        logger.warning(\"MySQL server has gone away. Reconnecting to the server.\")\n        connect_host_hook(self)\n        if self._in_transaction:\n            self.cancel_transaction()\n            raise errors.LostConnectionError(\n                \"Connection was lost during a transaction.\"\n            )\n        logger.debug(\"Re-executing\")\n        cursor = self._conn.cursor(cursor=cursor_class)\n        self._execute_query(cursor, query, args, suppress_warnings)\n\n    if use_query_cache:\n        data = cursor.fetchall()\n        cache_path.write_bytes(pack(data))\n        return EmulatedCursor(data)\n\n    return cursor\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.get_user", "title": "<code>get_user()</code>", "text": "<p>Returns:</p> Type Description <p>the user name and host name provided by the client to the server.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def get_user(self):\n\"\"\"\n    :return: the user name and host name provided by the client to the server.\n    \"\"\"\n    return self.query(\"SELECT user()\").fetchone()[0]\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.in_transaction", "title": "<code>in_transaction</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True if there is an open transaction.</p>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.start_transaction", "title": "<code>start_transaction()</code>", "text": "<p>Starts a transaction error.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def start_transaction(self):\n\"\"\"\n    Starts a transaction error.\n    \"\"\"\n    if self.in_transaction:\n        raise errors.DataJointError(\"Nested connections are not supported.\")\n    self.query(\"START TRANSACTION WITH CONSISTENT SNAPSHOT\")\n    self._in_transaction = True\n    logger.debug(\"Transaction started\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.cancel_transaction", "title": "<code>cancel_transaction()</code>", "text": "<p>Cancels the current transaction and rolls back all changes made during the transaction.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def cancel_transaction(self):\n\"\"\"\n    Cancels the current transaction and rolls back all changes made during the transaction.\n    \"\"\"\n    self.query(\"ROLLBACK\")\n    self._in_transaction = False\n    logger.debug(\"Transaction cancelled. Rolling back ...\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.commit_transaction", "title": "<code>commit_transaction()</code>", "text": "<p>Commit all changes made during the transaction and close it.</p> Source code in <code>datajoint/connection.py</code> <pre><code>def commit_transaction(self):\n\"\"\"\n    Commit all changes made during the transaction and close it.\n\n    \"\"\"\n    self.query(\"COMMIT\")\n    self._in_transaction = False\n    logger.debug(\"Transaction committed and closed.\")\n</code></pre>"}, {"location": "api/datajoint/connection/#datajoint.connection.Connection.transaction", "title": "<code>transaction</code>  <code>property</code>", "text": "<p>Context manager for transactions. Opens an transaction and closes it after the with statement. If an error is caught during the transaction, the commits are automatically rolled back. All errors are raised again.</p> <p>Example:</p> <p>import datajoint as dj with dj.conn().transaction as conn:     # transaction is open here</p>"}, {"location": "api/datajoint/declare/", "title": "declare.py", "text": "<p>This module hosts functions to convert DataJoint table definitions into mysql table definitions, and to declare the corresponding mysql tables.</p>"}, {"location": "api/datajoint/declare/#datajoint.declare.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.get_adapter", "title": "<code>get_adapter(context, adapter_name)</code>", "text": "<p>Extract the AttributeAdapter object by its name from the context and validate.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get_adapter(context, adapter_name):\n\"\"\"\n    Extract the AttributeAdapter object by its name from the context and validate.\n    \"\"\"\n    if not _support_adapted_types():\n        raise DataJointError(\"Support for Adapted Attribute types is disabled.\")\n    adapter_name = adapter_name.lstrip(\"&lt;\").rstrip(\"&gt;\")\n    try:\n        adapter = (\n            context[adapter_name]\n            if adapter_name in context\n            else type_plugins[adapter_name][\"object\"].load()\n        )\n    except KeyError:\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' is not defined.\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter, AttributeAdapter):\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' must be an instance of datajoint.AttributeAdapter\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter.attribute_type, str) or not re.match(\n        r\"^\\w\", adapter.attribute_type\n    ):\n        raise DataJointError(\n            \"Invalid attribute type {type} in attribute adapter '{adapter_name}'\".format(\n                type=adapter.attribute_type, adapter_name=adapter_name\n            )\n        )\n    return adapter\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.is_foreign_key", "title": "<code>is_foreign_key(line)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>line</code> <p>a line from the table definition</p> required <p>Returns:</p> Type Description <p>true if the line appears to be a foreign key definition</p> Source code in <code>datajoint/declare.py</code> <pre><code>def is_foreign_key(line):\n\"\"\"\n\n    :param line: a line from the table definition\n    :return: true if the line appears to be a foreign key definition\n    \"\"\"\n    arrow_position = line.find(\"-&gt;\")\n    return arrow_position &gt;= 0 and not any(c in line[:arrow_position] for c in \"\\\"#'\")\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_foreign_key", "title": "<code>compile_foreign_key(line, context, attributes, primary_key, attr_sql, foreign_key_sql, index_sql)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>line</code> <p>a line from a table definition</p> required <code>context</code> <p>namespace containing referenced objects</p> required <code>attributes</code> <p>list of attribute names already in the declaration -- to be updated by this function</p> required <code>primary_key</code> <p>None if the current foreign key is made from the dependent section. Otherwise it is the list of primary key attributes thus far -- to be updated by the function</p> required <code>attr_sql</code> <p>list of sql statements defining attributes -- to be updated by this function.</p> required <code>foreign_key_sql</code> <p>list of sql statements specifying foreign key constraints -- to be updated by this function.</p> required <code>index_sql</code> <p>list of INDEX declaration statements, duplicate or redundant indexes are ok.</p> required Source code in <code>datajoint/declare.py</code> <pre><code>def compile_foreign_key(\n    line, context, attributes, primary_key, attr_sql, foreign_key_sql, index_sql\n):\n\"\"\"\n    :param line: a line from a table definition\n    :param context: namespace containing referenced objects\n    :param attributes: list of attribute names already in the declaration -- to be updated by this function\n    :param primary_key: None if the current foreign key is made from the dependent section. Otherwise it is the list\n        of primary key attributes thus far -- to be updated by the function\n    :param attr_sql: list of sql statements defining attributes -- to be updated by this function.\n    :param foreign_key_sql: list of sql statements specifying foreign key constraints -- to be updated by this function.\n    :param index_sql: list of INDEX declaration statements, duplicate or redundant indexes are ok.\n    \"\"\"\n    # Parse and validate\n    from .table import Table\n    from .expression import QueryExpression\n\n    try:\n        result = foreign_key_parser.parseString(line)\n    except pp.ParseException as err:\n        raise DataJointError('Parsing error in line \"%s\". %s.' % (line, err))\n\n    try:\n        ref = eval(result.ref_table, context)\n    except Exception:\n        raise DataJointError(\n            \"Foreign key reference %s could not be resolved\" % result.ref_table\n        )\n\n    options = [opt.upper() for opt in result.options]\n    for opt in options:  # check for invalid options\n        if opt not in {\"NULLABLE\", \"UNIQUE\"}:\n            raise DataJointError('Invalid foreign key option \"{opt}\"'.format(opt=opt))\n    is_nullable = \"NULLABLE\" in options\n    is_unique = \"UNIQUE\" in options\n    if is_nullable and primary_key is not None:\n        raise DataJointError(\n            'Primary dependencies cannot be nullable in line \"{line}\"'.format(line=line)\n        )\n\n    if isinstance(ref, type) and issubclass(ref, Table):\n        ref = ref()\n\n    # check that dependency is of a supported type\n    if (\n        not isinstance(ref, QueryExpression)\n        or len(ref.restriction)\n        or len(ref.support) != 1\n        or not isinstance(ref.support[0], str)\n    ):\n        raise DataJointError(\n            'Dependency \"%s\" is not supported (yet). Use a base table or its projection.'\n            % result.ref_table\n        )\n\n    # declare new foreign key attributes\n    for attr in ref.primary_key:\n        if attr not in attributes:\n            attributes.append(attr)\n            if primary_key is not None:\n                primary_key.append(attr)\n            attr_sql.append(\n                ref.heading[attr].sql.replace(\"NOT NULL \", \"\", int(is_nullable))\n            )\n\n    # declare the foreign key\n    foreign_key_sql.append(\n        \"FOREIGN KEY (`{fk}`) REFERENCES {ref} (`{pk}`) ON UPDATE CASCADE ON DELETE RESTRICT\".format(\n            fk=\"`,`\".join(ref.primary_key),\n            pk=\"`,`\".join(ref.heading[name].original_name for name in ref.primary_key),\n            ref=ref.support[0],\n        )\n    )\n\n    # declare unique index\n    if is_unique:\n        index_sql.append(\n            \"UNIQUE INDEX ({attrs})\".format(\n                attrs=\",\".join(\"`%s`\" % attr for attr in ref.primary_key)\n            )\n        )\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.declare", "title": "<code>declare(full_table_name, definition, context)</code>", "text": "<p>Parse declaration and generate the SQL CREATE TABLE code</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>full name of the table</p> required <code>definition</code> <p>DataJoint table definition</p> required <code>context</code> <p>dictionary of objects that might be referred to in the table</p> required <p>Returns:</p> Type Description <p>SQL CREATE TABLE statement, list of external stores used</p> Source code in <code>datajoint/declare.py</code> <pre><code>def declare(full_table_name, definition, context):\n\"\"\"\n    Parse declaration and generate the SQL CREATE TABLE code\n\n    :param full_table_name: full name of the table\n    :param definition: DataJoint table definition\n    :param context: dictionary of objects that might be referred to in the table\n    :return: SQL CREATE TABLE statement, list of external stores used\n    \"\"\"\n    table_name = full_table_name.strip(\"`\").split(\".\")[1]\n    if len(table_name) &gt; MAX_TABLE_NAME_LENGTH:\n        raise DataJointError(\n            \"Table name `{name}` exceeds the max length of {max_length}\".format(\n                name=table_name, max_length=MAX_TABLE_NAME_LENGTH\n            )\n        )\n\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n\n    if not primary_key:\n        raise DataJointError(\"Table must have a primary key\")\n\n    return (\n        \"CREATE TABLE IF NOT EXISTS %s (\\n\" % full_table_name\n        + \",\\n\".join(\n            attribute_sql\n            + [\"PRIMARY KEY (`\" + \"`,`\".join(primary_key) + \"`)\"]\n            + foreign_key_sql\n            + index_sql\n        )\n        + '\\n) ENGINE=InnoDB, COMMENT \"%s\"' % table_comment\n    ), external_stores\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.alter", "title": "<code>alter(definition, old_definition, context)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>definition</code> <p>new table definition</p> required <code>old_definition</code> <p>current table definition</p> required <code>context</code> <p>the context in which to evaluate foreign key definitions</p> required <p>Returns:</p> Type Description <p>string SQL ALTER command, list of new stores used for external storage</p> Source code in <code>datajoint/declare.py</code> <pre><code>def alter(definition, old_definition, context):\n\"\"\"\n    :param definition: new table definition\n    :param old_definition: current table definition\n    :param context: the context in which to evaluate foreign key definitions\n    :return: string SQL ALTER command, list of new stores used for external storage\n    \"\"\"\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n    (\n        table_comment_,\n        primary_key_,\n        attribute_sql_,\n        foreign_key_sql_,\n        index_sql_,\n        external_stores_,\n    ) = prepare_declare(old_definition, context)\n\n    # analyze differences between declarations\n    sql = list()\n    if primary_key != primary_key_:\n        raise NotImplementedError(\"table.alter cannot alter the primary key (yet).\")\n    if foreign_key_sql != foreign_key_sql_:\n        raise NotImplementedError(\"table.alter cannot alter foreign keys (yet).\")\n    if index_sql != index_sql_:\n        raise NotImplementedError(\"table.alter cannot alter indexes (yet)\")\n    if attribute_sql != attribute_sql_:\n        sql.extend(_make_attribute_alter(attribute_sql, attribute_sql_, primary_key))\n    if table_comment != table_comment_:\n        sql.append('COMMENT=\"%s\"' % table_comment)\n    return sql, [e for e in external_stores if e not in external_stores_]\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.substitute_special_type", "title": "<code>substitute_special_type(match, category, foreign_key_sql, context)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>match</code> <p>dict containing with keys \"type\" and \"comment\" -- will be modified in place</p> required <code>category</code> <p>attribute type category from TYPE_PATTERN</p> required <code>foreign_key_sql</code> <p>list of foreign key declarations to add to</p> required <code>context</code> <p>context for looking up user-defined attribute_type adapters</p> required Source code in <code>datajoint/declare.py</code> <pre><code>def substitute_special_type(match, category, foreign_key_sql, context):\n\"\"\"\n    :param match: dict containing with keys \"type\" and \"comment\" -- will be modified in place\n    :param category: attribute type category from TYPE_PATTERN\n    :param foreign_key_sql: list of foreign key declarations to add to\n    :param context: context for looking up user-defined attribute_type adapters\n    \"\"\"\n    if category == \"UUID\":\n        match[\"type\"] = UUID_DATA_TYPE\n    elif category == \"INTERNAL_ATTACH\":\n        match[\"type\"] = \"LONGBLOB\"\n    elif category in EXTERNAL_TYPES:\n        if category == \"FILEPATH\" and not _support_filepath_types():\n            raise DataJointError(\n\"\"\"\n            The filepath data type is disabled until complete validation.\n            To turn it on as experimental feature, set the environment variable\n            {env} = TRUE or upgrade datajoint.\n            \"\"\".format(\n                    env=FILEPATH_FEATURE_SWITCH\n                )\n            )\n        match[\"store\"] = match[\"type\"].split(\"@\", 1)[1]\n        match[\"type\"] = UUID_DATA_TYPE\n        foreign_key_sql.append(\n            \"FOREIGN KEY (`{name}`) REFERENCES `{{database}}`.`{external_table_root}_{store}` (`hash`) \"\n            \"ON UPDATE RESTRICT ON DELETE RESTRICT\".format(\n                external_table_root=EXTERNAL_TABLE_ROOT, **match\n            )\n        )\n    elif category == \"ADAPTED\":\n        adapter = get_adapter(context, match[\"type\"])\n        match[\"type\"] = adapter.attribute_type\n        category = match_type(match[\"type\"])\n        if category in SPECIAL_TYPES:\n            # recursive redefinition from user-defined datatypes.\n            substitute_special_type(match, category, foreign_key_sql, context)\n    else:\n        assert False, \"Unknown special type\"\n</code></pre>"}, {"location": "api/datajoint/declare/#datajoint.declare.compile_attribute", "title": "<code>compile_attribute(line, in_key, foreign_key_sql, context)</code>", "text": "<p>Convert attribute definition from DataJoint format to SQL</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <p>attribution line</p> required <code>in_key</code> <p>set to True if attribute is in primary key set</p> required <code>foreign_key_sql</code> <p>the list of foreign key declarations to add to</p> required <code>context</code> <p>context in which to look up user-defined attribute type adapterss</p> required <p>Returns:</p> Type Description <p>(name, sql, is_external) -- attribute name and sql code for its declaration</p> Source code in <code>datajoint/declare.py</code> <pre><code>def compile_attribute(line, in_key, foreign_key_sql, context):\n\"\"\"\n    Convert attribute definition from DataJoint format to SQL\n\n    :param line: attribution line\n    :param in_key: set to True if attribute is in primary key set\n    :param foreign_key_sql: the list of foreign key declarations to add to\n    :param context: context in which to look up user-defined attribute type adapterss\n    :returns: (name, sql, is_external) -- attribute name and sql code for its declaration\n    \"\"\"\n    try:\n        match = attribute_parser.parseString(line + \"#\", parseAll=True)\n    except pp.ParseException as err:\n        raise DataJointError(\n            \"Declaration error in position {pos} in line:\\n  {line}\\n{msg}\".format(\n                line=err.args[0], pos=err.args[1], msg=err.args[2]\n            )\n        )\n    match[\"comment\"] = match[\"comment\"].rstrip(\"#\")\n    if \"default\" not in match:\n        match[\"default\"] = \"\"\n    match = {k: v.strip() for k, v in match.items()}\n    match[\"nullable\"] = match[\"default\"].lower() == \"null\"\n\n    if match[\"nullable\"]:\n        if in_key:\n            raise DataJointError(\n                'Primary key attributes cannot be nullable in line \"%s\"' % line\n            )\n        match[\"default\"] = \"DEFAULT NULL\"  # nullable attributes default to null\n    else:\n        if match[\"default\"]:\n            quote = (\n                match[\"default\"].split(\"(\")[0].upper() not in CONSTANT_LITERALS\n                and match[\"default\"][0] not in \"\\\"'\"\n            )\n            match[\"default\"] = (\n                \"NOT NULL DEFAULT \" + ('\"%s\"' if quote else \"%s\") % match[\"default\"]\n            )\n        else:\n            match[\"default\"] = \"NOT NULL\"\n\n    match[\"comment\"] = match[\"comment\"].replace(\n        '\"', '\\\\\"'\n    )  # escape double quotes in comment\n\n    if match[\"comment\"].startswith(\":\"):\n        raise DataJointError(\n            'An attribute comment must not start with a colon in comment \"{comment}\"'.format(\n                **match\n            )\n        )\n\n    category = match_type(match[\"type\"])\n    if category in SPECIAL_TYPES:\n        match[\"comment\"] = \":{type}:{comment}\".format(\n            **match\n        )  # insert custom type into comment\n        substitute_special_type(match, category, foreign_key_sql, context)\n\n    if category in SERIALIZED_TYPES and match[\"default\"] not in {\n        \"DEFAULT NULL\",\n        \"NOT NULL\",\n    }:\n        raise DataJointError(\n            \"The default value for a blob or attachment attributes can only be NULL in:\\n{line}\".format(\n                line=line\n            )\n        )\n\n    sql = (\n        \"`{name}` {type} {default}\"\n        + (' COMMENT \"{comment}\"' if match[\"comment\"] else \"\")\n    ).format(**match)\n    return match[\"name\"], sql, match.get(\"store\")\n</code></pre>"}, {"location": "api/datajoint/dependencies/", "title": "dependencies.py", "text": ""}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.unite_master_parts", "title": "<code>unite_master_parts(lst)</code>", "text": "<p>re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts(     ['<code>s</code>.<code>a</code>', '<code>s</code>.<code>a__q</code>', '<code>s</code>.<code>b</code>', '<code>s</code>.<code>c</code>', '<code>s</code>.<code>c__q</code>', '<code>s</code>.<code>b__q</code>', '<code>s</code>.<code>d</code>', '<code>s</code>.<code>a__r</code>']) -&gt;     ['<code>s</code>.<code>a</code>', '<code>s</code>.<code>a__q</code>', '<code>s</code>.<code>a__r</code>', '<code>s</code>.<code>b</code>', '<code>s</code>.<code>b__q</code>', '<code>s</code>.<code>c</code>', '<code>s</code>.<code>c__q</code>', '<code>s</code>.<code>d</code>']</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def unite_master_parts(lst):\n\"\"\"\n    re-order a list of table names so that part tables immediately follow their master tables without breaking\n    the topological order.\n    Without this correction, a simple topological sort may insert other descendants between master and parts.\n    The input list must be topologically sorted.\n    :example:\n    unite_master_parts(\n        ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -&gt;\n        ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`']\n    \"\"\"\n    for i in range(2, len(lst)):\n        name = lst[i]\n        match = re.match(r\"(?P&lt;master&gt;`\\w+`.`#?\\w+)__\\w+`\", name)\n        if match:  # name is a part table\n            master = match.group(\"master\")\n            for j in range(i - 1, -1, -1):\n                if lst[j] == master + \"`\" or lst[j].startswith(master + \"__\"):\n                    # move from the ith position to the (j+1)th position\n                    lst[j + 1 : i + 1] = [name] + lst[j + 1 : i]\n                    break\n    return lst\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies", "title": "<code>Dependencies</code>", "text": "<p>         Bases: <code>nx.DiGraph</code></p> <p>The graph of dependencies (foreign keys) between loaded tables.</p> <p>Note: the 'connection' argument should normally be supplied; Empty use is permitted to facilitate use of networkx algorithms which internally create objects with the expectation of empty constructors. See also: https://github.com/datajoint/datajoint-python/pull/443</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>class Dependencies(nx.DiGraph):\n\"\"\"\n    The graph of dependencies (foreign keys) between loaded tables.\n\n    Note: the 'connection' argument should normally be supplied;\n    Empty use is permitted to facilitate use of networkx algorithms which\n    internally create objects with the expectation of empty constructors.\n    See also: https://github.com/datajoint/datajoint-python/pull/443\n    \"\"\"\n\n    def __init__(self, connection=None):\n        self._conn = connection\n        self._node_alias_count = itertools.count()\n        self._loaded = False\n        super().__init__(self)\n\n    def clear(self):\n        self._loaded = False\n        super().clear()\n\n    def load(self, force=True):\n\"\"\"\n        Load dependencies for all loaded schemas.\n        This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n        \"\"\"\n        # reload from scratch to prevent duplication of renamed edges\n        if self._loaded and not force:\n            return\n\n        self.clear()\n\n        # load primary key info\n        keys = self._conn.query(\n\"\"\"\n                SELECT\n                    concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n                FROM information_schema.key_column_usage\n                WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n                \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            )\n        )\n        pks = defaultdict(set)\n        for key in keys:\n            pks[key[0]].add(key[1])\n\n        # add nodes to the graph\n        for n, pk in pks.items():\n            self.add_node(n, primary_key=pk)\n\n        # load foreign keys\n        keys = (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in self._conn.query(\n\"\"\"\n        SELECT constraint_name,\n            concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n            concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n            column_name, referenced_column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n            referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n        \"\"\".format(\n                    schemas=\"','\".join(self._conn.schemas)\n                ),\n                as_dict=True,\n            )\n        )\n        fks = defaultdict(lambda: dict(attr_map=dict()))\n        for key in keys:\n            d = fks[\n                (\n                    key[\"constraint_name\"],\n                    key[\"referencing_table\"],\n                    key[\"referenced_table\"],\n                )\n            ]\n            d[\"referencing_table\"] = key[\"referencing_table\"]\n            d[\"referenced_table\"] = key[\"referenced_table\"]\n            d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n        # add edges to the graph\n        for fk in fks.values():\n            props = dict(\n                primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n                attr_map=fk[\"attr_map\"],\n                aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n                multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n            )\n            if not props[\"aliased\"]:\n                self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n            else:\n                # for aliased dependencies, add an extra node in the format '1', '2', etc\n                alias_node = \"%d\" % next(self._node_alias_count)\n                self.add_node(alias_node)\n                self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n                self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n        if not nx.is_directed_acyclic_graph(self):\n            raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n        self._loaded = True\n\n    def parents(self, table_name, primary=None):\n\"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referenced by the foreign keys of table\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[0]: p[2]\n            for p in self.in_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def children(self, table_name, primary=None):\n\"\"\"\n        :param table_name: `schema`.`table`\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n            attribute are considered.\n        :return: dict of tables referencing the table through foreign keys\n        \"\"\"\n        self.load(force=False)\n        return {\n            p[1]: p[2]\n            for p in self.out_edges(table_name, data=True)\n            if primary is None or p[2][\"primary\"] == primary\n        }\n\n    def descendants(self, full_table_name):\n\"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.algorithms.dag.descendants(self, full_table_name))\n        return unite_master_parts(\n            [full_table_name] + list(nx.algorithms.dag.topological_sort(nodes))\n        )\n\n    def ancestors(self, full_table_name):\n\"\"\"\n        :param full_table_name:  In form `schema`.`table_name`\n        :return: all dependent tables sorted in topological order.  Self is included.\n        \"\"\"\n        self.load(force=False)\n        nodes = self.subgraph(nx.algorithms.dag.ancestors(self, full_table_name))\n        return list(\n            reversed(\n                unite_master_parts(\n                    list(nx.algorithms.dag.topological_sort(nodes)) + [full_table_name]\n                )\n            )\n        )\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.load", "title": "<code>load(force=True)</code>", "text": "<p>Load dependencies for all loaded schemas. This method gets called before any operation that requires dependencies: delete, drop, populate, progress.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def load(self, force=True):\n\"\"\"\n    Load dependencies for all loaded schemas.\n    This method gets called before any operation that requires dependencies: delete, drop, populate, progress.\n    \"\"\"\n    # reload from scratch to prevent duplication of renamed edges\n    if self._loaded and not force:\n        return\n\n    self.clear()\n\n    # load primary key info\n    keys = self._conn.query(\n\"\"\"\n            SELECT\n                concat('`', table_schema, '`.`', table_name, '`') as tab, column_name\n            FROM information_schema.key_column_usage\n            WHERE table_name not LIKE \"~%%\" AND table_schema in ('{schemas}') AND constraint_name=\"PRIMARY\"\n            \"\"\".format(\n            schemas=\"','\".join(self._conn.schemas)\n        )\n    )\n    pks = defaultdict(set)\n    for key in keys:\n        pks[key[0]].add(key[1])\n\n    # add nodes to the graph\n    for n, pk in pks.items():\n        self.add_node(n, primary_key=pk)\n\n    # load foreign keys\n    keys = (\n        {k.lower(): v for k, v in elem.items()}\n        for elem in self._conn.query(\n\"\"\"\n    SELECT constraint_name,\n        concat('`', table_schema, '`.`', table_name, '`') as referencing_table,\n        concat('`', referenced_table_schema, '`.`',  referenced_table_name, '`') as referenced_table,\n        column_name, referenced_column_name\n    FROM information_schema.key_column_usage\n    WHERE referenced_table_name NOT LIKE \"~%%\" AND (referenced_table_schema in ('{schemas}') OR\n        referenced_table_schema is not NULL AND table_schema in ('{schemas}'))\n    \"\"\".format(\n                schemas=\"','\".join(self._conn.schemas)\n            ),\n            as_dict=True,\n        )\n    )\n    fks = defaultdict(lambda: dict(attr_map=dict()))\n    for key in keys:\n        d = fks[\n            (\n                key[\"constraint_name\"],\n                key[\"referencing_table\"],\n                key[\"referenced_table\"],\n            )\n        ]\n        d[\"referencing_table\"] = key[\"referencing_table\"]\n        d[\"referenced_table\"] = key[\"referenced_table\"]\n        d[\"attr_map\"][key[\"column_name\"]] = key[\"referenced_column_name\"]\n\n    # add edges to the graph\n    for fk in fks.values():\n        props = dict(\n            primary=set(fk[\"attr_map\"]) &lt;= set(pks[fk[\"referencing_table\"]]),\n            attr_map=fk[\"attr_map\"],\n            aliased=any(k != v for k, v in fk[\"attr_map\"].items()),\n            multi=set(fk[\"attr_map\"]) != set(pks[fk[\"referencing_table\"]]),\n        )\n        if not props[\"aliased\"]:\n            self.add_edge(fk[\"referenced_table\"], fk[\"referencing_table\"], **props)\n        else:\n            # for aliased dependencies, add an extra node in the format '1', '2', etc\n            alias_node = \"%d\" % next(self._node_alias_count)\n            self.add_node(alias_node)\n            self.add_edge(fk[\"referenced_table\"], alias_node, **props)\n            self.add_edge(alias_node, fk[\"referencing_table\"], **props)\n\n    if not nx.is_directed_acyclic_graph(self):\n        raise DataJointError(\"DataJoint can only work with acyclic dependencies\")\n    self._loaded = True\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.parents", "title": "<code>parents(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referenced by the foreign keys of table</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def parents(self, table_name, primary=None):\n\"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referenced by the foreign keys of table\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[0]: p[2]\n        for p in self.in_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.children", "title": "<code>children(table_name, primary=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>schema</code>.<code>table</code></p> required <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, the only foreign keys including at least one non-primary attribute are considered.</p> <code>None</code> <p>Returns:</p> Type Description <p>dict of tables referencing the table through foreign keys</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def children(self, table_name, primary=None):\n\"\"\"\n    :param table_name: `schema`.`table`\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, the only foreign keys including at least one non-primary\n        attribute are considered.\n    :return: dict of tables referencing the table through foreign keys\n    \"\"\"\n    self.load(force=False)\n    return {\n        p[1]: p[2]\n        for p in self.out_edges(table_name, data=True)\n        if primary is None or p[2][\"primary\"] == primary\n    }\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.descendants", "title": "<code>descendants(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def descendants(self, full_table_name):\n\"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.algorithms.dag.descendants(self, full_table_name))\n    return unite_master_parts(\n        [full_table_name] + list(nx.algorithms.dag.topological_sort(nodes))\n    )\n</code></pre>"}, {"location": "api/datajoint/dependencies/#datajoint.dependencies.Dependencies.ancestors", "title": "<code>ancestors(full_table_name)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>In form <code>schema</code>.<code>table_name</code></p> required <p>Returns:</p> Type Description <p>all dependent tables sorted in topological order.  Self is included.</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def ancestors(self, full_table_name):\n\"\"\"\n    :param full_table_name:  In form `schema`.`table_name`\n    :return: all dependent tables sorted in topological order.  Self is included.\n    \"\"\"\n    self.load(force=False)\n    nodes = self.subgraph(nx.algorithms.dag.ancestors(self, full_table_name))\n    return list(\n        reversed(\n            unite_master_parts(\n                list(nx.algorithms.dag.topological_sort(nodes)) + [full_table_name]\n            )\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/diagram/", "title": "diagram.py", "text": ""}, {"location": "api/datajoint/diagram/#datajoint.diagram.unite_master_parts", "title": "<code>unite_master_parts(lst)</code>", "text": "<p>re-order a list of table names so that part tables immediately follow their master tables without breaking the topological order. Without this correction, a simple topological sort may insert other descendants between master and parts. The input list must be topologically sorted. :example: unite_master_parts(     ['<code>s</code>.<code>a</code>', '<code>s</code>.<code>a__q</code>', '<code>s</code>.<code>b</code>', '<code>s</code>.<code>c</code>', '<code>s</code>.<code>c__q</code>', '<code>s</code>.<code>b__q</code>', '<code>s</code>.<code>d</code>', '<code>s</code>.<code>a__r</code>']) -&gt;     ['<code>s</code>.<code>a</code>', '<code>s</code>.<code>a__q</code>', '<code>s</code>.<code>a__r</code>', '<code>s</code>.<code>b</code>', '<code>s</code>.<code>b__q</code>', '<code>s</code>.<code>c</code>', '<code>s</code>.<code>c__q</code>', '<code>s</code>.<code>d</code>']</p> Source code in <code>datajoint/dependencies.py</code> <pre><code>def unite_master_parts(lst):\n\"\"\"\n    re-order a list of table names so that part tables immediately follow their master tables without breaking\n    the topological order.\n    Without this correction, a simple topological sort may insert other descendants between master and parts.\n    The input list must be topologically sorted.\n    :example:\n    unite_master_parts(\n        ['`s`.`a`', '`s`.`a__q`', '`s`.`b`', '`s`.`c`', '`s`.`c__q`', '`s`.`b__q`', '`s`.`d`', '`s`.`a__r`']) -&gt;\n        ['`s`.`a`', '`s`.`a__q`', '`s`.`a__r`', '`s`.`b`', '`s`.`b__q`', '`s`.`c`', '`s`.`c__q`', '`s`.`d`']\n    \"\"\"\n    for i in range(2, len(lst)):\n        name = lst[i]\n        match = re.match(r\"(?P&lt;master&gt;`\\w+`.`#?\\w+)__\\w+`\", name)\n        if match:  # name is a part table\n            master = match.group(\"master\")\n            for j in range(i - 1, -1, -1):\n                if lst[j] == master + \"`\" or lst[j].startswith(master + \"__\"):\n                    # move from the ith position to the (j+1)th position\n                    lst[j + 1 : i + 1] = [name] + lst[j + 1 : i]\n                    break\n    return lst\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/diagram/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram", "title": "<code>Diagram</code>", "text": "<p>         Bases: <code>nx.DiGraph</code></p> <p>Entity relationship diagram.</p> <p>Usage:</p> <p>diag = Diagram(source)</p> <p>source can be a base table object, a base table class, a schema, or a module that has a schema.</p> <p>diag.draw()</p> <p>draws the diagram using pyplot</p> <p>diag1 + diag2  - combines the two diagrams. diag + n   - expands n levels of successors diag - n   - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table</p> <p>Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed</p> Source code in <code>datajoint/diagram.py</code> <pre><code>class Diagram(nx.DiGraph):\n\"\"\"\n    Entity relationship diagram.\n\n    Usage:\n\n    &gt;&gt;&gt;  diag = Diagram(source)\n\n    source can be a base table object, a base table class, a schema, or a module that has a schema.\n\n    &gt;&gt;&gt; diag.draw()\n\n    draws the diagram using pyplot\n\n    diag1 + diag2  - combines the two diagrams.\n    diag + n   - expands n levels of successors\n    diag - n   - expands n levels of predecessors\n    Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table\n\n    Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth.\n    Only those tables that are loaded in the connection object are displayed\n    \"\"\"\n\n    def __init__(self, source, context=None):\n        if isinstance(source, Diagram):\n            # copy constructor\n            self.nodes_to_show = set(source.nodes_to_show)\n            self.context = source.context\n            super().__init__(source)\n            return\n\n        # get the caller's context\n        if context is None:\n            frame = inspect.currentframe().f_back\n            self.context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        else:\n            self.context = context\n\n        # find connection in the source\n        try:\n            connection = source.connection\n        except AttributeError:\n            try:\n                connection = source.schema.connection\n            except AttributeError:\n                raise DataJointError(\n                    \"Could not find database connection in %s\" % repr(source[0])\n                )\n\n        # initialize graph from dependencies\n        connection.dependencies.load()\n        super().__init__(connection.dependencies)\n\n        # Enumerate nodes from all the items in the list\n        self.nodes_to_show = set()\n        try:\n            self.nodes_to_show.add(source.full_table_name)\n        except AttributeError:\n            try:\n                database = source.database\n            except AttributeError:\n                try:\n                    database = source.schema.database\n                except AttributeError:\n                    raise DataJointError(\n                        \"Cannot plot Diagram for %s\" % repr(source)\n                    )\n            for node in self:\n                if node.startswith(\"`%s`\" % database):\n                    self.nodes_to_show.add(node)\n\n    @classmethod\n    def from_sequence(cls, sequence):\n\"\"\"\n        The join Diagram for all objects in sequence\n\n        :param sequence: a sequence (e.g. list, tuple)\n        :return: Diagram(arg1) + ... + Diagram(argn)\n        \"\"\"\n        return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n\n    def add_parts(self):\n\"\"\"\n        Adds to the diagram the part tables of tables already included in the diagram\n        :return:\n        \"\"\"\n\n        def is_part(part, master):\n\"\"\"\n            :param part:  `database`.`table_name`\n            :param master:   `database`.`table_name`\n            :return: True if part is part of master.\n            \"\"\"\n            part = [s.strip(\"`\") for s in part.split(\".\")]\n            master = [s.strip(\"`\") for s in master.split(\".\")]\n            return (\n                master[0] == part[0]\n                and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n            )\n\n        self = Diagram(self)  # copy\n        self.nodes_to_show.update(\n            n\n            for n in self.nodes()\n            if any(is_part(n, m) for m in self.nodes_to_show)\n        )\n        return self\n\n    def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n        return unite_master_parts(\n            list(\n                nx.algorithms.dag.topological_sort(\n                    nx.DiGraph(self).subgraph(self.nodes_to_show)\n                )\n            )\n        )\n\n    def __add__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Union of the diagrams when arg is another Diagram\n                 or an expansion downstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.add(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    new = nx.algorithms.boundary.node_boundary(\n                        self, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            self, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __sub__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Difference of the diagrams when arg is another Diagram or\n                 an expansion upstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.difference_update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.remove(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    graph = nx.DiGraph(self).reverse()\n                    new = nx.algorithms.boundary.node_boundary(\n                        graph, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            graph, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __mul__(self, arg):\n\"\"\"\n        Intersection of two diagrams\n        :param arg: another Diagram\n        :return: a new Diagram comprising nodes that are present in both operands.\n        \"\"\"\n        self = Diagram(self)  # copy\n        self.nodes_to_show.intersection_update(arg.nodes_to_show)\n        return self\n\n    def _make_graph(self):\n\"\"\"\n        Make the self.graph - a graph object ready for drawing\n        \"\"\"\n        # mark \"distinguished\" tables, i.e. those that introduce new primary key\n        # attributes\n        for name in self.nodes_to_show:\n            foreign_attributes = set(\n                attr\n                for p in self.in_edges(name, data=True)\n                for attr in p[2][\"attr_map\"]\n                if p[2][\"primary\"]\n            )\n            self.nodes[name][\"distinguished\"] = (\n                \"primary_key\" in self.nodes[name]\n                and foreign_attributes &lt; self.nodes[name][\"primary_key\"]\n            )\n        # include aliased nodes that are sandwiched between two displayed nodes\n        gaps = set(\n            nx.algorithms.boundary.node_boundary(self, self.nodes_to_show)\n        ).intersection(\n            nx.algorithms.boundary.node_boundary(\n                nx.DiGraph(self).reverse(), self.nodes_to_show\n            )\n        )\n        nodes = self.nodes_to_show.union(a for a in gaps if a.isdigit)\n        # construct subgraph and rename nodes to class names\n        graph = nx.DiGraph(nx.DiGraph(self).subgraph(nodes))\n        nx.set_node_attributes(\n            graph, name=\"node_type\", values={n: _get_tier(n) for n in graph}\n        )\n        # relabel nodes to class names\n        mapping = {\n            node: lookup_class_name(node, self.context) or node\n            for node in graph.nodes()\n        }\n        new_names = [mapping.values()]\n        if len(new_names) &gt; len(set(new_names)):\n            raise DataJointError(\n                \"Some classes have identical names. The Diagram cannot be plotted.\"\n            )\n        nx.relabel_nodes(graph, mapping, copy=False)\n        return graph\n\n    def make_dot(self):\n        graph = self._make_graph()\n        graph.nodes()\n\n        scale = 1.2  # scaling factor for fonts and boxes\n        label_props = {  # http://matplotlib.org/examples/color/named_colors.html\n            None: dict(\n                shape=\"circle\",\n                color=\"#FFFF0040\",\n                fontcolor=\"yellow\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            _AliasNode: dict(\n                shape=\"circle\",\n                color=\"#FF880080\",\n                fontcolor=\"#FF880080\",\n                fontsize=round(scale * 0),\n                size=0.05 * scale,\n                fixed=True,\n            ),\n            Manual: dict(\n                shape=\"box\",\n                color=\"#00FF0030\",\n                fontcolor=\"darkgreen\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Lookup: dict(\n                shape=\"plaintext\",\n                color=\"#00000020\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Computed: dict(\n                shape=\"ellipse\",\n                color=\"#FF000020\",\n                fontcolor=\"#7F0000A0\",\n                fontsize=round(scale * 10),\n                size=0.3 * scale,\n                fixed=True,\n            ),\n            Imported: dict(\n                shape=\"ellipse\",\n                color=\"#00007F40\",\n                fontcolor=\"#00007FA0\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Part: dict(\n                shape=\"plaintext\",\n                color=\"#0000000\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.1 * scale,\n                fixed=False,\n            ),\n        }\n        node_props = {\n            node: label_props[d[\"node_type\"]]\n            for node, d in dict(graph.nodes(data=True)).items()\n        }\n\n        dot = nx.drawing.nx_pydot.to_pydot(graph)\n        for node in dot.get_nodes():\n            node.set_shape(\"circle\")\n            name = node.get_name().strip('\"')\n            props = node_props[name]\n            node.set_fontsize(props[\"fontsize\"])\n            node.set_fontcolor(props[\"fontcolor\"])\n            node.set_shape(props[\"shape\"])\n            node.set_fontname(\"arial\")\n            node.set_fixedsize(\"shape\" if props[\"fixed\"] else False)\n            node.set_width(props[\"size\"])\n            node.set_height(props[\"size\"])\n            if name.split(\".\")[0] in self.context:\n                cls = eval(name, self.context)\n                assert issubclass(cls, Table)\n                description = cls().describe(context=self.context).split(\"\\n\")\n                description = (\n                    \"-\" * 30\n                    if q.startswith(\"---\")\n                    else q.replace(\"-&gt;\", \"&amp;#8594;\")\n                    if \"-&gt;\" in q\n                    else q.split(\":\")[0]\n                    for q in description\n                    if not q.startswith(\"#\")\n                )\n                node.set_tooltip(\"&amp;#13;\".join(description))\n            node.set_label(\n                \"&lt;&lt;u&gt;\" + name + \"&lt;/u&gt;&gt;\"\n                if node.get(\"distinguished\") == \"True\"\n                else name\n            )\n            node.set_color(props[\"color\"])\n            node.set_style(\"filled\")\n\n        for edge in dot.get_edges():\n            # see https://graphviz.org/doc/info/attrs.html\n            src = edge.get_source().strip('\"')\n            dest = edge.get_destination().strip('\"')\n            props = graph.get_edge_data(src, dest)\n            edge.set_color(\"#00000040\")\n            edge.set_style(\"solid\" if props[\"primary\"] else \"dashed\")\n            master_part = graph.nodes[dest][\n                \"node_type\"\n            ] is Part and dest.startswith(src + \".\")\n            edge.set_weight(3 if master_part else 1)\n            edge.set_arrowhead(\"none\")\n            edge.set_penwidth(0.75 if props[\"multi\"] else 2)\n\n        return dot\n\n    def make_svg(self):\n        from IPython.display import SVG\n\n        return SVG(self.make_dot().create_svg())\n\n    def make_png(self):\n        return io.BytesIO(self.make_dot().create_png())\n\n    def make_image(self):\n        if plot_active:\n            return plt.imread(self.make_png())\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def _repr_svg_(self):\n        return self.make_svg()._repr_svg_()\n\n    def draw(self):\n        if plot_active:\n            plt.imshow(self.make_image())\n            plt.gca().axis(\"off\")\n            plt.show()\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def save(self, filename, format=None):\n        if format is None:\n            if filename.lower().endswith(\".png\"):\n                format = \"png\"\n            elif filename.lower().endswith(\".svg\"):\n                format = \"svg\"\n        if format.lower() == \"png\":\n            with open(filename, \"wb\") as f:\n                f.write(self.make_png().getbuffer().tobytes())\n        elif format.lower() == \"svg\":\n            with open(filename, \"w\") as f:\n                f.write(self.make_svg().data)\n        else:\n            raise DataJointError(\"Unsupported file format\")\n\n    @staticmethod\n    def _layout(graph, **kwargs):\n        return pydot_layout(graph, prog=\"dot\", **kwargs)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.from_sequence", "title": "<code>from_sequence(sequence)</code>  <code>classmethod</code>", "text": "<p>The join Diagram for all objects in sequence</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <p>a sequence (e.g. list, tuple)</p> required <p>Returns:</p> Type Description <p>Diagram(arg1) + ... + Diagram(argn)</p> Source code in <code>datajoint/diagram.py</code> <pre><code>@classmethod\ndef from_sequence(cls, sequence):\n\"\"\"\n    The join Diagram for all objects in sequence\n\n    :param sequence: a sequence (e.g. list, tuple)\n    :return: Diagram(arg1) + ... + Diagram(argn)\n    \"\"\"\n    return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.add_parts", "title": "<code>add_parts()</code>", "text": "<p>Adds to the diagram the part tables of tables already included in the diagram</p> <p>Returns:</p> Type Description Source code in <code>datajoint/diagram.py</code> <pre><code>def add_parts(self):\n\"\"\"\n    Adds to the diagram the part tables of tables already included in the diagram\n    :return:\n    \"\"\"\n\n    def is_part(part, master):\n\"\"\"\n        :param part:  `database`.`table_name`\n        :param master:   `database`.`table_name`\n        :return: True if part is part of master.\n        \"\"\"\n        part = [s.strip(\"`\") for s in part.split(\".\")]\n        master = [s.strip(\"`\") for s in master.split(\".\")]\n        return (\n            master[0] == part[0]\n            and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n        )\n\n    self = Diagram(self)  # copy\n    self.nodes_to_show.update(\n        n\n        for n in self.nodes()\n        if any(is_part(n, m) for m in self.nodes_to_show)\n    )\n    return self\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Diagram.topological_sort", "title": "<code>topological_sort()</code>", "text": "<p>Returns:</p> Type Description <p>list of nodes in topological order</p> Source code in <code>datajoint/diagram.py</code> <pre><code>def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n    return unite_master_parts(\n        list(\n            nx.algorithms.dag.topological_sort(\n                nx.DiGraph(self).subgraph(self.nodes_to_show)\n            )\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Manual", "title": "<code>Manual</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Lookup", "title": "<code>Lookup</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Imported", "title": "<code>Imported</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Computed", "title": "<code>Computed</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.Part", "title": "<code>Part</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.user_tables.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.user_tables.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/diagram/#datajoint.diagram.lookup_class_name", "title": "<code>lookup_class_name(name, context, depth=3)</code>", "text": "<p>given a table name in the form <code>schema_name</code>.<code>table_name</code>, find its class in the context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p><code>schema_name</code>.<code>table_name</code></p> required <code>context</code> <p>dictionary representing the namespace</p> required <code>depth</code> <p>search depth into imported modules, helps avoid infinite recursion.</p> <code>3</code> <p>Returns:</p> Type Description <p>class name found in the context or None if not found</p> Source code in <code>datajoint/table.py</code> <pre><code>def lookup_class_name(name, context, depth=3):\n\"\"\"\n    given a table name in the form `schema_name`.`table_name`, find its class in the context.\n\n    :param name: `schema_name`.`table_name`\n    :param context: dictionary representing the namespace\n    :param depth: search depth into imported modules, helps avoid infinite recursion.\n    :return: class name found in the context or None if not found\n    \"\"\"\n    # breadth-first search\n    nodes = [dict(context=context, context_name=\"\", depth=depth)]\n    while nodes:\n        node = nodes.pop(0)\n        for member_name, member in node[\"context\"].items():\n            if not member_name.startswith(\"_\"):  # skip IPython's implicit variables\n                if inspect.isclass(member) and issubclass(member, Table):\n                    if member.full_table_name == name:  # found it!\n                        return \".\".join([node[\"context_name\"], member_name]).lstrip(\".\")\n                    try:  # look for part tables\n                        parts = member.__dict__\n                    except AttributeError:\n                        pass  # not a UserTable -- cannot have part tables.\n                    else:\n                        for part in (\n                            getattr(member, p)\n                            for p in parts\n                            if p[0].isupper() and hasattr(member, p)\n                        ):\n                            if (\n                                inspect.isclass(part)\n                                and issubclass(part, Table)\n                                and part.full_table_name == name\n                            ):\n                                return \".\".join(\n                                    [node[\"context_name\"], member_name, part.__name__]\n                                ).lstrip(\".\")\n                elif (\n                    node[\"depth\"] &gt; 0\n                    and inspect.ismodule(member)\n                    and member.__name__ != \"datajoint\"\n                ):\n                    try:\n                        nodes.append(\n                            dict(\n                                context=dict(inspect.getmembers(member)),\n                                context_name=node[\"context_name\"] + \".\" + member_name,\n                                depth=node[\"depth\"] - 1,\n                            )\n                        )\n                    except ImportError:\n                        pass  # could not import, so do not attempt\n    return None\n</code></pre>"}, {"location": "api/datajoint/errors/", "title": "errors.py", "text": "<p>Exception classes for the DataJoint library</p>"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.LostConnectionError", "title": "<code>LostConnectionError</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Loss of server connection</p> Source code in <code>datajoint/errors.py</code> <pre><code>class LostConnectionError(DataJointError):\n\"\"\"\n    Loss of server connection\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.QueryError", "title": "<code>QueryError</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Errors arising from queries to the database</p> Source code in <code>datajoint/errors.py</code> <pre><code>class QueryError(DataJointError):\n\"\"\"\n    Errors arising from queries to the database\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.QuerySyntaxError", "title": "<code>QuerySyntaxError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>Errors arising from incorrect query syntax</p> Source code in <code>datajoint/errors.py</code> <pre><code>class QuerySyntaxError(QueryError):\n\"\"\"\n    Errors arising from incorrect query syntax\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.AccessError", "title": "<code>AccessError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>User access error: insufficient privileges.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class AccessError(QueryError):\n\"\"\"\n    User access error: insufficient privileges.\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingTableError", "title": "<code>MissingTableError</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Query on a table that has not been declared</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingTableError(DataJointError):\n\"\"\"\n    Query on a table that has not been declared\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.DuplicateError", "title": "<code>DuplicateError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An integrity error caused by a duplicate entry into a unique key</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DuplicateError(QueryError):\n\"\"\"\n    An integrity error caused by a duplicate entry into a unique key\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.IntegrityError", "title": "<code>IntegrityError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An integrity error triggered by foreign key constraints</p> Source code in <code>datajoint/errors.py</code> <pre><code>class IntegrityError(QueryError):\n\"\"\"\n    An integrity error triggered by foreign key constraints\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.UnknownAttributeError", "title": "<code>UnknownAttributeError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>User requests an attribute name not found in query heading</p> Source code in <code>datajoint/errors.py</code> <pre><code>class UnknownAttributeError(QueryError):\n\"\"\"\n    User requests an attribute name not found in query heading\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingAttributeError", "title": "<code>MissingAttributeError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An error arising when a required attribute value is not provided in INSERT</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingAttributeError(QueryError):\n\"\"\"\n    An error arising when a required attribute value is not provided in INSERT\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.MissingExternalFile", "title": "<code>MissingExternalFile</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Error raised when an external file managed by DataJoint is no longer accessible</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingExternalFile(DataJointError):\n\"\"\"\n    Error raised when an external file managed by DataJoint is no longer accessible\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/errors/#datajoint.errors.BucketInaccessible", "title": "<code>BucketInaccessible</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Error raised when a S3 bucket is inaccessible</p> Source code in <code>datajoint/errors.py</code> <pre><code>class BucketInaccessible(DataJointError):\n\"\"\"\n    Error raised when a S3 bucket is inaccessible\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/expression/", "title": "expression.py", "text": ""}, {"location": "api/datajoint/expression/#datajoint.expression.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression", "title": "<code>QueryExpression</code>", "text": "<p>QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union.</p> <p>A QueryExpression object has a support, a restriction (an AndList), and heading. Property <code>heading</code> (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj.</p> <p>Property <code>support</code> is the list of table names or other QueryExpressions to be joined.</p> <p>The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute.</p> <p>Application of operators does not always lead to the creation of a subquery. A subquery is generated when:     1. A restriction is applied on any computed or renamed attributes     2. A projection is applied remapping remapped attributes     3. Subclasses: Join, Aggregation, and Union have additional specific rules.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class QueryExpression:\n\"\"\"\n    QueryExpression implements query operators to derive new entity set from its input.\n    A QueryExpression object generates a SELECT statement in SQL.\n    QueryExpression operators are restrict, join, proj, aggr, and union.\n\n    A QueryExpression object has a support, a restriction (an AndList), and heading.\n    Property `heading` (type dj.Heading) contains information about the attributes.\n    It is loaded from the database and updated by proj.\n\n    Property `support` is the list of table names or other QueryExpressions to be joined.\n\n    The restriction is applied first without having access to the attributes generated by the projection.\n    Then projection is applied by selecting modifying the heading attribute.\n\n    Application of operators does not always lead to the creation of a subquery.\n    A subquery is generated when:\n        1. A restriction is applied on any computed or renamed attributes\n        2. A projection is applied remapping remapped attributes\n        3. Subclasses: Join, Aggregation, and Union have additional specific rules.\n    \"\"\"\n\n    _restriction = None\n    _restriction_attributes = None\n    _left = []  # list of booleans True for left joins, False for inner joins\n    _original_heading = None  # heading before projections\n\n    # subclasses or instantiators must provide values\n    _connection = None\n    _heading = None\n    _support = None\n\n    # If the query will be using distinct\n    _distinct = False\n\n    @property\n    def connection(self):\n\"\"\"a dj.Connection object\"\"\"\n        assert self._connection is not None\n        return self._connection\n\n    @property\n    def support(self):\n\"\"\"A list of table names or subqueries to from the FROM clause\"\"\"\n        assert self._support is not None\n        return self._support\n\n    @property\n    def heading(self):\n\"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\"\n        return self._heading\n\n    @property\n    def original_heading(self):\n\"\"\"a dj.Heading object reflecting the attributes before projection\"\"\"\n        return self._original_heading or self.heading\n\n    @property\n    def restriction(self):\n\"\"\"a AndList object of restrictions applied to input to produce the result\"\"\"\n        if self._restriction is None:\n            self._restriction = AndList()\n        return self._restriction\n\n    @property\n    def restriction_attributes(self):\n\"\"\"the set of attribute names invoked in the WHERE clause\"\"\"\n        if self._restriction_attributes is None:\n            self._restriction_attributes = set()\n        return self._restriction_attributes\n\n    @property\n    def primary_key(self):\n        return self.heading.primary_key\n\n    _subquery_alias_count = count()  # count for alias names used in the FROM clause\n\n    def from_clause(self):\n        support = (\n            \"(\" + src.make_sql() + \") as `$%x`\" % next(self._subquery_alias_count)\n            if isinstance(src, QueryExpression)\n            else src\n            for src in self.support\n        )\n        clause = next(support)\n        for s, left in zip(support, self._left):\n            clause += \" NATURAL{left} JOIN {clause}\".format(\n                left=\" LEFT\" if left else \"\", clause=s\n            )\n        return clause\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self.restriction\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self.restriction)\n        )\n\n    def make_sql(self, fields=None):\n\"\"\"\n        Make the SQL SELECT statement.\n\n        :param fields: used to explicitly set the select attributes\n        \"\"\"\n        return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n            distinct=\"DISTINCT \" if self._distinct else \"\",\n            fields=self.heading.as_sql(fields or self.heading.names),\n            from_=self.from_clause(),\n            where=self.where_clause(),\n        )\n\n    # --------- query operators -----------\n    def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = [self]\n        result._heading = self.heading.make_subquery_heading()\n        return result\n\n    def restrict(self, restriction):\n\"\"\"\n        Produces a new expression with the new restriction applied.\n        rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n        rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n        The primary key of the result is unaffected.\n        Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n        Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n        (logical disjunction of conditions)\n        Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n        The expressions in each row equivalent:\n\n        rel &amp; True                          rel\n        rel &amp; False                         the empty entity set\n        rel &amp; 'TRUE'                        rel\n        rel &amp; 'FALSE'                       the empty entity set\n        rel - cond                          rel &amp; Not(cond)\n        rel - 'TRUE'                        rel &amp; False\n        rel - 'FALSE'                       rel\n        rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n        rel &amp; AndList()                     rel\n        rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n        rel &amp; []                            rel &amp; False\n        rel &amp; None                          rel &amp; False\n        rel &amp; any_empty_entity_set          rel &amp; False\n        rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n        rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n        rel - AndList()                     rel &amp; False\n        rel - []                            rel\n        rel - None                          rel\n        rel - any_empty_entity_set          rel\n\n        When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n        one element in arg (hence arg is treated as an OrList).\n        Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n        Two elements match when their common attributes have equal values or when they have no common attributes.\n        All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n        QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n        ultimately call restrict()\n\n        :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n        string, or an AndList.\n        \"\"\"\n        attributes = set()\n        new_condition = make_condition(self, restriction, attributes)\n        if new_condition is True:\n            return self  # restriction has no effect, return the same object\n        # check that all attributes in condition are present in the query\n        try:\n            raise DataJointError(\n                \"Attribute `%s` is not found in query.\"\n                % next(attr for attr in attributes if attr not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        # If the new condition uses any new attributes, a subquery is required.\n        # However, Aggregation's HAVING statement works fine with aliased attributes.\n        need_subquery = isinstance(self, Union) or (\n            not isinstance(self, Aggregation) and self.heading.new_attributes\n        )\n        if need_subquery:\n            result = self.make_subquery()\n        else:\n            result = copy.copy(self)\n            result._restriction = AndList(\n                self.restriction\n            )  # copy to preserve the original\n        result.restriction.append(new_condition)\n        result.restriction_attributes.update(attributes)\n        return result\n\n    def restrict_in_place(self, restriction):\n        self.__dict__.update(self.restrict(restriction).__dict__)\n\n    def __and__(self, restriction):\n\"\"\"\n        Restriction operator e.g. ``q1 &amp; q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(restriction)\n\n    def __xor__(self, restriction):\n\"\"\"\n        Permissive restriction operator ignoring compatibility check  e.g. ``q1 ^ q2``.\n        \"\"\"\n        if inspect.isclass(restriction) and issubclass(restriction, QueryExpression):\n            restriction = restriction()\n        if isinstance(restriction, Not):\n            return self.restrict(Not(PromiscuousOperand(restriction.restriction)))\n        return self.restrict(PromiscuousOperand(restriction))\n\n    def __sub__(self, restriction):\n\"\"\"\n        Inverted restriction e.g. ``q1 - q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(Not(restriction))\n\n    def __neg__(self):\n\"\"\"\n        Convert between restriction and inverted restriction e.g. ``-q1``.\n        :return: target restriction\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        if isinstance(self, Not):\n            return self.restriction\n        return Not(self)\n\n    def __mul__(self, other):\n\"\"\"\n        join of query expressions `self` and `other` e.g. ``q1 * q2``.\n        \"\"\"\n        return self.join(other)\n\n    def __matmul__(self, other):\n\"\"\"\n        Permissive join of query expressions `self` and `other` ignoring compatibility check\n            e.g. ``q1 @ q2``.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        return self.join(other, semantic_check=False)\n\n    def join(self, other, semantic_check=True, left=False):\n\"\"\"\n        create the joined QueryExpression.\n        a * b  is short for A.join(B)\n        a @ b  is short for A.join(B, semantic_check=False)\n        Additionally, left=True will retain the rows of self, effectively performing a left join.\n        \"\"\"\n        # trigger subqueries if joining on renamed attributes\n        if isinstance(other, U):\n            return other * self\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"The argument of join must be a QueryExpression\")\n        if semantic_check:\n            assert_join_compatibility(self, other)\n        join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n        # needs subquery if self's FROM clause has common attributes with other's FROM clause\n        need_subquery1 = need_subquery2 = bool(\n            (set(self.original_heading.names) &amp; set(other.original_heading.names))\n            - join_attributes\n        )\n        # need subquery if any of the join attributes are derived\n        need_subquery1 = (\n            need_subquery1\n            or isinstance(self, Aggregation)\n            or any(n in self.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        need_subquery2 = (\n            need_subquery2\n            or isinstance(other, Aggregation)\n            or any(n in other.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        if need_subquery1:\n            self = self.make_subquery()\n        if need_subquery2:\n            other = other.make_subquery()\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = self.support + other.support\n        result._left = self._left + [left] + other._left\n        result._heading = self.heading.join(other.heading)\n        result._restriction = AndList(self.restriction)\n        result._restriction.append(other.restriction)\n        result._original_heading = self.original_heading.join(other.original_heading)\n        assert len(result.support) == len(result._left) + 1\n        return result\n\n    def __add__(self, other):\n\"\"\"union e.g. ``q1 + q2``.\"\"\"\n        return Union.create(self, other)\n\n    def proj(self, *attributes, **named_attributes):\n\"\"\"\n        Projection operator.\n\n        :param attributes:  attributes to be included in the result. (The primary key is already included).\n        :param named_attributes: new attributes computed or renamed from existing attributes.\n        :return: the projected expression.\n        Primary key attributes cannot be excluded but may be renamed.\n        If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n        Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n        Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n        self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n        self.proj() -- include only primary key\n        self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n        self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n        self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n        self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n        self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n        from other attributes available before the projection.\n        Each attribute name can only be used once.\n        \"\"\"\n        named_attributes = {\n            k: translate_attribute(v)[1] for k, v in named_attributes.items()\n        }\n        # new attributes in parentheses are included again with the new name without removing original\n        duplication_pattern = re.compile(\n            rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n        )\n        # attributes without parentheses renamed\n        rename_pattern = re.compile(\n            rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n        )\n        replicate_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        rename_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        compute_map = {\n            k: v\n            for k, v in named_attributes.items()\n            if not duplication_pattern.match(v) and not rename_pattern.match(v)\n        }\n        attributes = set(attributes)\n        # include primary key\n        attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n        # include all secondary attributes with Ellipsis\n        if Ellipsis in attributes:\n            attributes.discard(Ellipsis)\n            attributes.update(\n                (\n                    a\n                    for a in self.heading.secondary_attributes\n                    if a not in attributes and a not in rename_map.values()\n                )\n            )\n        try:\n            raise DataJointError(\n                \"%s is not a valid data type for an attribute in .proj\"\n                % next(a for a in attributes if not isinstance(a, str))\n            )\n        except StopIteration:\n            pass  # normal case\n        # remove excluded attributes, specified as `-attr'\n        excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n        attributes.difference_update(excluded)\n        excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n        attributes.difference_update(excluded)\n        try:\n            raise DataJointError(\n                \"Cannot exclude primary key attribute %s\",\n                next(a for a in excluded if a in self.primary_key),\n            )\n        except StopIteration:\n            pass  # all ok\n        # check that all attributes exist in heading\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(a for a in attributes if a not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that all mentioned names are present in heading\n        mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n        try:\n            raise DataJointError(\n                \"Attribute '%s' not found.\"\n                % next(a for a in mentions if not self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that newly created attributes do not clash with any other selected attributes\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in rename_map\n                    if a in attributes.union(compute_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in compute_map\n                    if a in attributes.union(rename_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in replicate_map\n                    if a in attributes.union(rename_map).union(compute_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # need a subquery if the projection remaps any remapped attributes\n        used = set(q for v in compute_map.values() for q in extract_column_names(v))\n        used.update(rename_map.values())\n        used.update(replicate_map.values())\n        used.intersection_update(self.heading.names)\n        need_subquery = isinstance(self, Union) or any(\n            self.heading[name].attribute_expression is not None for name in used\n        )\n        if not need_subquery and self.restriction:\n            # need a subquery if the restriction applies to attributes that have been renamed\n            need_subquery = any(\n                name in self.restriction_attributes\n                for name in self.heading.new_attributes\n            )\n\n        result = self.make_subquery() if need_subquery else copy.copy(self)\n        result._original_heading = result.original_heading\n        result._heading = result.heading.select(\n            attributes,\n            rename_map=dict(**rename_map, **replicate_map),\n            compute_map=compute_map,\n        )\n        return result\n\n    def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if Ellipsis in attributes:\n            # expand ellipsis to include only attributes from the left table\n            attributes = set(attributes)\n            attributes.discard(Ellipsis)\n            attributes.update(self.heading.secondary_attributes)\n        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n            *attributes, **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n\n    # ---------- Fetch operators --------------------\n    @property\n    def fetch1(self):\n        return Fetch1(self)\n\n    @property\n    def fetch(self):\n        return Fetch(self)\n\n    def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the first few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n\n    def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the last few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n\n    def __len__(self):\n\"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\"\n        return self.connection.query(\n            \"SELECT {select_} FROM {from_}{where}\".format(\n                select_=(\n                    \"count(*)\"\n                    if any(self._left)\n                    else \"count(DISTINCT {fields})\".format(\n                        fields=self.heading.as_sql(\n                            self.primary_key, include_aliases=False\n                        )\n                    )\n                ),\n                from_=self.from_clause(),\n                where=self.where_clause(),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n\"\"\"\n        :return: True if the result is not empty. Equivalent to len(self) &gt; 0 but often\n            faster e.g. ``bool(q1)``.\n        \"\"\"\n        return bool(\n            self.connection.query(\n                \"SELECT EXISTS(SELECT 1 FROM {from_}{where})\".format(\n                    from_=self.from_clause(), where=self.where_clause()\n                )\n            ).fetchone()[0]\n        )\n\n    def __contains__(self, item):\n\"\"\"\n        returns True if the restriction in item matches any entries in self\n            e.g. ``restriction in q1``.\n\n        :param item: any restriction\n        (item in query_expression) is equivalent to bool(query_expression &amp; item) but may be\n        executed more efficiently.\n        \"\"\"\n        return bool(self &amp; item)  # May be optimized e.g. using an EXISTS query\n\n    def __iter__(self):\n\"\"\"\n        returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``.\n\n        :param self: iterator-compatible QueryExpression object\n        \"\"\"\n        self._iter_only_key = all(v.in_key for v in self.heading.attributes.values())\n        self._iter_keys = self.fetch(\"KEY\")\n        return self\n\n    def __next__(self):\n\"\"\"\n        returns the next record on an iterator-compatible QueryExpression object\n            e.g. ``next(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: dict\n        \"\"\"\n        try:\n            key = self._iter_keys.pop(0)\n        except AttributeError:\n            # self._iter_keys is missing because __iter__ has not been called.\n            raise TypeError(\n                \"A QueryExpression object is not an iterator. \"\n                \"Use iter(obj) to create an iterator.\"\n            )\n        except IndexError:\n            raise StopIteration\n        else:\n            if self._iter_only_key:\n                return key\n            else:\n                try:\n                    return (self &amp; key).fetch1()\n                except DataJointError:\n                    # The data may have been deleted since the moment the keys were fetched\n                    # -- move on to next entry.\n                    return next(self)\n\n    def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n        See expression.fetch() for input description.\n        :return: query cursor\n        \"\"\"\n        if offset and limit is None:\n            raise DataJointError(\"limit is required when offset is set\")\n        sql = self.make_sql()\n        if order_by is not None:\n            sql += \" ORDER BY \" + \", \".join(order_by)\n        if limit is not None:\n            sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n        logger.debug(sql)\n        return self.connection.query(sql, as_dict=as_dict)\n\n    def __repr__(self):\n\"\"\"\n        returns the string representation of a QueryExpression object e.g. ``str(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: str\n        \"\"\"\n        return (\n            super().__repr__()\n            if config[\"loglevel\"].lower() == \"debug\"\n            else self.preview()\n        )\n\n    def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n        return preview(self, limit, width)\n\n    def _repr_html_(self):\n\"\"\":return: HTML to display table in Jupyter notebook.\"\"\"\n        return repr_html(self)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.connection", "title": "<code>connection</code>  <code>property</code>", "text": "<p>a dj.Connection object</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.support", "title": "<code>support</code>  <code>property</code>", "text": "<p>A list of table names or subqueries to from the FROM clause</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.heading", "title": "<code>heading</code>  <code>property</code>", "text": "<p>a dj.Heading object, reflects the effects of the projection operator .proj</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.original_heading", "title": "<code>original_heading</code>  <code>property</code>", "text": "<p>a dj.Heading object reflecting the attributes before projection</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction", "title": "<code>restriction</code>  <code>property</code>", "text": "<p>a AndList object of restrictions applied to input to produce the result</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restriction_attributes", "title": "<code>restriction_attributes</code>  <code>property</code>", "text": "<p>the set of attribute names invoked in the WHERE clause</p>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_sql", "title": "<code>make_sql(fields=None)</code>", "text": "<p>Make the SQL SELECT statement.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>used to explicitly set the select attributes</p> <code>None</code> Source code in <code>datajoint/expression.py</code> <pre><code>def make_sql(self, fields=None):\n\"\"\"\n    Make the SQL SELECT statement.\n\n    :param fields: used to explicitly set the select attributes\n    \"\"\"\n    return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n        distinct=\"DISTINCT \" if self._distinct else \"\",\n        fields=self.heading.as_sql(fields or self.heading.names),\n        from_=self.from_clause(),\n        where=self.where_clause(),\n    )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.make_subquery", "title": "<code>make_subquery()</code>", "text": "<p>create a new SELECT statement where self is the FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = [self]\n    result._heading = self.heading.make_subquery_heading()\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.restrict", "title": "<code>restrict(restriction)</code>", "text": "<p>Produces a new expression with the new restriction applied. rel.restrict(restriction)  is equivalent to  rel &amp; restriction. rel.restrict(Not(restriction))  is equivalent to  rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class.</p> <p>The expressions in each row equivalent:</p> <p>rel &amp; True                          rel rel &amp; False                         the empty entity set rel &amp; 'TRUE'                        rel rel &amp; 'FALSE'                       the empty entity set rel - cond                          rel &amp; Not(cond) rel - 'TRUE'                        rel &amp; False rel - 'FALSE'                       rel rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2 rel &amp; AndList()                     rel rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2)) rel &amp; []                            rel &amp; False rel &amp; None                          rel &amp; False rel &amp; any_empty_entity_set          rel &amp; False rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)] rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2) rel - AndList()                     rel &amp; False rel - []                            rel rel - None                          rel rel - any_empty_entity_set          rel</p> <p>When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.</p> <p>QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict()</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList.</p> required Source code in <code>datajoint/expression.py</code> <pre><code>def restrict(self, restriction):\n\"\"\"\n    Produces a new expression with the new restriction applied.\n    rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n    rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n    The primary key of the result is unaffected.\n    Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n    Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n    (logical disjunction of conditions)\n    Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n    The expressions in each row equivalent:\n\n    rel &amp; True                          rel\n    rel &amp; False                         the empty entity set\n    rel &amp; 'TRUE'                        rel\n    rel &amp; 'FALSE'                       the empty entity set\n    rel - cond                          rel &amp; Not(cond)\n    rel - 'TRUE'                        rel &amp; False\n    rel - 'FALSE'                       rel\n    rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n    rel &amp; AndList()                     rel\n    rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n    rel &amp; []                            rel &amp; False\n    rel &amp; None                          rel &amp; False\n    rel &amp; any_empty_entity_set          rel &amp; False\n    rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n    rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n    rel - AndList()                     rel &amp; False\n    rel - []                            rel\n    rel - None                          rel\n    rel - any_empty_entity_set          rel\n\n    When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n    one element in arg (hence arg is treated as an OrList).\n    Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n    Two elements match when their common attributes have equal values or when they have no common attributes.\n    All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n    QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n    ultimately call restrict()\n\n    :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n    string, or an AndList.\n    \"\"\"\n    attributes = set()\n    new_condition = make_condition(self, restriction, attributes)\n    if new_condition is True:\n        return self  # restriction has no effect, return the same object\n    # check that all attributes in condition are present in the query\n    try:\n        raise DataJointError(\n            \"Attribute `%s` is not found in query.\"\n            % next(attr for attr in attributes if attr not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    # If the new condition uses any new attributes, a subquery is required.\n    # However, Aggregation's HAVING statement works fine with aliased attributes.\n    need_subquery = isinstance(self, Union) or (\n        not isinstance(self, Aggregation) and self.heading.new_attributes\n    )\n    if need_subquery:\n        result = self.make_subquery()\n    else:\n        result = copy.copy(self)\n        result._restriction = AndList(\n            self.restriction\n        )  # copy to preserve the original\n    result.restriction.append(new_condition)\n    result.restriction_attributes.update(attributes)\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.join", "title": "<code>join(other, semantic_check=True, left=False)</code>", "text": "<p>create the joined QueryExpression. a * b  is short for A.join(B) a @ b  is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, semantic_check=True, left=False):\n\"\"\"\n    create the joined QueryExpression.\n    a * b  is short for A.join(B)\n    a @ b  is short for A.join(B, semantic_check=False)\n    Additionally, left=True will retain the rows of self, effectively performing a left join.\n    \"\"\"\n    # trigger subqueries if joining on renamed attributes\n    if isinstance(other, U):\n        return other * self\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"The argument of join must be a QueryExpression\")\n    if semantic_check:\n        assert_join_compatibility(self, other)\n    join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n    # needs subquery if self's FROM clause has common attributes with other's FROM clause\n    need_subquery1 = need_subquery2 = bool(\n        (set(self.original_heading.names) &amp; set(other.original_heading.names))\n        - join_attributes\n    )\n    # need subquery if any of the join attributes are derived\n    need_subquery1 = (\n        need_subquery1\n        or isinstance(self, Aggregation)\n        or any(n in self.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    need_subquery2 = (\n        need_subquery2\n        or isinstance(other, Aggregation)\n        or any(n in other.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    if need_subquery1:\n        self = self.make_subquery()\n    if need_subquery2:\n        other = other.make_subquery()\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = self.support + other.support\n    result._left = self._left + [left] + other._left\n    result._heading = self.heading.join(other.heading)\n    result._restriction = AndList(self.restriction)\n    result._restriction.append(other.restriction)\n    result._original_heading = self.original_heading.join(other.original_heading)\n    assert len(result.support) == len(result._left) + 1\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.proj", "title": "<code>proj(*attributes, **named_attributes)</code>", "text": "<p>Projection operator.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <p>attributes to be included in the result. (The primary key is already included).</p> <code>()</code> <code>named_attributes</code> <p>new attributes computed or renamed from existing attributes.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def proj(self, *attributes, **named_attributes):\n\"\"\"\n    Projection operator.\n\n    :param attributes:  attributes to be included in the result. (The primary key is already included).\n    :param named_attributes: new attributes computed or renamed from existing attributes.\n    :return: the projected expression.\n    Primary key attributes cannot be excluded but may be renamed.\n    If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n    Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n    Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n    self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n    self.proj() -- include only primary key\n    self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n    self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n    self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n    self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n    self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n    from other attributes available before the projection.\n    Each attribute name can only be used once.\n    \"\"\"\n    named_attributes = {\n        k: translate_attribute(v)[1] for k, v in named_attributes.items()\n    }\n    # new attributes in parentheses are included again with the new name without removing original\n    duplication_pattern = re.compile(\n        rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n    )\n    # attributes without parentheses renamed\n    rename_pattern = re.compile(\n        rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n    )\n    replicate_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    rename_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    compute_map = {\n        k: v\n        for k, v in named_attributes.items()\n        if not duplication_pattern.match(v) and not rename_pattern.match(v)\n    }\n    attributes = set(attributes)\n    # include primary key\n    attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n    # include all secondary attributes with Ellipsis\n    if Ellipsis in attributes:\n        attributes.discard(Ellipsis)\n        attributes.update(\n            (\n                a\n                for a in self.heading.secondary_attributes\n                if a not in attributes and a not in rename_map.values()\n            )\n        )\n    try:\n        raise DataJointError(\n            \"%s is not a valid data type for an attribute in .proj\"\n            % next(a for a in attributes if not isinstance(a, str))\n        )\n    except StopIteration:\n        pass  # normal case\n    # remove excluded attributes, specified as `-attr'\n    excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n    attributes.difference_update(excluded)\n    excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n    attributes.difference_update(excluded)\n    try:\n        raise DataJointError(\n            \"Cannot exclude primary key attribute %s\",\n            next(a for a in excluded if a in self.primary_key),\n        )\n    except StopIteration:\n        pass  # all ok\n    # check that all attributes exist in heading\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(a for a in attributes if a not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that all mentioned names are present in heading\n    mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n    try:\n        raise DataJointError(\n            \"Attribute '%s' not found.\"\n            % next(a for a in mentions if not self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that newly created attributes do not clash with any other selected attributes\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in rename_map\n                if a in attributes.union(compute_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in compute_map\n                if a in attributes.union(rename_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in replicate_map\n                if a in attributes.union(rename_map).union(compute_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # need a subquery if the projection remaps any remapped attributes\n    used = set(q for v in compute_map.values() for q in extract_column_names(v))\n    used.update(rename_map.values())\n    used.update(replicate_map.values())\n    used.intersection_update(self.heading.names)\n    need_subquery = isinstance(self, Union) or any(\n        self.heading[name].attribute_expression is not None for name in used\n    )\n    if not need_subquery and self.restriction:\n        # need a subquery if the restriction applies to attributes that have been renamed\n        need_subquery = any(\n            name in self.restriction_attributes\n            for name in self.heading.new_attributes\n        )\n\n    result = self.make_subquery() if need_subquery else copy.copy(self)\n    result._original_heading = result.original_heading\n    result._heading = result.heading.select(\n        attributes,\n        rename_map=dict(**rename_map, **replicate_map),\n        compute_map=compute_map,\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.aggr", "title": "<code>aggr(group, *attributes, keep_all_rows=False, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>keep_all_rows</code> <p>True=keep all the rows from self. False=keep only rows that match entries in group.</p> <code>False</code> <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if Ellipsis in attributes:\n        # expand ellipsis to include only attributes from the left table\n        attributes = set(attributes)\n        attributes.discard(Ellipsis)\n        attributes.update(self.heading.secondary_attributes)\n    return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n        *attributes, **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.head", "title": "<code>head(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25)</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the first few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.tail", "title": "<code>tail(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the last few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.cursor", "title": "<code>cursor(offset=0, limit=None, order_by=None, as_dict=False)</code>", "text": "<p>See expression.fetch() for input description.</p> <p>Returns:</p> Type Description <p>query cursor</p> Source code in <code>datajoint/expression.py</code> <pre><code>def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n    See expression.fetch() for input description.\n    :return: query cursor\n    \"\"\"\n    if offset and limit is None:\n        raise DataJointError(\"limit is required when offset is set\")\n    sql = self.make_sql()\n    if order_by is not None:\n        sql += \" ORDER BY \" + \", \".join(order_by)\n    if limit is not None:\n        sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n    logger.debug(sql)\n    return self.connection.query(sql, as_dict=as_dict)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.QueryExpression.preview", "title": "<code>preview(limit=None, width=None)</code>", "text": "<p>Returns:</p> Type Description <p>a string of preview of the contents of the query.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n    return preview(self, limit, width)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.PromiscuousOperand", "title": "<code>PromiscuousOperand</code>", "text": "<p>A container for an operand to ignore join compatibility</p> Source code in <code>datajoint/condition.py</code> <pre><code>class PromiscuousOperand:\n\"\"\"\n    A container for an operand to ignore join compatibility\n    \"\"\"\n\n    def __init__(self, operand):\n        self.operand = operand\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.AndList", "title": "<code>AndList</code>", "text": "<p>         Bases: <code>list</code></p> <p>A list of conditions to by applied to a query expression by logical conjunction: the conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are applied by logical disjunction (OR).</p> <p>Example: expr2 = expr &amp; dj.AndList((cond1, cond2, cond3)) is equivalent to expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3</p> Source code in <code>datajoint/condition.py</code> <pre><code>class AndList(list):\n\"\"\"\n    A list of conditions to by applied to a query expression by logical conjunction: the\n    conditions are AND-ed. All other collections (lists, sets, other entity sets, etc) are\n    applied by logical disjunction (OR).\n\n    Example:\n    expr2 = expr &amp; dj.AndList((cond1, cond2, cond3))\n    is equivalent to\n    expr2 = expr &amp; cond1 &amp; cond2 &amp; cond3\n    \"\"\"\n\n    def append(self, restriction):\n        if isinstance(restriction, AndList):\n            # extend to reduce nesting\n            self.extend(restriction)\n        else:\n            super().append(restriction)\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Not", "title": "<code>Not</code>", "text": "<p>invert restriction</p> Source code in <code>datajoint/condition.py</code> <pre><code>class Not:\n\"\"\"invert restriction\"\"\"\n\n    def __init__(self, restriction):\n        self.restriction = restriction\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.assert_join_compatibility", "title": "<code>assert_join_compatibility(expr1, expr2)</code>", "text": "<p>Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible, the matching attributes in the two expressions must be in the primary key of one or the other expression. Raises an exception if not compatible.</p> <p>Parameters:</p> Name Type Description Default <code>expr1</code> <p>A QueryExpression object</p> required <code>expr2</code> <p>A QueryExpression object</p> required Source code in <code>datajoint/condition.py</code> <pre><code>def assert_join_compatibility(expr1, expr2):\n\"\"\"\n    Determine if expressions expr1 and expr2 are join-compatible.  To be join-compatible,\n    the matching attributes in the two expressions must be in the primary key of one or the\n    other expression.\n    Raises an exception if not compatible.\n\n    :param expr1: A QueryExpression object\n    :param expr2: A QueryExpression object\n    \"\"\"\n    from .expression import QueryExpression, U\n\n    for rel in (expr1, expr2):\n        if not isinstance(rel, (U, QueryExpression)):\n            raise DataJointError(\n                \"Object %r is not a QueryExpression and cannot be joined.\" % rel\n            )\n    if not isinstance(expr1, U) and not isinstance(\n        expr2, U\n    ):  # dj.U is always compatible\n        try:\n            raise DataJointError(\n                \"Cannot join query expressions on dependent attribute `%s`\"\n                % next(\n                    r\n                    for r in set(expr1.heading.secondary_attributes).intersection(\n                        expr2.heading.secondary_attributes\n                    )\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.make_condition", "title": "<code>make_condition(query_expression, condition, columns)</code>", "text": "<p>Translate the input condition into the equivalent SQL condition (a string)</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <p>a dj.QueryExpression object to apply condition</p> required <code>condition</code> <p>any valid restriction object.</p> required <code>columns</code> <p>a set passed by reference to collect all column names used in the condition.</p> required <p>Returns:</p> Type Description <p>an SQL condition string or a boolean value.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def make_condition(query_expression, condition, columns):\n\"\"\"\n    Translate the input condition into the equivalent SQL condition (a string)\n\n    :param query_expression: a dj.QueryExpression object to apply condition\n    :param condition: any valid restriction object.\n    :param columns: a set passed by reference to collect all column names used in the\n        condition.\n    :return: an SQL condition string or a boolean value.\n    \"\"\"\n    from .expression import QueryExpression, Aggregation, U\n\n    def prep_value(k, v):\n\"\"\"prepare SQL condition\"\"\"\n        key_match, k = translate_attribute(k)\n        if key_match[\"path\"] is None:\n            k = f\"`{k}`\"\n        if (\n            query_expression.heading[key_match[\"attr\"]].json\n            and key_match[\"path\"] is not None\n            and isinstance(v, dict)\n        ):\n            return f\"{k}='{json.dumps(v)}'\"\n        if v is None:\n            return f\"{k} IS NULL\"\n        if query_expression.heading[key_match[\"attr\"]].uuid:\n            if not isinstance(v, uuid.UUID):\n                try:\n                    v = uuid.UUID(v)\n                except (AttributeError, ValueError):\n                    raise DataJointError(\n                        \"Badly formed UUID {v} in restriction by `{k}`\".format(k=k, v=v)\n                    )\n            return f\"{k}=X'{v.bytes.hex()}'\"\n        if isinstance(\n            v,\n            (\n                datetime.date,\n                datetime.datetime,\n                datetime.time,\n                decimal.Decimal,\n                list,\n            ),\n        ):\n            return f'{k}=\"{v}\"'\n        if isinstance(v, str):\n            v = v.replace(\"%\", \"%%\").replace(\"\\\\\", \"\\\\\\\\\")\n            return f'{k}=\"{v}\"'\n        return f\"{k}={v}\"\n\n    def combine_conditions(negate, conditions):\n        return f\"{'NOT ' if negate else ''} ({')AND('.join(conditions)})\"\n\n    negate = False\n    while isinstance(condition, Not):\n        negate = not negate\n        condition = condition.restriction\n\n    # restrict by string\n    if isinstance(condition, str):\n        columns.update(extract_column_names(condition))\n        return combine_conditions(\n            negate, conditions=[condition.strip().replace(\"%\", \"%%\")]\n        )  # escape %, see issue #376\n\n    # restrict by AndList\n    if isinstance(condition, AndList):\n        # omit all conditions that evaluate to True\n        items = [\n            item\n            for item in (\n                make_condition(query_expression, cond, columns) for cond in condition\n            )\n            if item is not True\n        ]\n        if any(item is False for item in items):\n            return negate  # if any item is False, the whole thing is False\n        if not items:\n            return not negate  # and empty AndList is True\n        return combine_conditions(negate, conditions=items)\n\n    # restriction by dj.U evaluates to True\n    if isinstance(condition, U):\n        return not negate\n\n    # restrict by boolean\n    if isinstance(condition, bool):\n        return negate != condition\n\n    # restrict by a mapping/dict -- convert to an AndList of string equality conditions\n    if isinstance(condition, collections.abc.Mapping):\n        common_attributes = set(c.split(\".\", 1)[0] for c in condition).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluates to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[\n                prep_value(k, v)\n                for k, v in condition.items()\n                if k.split(\".\", 1)[0] in common_attributes  # handle json indexing\n            ],\n        )\n\n    # restrict by a numpy record -- convert to an AndList of string equality conditions\n    if isinstance(condition, numpy.void):\n        common_attributes = set(condition.dtype.fields).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluate to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[prep_value(k, condition[k]) for k in common_attributes],\n        )\n\n    # restrict by a QueryExpression subclass -- trigger instantiation and move on\n    if inspect.isclass(condition) and issubclass(condition, QueryExpression):\n        condition = condition()\n\n    # restrict by another expression (aka semijoin and antijoin)\n    check_compatibility = True\n    if isinstance(condition, PromiscuousOperand):\n        condition = condition.operand\n        check_compatibility = False\n\n    if isinstance(condition, QueryExpression):\n        if check_compatibility:\n            assert_join_compatibility(query_expression, condition)\n        common_attributes = [\n            q for q in condition.heading.names if q in query_expression.heading.names\n        ]\n        columns.update(common_attributes)\n        if isinstance(condition, Aggregation):\n            condition = condition.make_subquery()\n        return (\n            # without common attributes, any non-empty set matches everything\n            (not negate if condition else negate)\n            if not common_attributes\n            else \"({fields}) {not_}in ({subquery})\".format(\n                fields=\"`\" + \"`,`\".join(common_attributes) + \"`\",\n                not_=\"not \" if negate else \"\",\n                subquery=condition.make_sql(common_attributes),\n            )\n        )\n\n    # restrict by pandas.DataFrames\n    if isinstance(condition, pandas.DataFrame):\n        condition = condition.to_records()  # convert to numpy.recarray and move on\n\n    # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList\n    try:\n        or_list = [make_condition(query_expression, q, columns) for q in condition]\n    except TypeError:\n        raise DataJointError(\"Invalid restriction type %r\" % condition)\n    else:\n        or_list = [\n            item for item in or_list if item is not False\n        ]  # ignore False conditions\n        if any(item is True for item in or_list):  # if any item is True, entirely True\n            return not negate\n        return (\n            f\"{'NOT ' if negate else ''} ({' OR '.join(or_list)})\"\n            if or_list\n            else negate\n        )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Fetch", "title": "<code>Fetch</code>", "text": "<p>A fetch object that handles retrieving elements from the table expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>the QueryExpression object to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch:\n\"\"\"\n    A fetch object that handles retrieving elements from the table expression.\n\n    :param expression: the QueryExpression object to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(\n        self,\n        *attrs,\n        offset=None,\n        limit=None,\n        order_by=None,\n        format=None,\n        as_dict=None,\n        squeeze=False,\n        download_path=\".\"\n    ):\n\"\"\"\n        Fetches the expression results from the database into an np.array or list of dictionaries and\n        unpacks blob attributes.\n\n        :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this\n                        table. If provided, returns tuples with an entry for each attribute.\n        :param offset: the number of tuples to skip in the returned result\n        :param limit: the maximum number of tuples to return\n        :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed\n                        if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\",\n                        \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\"\n        :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or\n                        'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. .\n        :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to\n                        True for .fetch('KEY')\n        :param squeeze:  if True, remove extra dimensions from arrays\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the contents of the table in the form of a structured numpy.array or a dict list\n        \"\"\"\n        if order_by is not None:\n            # if 'order_by' passed in a string, make into list\n            if isinstance(order_by, str):\n                order_by = [order_by]\n            # expand \"KEY\" or \"KEY DESC\"\n            order_by = list(\n                _flatten_attribute_list(self._expression.primary_key, order_by)\n            )\n\n        attrs_as_dict = as_dict and attrs\n        if attrs_as_dict:\n            # absorb KEY into attrs and prepare to return attributes as dict (issue #595)\n            if any(is_key(k) for k in attrs):\n                attrs = list(self._expression.primary_key) + [\n                    a for a in attrs if a not in self._expression.primary_key\n                ]\n        if as_dict is None:\n            as_dict = bool(attrs)  # default to True for \"KEY\" and False otherwise\n        # format should not be specified with attrs or is_dict=True\n        if format is not None and (as_dict or attrs):\n            raise DataJointError(\n                \"Cannot specify output format when as_dict=True or \"\n                \"when attributes are selected to be fetched separately.\"\n            )\n        if format not in {None, \"array\", \"frame\"}:\n            raise DataJointError(\n                \"Fetch output format must be in \"\n                '{{\"array\", \"frame\"}} but \"{}\" was given'.format(format)\n            )\n\n        if not (attrs or as_dict) and format is None:\n            format = config[\"fetch_format\"]  # default to array\n            if format not in {\"array\", \"frame\"}:\n                raise DataJointError(\n                    'Invalid entry \"{}\" in datajoint.config[\"fetch_format\"]: '\n                    'use \"array\" or \"frame\"'.format(format)\n                )\n\n        if limit is None and offset is not None:\n            logger.warning(\n                \"Offset set, but no limit. Setting limit to a large number. \"\n                \"Consider setting a limit explicitly.\"\n            )\n            limit = 8000000000  # just a very large number to effect no limit\n\n        get = partial(\n            _get,\n            self._expression.connection,\n            squeeze=squeeze,\n            download_path=download_path,\n        )\n        if attrs:  # a list of attributes provided\n            attributes = [a for a in attrs if not is_key(a)]\n            ret = self._expression.proj(*attributes)\n            ret = ret.fetch(\n                offset=offset,\n                limit=limit,\n                order_by=order_by,\n                as_dict=False,\n                squeeze=squeeze,\n                download_path=download_path,\n                format=\"array\",\n            )\n            if attrs_as_dict:\n                ret = [\n                    {k: v for k, v in zip(ret.dtype.names, x) if k in attrs}\n                    for x in ret\n                ]\n            else:\n                return_values = [\n                    list(\n                        (to_dicts if as_dict else lambda x: x)(\n                            ret[self._expression.primary_key]\n                        )\n                    )\n                    if is_key(attribute)\n                    else ret[attribute]\n                    for attribute in attrs\n                ]\n                ret = return_values[0] if len(attrs) == 1 else return_values\n        else:  # fetch all attributes as a numpy.record_array or pandas.DataFrame\n            cur = self._expression.cursor(\n                as_dict=as_dict, limit=limit, offset=offset, order_by=order_by\n            )\n            heading = self._expression.heading\n            if as_dict:\n                ret = [\n                    dict((name, get(heading[name], d[name])) for name in heading.names)\n                    for d in cur\n                ]\n            else:\n                ret = list(cur.fetchall())\n                record_type = (\n                    heading.as_dtype\n                    if not ret\n                    else np.dtype(\n                        [\n                            (\n                                name,\n                                type(value),\n                            )  # use the first element to determine blob type\n                            if heading[name].is_blob\n                            and isinstance(value, numbers.Number)\n                            else (name, heading.as_dtype[name])\n                            for value, name in zip(ret[0], heading.as_dtype.names)\n                        ]\n                    )\n                )\n                try:\n                    ret = np.array(ret, dtype=record_type)\n                except Exception as e:\n                    raise e\n                for name in heading:\n                    # unpack blobs and externals\n                    ret[name] = list(map(partial(get, heading[name]), ret[name]))\n                if format == \"frame\":\n                    ret = pandas.DataFrame(ret).set_index(heading.primary_key)\n        return ret\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.extract_column_names", "title": "<code>extract_column_names(sql_expression)</code>", "text": "<p>extract all presumed column names from an sql expression such as the WHERE clause, for example.</p> <p>Parameters:</p> Name Type Description Default <code>sql_expression</code> <p>a string containing an SQL expression</p> required <p>Returns:</p> Type Description <p>set of extracted column names This may be MySQL-specific for now.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def extract_column_names(sql_expression):\n\"\"\"\n    extract all presumed column names from an sql expression such as the WHERE clause,\n    for example.\n\n    :param sql_expression: a string containing an SQL expression\n    :return: set of extracted column names\n    This may be MySQL-specific for now.\n    \"\"\"\n    assert isinstance(sql_expression, str)\n    result = set()\n    s = sql_expression  # for terseness\n    # remove escaped quotes\n    s = re.sub(r\"(\\\\\\\")|(\\\\\\')\", \"\", s)\n    # remove quoted text\n    s = re.sub(r\"'[^']*'\", \"\", s)\n    s = re.sub(r'\"[^\"]*\"', \"\", s)\n    # find all tokens in back quotes and remove them\n    result.update(re.findall(r\"`([a-z][a-z_0-9]*)`\", s))\n    s = re.sub(r\"`[a-z][a-z_0-9]*`\", \"\", s)\n    # remove space before parentheses\n    s = re.sub(r\"\\s*\\(\", \"(\", s)\n    # remove tokens followed by ( since they must be functions\n    s = re.sub(r\"(\\b[a-z][a-z_0-9]*)\\(\", \"(\", s)\n    remaining_tokens = set(re.findall(r\"\\b[a-z][a-z_0-9]*\\b\", s))\n    # update result removing reserved words\n    result.update(\n        remaining_tokens\n        - {\n            \"is\",\n            \"in\",\n            \"between\",\n            \"like\",\n            \"and\",\n            \"or\",\n            \"null\",\n            \"not\",\n            \"interval\",\n            \"second\",\n            \"minute\",\n            \"hour\",\n            \"day\",\n            \"month\",\n            \"week\",\n            \"year\",\n        }\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Fetch1", "title": "<code>Fetch1</code>", "text": "<p>Fetch object for fetching the result of a query yielding one row.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>a query expression to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch1:\n\"\"\"\n    Fetch object for fetching the result of a query yielding one row.\n\n    :param expression: a query expression to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(self, *attrs, squeeze=False, download_path=\".\"):\n\"\"\"\n        Fetches the result of a query expression that yields one entry.\n\n        If no attributes are specified, returns the result as a dict.\n        If attributes are specified returns the corresponding results as a tuple.\n\n        Examples:\n        d = rel.fetch1()   # as a dictionary\n        a, b = rel.fetch1('a', 'b')   # as a tuple\n\n        :params *attrs: attributes to return when expanding into a tuple.\n                 If attrs is empty, the return result is a dict\n        :param squeeze:  When true, remove extra dimensions from arrays in attributes\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the one tuple in the table in the form of a dict\n        \"\"\"\n        heading = self._expression.heading\n\n        if not attrs:  # fetch all attributes, return as ordered dict\n            cur = self._expression.cursor(as_dict=True)\n            ret = cur.fetchone()\n            if not ret or cur.fetchone():\n                raise DataJointError(\n                    \"fetch1 requires exactly one tuple in the input set.\"\n                )\n            ret = dict(\n                (\n                    name,\n                    _get(\n                        self._expression.connection,\n                        heading[name],\n                        ret[name],\n                        squeeze=squeeze,\n                        download_path=download_path,\n                    ),\n                )\n                for name in heading.names\n            )\n        else:  # fetch some attributes, return as tuple\n            attributes = [a for a in attrs if not is_key(a)]\n            result = self._expression.proj(*attributes).fetch(\n                squeeze=squeeze, download_path=download_path, format=\"array\"\n            )\n            if len(result) != 1:\n                raise DataJointError(\n                    \"fetch1 should only return one tuple. %d tuples found\" % len(result)\n                )\n            return_values = tuple(\n                next(to_dicts(result[self._expression.primary_key]))\n                if is_key(attribute)\n                else result[attribute][0]\n                for attribute in attrs\n            )\n            ret = return_values[0] if len(attrs) == 1 else return_values\n        return ret\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Aggregation", "title": "<code>Aggregation</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class Aggregation(QueryExpression):\n\"\"\"\n    Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set\n    with primary key from arg.\n    The computed arguments comp1, ..., compn use aggregation calculations on the attributes of\n    group or simple projections and calculations on the attributes of arg.\n    Aggregation is used QueryExpression.aggr and U.aggr.\n    Aggregation is a private class in DataJoint, not exposed to users.\n    \"\"\"\n\n    _left_restrict = None  # the pre-GROUP BY conditions for the WHERE clause\n    _subquery_alias_count = count()\n\n    @classmethod\n    def create(cls, arg, group, keep_all_rows=False):\n        if inspect.isclass(group) and issubclass(group, QueryExpression):\n            group = group()  # instantiate if a class\n        assert isinstance(group, QueryExpression)\n        if keep_all_rows and len(group.support) &gt; 1 or group.heading.new_attributes:\n            group = group.make_subquery()  # subquery if left joining a join\n        join = arg.join(group, left=keep_all_rows)  # reuse the join logic\n        result = cls()\n        result._connection = join.connection\n        result._heading = join.heading.set_primary_key(\n            arg.primary_key\n        )  # use left operand's primary key\n        result._support = join.support\n        result._left = join._left\n        result._left_restrict = join.restriction  # WHERE clause applied before GROUP BY\n        result._grouping_attributes = result.primary_key\n\n        return result\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self._left_restrict\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self._left_restrict)\n        )\n\n    def make_sql(self, fields=None):\n        fields = self.heading.as_sql(fields or self.heading.names)\n        assert self._grouping_attributes or not self.restriction\n        distinct = set(self.heading.names) == set(self.primary_key)\n        return \"SELECT {distinct}{fields} FROM {from_}{where}{group_by}\".format(\n            distinct=\"DISTINCT \" if distinct else \"\",\n            fields=fields,\n            from_=self.from_clause(),\n            where=self.where_clause(),\n            group_by=\"\"\n            if not self.primary_key\n            else (\n                \" GROUP BY `%s`\" % \"`,`\".join(self._grouping_attributes)\n                + (\n                    \"\"\n                    if not self.restriction\n                    else \" HAVING (%s)\" % \")AND(\".join(self.restriction)\n                )\n            ),\n        )\n\n    def __len__(self):\n        return self.connection.query(\n            \"SELECT count(1) FROM ({subquery}) `${alias:x}`\".format(\n                subquery=self.make_sql(), alias=next(self._subquery_alias_count)\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n        return bool(\n            self.connection.query(\"SELECT EXISTS({sql})\".format(sql=self.make_sql()))\n        )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union", "title": "<code>Union</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Union is the private DataJoint class that implements the union operator.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class Union(QueryExpression):\n\"\"\"\n    Union is the private DataJoint class that implements the union operator.\n    \"\"\"\n\n    __count = count()\n\n    @classmethod\n    def create(cls, arg1, arg2):\n        if inspect.isclass(arg2) and issubclass(arg2, QueryExpression):\n            arg2 = arg2()  # instantiate if a class\n        if not isinstance(arg2, QueryExpression):\n            raise DataJointError(\n                \"A QueryExpression can only be unioned with another QueryExpression\"\n            )\n        if arg1.connection != arg2.connection:\n            raise DataJointError(\n                \"Cannot operate on QueryExpressions originating from different connections.\"\n            )\n        if set(arg1.primary_key) != set(arg2.primary_key):\n            raise DataJointError(\n                \"The operands of a union must share the same primary key.\"\n            )\n        if set(arg1.heading.secondary_attributes) &amp; set(\n            arg2.heading.secondary_attributes\n        ):\n            raise DataJointError(\n                \"The operands of a union must not share any secondary attributes.\"\n            )\n        result = cls()\n        result._connection = arg1.connection\n        result._heading = arg1.heading.join(arg2.heading)\n        result._support = [arg1, arg2]\n        return result\n\n    def make_sql(self):\n        arg1, arg2 = self._support\n        if (\n            not arg1.heading.secondary_attributes\n            and not arg2.heading.secondary_attributes\n        ):\n            # no secondary attributes: use UNION DISTINCT\n            fields = arg1.primary_key\n            return \"SELECT * FROM (({sql1}) UNION ({sql2})) as `_u{alias}`\".format(\n                sql1=arg1.make_sql()\n                if isinstance(arg1, Union)\n                else arg1.make_sql(fields),\n                sql2=arg2.make_sql()\n                if isinstance(arg2, Union)\n                else arg2.make_sql(fields),\n                alias=next(self.__count),\n            )\n        # with secondary attributes, use union of left join with antijoin\n        fields = self.heading.names\n        sql1 = arg1.join(arg2, left=True).make_sql(fields)\n        sql2 = (\n            (arg2 - arg1)\n            .proj(..., **{k: \"NULL\" for k in arg1.heading.secondary_attributes})\n            .make_sql(fields)\n        )\n        return \"({sql1})  UNION ({sql2})\".format(sql1=sql1, sql2=sql2)\n\n    def from_clause(self):\n\"\"\"The union does not use a FROM clause\"\"\"\n        assert False\n\n    def where_clause(self):\n\"\"\"The union does not use a WHERE clause\"\"\"\n        assert False\n\n    def __len__(self):\n        return self.connection.query(\n            \"SELECT count(1) FROM ({subquery}) `${alias:x}`\".format(\n                subquery=self.make_sql(),\n                alias=next(QueryExpression._subquery_alias_count),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n        return bool(\n            self.connection.query(\"SELECT EXISTS({sql})\".format(sql=self.make_sql()))\n        )\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.from_clause", "title": "<code>from_clause()</code>", "text": "<p>The union does not use a FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def from_clause(self):\n\"\"\"The union does not use a FROM clause\"\"\"\n    assert False\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.Union.where_clause", "title": "<code>where_clause()</code>", "text": "<p>The union does not use a WHERE clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def where_clause(self):\n\"\"\"The union does not use a WHERE clause\"\"\"\n    assert False\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U", "title": "<code>U</code>", "text": "<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expressio <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class U:\n\"\"\"\n    dj.U objects are the universal sets representing all possible values of their attributes.\n    dj.U objects cannot be queried on their own but are useful for forming some queries.\n    dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn.\n    The universal set is the set of all possible combinations of values of the attributes.\n    Without any attributes, dj.U() represents the set with one element that has no attributes.\n\n    Restriction:\n\n    dj.U can be used to enumerate unique combinations of values of attributes from other expressions.\n\n    The following expression yields all unique combinations of contrast and brightness found in the `stimulus` set:\n\n    &gt;&gt;&gt; dj.U('contrast', 'brightness') &amp; stimulus\n\n    Aggregation:\n\n    In aggregation, dj.U is used for summary calculation over an entire set:\n\n    The following expression yields one element with one attribute `s` containing the total number of elements in\n    query expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(*)')\n\n    The following expressions both yield one element containing the number `n` of distinct values of attribute `attr` in\n    query expressio `expr`.\n\n    &gt;&gt;&gt; dj.U().aggr(expr, n='count(distinct attr)')\n    &gt;&gt;&gt; dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')\n\n    The following expression yields one element and one attribute `s` containing the sum of values of attribute `attr`\n    over entire result set of expression `expr`:\n\n    &gt;&gt;&gt; dj.U().aggr(expr, s='sum(attr)')\n\n    The following expression yields the set of all unique combinations of attributes `attr1`, `attr2` and the number of\n    their occurrences in the result set of query expression `expr`.\n\n    &gt;&gt;&gt; dj.U(attr1,attr2).aggr(expr, n='count(*)')\n\n    Joins:\n\n    If expression `expr` has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result\n    as `expr` but `attr1` and `attr2` are promoted to the the primary key.  This is useful for producing a join on\n    non-primary key attributes.\n    For example, if `attr` is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw\n    an error because in most cases, it does not make sense to join on non-primary key attributes and users must first\n    rename `attr` in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.\n    \"\"\"\n\n    def __init__(self, *primary_key):\n        self._primary_key = primary_key\n\n    @property\n    def primary_key(self):\n        return self._primary_key\n\n    def __and__(self, other):\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be restricted with a QueryExpression.\")\n        result = copy.copy(other)\n        result._distinct = True\n        result._heading = result.heading.set_primary_key(self.primary_key)\n        result = result.proj()\n        return result\n\n    def join(self, other, left=False):\n\"\"\"\n        Joining U with a query expression has the effect of promoting the attributes of U to\n        the primary key of the other query expression.\n\n        :param other: the other query expression to join with.\n        :param left: ignored. dj.U always acts as if left=False\n        :return: a copy of the other query expression with the primary key extended.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate if a class\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found\"\n                % next(k for k in self.primary_key if k not in other.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        result = copy.copy(other)\n        result._heading = result.heading.set_primary_key(\n            other.primary_key\n            + [k for k in self.primary_key if k not in other.primary_key]\n        )\n        return result\n\n    def __mul__(self, other):\n\"\"\"shorthand for join\"\"\"\n        return self.join(other)\n\n    def aggr(self, group, **named_attributes):\n\"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if named_attributes.get(\"keep_all_rows\", False):\n            raise DataJointError(\n                \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n            )\n        return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n            **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U.join", "title": "<code>join(other, left=False)</code>", "text": "<p>Joining U with a query expression has the effect of promoting the attributes of U to the primary key of the other query expression.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>the other query expression to join with.</p> required <code>left</code> <p>ignored. dj.U always acts as if left=False</p> <code>False</code> <p>Returns:</p> Type Description <p>a copy of the other query expression with the primary key extended.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, left=False):\n\"\"\"\n    Joining U with a query expression has the effect of promoting the attributes of U to\n    the primary key of the other query expression.\n\n    :param other: the other query expression to join with.\n    :param left: ignored. dj.U always acts as if left=False\n    :return: a copy of the other query expression with the primary key extended.\n    \"\"\"\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate if a class\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"Set U can only be joined with a QueryExpression.\")\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found\"\n            % next(k for k in self.primary_key if k not in other.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    result = copy.copy(other)\n    result._heading = result.heading.set_primary_key(\n        other.primary_key\n        + [k for k in self.primary_key if k not in other.primary_key]\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/expression/#datajoint.expression.U.aggr", "title": "<code>aggr(group, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, **named_attributes):\n\"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if named_attributes.get(\"keep_all_rows\", False):\n        raise DataJointError(\n            \"Cannot set keep_all_rows=True when aggregating on a universal set.\"\n        )\n    return Aggregation.create(self, group=group, keep_all_rows=False).proj(\n        **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/external/", "title": "external.py", "text": ""}, {"location": "api/datajoint/external/#datajoint.external.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.subfold", "title": "<code>subfold(name, folds)</code>", "text": "<p>subfolding for external storage:   e.g.  subfold('aBCdefg', (2, 3))  --&gt;  ['ab','cde']</p> Source code in <code>datajoint/external.py</code> <pre><code>def subfold(name, folds):\n\"\"\"\n    subfolding for external storage:   e.g.  subfold('aBCdefg', (2, 3))  --&gt;  ['ab','cde']\n    \"\"\"\n    return (\n        (name[: folds[0]].lower(),) + subfold(name[folds[0] :], folds[1:])\n        if folds\n        else ()\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable", "title": "<code>ExternalTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>The table tracking externally stored objects. Declare as ExternalTable(connection, database)</p> Source code in <code>datajoint/external.py</code> <pre><code>class ExternalTable(Table):\n\"\"\"\n    The table tracking externally stored objects.\n    Declare as ExternalTable(connection, database)\n    \"\"\"\n\n    def __init__(self, connection, store, database):\n        self.store = store\n        self.spec = config.get_store_spec(store)\n        self._s3 = None\n        self.database = database\n        self._connection = connection\n        self._heading = Heading(\n            table_info=dict(\n                conn=connection,\n                database=database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n        self._support = [self.full_table_name]\n        if not self.is_declared:\n            self.declare()\n        self._s3 = None\n        if self.spec[\"protocol\"] == \"file\" and not Path(self.spec[\"location\"]).is_dir():\n            raise FileNotFoundError(\n                \"Inaccessible local directory %s\" % self.spec[\"location\"]\n            ) from None\n\n    @property\n    def definition(self):\n        return \"\"\"\n        # external storage tracking\n        hash  : uuid    #  hash of contents (blob), of filename + contents (attach), or relative filepath (filepath)\n        ---\n        size      :bigint unsigned     # size of object in bytes\n        attachment_name=null : varchar(255)  # the filename of an attachment\n        filepath=null : varchar(1000)  # relative filepath or attachment filename\n        contents_hash=null : uuid      # used for the filepath datatype\n        timestamp=CURRENT_TIMESTAMP  :timestamp   # automatic timestamp\n        \"\"\"\n\n    @property\n    def table_name(self):\n        return f\"{EXTERNAL_TABLE_ROOT}_{self.store}\"\n\n    @property\n    def s3(self):\n        if self._s3 is None:\n            self._s3 = s3.Folder(**self.spec)\n        return self._s3\n\n    # - low-level operations - private\n\n    def _make_external_filepath(self, relative_filepath):\n\"\"\"resolve the complete external path based on the relative path\"\"\"\n        # Strip root\n        if self.spec[\"protocol\"] == \"s3\":\n            posix_path = PurePosixPath(PureWindowsPath(self.spec[\"location\"]))\n            location_path = (\n                Path(*posix_path.parts[1:])\n                if len(self.spec[\"location\"]) &gt; 0\n                and any(case in posix_path.parts[0] for case in (\"\\\\\", \":\"))\n                else Path(posix_path)\n            )\n            return PurePosixPath(location_path, relative_filepath)\n        # Preserve root\n        elif self.spec[\"protocol\"] == \"file\":\n            return PurePosixPath(Path(self.spec[\"location\"]), relative_filepath)\n        else:\n            assert False\n\n    def _make_uuid_path(self, uuid, suffix=\"\"):\n\"\"\"create external path based on the uuid hash\"\"\"\n        return self._make_external_filepath(\n            PurePosixPath(\n                self.database,\n                \"/\".join(subfold(uuid.hex, self.spec[\"subfolding\"])),\n                uuid.hex,\n            ).with_suffix(suffix)\n        )\n\n    def _upload_file(self, local_path, external_path, metadata=None):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.fput(local_path, external_path, metadata)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_copy(local_path, external_path, overwrite=True)\n        else:\n            assert False\n\n    def _download_file(self, external_path, download_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.fget(external_path, download_path)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_copy(external_path, download_path)\n        else:\n            assert False\n\n    def _upload_buffer(self, buffer, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.put(external_path, buffer)\n        elif self.spec[\"protocol\"] == \"file\":\n            safe_write(external_path, buffer)\n        else:\n            assert False\n\n    def _download_buffer(self, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            return self.s3.get(external_path)\n        if self.spec[\"protocol\"] == \"file\":\n            return Path(external_path).read_bytes()\n        assert False\n\n    def _remove_external_file(self, external_path):\n        if self.spec[\"protocol\"] == \"s3\":\n            self.s3.remove_object(external_path)\n        elif self.spec[\"protocol\"] == \"file\":\n            try:\n                Path(external_path).unlink()\n            except FileNotFoundError:\n                pass\n\n    def exists(self, external_filepath):\n\"\"\"\n        :return: True if the external file is accessible\n        \"\"\"\n        if self.spec[\"protocol\"] == \"s3\":\n            return self.s3.exists(external_filepath)\n        if self.spec[\"protocol\"] == \"file\":\n            return Path(external_filepath).is_file()\n        assert False\n\n    # --- BLOBS ----\n\n    def put(self, blob):\n\"\"\"\n        put a binary string (blob) in external store\n        \"\"\"\n        uuid = uuid_from_buffer(blob)\n        self._upload_buffer(blob, self._make_uuid_path(uuid))\n        # insert tracking info\n        self.connection.query(\n            \"INSERT INTO {tab} (hash, size) VALUES (%s, {size}) ON DUPLICATE KEY \"\n            \"UPDATE timestamp=CURRENT_TIMESTAMP\".format(\n                tab=self.full_table_name, size=len(blob)\n            ),\n            args=(uuid.bytes,),\n        )\n        return uuid\n\n    def get(self, uuid):\n\"\"\"\n        get an object from external store.\n        \"\"\"\n        if uuid is None:\n            return None\n        # attempt to get object from cache\n        blob = None\n        cache_folder = config.get(\"cache\", None)\n        if cache_folder:\n            try:\n                cache_path = Path(cache_folder, *subfold(uuid.hex, CACHE_SUBFOLDING))\n                cache_file = Path(cache_path, uuid.hex)\n                blob = cache_file.read_bytes()\n            except FileNotFoundError:\n                pass  # not cached\n        # download blob from external store\n        if blob is None:\n            try:\n                blob = self._download_buffer(self._make_uuid_path(uuid))\n            except MissingExternalFile:\n                if not SUPPORT_MIGRATED_BLOBS:\n                    raise\n                # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths\n                relative_filepath, contents_hash = (self &amp; {\"hash\": uuid}).fetch1(\n                    \"filepath\", \"contents_hash\"\n                )\n                if relative_filepath is None:\n                    raise\n                blob = self._download_buffer(\n                    self._make_external_filepath(relative_filepath)\n                )\n            if cache_folder:\n                cache_path.mkdir(parents=True, exist_ok=True)\n                safe_write(cache_path / uuid.hex, blob)\n        return blob\n\n    # --- ATTACHMENTS ---\n\n    def upload_attachment(self, local_path):\n        attachment_name = Path(local_path).name\n        uuid = uuid_from_file(local_path, init_string=attachment_name + \"\\0\")\n        external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n        self._upload_file(local_path, external_path)\n        # insert tracking info\n        self.connection.query(\n\"\"\"\n        INSERT INTO {tab} (hash, size, attachment_name)\n        VALUES (%s, {size}, \"{attachment_name}\")\n        ON DUPLICATE KEY UPDATE timestamp=CURRENT_TIMESTAMP\"\"\".format(\n                tab=self.full_table_name,\n                size=Path(local_path).stat().st_size,\n                attachment_name=attachment_name,\n            ),\n            args=[uuid.bytes],\n        )\n        return uuid\n\n    def get_attachment_name(self, uuid):\n        return (self &amp; {\"hash\": uuid}).fetch1(\"attachment_name\")\n\n    def download_attachment(self, uuid, attachment_name, download_path):\n\"\"\"save attachment from memory buffer into the save_path\"\"\"\n        external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n        self._download_file(external_path, download_path)\n\n    # --- FILEPATH ---\n\n    def upload_filepath(self, local_filepath):\n\"\"\"\n        Raise exception if an external entry already exists with a different contents checksum.\n        Otherwise, copy (with overwrite) file to remote and\n        If an external entry exists with the same checksum, then no copying should occur\n        \"\"\"\n        local_filepath = Path(local_filepath)\n        try:\n            relative_filepath = str(\n                local_filepath.relative_to(self.spec[\"stage\"]).as_posix()\n            )\n        except ValueError:\n            raise DataJointError(\n                \"The path {path} is not in stage {stage}\".format(\n                    path=local_filepath.parent, **self.spec\n                )\n            )\n        uuid = uuid_from_buffer(\n            init_string=relative_filepath\n        )  # hash relative path, not contents\n        contents_hash = uuid_from_file(local_filepath)\n\n        # check if the remote file already exists and verify that it matches\n        check_hash = (self &amp; {\"hash\": uuid}).fetch(\"contents_hash\")\n        if check_hash:\n            # the tracking entry exists, check that it's the same file as before\n            if contents_hash != check_hash[0]:\n                raise DataJointError(\n                    f\"A different version of '{relative_filepath}' has already been placed.\"\n                )\n        else:\n            # upload the file and create its tracking entry\n            self._upload_file(\n                local_filepath,\n                self._make_external_filepath(relative_filepath),\n                metadata={\"contents_hash\": str(contents_hash)},\n            )\n            self.connection.query(\n                \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES (%s, {size}, '{filepath}', %s)\".format(\n                    tab=self.full_table_name,\n                    size=Path(local_filepath).stat().st_size,\n                    filepath=relative_filepath,\n                ),\n                args=(uuid.bytes, contents_hash.bytes),\n            )\n        return uuid\n\n    def download_filepath(self, filepath_hash):\n\"\"\"\n        sync a file from external store to the local stage\n\n        :param filepath_hash: The hash (UUID) of the relative_path\n        :return: hash (UUID) of the contents of the downloaded file or Nones\n        \"\"\"\n\n        def _need_checksum(local_filepath, expected_size):\n            limit = config.get(\"filepath_checksum_size_limit\")\n            actual_size = Path(local_filepath).stat().st_size\n            if expected_size != actual_size:\n                # this should never happen without outside interference\n                raise DataJointError(\n                    f\"'{local_filepath}' downloaded but size did not match.\"\n                )\n            return limit is None or actual_size &lt; limit\n\n        if filepath_hash is not None:\n            relative_filepath, contents_hash, size = (\n                self &amp; {\"hash\": filepath_hash}\n            ).fetch1(\"filepath\", \"contents_hash\", \"size\")\n            external_path = self._make_external_filepath(relative_filepath)\n            local_filepath = Path(self.spec[\"stage\"]).absolute() / relative_filepath\n\n            file_exists = Path(local_filepath).is_file() and (\n                not _need_checksum(local_filepath, size)\n                or uuid_from_file(local_filepath) == contents_hash\n            )\n\n            if not file_exists:\n                self._download_file(external_path, local_filepath)\n                if (\n                    _need_checksum(local_filepath, size)\n                    and uuid_from_file(local_filepath) != contents_hash\n                ):\n                    # this should never happen without outside interference\n                    raise DataJointError(\n                        f\"'{local_filepath}' downloaded but did not pass checksum.\"\n                    )\n            if not _need_checksum(local_filepath, size):\n                logger.warning(\n                    f\"Skipped checksum for file with hash: {contents_hash}, and path: {local_filepath}\"\n                )\n            return str(local_filepath), contents_hash\n\n    # --- UTILITIES ---\n\n    @property\n    def references(self):\n\"\"\"\n        :return: generator of referencing table names and their referencing columns\n        \"\"\"\n        return (\n            {k.lower(): v for k, v in elem.items()}\n            for elem in self.connection.query(\n\"\"\"\n        SELECT concat('`', table_schema, '`.`', table_name, '`') as referencing_table, column_name\n        FROM information_schema.key_column_usage\n        WHERE referenced_table_name=\"{tab}\" and referenced_table_schema=\"{db}\"\n        \"\"\".format(\n                    tab=self.table_name, db=self.database\n                ),\n                as_dict=True,\n            )\n        )\n\n    def fetch_external_paths(self, **fetch_kwargs):\n\"\"\"\n        generate complete external filepaths from the query.\n        Each element is a tuple: (uuid, path)\n\n        :param fetch_kwargs: keyword arguments to pass to fetch\n        \"\"\"\n        fetch_kwargs.update(as_dict=True)\n        paths = []\n        for item in self.fetch(\"hash\", \"attachment_name\", \"filepath\", **fetch_kwargs):\n            if item[\"attachment_name\"]:\n                # attachments\n                path = self._make_uuid_path(item[\"hash\"], \".\" + item[\"attachment_name\"])\n            elif item[\"filepath\"]:\n                # external filepaths\n                path = self._make_external_filepath(item[\"filepath\"])\n            else:\n                # blobs\n                path = self._make_uuid_path(item[\"hash\"])\n            paths.append((item[\"hash\"], path))\n        return paths\n\n    def unused(self):\n\"\"\"\n        query expression for unused hashes\n\n        :return: self restricted to elements that are not in use by any tables in the schema\n        \"\"\"\n        return self - [\n            FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n                hash=ref[\"column_name\"]\n            )\n            for ref in self.references\n        ]\n\n    def used(self):\n\"\"\"\n        query expression for used hashes\n\n        :return: self restricted to elements that in use by tables in the schema\n        \"\"\"\n        return self &amp; [\n            FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n                hash=ref[\"column_name\"]\n            )\n            for ref in self.references\n        ]\n\n    def delete(\n        self,\n        *,\n        delete_external_files=None,\n        limit=None,\n        display_progress=True,\n        errors_as_string=True,\n    ):\n\"\"\"\n\n        :param delete_external_files: True or False. If False, only the tracking info is removed from the external\n                store table but the external files remain intact. If True, then the external files themselves are deleted too.\n        :param errors_as_string: If True any errors returned when deleting from external files will be strings\n        :param limit: (integer) limit the number of items to delete\n        :param display_progress: if True, display progress as files are cleaned up\n        :return: if deleting external files, returns errors\n        \"\"\"\n        if delete_external_files not in (True, False):\n            raise DataJointError(\n                \"The delete_external_files argument must be set to either \"\n                \"True or False in delete()\"\n            )\n\n        if not delete_external_files:\n            self.unused().delete_quick()\n        else:\n            items = self.unused().fetch_external_paths(limit=limit)\n            if display_progress:\n                items = tqdm(items)\n            # delete items one by one, close to transaction-safe\n            error_list = []\n            for uuid, external_path in items:\n                row = (self &amp; {\"hash\": uuid}).fetch()\n                if row.size:\n                    try:\n                        (self &amp; {\"hash\": uuid}).delete_quick()\n                    except Exception:\n                        pass  # if delete failed, do not remove the external file\n                    else:\n                        try:\n                            self._remove_external_file(external_path)\n                        except Exception as error:\n                            # adding row back into table after failed delete\n                            self.insert1(row[0], skip_duplicates=True)\n                            error_list.append(\n                                (\n                                    uuid,\n                                    external_path,\n                                    str(error) if errors_as_string else error,\n                                )\n                            )\n            return error_list\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.exists", "title": "<code>exists(external_filepath)</code>", "text": "<p>Returns:</p> Type Description <p>True if the external file is accessible</p> Source code in <code>datajoint/external.py</code> <pre><code>def exists(self, external_filepath):\n\"\"\"\n    :return: True if the external file is accessible\n    \"\"\"\n    if self.spec[\"protocol\"] == \"s3\":\n        return self.s3.exists(external_filepath)\n    if self.spec[\"protocol\"] == \"file\":\n        return Path(external_filepath).is_file()\n    assert False\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.put", "title": "<code>put(blob)</code>", "text": "<p>put a binary string (blob) in external store</p> Source code in <code>datajoint/external.py</code> <pre><code>def put(self, blob):\n\"\"\"\n    put a binary string (blob) in external store\n    \"\"\"\n    uuid = uuid_from_buffer(blob)\n    self._upload_buffer(blob, self._make_uuid_path(uuid))\n    # insert tracking info\n    self.connection.query(\n        \"INSERT INTO {tab} (hash, size) VALUES (%s, {size}) ON DUPLICATE KEY \"\n        \"UPDATE timestamp=CURRENT_TIMESTAMP\".format(\n            tab=self.full_table_name, size=len(blob)\n        ),\n        args=(uuid.bytes,),\n    )\n    return uuid\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.get", "title": "<code>get(uuid)</code>", "text": "<p>get an object from external store.</p> Source code in <code>datajoint/external.py</code> <pre><code>def get(self, uuid):\n\"\"\"\n    get an object from external store.\n    \"\"\"\n    if uuid is None:\n        return None\n    # attempt to get object from cache\n    blob = None\n    cache_folder = config.get(\"cache\", None)\n    if cache_folder:\n        try:\n            cache_path = Path(cache_folder, *subfold(uuid.hex, CACHE_SUBFOLDING))\n            cache_file = Path(cache_path, uuid.hex)\n            blob = cache_file.read_bytes()\n        except FileNotFoundError:\n            pass  # not cached\n    # download blob from external store\n    if blob is None:\n        try:\n            blob = self._download_buffer(self._make_uuid_path(uuid))\n        except MissingExternalFile:\n            if not SUPPORT_MIGRATED_BLOBS:\n                raise\n            # blobs migrated from datajoint 0.11 are stored at explicitly defined filepaths\n            relative_filepath, contents_hash = (self &amp; {\"hash\": uuid}).fetch1(\n                \"filepath\", \"contents_hash\"\n            )\n            if relative_filepath is None:\n                raise\n            blob = self._download_buffer(\n                self._make_external_filepath(relative_filepath)\n            )\n        if cache_folder:\n            cache_path.mkdir(parents=True, exist_ok=True)\n            safe_write(cache_path / uuid.hex, blob)\n    return blob\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_attachment", "title": "<code>download_attachment(uuid, attachment_name, download_path)</code>", "text": "<p>save attachment from memory buffer into the save_path</p> Source code in <code>datajoint/external.py</code> <pre><code>def download_attachment(self, uuid, attachment_name, download_path):\n\"\"\"save attachment from memory buffer into the save_path\"\"\"\n    external_path = self._make_uuid_path(uuid, \".\" + attachment_name)\n    self._download_file(external_path, download_path)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.upload_filepath", "title": "<code>upload_filepath(local_filepath)</code>", "text": "<p>Raise exception if an external entry already exists with a different contents checksum. Otherwise, copy (with overwrite) file to remote and If an external entry exists with the same checksum, then no copying should occur</p> Source code in <code>datajoint/external.py</code> <pre><code>def upload_filepath(self, local_filepath):\n\"\"\"\n    Raise exception if an external entry already exists with a different contents checksum.\n    Otherwise, copy (with overwrite) file to remote and\n    If an external entry exists with the same checksum, then no copying should occur\n    \"\"\"\n    local_filepath = Path(local_filepath)\n    try:\n        relative_filepath = str(\n            local_filepath.relative_to(self.spec[\"stage\"]).as_posix()\n        )\n    except ValueError:\n        raise DataJointError(\n            \"The path {path} is not in stage {stage}\".format(\n                path=local_filepath.parent, **self.spec\n            )\n        )\n    uuid = uuid_from_buffer(\n        init_string=relative_filepath\n    )  # hash relative path, not contents\n    contents_hash = uuid_from_file(local_filepath)\n\n    # check if the remote file already exists and verify that it matches\n    check_hash = (self &amp; {\"hash\": uuid}).fetch(\"contents_hash\")\n    if check_hash:\n        # the tracking entry exists, check that it's the same file as before\n        if contents_hash != check_hash[0]:\n            raise DataJointError(\n                f\"A different version of '{relative_filepath}' has already been placed.\"\n            )\n    else:\n        # upload the file and create its tracking entry\n        self._upload_file(\n            local_filepath,\n            self._make_external_filepath(relative_filepath),\n            metadata={\"contents_hash\": str(contents_hash)},\n        )\n        self.connection.query(\n            \"INSERT INTO {tab} (hash, size, filepath, contents_hash) VALUES (%s, {size}, '{filepath}', %s)\".format(\n                tab=self.full_table_name,\n                size=Path(local_filepath).stat().st_size,\n                filepath=relative_filepath,\n            ),\n            args=(uuid.bytes, contents_hash.bytes),\n        )\n    return uuid\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.download_filepath", "title": "<code>download_filepath(filepath_hash)</code>", "text": "<p>sync a file from external store to the local stage</p> <p>Parameters:</p> Name Type Description Default <code>filepath_hash</code> <p>The hash (UUID) of the relative_path</p> required <p>Returns:</p> Type Description <p>hash (UUID) of the contents of the downloaded file or Nones</p> Source code in <code>datajoint/external.py</code> <pre><code>def download_filepath(self, filepath_hash):\n\"\"\"\n    sync a file from external store to the local stage\n\n    :param filepath_hash: The hash (UUID) of the relative_path\n    :return: hash (UUID) of the contents of the downloaded file or Nones\n    \"\"\"\n\n    def _need_checksum(local_filepath, expected_size):\n        limit = config.get(\"filepath_checksum_size_limit\")\n        actual_size = Path(local_filepath).stat().st_size\n        if expected_size != actual_size:\n            # this should never happen without outside interference\n            raise DataJointError(\n                f\"'{local_filepath}' downloaded but size did not match.\"\n            )\n        return limit is None or actual_size &lt; limit\n\n    if filepath_hash is not None:\n        relative_filepath, contents_hash, size = (\n            self &amp; {\"hash\": filepath_hash}\n        ).fetch1(\"filepath\", \"contents_hash\", \"size\")\n        external_path = self._make_external_filepath(relative_filepath)\n        local_filepath = Path(self.spec[\"stage\"]).absolute() / relative_filepath\n\n        file_exists = Path(local_filepath).is_file() and (\n            not _need_checksum(local_filepath, size)\n            or uuid_from_file(local_filepath) == contents_hash\n        )\n\n        if not file_exists:\n            self._download_file(external_path, local_filepath)\n            if (\n                _need_checksum(local_filepath, size)\n                and uuid_from_file(local_filepath) != contents_hash\n            ):\n                # this should never happen without outside interference\n                raise DataJointError(\n                    f\"'{local_filepath}' downloaded but did not pass checksum.\"\n                )\n        if not _need_checksum(local_filepath, size):\n            logger.warning(\n                f\"Skipped checksum for file with hash: {contents_hash}, and path: {local_filepath}\"\n            )\n        return str(local_filepath), contents_hash\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.references", "title": "<code>references</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>generator of referencing table names and their referencing columns</p>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.fetch_external_paths", "title": "<code>fetch_external_paths(**fetch_kwargs)</code>", "text": "<p>generate complete external filepaths from the query. Each element is a tuple: (uuid, path)</p> <p>Parameters:</p> Name Type Description Default <code>fetch_kwargs</code> <p>keyword arguments to pass to fetch</p> <code>{}</code> Source code in <code>datajoint/external.py</code> <pre><code>def fetch_external_paths(self, **fetch_kwargs):\n\"\"\"\n    generate complete external filepaths from the query.\n    Each element is a tuple: (uuid, path)\n\n    :param fetch_kwargs: keyword arguments to pass to fetch\n    \"\"\"\n    fetch_kwargs.update(as_dict=True)\n    paths = []\n    for item in self.fetch(\"hash\", \"attachment_name\", \"filepath\", **fetch_kwargs):\n        if item[\"attachment_name\"]:\n            # attachments\n            path = self._make_uuid_path(item[\"hash\"], \".\" + item[\"attachment_name\"])\n        elif item[\"filepath\"]:\n            # external filepaths\n            path = self._make_external_filepath(item[\"filepath\"])\n        else:\n            # blobs\n            path = self._make_uuid_path(item[\"hash\"])\n        paths.append((item[\"hash\"], path))\n    return paths\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.unused", "title": "<code>unused()</code>", "text": "<p>query expression for unused hashes</p> <p>Returns:</p> Type Description <p>self restricted to elements that are not in use by any tables in the schema</p> Source code in <code>datajoint/external.py</code> <pre><code>def unused(self):\n\"\"\"\n    query expression for unused hashes\n\n    :return: self restricted to elements that are not in use by any tables in the schema\n    \"\"\"\n    return self - [\n        FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n            hash=ref[\"column_name\"]\n        )\n        for ref in self.references\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.used", "title": "<code>used()</code>", "text": "<p>query expression for used hashes</p> <p>Returns:</p> Type Description <p>self restricted to elements that in use by tables in the schema</p> Source code in <code>datajoint/external.py</code> <pre><code>def used(self):\n\"\"\"\n    query expression for used hashes\n\n    :return: self restricted to elements that in use by tables in the schema\n    \"\"\"\n    return self &amp; [\n        FreeTable(self.connection, ref[\"referencing_table\"]).proj(\n            hash=ref[\"column_name\"]\n        )\n        for ref in self.references\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalTable.delete", "title": "<code>delete(*, delete_external_files=None, limit=None, display_progress=True, errors_as_string=True)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>delete_external_files</code> <p>True or False. If False, only the tracking info is removed from the external store table but the external files remain intact. If True, then the external files themselves are deleted too.</p> <code>None</code> <code>errors_as_string</code> <p>If True any errors returned when deleting from external files will be strings</p> <code>True</code> <code>limit</code> <p>(integer) limit the number of items to delete</p> <code>None</code> <code>display_progress</code> <p>if True, display progress as files are cleaned up</p> <code>True</code> <p>Returns:</p> Type Description <p>if deleting external files, returns errors</p> Source code in <code>datajoint/external.py</code> <pre><code>def delete(\n    self,\n    *,\n    delete_external_files=None,\n    limit=None,\n    display_progress=True,\n    errors_as_string=True,\n):\n\"\"\"\n\n    :param delete_external_files: True or False. If False, only the tracking info is removed from the external\n            store table but the external files remain intact. If True, then the external files themselves are deleted too.\n    :param errors_as_string: If True any errors returned when deleting from external files will be strings\n    :param limit: (integer) limit the number of items to delete\n    :param display_progress: if True, display progress as files are cleaned up\n    :return: if deleting external files, returns errors\n    \"\"\"\n    if delete_external_files not in (True, False):\n        raise DataJointError(\n            \"The delete_external_files argument must be set to either \"\n            \"True or False in delete()\"\n        )\n\n    if not delete_external_files:\n        self.unused().delete_quick()\n    else:\n        items = self.unused().fetch_external_paths(limit=limit)\n        if display_progress:\n            items = tqdm(items)\n        # delete items one by one, close to transaction-safe\n        error_list = []\n        for uuid, external_path in items:\n            row = (self &amp; {\"hash\": uuid}).fetch()\n            if row.size:\n                try:\n                    (self &amp; {\"hash\": uuid}).delete_quick()\n                except Exception:\n                    pass  # if delete failed, do not remove the external file\n                else:\n                    try:\n                        self._remove_external_file(external_path)\n                    except Exception as error:\n                        # adding row back into table after failed delete\n                        self.insert1(row[0], skip_duplicates=True)\n                        error_list.append(\n                            (\n                                uuid,\n                                external_path,\n                                str(error) if errors_as_string else error,\n                            )\n                        )\n        return error_list\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/external/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/external/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/external/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n\"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n\"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return self._attributes\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n\"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n\"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n\"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n\"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n\"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n\"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=attr[\"type\"].split(\"@\")[1]\n                    if category in EXTERNAL_TYPES\n                    else None,\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n\"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n\"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n\"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n\"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        \"`%s`\" % name\n        if self.attributes[name].attribute_expression is None\n        else self.attributes[name].attribute_expression\n        + (\" as `%s`\" % name if include_aliases else \"\")\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n\"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n\"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n\"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.safe_write", "title": "<code>safe_write(filepath, blob)</code>", "text": "<p>A two-step write.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>full path</p> required <code>blob</code> <p>binary data</p> required Source code in <code>datajoint/utils.py</code> <pre><code>def safe_write(filepath, blob):\n\"\"\"\n    A two-step write.\n\n    :param filename: full path\n    :param blob: binary data\n    \"\"\"\n    filepath = Path(filepath)\n    if not filepath.is_file():\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = filepath.with_suffix(filepath.suffix + \".saving\")\n        temp_file.write_bytes(blob)\n        temp_file.rename(filepath)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.MissingExternalFile", "title": "<code>MissingExternalFile</code>", "text": "<p>         Bases: <code>DataJointError</code></p> <p>Error raised when an external file managed by DataJoint is no longer accessible</p> Source code in <code>datajoint/errors.py</code> <pre><code>class MissingExternalFile(DataJointError):\n\"\"\"\n    Error raised when an external file managed by DataJoint is no longer accessible\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.safe_copy", "title": "<code>safe_copy(src, dest, overwrite=False)</code>", "text": "<p>Copy the contents of src file into dest file as a two-step process. Skip if dest exists already</p> Source code in <code>datajoint/utils.py</code> <pre><code>def safe_copy(src, dest, overwrite=False):\n\"\"\"\n    Copy the contents of src file into dest file as a two-step process. Skip if dest exists already\n    \"\"\"\n    src, dest = Path(src), Path(dest)\n    if not (dest.exists() and src.samefile(dest)) and (overwrite or not dest.is_file()):\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = dest.with_suffix(dest.suffix + \".copying\")\n        shutil.copyfile(str(src), str(temp_file))\n        temp_file.rename(dest)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.ExternalMapping", "title": "<code>ExternalMapping</code>", "text": "<p>         Bases: <code>Mapping</code></p> <p>The external manager contains all the tables for all external stores for a given schema :Example:     e = ExternalMapping(schema)     external_table = e[store]</p> Source code in <code>datajoint/external.py</code> <pre><code>class ExternalMapping(Mapping):\n\"\"\"\n    The external manager contains all the tables for all external stores for a given schema\n    :Example:\n        e = ExternalMapping(schema)\n        external_table = e[store]\n    \"\"\"\n\n    def __init__(self, schema):\n        self.schema = schema\n        self._tables = {}\n\n    def __repr__(self):\n        return \"External file tables for schema `{schema}`:\\n    \".format(\n            schema=self.schema.database\n        ) + \"\\n    \".join(\n            '\"{store}\" {protocol}:{location}'.format(store=k, **v.spec)\n            for k, v in self.items()\n        )\n\n    def __getitem__(self, store):\n\"\"\"\n        Triggers the creation of an external table.\n        Should only be used when ready to save or read from external storage.\n\n        :param store: the name of the store\n        :return: the ExternalTable object for the store\n        \"\"\"\n        if store not in self._tables:\n            self._tables[store] = ExternalTable(\n                connection=self.schema.connection,\n                store=store,\n                database=self.schema.database,\n            )\n        return self._tables[store]\n\n    def __len__(self):\n        return len(self._tables)\n\n    def __iter__(self):\n        return iter(self._tables)\n</code></pre>"}, {"location": "api/datajoint/external/#datajoint.external.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n\"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/fetch/", "title": "fetch.py", "text": ""}, {"location": "api/datajoint/fetch/#datajoint.fetch.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.key", "title": "<code>key</code>", "text": "<p>object that allows requesting the primary key as an argument in expression.fetch() The string \"KEY\" can be used instead of the class key</p> Source code in <code>datajoint/fetch.py</code> <pre><code>class key:\n\"\"\"\n    object that allows requesting the primary key as an argument in expression.fetch()\n    The string \"KEY\" can be used instead of the class key\n    \"\"\"\n\n    pass\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.to_dicts", "title": "<code>to_dicts(recarray)</code>", "text": "<p>convert record array to a dictionaries</p> Source code in <code>datajoint/fetch.py</code> <pre><code>def to_dicts(recarray):\n\"\"\"convert record array to a dictionaries\"\"\"\n    for rec in recarray:\n        yield dict(zip(recarray.dtype.names, rec.tolist()))\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.safe_write", "title": "<code>safe_write(filepath, blob)</code>", "text": "<p>A two-step write.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>full path</p> required <code>blob</code> <p>binary data</p> required Source code in <code>datajoint/utils.py</code> <pre><code>def safe_write(filepath, blob):\n\"\"\"\n    A two-step write.\n\n    :param filename: full path\n    :param blob: binary data\n    \"\"\"\n    filepath = Path(filepath)\n    if not filepath.is_file():\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = filepath.with_suffix(filepath.suffix + \".saving\")\n        temp_file.write_bytes(blob)\n        temp_file.rename(filepath)\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch", "title": "<code>Fetch</code>", "text": "<p>A fetch object that handles retrieving elements from the table expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>the QueryExpression object to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch:\n\"\"\"\n    A fetch object that handles retrieving elements from the table expression.\n\n    :param expression: the QueryExpression object to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(\n        self,\n        *attrs,\n        offset=None,\n        limit=None,\n        order_by=None,\n        format=None,\n        as_dict=None,\n        squeeze=False,\n        download_path=\".\"\n    ):\n\"\"\"\n        Fetches the expression results from the database into an np.array or list of dictionaries and\n        unpacks blob attributes.\n\n        :param attrs: zero or more attributes to fetch. If not provided, the call will return all attributes of this\n                        table. If provided, returns tuples with an entry for each attribute.\n        :param offset: the number of tuples to skip in the returned result\n        :param limit: the maximum number of tuples to return\n        :param order_by: a single attribute or the list of attributes to order the results. No ordering should be assumed\n                        if order_by=None. To reverse the order, add DESC to the attribute name or names: e.g. (\"age DESC\",\n                        \"frequency\") To order by primary key, use \"KEY\" or \"KEY DESC\"\n        :param format: Effective when as_dict=None and when attrs is empty None: default from config['fetch_format'] or\n                        'array' if not configured \"array\": use numpy.key_array \"frame\": output pandas.DataFrame. .\n        :param as_dict: returns a list of dictionaries instead of a record array. Defaults to False for .fetch() and to\n                        True for .fetch('KEY')\n        :param squeeze:  if True, remove extra dimensions from arrays\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the contents of the table in the form of a structured numpy.array or a dict list\n        \"\"\"\n        if order_by is not None:\n            # if 'order_by' passed in a string, make into list\n            if isinstance(order_by, str):\n                order_by = [order_by]\n            # expand \"KEY\" or \"KEY DESC\"\n            order_by = list(\n                _flatten_attribute_list(self._expression.primary_key, order_by)\n            )\n\n        attrs_as_dict = as_dict and attrs\n        if attrs_as_dict:\n            # absorb KEY into attrs and prepare to return attributes as dict (issue #595)\n            if any(is_key(k) for k in attrs):\n                attrs = list(self._expression.primary_key) + [\n                    a for a in attrs if a not in self._expression.primary_key\n                ]\n        if as_dict is None:\n            as_dict = bool(attrs)  # default to True for \"KEY\" and False otherwise\n        # format should not be specified with attrs or is_dict=True\n        if format is not None and (as_dict or attrs):\n            raise DataJointError(\n                \"Cannot specify output format when as_dict=True or \"\n                \"when attributes are selected to be fetched separately.\"\n            )\n        if format not in {None, \"array\", \"frame\"}:\n            raise DataJointError(\n                \"Fetch output format must be in \"\n                '{{\"array\", \"frame\"}} but \"{}\" was given'.format(format)\n            )\n\n        if not (attrs or as_dict) and format is None:\n            format = config[\"fetch_format\"]  # default to array\n            if format not in {\"array\", \"frame\"}:\n                raise DataJointError(\n                    'Invalid entry \"{}\" in datajoint.config[\"fetch_format\"]: '\n                    'use \"array\" or \"frame\"'.format(format)\n                )\n\n        if limit is None and offset is not None:\n            logger.warning(\n                \"Offset set, but no limit. Setting limit to a large number. \"\n                \"Consider setting a limit explicitly.\"\n            )\n            limit = 8000000000  # just a very large number to effect no limit\n\n        get = partial(\n            _get,\n            self._expression.connection,\n            squeeze=squeeze,\n            download_path=download_path,\n        )\n        if attrs:  # a list of attributes provided\n            attributes = [a for a in attrs if not is_key(a)]\n            ret = self._expression.proj(*attributes)\n            ret = ret.fetch(\n                offset=offset,\n                limit=limit,\n                order_by=order_by,\n                as_dict=False,\n                squeeze=squeeze,\n                download_path=download_path,\n                format=\"array\",\n            )\n            if attrs_as_dict:\n                ret = [\n                    {k: v for k, v in zip(ret.dtype.names, x) if k in attrs}\n                    for x in ret\n                ]\n            else:\n                return_values = [\n                    list(\n                        (to_dicts if as_dict else lambda x: x)(\n                            ret[self._expression.primary_key]\n                        )\n                    )\n                    if is_key(attribute)\n                    else ret[attribute]\n                    for attribute in attrs\n                ]\n                ret = return_values[0] if len(attrs) == 1 else return_values\n        else:  # fetch all attributes as a numpy.record_array or pandas.DataFrame\n            cur = self._expression.cursor(\n                as_dict=as_dict, limit=limit, offset=offset, order_by=order_by\n            )\n            heading = self._expression.heading\n            if as_dict:\n                ret = [\n                    dict((name, get(heading[name], d[name])) for name in heading.names)\n                    for d in cur\n                ]\n            else:\n                ret = list(cur.fetchall())\n                record_type = (\n                    heading.as_dtype\n                    if not ret\n                    else np.dtype(\n                        [\n                            (\n                                name,\n                                type(value),\n                            )  # use the first element to determine blob type\n                            if heading[name].is_blob\n                            and isinstance(value, numbers.Number)\n                            else (name, heading.as_dtype[name])\n                            for value, name in zip(ret[0], heading.as_dtype.names)\n                        ]\n                    )\n                )\n                try:\n                    ret = np.array(ret, dtype=record_type)\n                except Exception as e:\n                    raise e\n                for name in heading:\n                    # unpack blobs and externals\n                    ret[name] = list(map(partial(get, heading[name]), ret[name]))\n                if format == \"frame\":\n                    ret = pandas.DataFrame(ret).set_index(heading.primary_key)\n        return ret\n</code></pre>"}, {"location": "api/datajoint/fetch/#datajoint.fetch.Fetch1", "title": "<code>Fetch1</code>", "text": "<p>Fetch object for fetching the result of a query yielding one row.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <p>a query expression to fetch from.</p> required Source code in <code>datajoint/fetch.py</code> <pre><code>class Fetch1:\n\"\"\"\n    Fetch object for fetching the result of a query yielding one row.\n\n    :param expression: a query expression to fetch from.\n    \"\"\"\n\n    def __init__(self, expression):\n        self._expression = expression\n\n    def __call__(self, *attrs, squeeze=False, download_path=\".\"):\n\"\"\"\n        Fetches the result of a query expression that yields one entry.\n\n        If no attributes are specified, returns the result as a dict.\n        If attributes are specified returns the corresponding results as a tuple.\n\n        Examples:\n        d = rel.fetch1()   # as a dictionary\n        a, b = rel.fetch1('a', 'b')   # as a tuple\n\n        :params *attrs: attributes to return when expanding into a tuple.\n                 If attrs is empty, the return result is a dict\n        :param squeeze:  When true, remove extra dimensions from arrays in attributes\n        :param download_path: for fetches that download data, e.g. attachments\n        :return: the one tuple in the table in the form of a dict\n        \"\"\"\n        heading = self._expression.heading\n\n        if not attrs:  # fetch all attributes, return as ordered dict\n            cur = self._expression.cursor(as_dict=True)\n            ret = cur.fetchone()\n            if not ret or cur.fetchone():\n                raise DataJointError(\n                    \"fetch1 requires exactly one tuple in the input set.\"\n                )\n            ret = dict(\n                (\n                    name,\n                    _get(\n                        self._expression.connection,\n                        heading[name],\n                        ret[name],\n                        squeeze=squeeze,\n                        download_path=download_path,\n                    ),\n                )\n                for name in heading.names\n            )\n        else:  # fetch some attributes, return as tuple\n            attributes = [a for a in attrs if not is_key(a)]\n            result = self._expression.proj(*attributes).fetch(\n                squeeze=squeeze, download_path=download_path, format=\"array\"\n            )\n            if len(result) != 1:\n                raise DataJointError(\n                    \"fetch1 should only return one tuple. %d tuples found\" % len(result)\n                )\n            return_values = tuple(\n                next(to_dicts(result[self._expression.primary_key]))\n                if is_key(attribute)\n                else result[attribute][0]\n                for attribute in attrs\n            )\n            ret = return_values[0] if len(attrs) == 1 else return_values\n        return ret\n</code></pre>"}, {"location": "api/datajoint/hash/", "title": "hash.py", "text": ""}, {"location": "api/datajoint/hash/#datajoint.hash.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n\"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/hash/#datajoint.hash.uuid_from_stream", "title": "<code>uuid_from_stream(stream, *, init_string='')</code>", "text": "<p>:stream: stream object or open file handle :init_string: string to initialize the checksum</p> <p>Returns:</p> Type Description <p>16-byte digest of stream data</p> Source code in <code>datajoint/hash.py</code> <pre><code>def uuid_from_stream(stream, *, init_string=\"\"):\n\"\"\"\n    :return: 16-byte digest of stream data\n    :stream: stream object or open file handle\n    :init_string: string to initialize the checksum\n    \"\"\"\n    hashed = hashlib.md5(init_string.encode())\n    chunk = True\n    chunk_size = 1 &lt;&lt; 14\n    while chunk:\n        chunk = stream.read(chunk_size)\n        hashed.update(chunk)\n    return uuid.UUID(bytes=hashed.digest())\n</code></pre>"}, {"location": "api/datajoint/heading/", "title": "heading.py", "text": ""}, {"location": "api/datajoint/heading/#datajoint.heading.AttributeAdapter", "title": "<code>AttributeAdapter</code>", "text": "<p>Base class for adapter objects for user-defined attribute types.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>class AttributeAdapter:\n\"\"\"\n    Base class for adapter objects for user-defined attribute types.\n    \"\"\"\n\n    @property\n    def attribute_type(self):\n\"\"\"\n        :return: a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def get(self, value):\n\"\"\"\n        convert value retrieved from the the attribute in a table into the adapted type\n\n        :param value: value from the database\n\n        :return: object of the adapted type\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n\n    def put(self, obj):\n\"\"\"\n        convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n        :param obj: an object of the adapted type\n        :return: value to store in the database\n        \"\"\"\n        raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.attribute_adapter.AttributeAdapter.attribute_type", "title": "<code>attribute_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a supported DataJoint attribute type to use; e.g. \"longblob\", \"blob@store\"</p>"}, {"location": "api/datajoint/heading/#datajoint.attribute_adapter.AttributeAdapter.get", "title": "<code>get(value)</code>", "text": "<p>convert value retrieved from the the attribute in a table into the adapted type</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>value from the database</p> required <p>Returns:</p> Type Description <p>object of the adapted type</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get(self, value):\n\"\"\"\n    convert value retrieved from the the attribute in a table into the adapted type\n\n    :param value: value from the database\n\n    :return: object of the adapted type\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.attribute_adapter.AttributeAdapter.put", "title": "<code>put(obj)</code>", "text": "<p>convert an object of the adapted type into a value that DataJoint can store in a table attribute</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>an object of the adapted type</p> required <p>Returns:</p> Type Description <p>value to store in the database</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def put(self, obj):\n\"\"\"\n    convert an object of the adapted type into a value that DataJoint can store in a table attribute\n\n    :param obj: an object of the adapted type\n    :return: value to store in the database\n    \"\"\"\n    raise NotImplementedError(\"Undefined attribute adapter\")\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.get_adapter", "title": "<code>get_adapter(context, adapter_name)</code>", "text": "<p>Extract the AttributeAdapter object by its name from the context and validate.</p> Source code in <code>datajoint/attribute_adapter.py</code> <pre><code>def get_adapter(context, adapter_name):\n\"\"\"\n    Extract the AttributeAdapter object by its name from the context and validate.\n    \"\"\"\n    if not _support_adapted_types():\n        raise DataJointError(\"Support for Adapted Attribute types is disabled.\")\n    adapter_name = adapter_name.lstrip(\"&lt;\").rstrip(\"&gt;\")\n    try:\n        adapter = (\n            context[adapter_name]\n            if adapter_name in context\n            else type_plugins[adapter_name][\"object\"].load()\n        )\n    except KeyError:\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' is not defined.\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter, AttributeAdapter):\n        raise DataJointError(\n            \"Attribute adapter '{adapter_name}' must be an instance of datajoint.AttributeAdapter\".format(\n                adapter_name=adapter_name\n            )\n        )\n    if not isinstance(adapter.attribute_type, str) or not re.match(\n        r\"^\\w\", adapter.attribute_type\n    ):\n        raise DataJointError(\n            \"Invalid attribute type {type} in attribute adapter '{adapter_name}'\".format(\n                type=adapter.attribute_type, adapter_name=adapter_name\n            )\n        )\n    return adapter\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute", "title": "<code>Attribute</code>", "text": "<p>         Bases: <code>namedtuple('_Attribute', default_attribute_properties)</code></p> <p>Properties of a table column (attribute)</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Attribute(namedtuple(\"_Attribute\", default_attribute_properties)):\n\"\"\"\n    Properties of a table column (attribute)\n    \"\"\"\n\n    def todict(self):\n\"\"\"Convert namedtuple to dict.\"\"\"\n        return dict((name, self[i]) for i, name in enumerate(self._fields))\n\n    @property\n    def sql_type(self):\n\"\"\":return: datatype (as string) in database. In most cases, it is the same as self.type\"\"\"\n        return UUID_DATA_TYPE if self.uuid else self.type\n\n    @property\n    def sql_comment(self):\n\"\"\":return: full comment for the SQL declaration. Includes custom type specification\"\"\"\n        return (\":uuid:\" if self.uuid else \"\") + self.comment\n\n    @property\n    def sql(self):\n\"\"\"\n        Convert primary key attribute tuple into its SQL CREATE TABLE clause.\n        Default values are not reflected.\n        This is used for declaring foreign keys in referencing tables\n\n        :return: SQL code for attribute declaration\n        \"\"\"\n        return '`{name}` {type} NOT NULL COMMENT \"{comment}\"'.format(\n            name=self.name, type=self.sql_type, comment=self.sql_comment\n        )\n\n    @property\n    def original_name(self):\n        if self.attribute_expression is None:\n            return self.name\n        assert self.attribute_expression.startswith(\"`\")\n        return self.attribute_expression.strip(\"`\")\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.todict", "title": "<code>todict()</code>", "text": "<p>Convert namedtuple to dict.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def todict(self):\n\"\"\"Convert namedtuple to dict.\"\"\"\n    return dict((name, self[i]) for i, name in enumerate(self._fields))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_type", "title": "<code>sql_type</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>datatype (as string) in database. In most cases, it is the same as self.type</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql_comment", "title": "<code>sql_comment</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full comment for the SQL declaration. Includes custom type specification</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Attribute.sql", "title": "<code>sql</code>  <code>property</code>", "text": "<p>Convert primary key attribute tuple into its SQL CREATE TABLE clause. Default values are not reflected. This is used for declaring foreign keys in referencing tables</p> <p>Returns:</p> Type Description <p>SQL code for attribute declaration</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n\"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n\"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return self._attributes\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n\"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n\"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n\"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n\"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n\"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n\"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=attr[\"type\"].split(\"@\")[1]\n                    if category in EXTERNAL_TYPES\n                    else None,\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n\"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n\"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n\"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n\"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        \"`%s`\" % name\n        if self.attributes[name].attribute_expression is None\n        else self.attributes[name].attribute_expression\n        + (\" as `%s`\" % name if include_aliases else \"\")\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n\"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n\"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/heading/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n\"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/", "title": "jobs.py", "text": ""}, {"location": "api/datajoint/jobs/#datajoint.jobs.key_hash", "title": "<code>key_hash(mapping)</code>", "text": "<p>32-byte hash of the mapping's key values sorted by the key name. This is often used to convert a long primary key value into a shorter hash. For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.</p> Source code in <code>datajoint/hash.py</code> <pre><code>def key_hash(mapping):\n\"\"\"\n    32-byte hash of the mapping's key values sorted by the key name.\n    This is often used to convert a long primary key value into a shorter hash.\n    For example, the JobTable in datajoint.jobs uses this function to hash the primary key of autopopulated tables.\n    \"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(mapping.items()):\n        hashed.update(str(v).encode())\n    return hashed.hexdigest()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable", "title": "<code>JobTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table with no definition. Allows reserving jobs</p> Source code in <code>datajoint/jobs.py</code> <pre><code>class JobTable(Table):\n\"\"\"\n    A base table with no definition. Allows reserving jobs\n    \"\"\"\n\n    def __init__(self, conn, database):\n        self.database = database\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # job reservation table for `{database}`\n        table_name  :varchar(255)  # className of the table\n        key_hash  :char(32)  # key hash\n        ---\n        status  :enum('reserved','error','ignore')  # if tuple is missing, the job is available\n        key=null  :blob  # structure containing the key\n        error_message=\"\"  :varchar({error_message_length})  # error message returned if failed\n        error_stack=null  :mediumblob  # error stack if failed\n        user=\"\" :varchar(255) # database user\n        host=\"\"  :varchar(255)  # system hostname\n        pid=0  :int unsigned  # system process id\n        connection_id = 0  : bigint unsigned          # connection_id()\n        timestamp=CURRENT_TIMESTAMP  :timestamp   # automatic timestamp\n        \"\"\".format(\n            database=database, error_message_length=ERROR_MESSAGE_LENGTH\n        )\n        if not self.is_declared:\n            self.declare()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    @property\n    def table_name(self):\n        return \"~jobs\"\n\n    def delete(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.delete_quick()\n\n    def drop(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.drop_quick()\n\n    def reserve(self, table_name, key):\n\"\"\"\n        Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n        job key, identified by its hash. When jobs are completed, the entry is removed.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :return: True if reserved job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"reserved\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def ignore(self, table_name, key):\n\"\"\"\n        Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n        job key, identified by its hash, with status \"ignore\".\n\n        Args:\n        table_name:\n            Table name (str) - `database`.`table_name`\n        key:\n            The dict of the job's primary key\n\n        Returns:\n            True if ignore job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"ignore\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def complete(self, table_name, key):\n\"\"\"\n        Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        \"\"\"\n        job_key = dict(table_name=table_name, key_hash=key_hash(key))\n        (self &amp; job_key).delete_quick()\n\n    def error(self, table_name, key, error_message, error_stack=None):\n\"\"\"\n        Log an error message.  The job reservation is replaced with an error entry.\n        if an error occurs, leave an entry describing the problem\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :param error_message: string error message\n        :param error_stack: stack trace\n        \"\"\"\n        if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n            error_message = (\n                error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n                + TRUNCATION_APPENDIX\n            )\n        with config(enable_python_native_blobs=True):\n            self.insert1(\n                dict(\n                    table_name=table_name,\n                    key_hash=key_hash(key),\n                    status=\"error\",\n                    host=platform.node(),\n                    pid=os.getpid(),\n                    connection_id=self.connection.connection_id,\n                    user=self._user,\n                    key=key,\n                    error_message=error_message,\n                    error_stack=error_stack,\n                ),\n                replace=True,\n                ignore_extra_fields=True,\n            )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def delete(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.delete_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def drop(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.reserve", "title": "<code>reserve(table_name, key)</code>", "text": "<p>Reserve a job for computation.  When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <p>Returns:</p> Type Description <p>True if reserved job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def reserve(self, table_name, key):\n\"\"\"\n    Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n    job key, identified by its hash. When jobs are completed, the entry is removed.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :return: True if reserved job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"reserved\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.ignore", "title": "<code>ignore(table_name, key)</code>", "text": "<p>Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\".</p> <p>Args: table_name:     Table name (str) - <code>database</code>.<code>table_name</code> key:     The dict of the job's primary key</p> <p>Returns:     True if ignore job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def ignore(self, table_name, key):\n\"\"\"\n    Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n    job key, identified by its hash, with status \"ignore\".\n\n    Args:\n    table_name:\n        Table name (str) - `database`.`table_name`\n    key:\n        The dict of the job's primary key\n\n    Returns:\n        True if ignore job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"ignore\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.complete", "title": "<code>complete(table_name, key)</code>", "text": "<p>Log a completed job.  When a job is completed, its reservation entry is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required Source code in <code>datajoint/jobs.py</code> <pre><code>def complete(self, table_name, key):\n\"\"\"\n    Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    \"\"\"\n    job_key = dict(table_name=table_name, key_hash=key_hash(key))\n    (self &amp; job_key).delete_quick()\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.JobTable.error", "title": "<code>error(table_name, key, error_message, error_stack=None)</code>", "text": "<p>Log an error message.  The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <code>error_message</code> <p>string error message</p> required <code>error_stack</code> <p>stack trace</p> <code>None</code> Source code in <code>datajoint/jobs.py</code> <pre><code>def error(self, table_name, key, error_message, error_stack=None):\n\"\"\"\n    Log an error message.  The job reservation is replaced with an error entry.\n    if an error occurs, leave an entry describing the problem\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :param error_message: string error message\n    :param error_stack: stack trace\n    \"\"\"\n    if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n        error_message = (\n            error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n            + TRUNCATION_APPENDIX\n        )\n    with config(enable_python_native_blobs=True):\n        self.insert1(\n            dict(\n                table_name=table_name,\n                key_hash=key_hash(key),\n                status=\"error\",\n                host=platform.node(),\n                pid=os.getpid(),\n                connection_id=self.connection.connection_id,\n                user=self._user,\n                key=key,\n                error_message=error_message,\n                error_stack=error_stack,\n            ),\n            replace=True,\n            ignore_extra_fields=True,\n        )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/jobs/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.DuplicateError", "title": "<code>DuplicateError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An integrity error caused by a duplicate entry into a unique key</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DuplicateError(QueryError):\n\"\"\"\n    An integrity error caused by a duplicate entry into a unique key\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.jobs.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n\"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n\"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return self._attributes\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n\"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n\"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n\"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n\"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n\"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n\"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=attr[\"type\"].split(\"@\")[1]\n                    if category in EXTERNAL_TYPES\n                    else None,\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n\"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n\"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n\"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n\"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        \"`%s`\" % name\n        if self.attributes[name].attribute_expression is None\n        else self.attributes[name].attribute_expression\n        + (\" as `%s`\" % name if include_aliases else \"\")\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n\"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n\"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/jobs/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n\"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/logging/", "title": "logging.py", "text": ""}, {"location": "api/datajoint/plugin/", "title": "plugin.py", "text": ""}, {"location": "api/datajoint/preview/", "title": "preview.py", "text": "<p>methods for generating previews of query expression results in python command line and Jupyter</p>"}, {"location": "api/datajoint/s3/", "title": "s3.py", "text": "<p>AWS S3 operations</p>"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder", "title": "<code>Folder</code>", "text": "<p>A Folder instance manipulates a flat folder of objects within an S3-compatible object store</p> Source code in <code>datajoint/s3.py</code> <pre><code>class Folder:\n\"\"\"\n    A Folder instance manipulates a flat folder of objects within an S3-compatible object store\n    \"\"\"\n\n    def __init__(\n        self,\n        endpoint,\n        bucket,\n        access_key,\n        secret_key,\n        *,\n        secure=False,\n        proxy_server=None,\n        **_\n    ):\n        # from https://docs.min.io/docs/python-client-api-reference\n        self.client = minio.Minio(\n            endpoint,\n            access_key=access_key,\n            secret_key=secret_key,\n            secure=secure,\n            http_client=(\n                urllib3.ProxyManager(\n                    proxy_server,\n                    timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n                    cert_reqs=\"CERT_REQUIRED\",\n                    retries=urllib3.Retry(\n                        total=5,\n                        backoff_factor=0.2,\n                        status_forcelist=[500, 502, 503, 504],\n                    ),\n                )\n                if proxy_server\n                else None\n            ),\n        )\n        self.bucket = bucket\n        if not self.client.bucket_exists(bucket):\n            raise errors.BucketInaccessible(\"Inaccessible s3 bucket %s\" % bucket)\n\n    def put(self, name, buffer):\n        logger.debug(\"put: {}:{}\".format(self.bucket, name))\n        return self.client.put_object(\n            self.bucket, str(name), BytesIO(buffer), length=len(buffer)\n        )\n\n    def fput(self, local_file, name, metadata=None):\n        logger.debug(\"fput: {} -&gt; {}:{}\".format(self.bucket, local_file, name))\n        return self.client.fput_object(\n            self.bucket, str(name), str(local_file), metadata=metadata\n        )\n\n    def get(self, name):\n        logger.debug(\"get: {}:{}\".format(self.bucket, name))\n        try:\n            with self.client.get_object(self.bucket, str(name)) as result:\n                data = [d for d in result.stream()]\n            return b\"\".join(data)\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                raise errors.MissingExternalFile(\"Missing s3 key %s\" % name)\n            else:\n                raise e\n\n    def fget(self, name, local_filepath):\n\"\"\"get file from object name to local filepath\"\"\"\n        logger.debug(\"fget: {}:{}\".format(self.bucket, name))\n        name = str(name)\n        stat = self.client.stat_object(self.bucket, name)\n        meta = {k.lower().lstrip(\"x-amz-meta\"): v for k, v in stat.metadata.items()}\n        data = self.client.get_object(self.bucket, name)\n        local_filepath = Path(local_filepath)\n        local_filepath.parent.mkdir(parents=True, exist_ok=True)\n        with local_filepath.open(\"wb\") as f:\n            for d in data.stream(1 &lt;&lt; 16):\n                f.write(d)\n        if \"contents_hash\" in meta:\n            return uuid.UUID(meta[\"contents_hash\"])\n\n    def exists(self, name):\n        logger.debug(\"exists: {}:{}\".format(self.bucket, name))\n        try:\n            self.client.stat_object(self.bucket, str(name))\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                return False\n            else:\n                raise e\n        return True\n\n    def get_size(self, name):\n        logger.debug(\"get_size: {}:{}\".format(self.bucket, name))\n        try:\n            return self.client.stat_object(self.bucket, str(name)).size\n        except minio.error.S3Error as e:\n            if e.code == \"NoSuchKey\":\n                raise errors.MissingExternalFile\n            raise e\n\n    def remove_object(self, name):\n        logger.debug(\"remove_object: {}:{}\".format(self.bucket, name))\n        try:\n            self.client.remove_object(self.bucket, str(name))\n        except minio.error.MinioException:\n            raise errors.DataJointError(\"Failed to delete %s from s3 storage\" % name)\n</code></pre>"}, {"location": "api/datajoint/s3/#datajoint.s3.Folder.fget", "title": "<code>fget(name, local_filepath)</code>", "text": "<p>get file from object name to local filepath</p> Source code in <code>datajoint/s3.py</code> <pre><code>def fget(self, name, local_filepath):\n\"\"\"get file from object name to local filepath\"\"\"\n    logger.debug(\"fget: {}:{}\".format(self.bucket, name))\n    name = str(name)\n    stat = self.client.stat_object(self.bucket, name)\n    meta = {k.lower().lstrip(\"x-amz-meta\"): v for k, v in stat.metadata.items()}\n    data = self.client.get_object(self.bucket, name)\n    local_filepath = Path(local_filepath)\n    local_filepath.parent.mkdir(parents=True, exist_ok=True)\n    with local_filepath.open(\"wb\") as f:\n        for d in data.stream(1 &lt;&lt; 16):\n            f.write(d)\n    if \"contents_hash\" in meta:\n        return uuid.UUID(meta[\"contents_hash\"])\n</code></pre>"}, {"location": "api/datajoint/schemas/", "title": "schemas.py", "text": ""}, {"location": "api/datajoint/schemas/#datajoint.schemas.JobTable", "title": "<code>JobTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table with no definition. Allows reserving jobs</p> Source code in <code>datajoint/jobs.py</code> <pre><code>class JobTable(Table):\n\"\"\"\n    A base table with no definition. Allows reserving jobs\n    \"\"\"\n\n    def __init__(self, conn, database):\n        self.database = database\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # job reservation table for `{database}`\n        table_name  :varchar(255)  # className of the table\n        key_hash  :char(32)  # key hash\n        ---\n        status  :enum('reserved','error','ignore')  # if tuple is missing, the job is available\n        key=null  :blob  # structure containing the key\n        error_message=\"\"  :varchar({error_message_length})  # error message returned if failed\n        error_stack=null  :mediumblob  # error stack if failed\n        user=\"\" :varchar(255) # database user\n        host=\"\"  :varchar(255)  # system hostname\n        pid=0  :int unsigned  # system process id\n        connection_id = 0  : bigint unsigned          # connection_id()\n        timestamp=CURRENT_TIMESTAMP  :timestamp   # automatic timestamp\n        \"\"\".format(\n            database=database, error_message_length=ERROR_MESSAGE_LENGTH\n        )\n        if not self.is_declared:\n            self.declare()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    @property\n    def table_name(self):\n        return \"~jobs\"\n\n    def delete(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.delete_quick()\n\n    def drop(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n        self.drop_quick()\n\n    def reserve(self, table_name, key):\n\"\"\"\n        Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n        job key, identified by its hash. When jobs are completed, the entry is removed.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :return: True if reserved job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"reserved\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def ignore(self, table_name, key):\n\"\"\"\n        Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n        job key, identified by its hash, with status \"ignore\".\n\n        Args:\n        table_name:\n            Table name (str) - `database`.`table_name`\n        key:\n            The dict of the job's primary key\n\n        Returns:\n            True if ignore job successfully. False = the jobs is already taken\n        \"\"\"\n        job = dict(\n            table_name=table_name,\n            key_hash=key_hash(key),\n            status=\"ignore\",\n            host=platform.node(),\n            pid=os.getpid(),\n            connection_id=self.connection.connection_id,\n            key=key,\n            user=self._user,\n        )\n        try:\n            with config(enable_python_native_blobs=True):\n                self.insert1(job, ignore_extra_fields=True)\n        except DuplicateError:\n            return False\n        return True\n\n    def complete(self, table_name, key):\n\"\"\"\n        Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        \"\"\"\n        job_key = dict(table_name=table_name, key_hash=key_hash(key))\n        (self &amp; job_key).delete_quick()\n\n    def error(self, table_name, key, error_message, error_stack=None):\n\"\"\"\n        Log an error message.  The job reservation is replaced with an error entry.\n        if an error occurs, leave an entry describing the problem\n\n        :param table_name: `database`.`table_name`\n        :param key: the dict of the job's primary key\n        :param error_message: string error message\n        :param error_stack: stack trace\n        \"\"\"\n        if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n            error_message = (\n                error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n                + TRUNCATION_APPENDIX\n            )\n        with config(enable_python_native_blobs=True):\n            self.insert1(\n                dict(\n                    table_name=table_name,\n                    key_hash=key_hash(key),\n                    status=\"error\",\n                    host=platform.node(),\n                    pid=os.getpid(),\n                    connection_id=self.connection.connection_id,\n                    user=self._user,\n                    key=key,\n                    error_message=error_message,\n                    error_stack=error_stack,\n                ),\n                replace=True,\n                ignore_extra_fields=True,\n            )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def delete(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.delete_quick()\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and dependencies</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def drop(self):\n\"\"\"bypass interactive prompts and dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.reserve", "title": "<code>reserve(table_name, key)</code>", "text": "<p>Reserve a job for computation.  When a job is reserved, the job table contains an entry for the job key, identified by its hash. When jobs are completed, the entry is removed.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <p>Returns:</p> Type Description <p>True if reserved job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def reserve(self, table_name, key):\n\"\"\"\n    Reserve a job for computation.  When a job is reserved, the job table contains an entry for the\n    job key, identified by its hash. When jobs are completed, the entry is removed.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :return: True if reserved job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"reserved\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.ignore", "title": "<code>ignore(table_name, key)</code>", "text": "<p>Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the job key, identified by its hash, with status \"ignore\".</p> <p>Args: table_name:     Table name (str) - <code>database</code>.<code>table_name</code> key:     The dict of the job's primary key</p> <p>Returns:     True if ignore job successfully. False = the jobs is already taken</p> Source code in <code>datajoint/jobs.py</code> <pre><code>def ignore(self, table_name, key):\n\"\"\"\n    Set a job to be ignored for computation.  When a job is ignored, the job table contains an entry for the\n    job key, identified by its hash, with status \"ignore\".\n\n    Args:\n    table_name:\n        Table name (str) - `database`.`table_name`\n    key:\n        The dict of the job's primary key\n\n    Returns:\n        True if ignore job successfully. False = the jobs is already taken\n    \"\"\"\n    job = dict(\n        table_name=table_name,\n        key_hash=key_hash(key),\n        status=\"ignore\",\n        host=platform.node(),\n        pid=os.getpid(),\n        connection_id=self.connection.connection_id,\n        key=key,\n        user=self._user,\n    )\n    try:\n        with config(enable_python_native_blobs=True):\n            self.insert1(job, ignore_extra_fields=True)\n    except DuplicateError:\n        return False\n    return True\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.complete", "title": "<code>complete(table_name, key)</code>", "text": "<p>Log a completed job.  When a job is completed, its reservation entry is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required Source code in <code>datajoint/jobs.py</code> <pre><code>def complete(self, table_name, key):\n\"\"\"\n    Log a completed job.  When a job is completed, its reservation entry is deleted.\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    \"\"\"\n    job_key = dict(table_name=table_name, key_hash=key_hash(key))\n    (self &amp; job_key).delete_quick()\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.jobs.JobTable.error", "title": "<code>error(table_name, key, error_message, error_stack=None)</code>", "text": "<p>Log an error message.  The job reservation is replaced with an error entry. if an error occurs, leave an entry describing the problem</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <p><code>database</code>.<code>table_name</code></p> required <code>key</code> <p>the dict of the job's primary key</p> required <code>error_message</code> <p>string error message</p> required <code>error_stack</code> <p>stack trace</p> <code>None</code> Source code in <code>datajoint/jobs.py</code> <pre><code>def error(self, table_name, key, error_message, error_stack=None):\n\"\"\"\n    Log an error message.  The job reservation is replaced with an error entry.\n    if an error occurs, leave an entry describing the problem\n\n    :param table_name: `database`.`table_name`\n    :param key: the dict of the job's primary key\n    :param error_message: string error message\n    :param error_stack: stack trace\n    \"\"\"\n    if len(error_message) &gt; ERROR_MESSAGE_LENGTH:\n        error_message = (\n            error_message[: ERROR_MESSAGE_LENGTH - len(TRUNCATION_APPENDIX)]\n            + TRUNCATION_APPENDIX\n        )\n    with config(enable_python_native_blobs=True):\n        self.insert1(\n            dict(\n                table_name=table_name,\n                key_hash=key_hash(key),\n                status=\"error\",\n                host=platform.node(),\n                pid=os.getpid(),\n                connection_id=self.connection.connection_id,\n                user=self._user,\n                key=key,\n                error_message=error_message,\n                error_stack=error_stack,\n            ),\n            replace=True,\n            ignore_extra_fields=True,\n        )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.user_choice", "title": "<code>user_choice(prompt, choices=('yes', 'no'), default=None)</code>", "text": "<p>Prompts the user for confirmation.  The default value, if any, is capitalized.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>Information to display to the user.</p> required <code>choices</code> <p>an iterable of possible choices.</p> <code>('yes', 'no')</code> <code>default</code> <p>default choice</p> <code>None</code> <p>Returns:</p> Type Description <p>the user's choice</p> Source code in <code>datajoint/utils.py</code> <pre><code>def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n\"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = \", \".join(\n        (choice.title() if choice == default else choice for choice in choices)\n    )\n    response = None\n    while response not in choices:\n        response = input(prompt + \" [\" + choice_list + \"]: \")\n        response = response.lower() if response else default\n    return response\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.ordered_dir", "title": "<code>ordered_dir(class_)</code>", "text": "<p>List (most) attributes of the class including inherited ones, similar to <code>dir</code> build-in function, but respects order of attribute declaration as much as possible.</p> <p>Parameters:</p> Name Type Description Default <code>class_</code> <p>class to list members for</p> required <p>Returns:</p> Type Description <p>a list of attributes declared in class_ and its superclasses</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def ordered_dir(class_):\n\"\"\"\n    List (most) attributes of the class including inherited ones, similar to `dir` build-in function,\n    but respects order of attribute declaration as much as possible.\n\n    :param class_: class to list members for\n    :return: a list of attributes declared in class_ and its superclasses\n    \"\"\"\n    attr_list = list()\n    for c in reversed(class_.mro()):\n        attr_list.extend(e for e in c.__dict__ if e not in attr_list)\n    return attr_list\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema", "title": "<code>Schema</code>", "text": "<p>A schema object is a decorator for UserTable classes that binds them to their database. It also specifies the namespace <code>context</code> in which other UserTable classes are defined.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class Schema:\n\"\"\"\n    A schema object is a decorator for UserTable classes that binds them to their database.\n    It also specifies the namespace `context` in which other UserTable classes are defined.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_name=None,\n        context=None,\n        *,\n        connection=None,\n        create_schema=True,\n        create_tables=True,\n        add_objects=None,\n    ):\n\"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        If the schema_name is omitted, then schema.activate(..) must be called later\n        to associate with the database.\n\n        :param schema_name: the database schema to associate.\n        :param context: dictionary for looking up foreign key references, leave None to use local context.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: When False, do not create the schema and raise an error if missing.\n        :param create_tables: When False, do not create tables and raise errors when accessing missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context in which table classes\n        are declared.\n        \"\"\"\n        self._log = None\n        self.connection = connection\n        self.database = None\n        self.context = context\n        self.create_schema = create_schema\n        self.create_tables = create_tables\n        self._jobs = None\n        self.external = ExternalMapping(self)\n        self.add_objects = add_objects\n        self.declare_list = []\n        if schema_name:\n            self.activate(schema_name)\n\n    def is_activated(self):\n        return self.database is not None\n\n    def activate(\n        self,\n        schema_name=None,\n        *,\n        connection=None,\n        create_schema=None,\n        create_tables=None,\n        add_objects=None,\n    ):\n\"\"\"\n        Associate database schema `schema_name`. If the schema does not exist, attempt to\n        create it on the server.\n\n        :param schema_name: the database schema to associate.\n            schema_name=None is used to assert that the schema has already been activated.\n        :param connection: Connection object. Defaults to datajoint.conn().\n        :param create_schema: If False, do not create the schema and raise an error if missing.\n        :param create_tables: If False, do not create tables and raise errors when attempting\n            to access missing tables.\n        :param add_objects: a mapping with additional objects to make available to the context\n            in which table classes are declared.\n        \"\"\"\n        if schema_name is None:\n            if self.exists:\n                return\n            raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n        if self.database is not None and self.exists:\n            if self.database == schema_name:  # already activated\n                return\n            raise DataJointError(\n                \"The schema is already activated for schema {db}.\".format(\n                    db=self.database\n                )\n            )\n        if connection is not None:\n            self.connection = connection\n        if self.connection is None:\n            self.connection = conn()\n        self.database = schema_name\n        if create_schema is not None:\n            self.create_schema = create_schema\n        if create_tables is not None:\n            self.create_tables = create_tables\n        if add_objects:\n            self.add_objects = add_objects\n        if not self.exists:\n            if not self.create_schema or not self.database:\n                raise DataJointError(\n                    \"Database `{name}` has not yet been declared. \"\n                    \"Set argument create_schema=True to create it.\".format(\n                        name=schema_name\n                    )\n                )\n            # create database\n            logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n            try:\n                self.connection.query(\n                    \"CREATE DATABASE `{name}`\".format(name=schema_name)\n                )\n            except AccessError:\n                raise DataJointError(\n                    \"Schema `{name}` does not exist and could not be created. \"\n                    \"Check permissions.\".format(name=schema_name)\n                )\n            else:\n                self.log(\"created\")\n        self.connection.register(self)\n\n        # decorate all tables already decorated\n        for cls, context in self.declare_list:\n            if self.add_objects:\n                context = dict(context, **self.add_objects)\n            self._decorate_master(cls, context)\n\n    def _assert_exists(self, message=None):\n        if not self.exists:\n            raise DataJointError(\n                message\n                or \"Schema `{db}` has not been created.\".format(db=self.database)\n            )\n\n    def __call__(self, cls, *, context=None):\n\"\"\"\n        Binds the supplied class to a schema. This is intended to be used as a decorator.\n\n        :param cls: class to decorate.\n        :param context: supplied when called from spawn_missing_classes\n        \"\"\"\n        context = context or self.context or inspect.currentframe().f_back.f_locals\n        if issubclass(cls, Part):\n            raise DataJointError(\n                \"The schema decorator should not be applied to Part tables.\"\n            )\n        if self.is_activated():\n            self._decorate_master(cls, context)\n        else:\n            self.declare_list.append((cls, context))\n        return cls\n\n    def _decorate_master(self, cls, context):\n\"\"\"\n\n        :param cls: the master class to process\n        :param context: the class' declaration context\n        \"\"\"\n        self._decorate_table(\n            cls, context=dict(context, self=cls, **{cls.__name__: cls})\n        )\n        # Process part tables\n        for part in ordered_dir(cls):\n            if part[0].isupper():\n                part = getattr(cls, part)\n                if inspect.isclass(part) and issubclass(part, Part):\n                    part._master = cls\n                    # allow addressing master by name or keyword 'master'\n                    self._decorate_table(\n                        part,\n                        context=dict(\n                            context, master=cls, self=part, **{cls.__name__: cls}\n                        ),\n                    )\n\n    def _decorate_table(self, table_class, context, assert_declared=False):\n\"\"\"\n        assign schema properties to the table class and declare the table\n        \"\"\"\n        table_class.database = self.database\n        table_class._connection = self.connection\n        table_class._heading = Heading(\n            table_info=dict(\n                conn=self.connection,\n                database=self.database,\n                table_name=table_class.table_name,\n                context=context,\n            )\n        )\n        table_class._support = [table_class.full_table_name]\n        table_class.declaration_context = context\n\n        # instantiate the class, declare the table if not already\n        instance = table_class()\n        is_declared = instance.is_declared\n        if not is_declared and not assert_declared and self.create_tables:\n            instance.declare(context)\n            self.connection.dependencies.clear()\n        is_declared = is_declared or instance.is_declared\n\n        # add table definition to the doc string\n        if isinstance(table_class.definition, str):\n            table_class.__doc__ = (\n                (table_class.__doc__ or \"\")\n                + \"\\nTable definition:\\n\\n\"\n                + table_class.definition\n            )\n\n        # fill values in Lookup tables from their contents property\n        if (\n            isinstance(instance, Lookup)\n            and hasattr(instance, \"contents\")\n            and is_declared\n        ):\n            contents = list(instance.contents)\n            if len(contents) &gt; len(instance):\n                if instance.heading.has_autoincrement:\n                    warnings.warn(\n                        (\n                            \"Contents has changed but cannot be inserted because \"\n                            \"{table} has autoincrement.\"\n                        ).format(table=instance.__class__.__name__)\n                    )\n                else:\n                    instance.insert(contents, skip_duplicates=True)\n\n    @property\n    def log(self):\n        self._assert_exists()\n        if self._log is None:\n            self._log = Log(self.connection, self.database)\n        return self._log\n\n    def __repr__(self):\n        return \"Schema `{name}`\\n\".format(name=self.database)\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of the entire schema in bytes\n        \"\"\"\n        self._assert_exists()\n        return int(\n            self.connection.query(\n\"\"\"\n            SELECT SUM(data_length + index_length)\n            FROM information_schema.tables WHERE table_schema='{db}'\n            \"\"\".format(\n                    db=self.database\n                )\n            ).fetchone()[0]\n        )\n\n    def spawn_missing_classes(self, context=None):\n\"\"\"\n        Creates the appropriate python user table classes from tables in the schema and places them\n        in the context.\n\n        :param context: alternative context to place the missing classes into, e.g. locals()\n        \"\"\"\n        self._assert_exists()\n        if context is None:\n            if self.context is not None:\n                context = self.context\n            else:\n                # if context is missing, use the calling namespace\n                frame = inspect.currentframe().f_back\n                context = frame.f_locals\n                del frame\n        tables = [\n            row[0]\n            for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n            if lookup_class_name(\n                \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n            )\n            is None\n        ]\n        master_classes = (Lookup, Manual, Imported, Computed)\n        part_tables = []\n        for table_name in tables:\n            class_name = to_camel_case(table_name)\n            if class_name not in context:\n                try:\n                    cls = next(\n                        cls\n                        for cls in master_classes\n                        if re.fullmatch(cls.tier_regexp, table_name)\n                    )\n                except StopIteration:\n                    if re.fullmatch(Part.tier_regexp, table_name):\n                        part_tables.append(table_name)\n                else:\n                    # declare and decorate master table classes\n                    context[class_name] = self(\n                        type(class_name, (cls,), dict()), context=context\n                    )\n\n        # attach parts to masters\n        for table_name in part_tables:\n            groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n            class_name = to_camel_case(groups[\"part\"])\n            try:\n                master_class = context[to_camel_case(groups[\"master\"])]\n            except KeyError:\n                raise DataJointError(\n                    \"The table %s does not follow DataJoint naming conventions\"\n                    % table_name\n                )\n            part_class = type(class_name, (Part,), dict(definition=...))\n            part_class._master = master_class\n            self._decorate_table(part_class, context=context, assert_declared=True)\n            setattr(master_class, class_name, part_class)\n\n    def drop(self, force=False):\n\"\"\"\n        Drop the associated schema if it exists\n        \"\"\"\n        if not self.exists:\n            logger.info(\n                \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                    database=self.database\n                )\n            )\n        elif (\n            not config[\"safemode\"]\n            or force\n            or user_choice(\n                \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n            )\n            == \"yes\"\n        ):\n            logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n            try:\n                self.connection.query(\n                    \"DROP DATABASE `{database}`\".format(database=self.database)\n                )\n                logger.debug(\n                    \"Schema `{database}` was dropped successfully.\".format(\n                        database=self.database\n                    )\n                )\n            except AccessError:\n                raise AccessError(\n                    \"An attempt to drop schema `{database}` \"\n                    \"has failed. Check permissions.\".format(database=self.database)\n                )\n\n    @property\n    def exists(self):\n\"\"\"\n        :return: true if the associated schema exists on the server\n        \"\"\"\n        if self.database is None:\n            raise DataJointError(\"Schema must be activated first.\")\n        return bool(\n            self.connection.query(\n                \"SELECT schema_name \"\n                \"FROM information_schema.schemata \"\n                \"WHERE schema_name = '{database}'\".format(database=self.database)\n            ).rowcount\n        )\n\n    @property\n    def jobs(self):\n\"\"\"\n        schema.jobs provides a view of the job reservation table for the schema\n\n        :return: jobs table\n        \"\"\"\n        self._assert_exists()\n        if self._jobs is None:\n            self._jobs = JobTable(self.connection, self.database)\n        return self._jobs\n\n    @property\n    def code(self):\n        self._assert_exists()\n        return self.save()\n\n    def save(self, python_filename=None):\n\"\"\"\n        Generate the code for a module that recreates the schema.\n        This method is in preparation for a future release and is not officially supported.\n\n        :return: a string containing the body of a complete Python module defining this schema.\n        \"\"\"\n        self._assert_exists()\n        module_count = itertools.count()\n        # add virtual modules for referenced modules with names vmod0, vmod1, ...\n        module_lookup = collections.defaultdict(\n            lambda: \"vmod\" + str(next(module_count))\n        )\n        db = self.database\n\n        def make_class_definition(table):\n            tier = _get_tier(table).__name__\n            class_name = table.split(\".\")[1].strip(\"`\")\n            indent = \"\"\n            if tier == \"Part\":\n                class_name = class_name.split(\"__\")[-1]\n                indent += \"    \"\n            class_name = to_camel_case(class_name)\n\n            def replace(s):\n                d, tabs = s.group(1), s.group(2)\n                return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                    to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n                )\n\n            return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n                \"{indent}class {class_name}(dj.{tier}):\\n\"\n                '{indent}    definition = \"\"\"\\n'\n                '{indent}    {defi}\"\"\"'\n            ).format(\n                class_name=class_name,\n                indent=indent,\n                tier=tier,\n                defi=re.sub(\n                    r\"`([^`]+)`.`([^`]+)`\",\n                    replace,\n                    FreeTable(self.connection, table).describe(),\n                ).replace(\"\\n\", \"\\n    \" + indent),\n            )\n\n        diagram = Diagram(self)\n        body = \"\\n\\n\".join(\n            make_class_definition(table) for table in diagram.topological_sort()\n        )\n        python_code = \"\\n\\n\".join(\n            (\n                '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n                \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n                \"\\n\".join(\n                    \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                        module=v, schema_name=k\n                    )\n                    for k, v in module_lookup.items()\n                ),\n                body,\n            )\n        )\n        if python_filename is None:\n            return python_code\n        with open(python_filename, \"wt\") as f:\n            f.write(python_code)\n\n    def list_tables(self):\n\"\"\"\n        Return a list of all tables in the schema except tables with ~ in first character such\n        as ~logs and ~job\n\n        :return: A list of table names from the database schema.\n        \"\"\"\n        return [\n            t\n            for d, t in (\n                full_t.replace(\"`\", \"\").split(\".\")\n                for full_t in Diagram(self).topological_sort()\n            )\n            if d == self.database\n        ]\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.activate", "title": "<code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)</code>", "text": "<p>Associate database schema <code>schema_name</code>. If the schema does not exist, attempt to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <p>the database schema to associate. schema_name=None is used to assert that the schema has already been activated.</p> <code>None</code> <code>connection</code> <p>Connection object. Defaults to datajoint.conn().</p> <code>None</code> <code>create_schema</code> <p>If False, do not create the schema and raise an error if missing.</p> <code>None</code> <code>create_tables</code> <p>If False, do not create tables and raise errors when attempting to access missing tables.</p> <code>None</code> <code>add_objects</code> <p>a mapping with additional objects to make available to the context in which table classes are declared.</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def activate(\n    self,\n    schema_name=None,\n    *,\n    connection=None,\n    create_schema=None,\n    create_tables=None,\n    add_objects=None,\n):\n\"\"\"\n    Associate database schema `schema_name`. If the schema does not exist, attempt to\n    create it on the server.\n\n    :param schema_name: the database schema to associate.\n        schema_name=None is used to assert that the schema has already been activated.\n    :param connection: Connection object. Defaults to datajoint.conn().\n    :param create_schema: If False, do not create the schema and raise an error if missing.\n    :param create_tables: If False, do not create tables and raise errors when attempting\n        to access missing tables.\n    :param add_objects: a mapping with additional objects to make available to the context\n        in which table classes are declared.\n    \"\"\"\n    if schema_name is None:\n        if self.exists:\n            return\n        raise DataJointError(\"Please provide a schema_name to activate the schema.\")\n    if self.database is not None and self.exists:\n        if self.database == schema_name:  # already activated\n            return\n        raise DataJointError(\n            \"The schema is already activated for schema {db}.\".format(\n                db=self.database\n            )\n        )\n    if connection is not None:\n        self.connection = connection\n    if self.connection is None:\n        self.connection = conn()\n    self.database = schema_name\n    if create_schema is not None:\n        self.create_schema = create_schema\n    if create_tables is not None:\n        self.create_tables = create_tables\n    if add_objects:\n        self.add_objects = add_objects\n    if not self.exists:\n        if not self.create_schema or not self.database:\n            raise DataJointError(\n                \"Database `{name}` has not yet been declared. \"\n                \"Set argument create_schema=True to create it.\".format(\n                    name=schema_name\n                )\n            )\n        # create database\n        logger.debug(\"Creating schema `{name}`.\".format(name=schema_name))\n        try:\n            self.connection.query(\n                \"CREATE DATABASE `{name}`\".format(name=schema_name)\n            )\n        except AccessError:\n            raise DataJointError(\n                \"Schema `{name}` does not exist and could not be created. \"\n                \"Check permissions.\".format(name=schema_name)\n            )\n        else:\n            self.log(\"created\")\n    self.connection.register(self)\n\n    # decorate all tables already decorated\n    for cls, context in self.declare_list:\n        if self.add_objects:\n            context = dict(context, **self.add_objects)\n        self._decorate_master(cls, context)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of the entire schema in bytes</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.spawn_missing_classes", "title": "<code>spawn_missing_classes(context=None)</code>", "text": "<p>Creates the appropriate python user table classes from tables in the schema and places them in the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>alternative context to place the missing classes into, e.g. locals()</p> <code>None</code> Source code in <code>datajoint/schemas.py</code> <pre><code>def spawn_missing_classes(self, context=None):\n\"\"\"\n    Creates the appropriate python user table classes from tables in the schema and places them\n    in the context.\n\n    :param context: alternative context to place the missing classes into, e.g. locals()\n    \"\"\"\n    self._assert_exists()\n    if context is None:\n        if self.context is not None:\n            context = self.context\n        else:\n            # if context is missing, use the calling namespace\n            frame = inspect.currentframe().f_back\n            context = frame.f_locals\n            del frame\n    tables = [\n        row[0]\n        for row in self.connection.query(\"SHOW TABLES in `%s`\" % self.database)\n        if lookup_class_name(\n            \"`{db}`.`{tab}`\".format(db=self.database, tab=row[0]), context, 0\n        )\n        is None\n    ]\n    master_classes = (Lookup, Manual, Imported, Computed)\n    part_tables = []\n    for table_name in tables:\n        class_name = to_camel_case(table_name)\n        if class_name not in context:\n            try:\n                cls = next(\n                    cls\n                    for cls in master_classes\n                    if re.fullmatch(cls.tier_regexp, table_name)\n                )\n            except StopIteration:\n                if re.fullmatch(Part.tier_regexp, table_name):\n                    part_tables.append(table_name)\n            else:\n                # declare and decorate master table classes\n                context[class_name] = self(\n                    type(class_name, (cls,), dict()), context=context\n                )\n\n    # attach parts to masters\n    for table_name in part_tables:\n        groups = re.fullmatch(Part.tier_regexp, table_name).groupdict()\n        class_name = to_camel_case(groups[\"part\"])\n        try:\n            master_class = context[to_camel_case(groups[\"master\"])]\n        except KeyError:\n            raise DataJointError(\n                \"The table %s does not follow DataJoint naming conventions\"\n                % table_name\n            )\n        part_class = type(class_name, (Part,), dict(definition=...))\n        part_class._master = master_class\n        self._decorate_table(part_class, context=context, assert_declared=True)\n        setattr(master_class, class_name, part_class)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.drop", "title": "<code>drop(force=False)</code>", "text": "<p>Drop the associated schema if it exists</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    Drop the associated schema if it exists\n    \"\"\"\n    if not self.exists:\n        logger.info(\n            \"Schema named `{database}` does not exist. Doing nothing.\".format(\n                database=self.database\n            )\n        )\n    elif (\n        not config[\"safemode\"]\n        or force\n        or user_choice(\n            \"Proceed to delete entire schema `%s`?\" % self.database, default=\"no\"\n        )\n        == \"yes\"\n    ):\n        logger.debug(\"Dropping `{database}`.\".format(database=self.database))\n        try:\n            self.connection.query(\n                \"DROP DATABASE `{database}`\".format(database=self.database)\n            )\n            logger.debug(\n                \"Schema `{database}` was dropped successfully.\".format(\n                    database=self.database\n                )\n            )\n        except AccessError:\n            raise AccessError(\n                \"An attempt to drop schema `{database}` \"\n                \"has failed. Check permissions.\".format(database=self.database)\n            )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.exists", "title": "<code>exists</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>true if the associated schema exists on the server</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.jobs", "title": "<code>jobs</code>  <code>property</code>", "text": "<p>schema.jobs provides a view of the job reservation table for the schema</p> <p>Returns:</p> Type Description <p>jobs table</p>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.save", "title": "<code>save(python_filename=None)</code>", "text": "<p>Generate the code for a module that recreates the schema. This method is in preparation for a future release and is not officially supported.</p> <p>Returns:</p> Type Description <p>a string containing the body of a complete Python module defining this schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def save(self, python_filename=None):\n\"\"\"\n    Generate the code for a module that recreates the schema.\n    This method is in preparation for a future release and is not officially supported.\n\n    :return: a string containing the body of a complete Python module defining this schema.\n    \"\"\"\n    self._assert_exists()\n    module_count = itertools.count()\n    # add virtual modules for referenced modules with names vmod0, vmod1, ...\n    module_lookup = collections.defaultdict(\n        lambda: \"vmod\" + str(next(module_count))\n    )\n    db = self.database\n\n    def make_class_definition(table):\n        tier = _get_tier(table).__name__\n        class_name = table.split(\".\")[1].strip(\"`\")\n        indent = \"\"\n        if tier == \"Part\":\n            class_name = class_name.split(\"__\")[-1]\n            indent += \"    \"\n        class_name = to_camel_case(class_name)\n\n        def replace(s):\n            d, tabs = s.group(1), s.group(2)\n            return (\"\" if d == db else (module_lookup[d] + \".\")) + \".\".join(\n                to_camel_case(tab) for tab in tabs.lstrip(\"__\").split(\"__\")\n            )\n\n        return (\"\" if tier == \"Part\" else \"\\n@schema\\n\") + (\n            \"{indent}class {class_name}(dj.{tier}):\\n\"\n            '{indent}    definition = \"\"\"\\n'\n            '{indent}    {defi}\"\"\"'\n        ).format(\n            class_name=class_name,\n            indent=indent,\n            tier=tier,\n            defi=re.sub(\n                r\"`([^`]+)`.`([^`]+)`\",\n                replace,\n                FreeTable(self.connection, table).describe(),\n            ).replace(\"\\n\", \"\\n    \" + indent),\n        )\n\n    diagram = Diagram(self)\n    body = \"\\n\\n\".join(\n        make_class_definition(table) for table in diagram.topological_sort()\n    )\n    python_code = \"\\n\\n\".join(\n        (\n            '\"\"\"This module was auto-generated by datajoint from an existing schema\"\"\"',\n            \"import datajoint as dj\\n\\nschema = dj.Schema('{db}')\".format(db=db),\n            \"\\n\".join(\n                \"{module} = dj.VirtualModule('{module}', '{schema_name}')\".format(\n                    module=v, schema_name=k\n                )\n                for k, v in module_lookup.items()\n            ),\n            body,\n        )\n    )\n    if python_filename is None:\n        return python_code\n    with open(python_filename, \"wt\") as f:\n        f.write(python_code)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Schema.list_tables", "title": "<code>list_tables()</code>", "text": "<p>Return a list of all tables in the schema except tables with ~ in first character such as ~logs and ~job</p> <p>Returns:</p> Type Description <p>A list of table names from the database schema.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_tables(self):\n\"\"\"\n    Return a list of all tables in the schema except tables with ~ in first character such\n    as ~logs and ~job\n\n    :return: A list of table names from the database schema.\n    \"\"\"\n    return [\n        t\n        for d, t in (\n            full_t.replace(\"`\", \"\").split(\".\")\n            for full_t in Diagram(self).topological_sort()\n        )\n        if d == self.database\n    ]\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.to_camel_case", "title": "<code>to_camel_case(s)</code>", "text": "<p>Convert names with under score (_) separation into camel case names.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in under_score notation</p> required <p>Returns:</p> Type Description <p>string in CamelCase notation Example: &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def to_camel_case(s):\n\"\"\"\n    Convert names with under score (_) separation into camel case names.\n\n    :param s: string in under_score notation\n    :returns: string in CamelCase notation\n    Example:\n    &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"\n    \"\"\"\n\n    def to_upper(match):\n        return match.group(0)[-1].upper()\n\n    return re.sub(r\"(^|[_\\W])+[a-zA-Z]\", to_upper, s)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.AccessError", "title": "<code>AccessError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>User access error: insufficient privileges.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class AccessError(QueryError):\n\"\"\"\n    User access error: insufficient privileges.\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Diagram", "title": "<code>Diagram</code>", "text": "<p>         Bases: <code>nx.DiGraph</code></p> <p>Entity relationship diagram.</p> <p>Usage:</p> <p>diag = Diagram(source)</p> <p>source can be a base table object, a base table class, a schema, or a module that has a schema.</p> <p>diag.draw()</p> <p>draws the diagram using pyplot</p> <p>diag1 + diag2  - combines the two diagrams. diag + n   - expands n levels of successors diag - n   - expands n levels of predecessors Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table</p> <p>Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth. Only those tables that are loaded in the connection object are displayed</p> Source code in <code>datajoint/diagram.py</code> <pre><code>class Diagram(nx.DiGraph):\n\"\"\"\n    Entity relationship diagram.\n\n    Usage:\n\n    &gt;&gt;&gt;  diag = Diagram(source)\n\n    source can be a base table object, a base table class, a schema, or a module that has a schema.\n\n    &gt;&gt;&gt; diag.draw()\n\n    draws the diagram using pyplot\n\n    diag1 + diag2  - combines the two diagrams.\n    diag + n   - expands n levels of successors\n    diag - n   - expands n levels of predecessors\n    Thus dj.Diagram(schema.Table)+1-1 defines the diagram of immediate ancestors and descendants of schema.Table\n\n    Note that diagram + 1 - 1  may differ from diagram - 1 + 1 and so forth.\n    Only those tables that are loaded in the connection object are displayed\n    \"\"\"\n\n    def __init__(self, source, context=None):\n        if isinstance(source, Diagram):\n            # copy constructor\n            self.nodes_to_show = set(source.nodes_to_show)\n            self.context = source.context\n            super().__init__(source)\n            return\n\n        # get the caller's context\n        if context is None:\n            frame = inspect.currentframe().f_back\n            self.context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        else:\n            self.context = context\n\n        # find connection in the source\n        try:\n            connection = source.connection\n        except AttributeError:\n            try:\n                connection = source.schema.connection\n            except AttributeError:\n                raise DataJointError(\n                    \"Could not find database connection in %s\" % repr(source[0])\n                )\n\n        # initialize graph from dependencies\n        connection.dependencies.load()\n        super().__init__(connection.dependencies)\n\n        # Enumerate nodes from all the items in the list\n        self.nodes_to_show = set()\n        try:\n            self.nodes_to_show.add(source.full_table_name)\n        except AttributeError:\n            try:\n                database = source.database\n            except AttributeError:\n                try:\n                    database = source.schema.database\n                except AttributeError:\n                    raise DataJointError(\n                        \"Cannot plot Diagram for %s\" % repr(source)\n                    )\n            for node in self:\n                if node.startswith(\"`%s`\" % database):\n                    self.nodes_to_show.add(node)\n\n    @classmethod\n    def from_sequence(cls, sequence):\n\"\"\"\n        The join Diagram for all objects in sequence\n\n        :param sequence: a sequence (e.g. list, tuple)\n        :return: Diagram(arg1) + ... + Diagram(argn)\n        \"\"\"\n        return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n\n    def add_parts(self):\n\"\"\"\n        Adds to the diagram the part tables of tables already included in the diagram\n        :return:\n        \"\"\"\n\n        def is_part(part, master):\n\"\"\"\n            :param part:  `database`.`table_name`\n            :param master:   `database`.`table_name`\n            :return: True if part is part of master.\n            \"\"\"\n            part = [s.strip(\"`\") for s in part.split(\".\")]\n            master = [s.strip(\"`\") for s in master.split(\".\")]\n            return (\n                master[0] == part[0]\n                and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n            )\n\n        self = Diagram(self)  # copy\n        self.nodes_to_show.update(\n            n\n            for n in self.nodes()\n            if any(is_part(n, m) for m in self.nodes_to_show)\n        )\n        return self\n\n    def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n        return unite_master_parts(\n            list(\n                nx.algorithms.dag.topological_sort(\n                    nx.DiGraph(self).subgraph(self.nodes_to_show)\n                )\n            )\n        )\n\n    def __add__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Union of the diagrams when arg is another Diagram\n                 or an expansion downstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.add(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    new = nx.algorithms.boundary.node_boundary(\n                        self, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            self, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __sub__(self, arg):\n\"\"\"\n        :param arg: either another Diagram or a positive integer.\n        :return: Difference of the diagrams when arg is another Diagram or\n                 an expansion upstream when arg is a positive integer.\n        \"\"\"\n        self = Diagram(self)  # copy\n        try:\n            self.nodes_to_show.difference_update(arg.nodes_to_show)\n        except AttributeError:\n            try:\n                self.nodes_to_show.remove(arg.full_table_name)\n            except AttributeError:\n                for i in range(arg):\n                    graph = nx.DiGraph(self).reverse()\n                    new = nx.algorithms.boundary.node_boundary(\n                        graph, self.nodes_to_show\n                    )\n                    if not new:\n                        break\n                    # add nodes referenced by aliased nodes\n                    new.update(\n                        nx.algorithms.boundary.node_boundary(\n                            graph, (a for a in new if a.isdigit())\n                        )\n                    )\n                    self.nodes_to_show.update(new)\n        return self\n\n    def __mul__(self, arg):\n\"\"\"\n        Intersection of two diagrams\n        :param arg: another Diagram\n        :return: a new Diagram comprising nodes that are present in both operands.\n        \"\"\"\n        self = Diagram(self)  # copy\n        self.nodes_to_show.intersection_update(arg.nodes_to_show)\n        return self\n\n    def _make_graph(self):\n\"\"\"\n        Make the self.graph - a graph object ready for drawing\n        \"\"\"\n        # mark \"distinguished\" tables, i.e. those that introduce new primary key\n        # attributes\n        for name in self.nodes_to_show:\n            foreign_attributes = set(\n                attr\n                for p in self.in_edges(name, data=True)\n                for attr in p[2][\"attr_map\"]\n                if p[2][\"primary\"]\n            )\n            self.nodes[name][\"distinguished\"] = (\n                \"primary_key\" in self.nodes[name]\n                and foreign_attributes &lt; self.nodes[name][\"primary_key\"]\n            )\n        # include aliased nodes that are sandwiched between two displayed nodes\n        gaps = set(\n            nx.algorithms.boundary.node_boundary(self, self.nodes_to_show)\n        ).intersection(\n            nx.algorithms.boundary.node_boundary(\n                nx.DiGraph(self).reverse(), self.nodes_to_show\n            )\n        )\n        nodes = self.nodes_to_show.union(a for a in gaps if a.isdigit)\n        # construct subgraph and rename nodes to class names\n        graph = nx.DiGraph(nx.DiGraph(self).subgraph(nodes))\n        nx.set_node_attributes(\n            graph, name=\"node_type\", values={n: _get_tier(n) for n in graph}\n        )\n        # relabel nodes to class names\n        mapping = {\n            node: lookup_class_name(node, self.context) or node\n            for node in graph.nodes()\n        }\n        new_names = [mapping.values()]\n        if len(new_names) &gt; len(set(new_names)):\n            raise DataJointError(\n                \"Some classes have identical names. The Diagram cannot be plotted.\"\n            )\n        nx.relabel_nodes(graph, mapping, copy=False)\n        return graph\n\n    def make_dot(self):\n        graph = self._make_graph()\n        graph.nodes()\n\n        scale = 1.2  # scaling factor for fonts and boxes\n        label_props = {  # http://matplotlib.org/examples/color/named_colors.html\n            None: dict(\n                shape=\"circle\",\n                color=\"#FFFF0040\",\n                fontcolor=\"yellow\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            _AliasNode: dict(\n                shape=\"circle\",\n                color=\"#FF880080\",\n                fontcolor=\"#FF880080\",\n                fontsize=round(scale * 0),\n                size=0.05 * scale,\n                fixed=True,\n            ),\n            Manual: dict(\n                shape=\"box\",\n                color=\"#00FF0030\",\n                fontcolor=\"darkgreen\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Lookup: dict(\n                shape=\"plaintext\",\n                color=\"#00000020\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Computed: dict(\n                shape=\"ellipse\",\n                color=\"#FF000020\",\n                fontcolor=\"#7F0000A0\",\n                fontsize=round(scale * 10),\n                size=0.3 * scale,\n                fixed=True,\n            ),\n            Imported: dict(\n                shape=\"ellipse\",\n                color=\"#00007F40\",\n                fontcolor=\"#00007FA0\",\n                fontsize=round(scale * 10),\n                size=0.4 * scale,\n                fixed=False,\n            ),\n            Part: dict(\n                shape=\"plaintext\",\n                color=\"#0000000\",\n                fontcolor=\"black\",\n                fontsize=round(scale * 8),\n                size=0.1 * scale,\n                fixed=False,\n            ),\n        }\n        node_props = {\n            node: label_props[d[\"node_type\"]]\n            for node, d in dict(graph.nodes(data=True)).items()\n        }\n\n        dot = nx.drawing.nx_pydot.to_pydot(graph)\n        for node in dot.get_nodes():\n            node.set_shape(\"circle\")\n            name = node.get_name().strip('\"')\n            props = node_props[name]\n            node.set_fontsize(props[\"fontsize\"])\n            node.set_fontcolor(props[\"fontcolor\"])\n            node.set_shape(props[\"shape\"])\n            node.set_fontname(\"arial\")\n            node.set_fixedsize(\"shape\" if props[\"fixed\"] else False)\n            node.set_width(props[\"size\"])\n            node.set_height(props[\"size\"])\n            if name.split(\".\")[0] in self.context:\n                cls = eval(name, self.context)\n                assert issubclass(cls, Table)\n                description = cls().describe(context=self.context).split(\"\\n\")\n                description = (\n                    \"-\" * 30\n                    if q.startswith(\"---\")\n                    else q.replace(\"-&gt;\", \"&amp;#8594;\")\n                    if \"-&gt;\" in q\n                    else q.split(\":\")[0]\n                    for q in description\n                    if not q.startswith(\"#\")\n                )\n                node.set_tooltip(\"&amp;#13;\".join(description))\n            node.set_label(\n                \"&lt;&lt;u&gt;\" + name + \"&lt;/u&gt;&gt;\"\n                if node.get(\"distinguished\") == \"True\"\n                else name\n            )\n            node.set_color(props[\"color\"])\n            node.set_style(\"filled\")\n\n        for edge in dot.get_edges():\n            # see https://graphviz.org/doc/info/attrs.html\n            src = edge.get_source().strip('\"')\n            dest = edge.get_destination().strip('\"')\n            props = graph.get_edge_data(src, dest)\n            edge.set_color(\"#00000040\")\n            edge.set_style(\"solid\" if props[\"primary\"] else \"dashed\")\n            master_part = graph.nodes[dest][\n                \"node_type\"\n            ] is Part and dest.startswith(src + \".\")\n            edge.set_weight(3 if master_part else 1)\n            edge.set_arrowhead(\"none\")\n            edge.set_penwidth(0.75 if props[\"multi\"] else 2)\n\n        return dot\n\n    def make_svg(self):\n        from IPython.display import SVG\n\n        return SVG(self.make_dot().create_svg())\n\n    def make_png(self):\n        return io.BytesIO(self.make_dot().create_png())\n\n    def make_image(self):\n        if plot_active:\n            return plt.imread(self.make_png())\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def _repr_svg_(self):\n        return self.make_svg()._repr_svg_()\n\n    def draw(self):\n        if plot_active:\n            plt.imshow(self.make_image())\n            plt.gca().axis(\"off\")\n            plt.show()\n        else:\n            raise DataJointError(\"pyplot was not imported\")\n\n    def save(self, filename, format=None):\n        if format is None:\n            if filename.lower().endswith(\".png\"):\n                format = \"png\"\n            elif filename.lower().endswith(\".svg\"):\n                format = \"svg\"\n        if format.lower() == \"png\":\n            with open(filename, \"wb\") as f:\n                f.write(self.make_png().getbuffer().tobytes())\n        elif format.lower() == \"svg\":\n            with open(filename, \"w\") as f:\n                f.write(self.make_svg().data)\n        else:\n            raise DataJointError(\"Unsupported file format\")\n\n    @staticmethod\n    def _layout(graph, **kwargs):\n        return pydot_layout(graph, prog=\"dot\", **kwargs)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.diagram.Diagram.from_sequence", "title": "<code>from_sequence(sequence)</code>  <code>classmethod</code>", "text": "<p>The join Diagram for all objects in sequence</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <p>a sequence (e.g. list, tuple)</p> required <p>Returns:</p> Type Description <p>Diagram(arg1) + ... + Diagram(argn)</p> Source code in <code>datajoint/diagram.py</code> <pre><code>@classmethod\ndef from_sequence(cls, sequence):\n\"\"\"\n    The join Diagram for all objects in sequence\n\n    :param sequence: a sequence (e.g. list, tuple)\n    :return: Diagram(arg1) + ... + Diagram(argn)\n    \"\"\"\n    return functools.reduce(lambda x, y: x + y, map(Diagram, sequence))\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.diagram.Diagram.add_parts", "title": "<code>add_parts()</code>", "text": "<p>Adds to the diagram the part tables of tables already included in the diagram</p> <p>Returns:</p> Type Description Source code in <code>datajoint/diagram.py</code> <pre><code>def add_parts(self):\n\"\"\"\n    Adds to the diagram the part tables of tables already included in the diagram\n    :return:\n    \"\"\"\n\n    def is_part(part, master):\n\"\"\"\n        :param part:  `database`.`table_name`\n        :param master:   `database`.`table_name`\n        :return: True if part is part of master.\n        \"\"\"\n        part = [s.strip(\"`\") for s in part.split(\".\")]\n        master = [s.strip(\"`\") for s in master.split(\".\")]\n        return (\n            master[0] == part[0]\n            and master[1] + \"__\" == part[1][: len(master[1]) + 2]\n        )\n\n    self = Diagram(self)  # copy\n    self.nodes_to_show.update(\n        n\n        for n in self.nodes()\n        if any(is_part(n, m) for m in self.nodes_to_show)\n    )\n    return self\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.diagram.Diagram.topological_sort", "title": "<code>topological_sort()</code>", "text": "<p>Returns:</p> Type Description <p>list of nodes in topological order</p> Source code in <code>datajoint/diagram.py</code> <pre><code>def topological_sort(self):\n\"\"\":return:  list of nodes in topological order\"\"\"\n    return unite_master_parts(\n        list(\n            nx.algorithms.dag.topological_sort(\n                nx.DiGraph(self).subgraph(self.nodes_to_show)\n            )\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n\"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n\"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return self._attributes\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n\"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n\"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n\"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n\"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n\"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n\"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=attr[\"type\"].split(\"@\")[1]\n                    if category in EXTERNAL_TYPES\n                    else None,\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n\"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n\"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n\"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n\"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        \"`%s`\" % name\n        if self.attributes[name].attribute_expression is None\n        else self.attributes[name].attribute_expression\n        + (\" as `%s`\" % name if include_aliases else \"\")\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n\"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n\"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n\"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.conn", "title": "<code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)</code>", "text": "<p>Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <p>hostname</p> <code>None</code> <code>user</code> <p>mysql user</p> <code>None</code> <code>password</code> <p>mysql password</p> <code>None</code> <code>init_fun</code> <p>initialization function</p> <code>None</code> <code>reset</code> <p>whether the connection should be reset or not</p> <code>False</code> <code>use_tls</code> <p>TLS encryption option. Valid options are: True (required), False (required no TLS), None (TLS prefered, default), dict (Manually specify values per https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).</p> <code>None</code> Source code in <code>datajoint/connection.py</code> <pre><code>def conn(\n    host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None\n):\n\"\"\"\n    Returns a persistent connection object to be shared by multiple modules.\n    If the connection is not yet established or reset=True, a new connection is set up.\n    If connection information is not provided, it is taken from config which takes the\n    information from dj_local_conf.json. If the password is not specified in that file\n    datajoint prompts for the password.\n\n    :param host: hostname\n    :param user: mysql user\n    :param password: mysql password\n    :param init_fun: initialization function\n    :param reset: whether the connection should be reset or not\n    :param use_tls: TLS encryption option. Valid options are: True (required), False\n        (required no TLS), None (TLS prefered, default), dict (Manually specify values per\n        https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#encrypted-connection-options).\n    \"\"\"\n    if not hasattr(conn, \"connection\") or reset:\n        host = host if host is not None else config[\"database.host\"]\n        user = user if user is not None else config[\"database.user\"]\n        password = password if password is not None else config[\"database.password\"]\n        if user is None:\n            user = input(\"Please enter DataJoint username: \")\n        if password is None:\n            password = getpass(prompt=\"Please enter DataJoint password: \")\n        init_fun = (\n            init_fun if init_fun is not None else config[\"connection.init_function\"]\n        )\n        use_tls = use_tls if use_tls is not None else config[\"database.use_tls\"]\n        conn.connection = Connection(host, user, password, None, init_fun, use_tls)\n    return conn.connection\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Manual", "title": "<code>Manual</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Lookup", "title": "<code>Lookup</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Imported", "title": "<code>Imported</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Computed", "title": "<code>Computed</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Part", "title": "<code>Part</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.user_tables.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.user_tables.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.ExternalMapping", "title": "<code>ExternalMapping</code>", "text": "<p>         Bases: <code>Mapping</code></p> <p>The external manager contains all the tables for all external stores for a given schema :Example:     e = ExternalMapping(schema)     external_table = e[store]</p> Source code in <code>datajoint/external.py</code> <pre><code>class ExternalMapping(Mapping):\n\"\"\"\n    The external manager contains all the tables for all external stores for a given schema\n    :Example:\n        e = ExternalMapping(schema)\n        external_table = e[store]\n    \"\"\"\n\n    def __init__(self, schema):\n        self.schema = schema\n        self._tables = {}\n\n    def __repr__(self):\n        return \"External file tables for schema `{schema}`:\\n    \".format(\n            schema=self.schema.database\n        ) + \"\\n    \".join(\n            '\"{store}\" {protocol}:{location}'.format(store=k, **v.spec)\n            for k, v in self.items()\n        )\n\n    def __getitem__(self, store):\n\"\"\"\n        Triggers the creation of an external table.\n        Should only be used when ready to save or read from external storage.\n\n        :param store: the name of the store\n        :return: the ExternalTable object for the store\n        \"\"\"\n        if store not in self._tables:\n            self._tables[store] = ExternalTable(\n                connection=self.schema.connection,\n                store=store,\n                database=self.schema.database,\n            )\n        return self._tables[store]\n\n    def __len__(self):\n        return len(self._tables)\n\n    def __iter__(self):\n        return iter(self._tables)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.VirtualModule", "title": "<code>VirtualModule</code>", "text": "<p>         Bases: <code>types.ModuleType</code></p> <p>A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database. It declares the schema objects and a class for each table.</p> Source code in <code>datajoint/schemas.py</code> <pre><code>class VirtualModule(types.ModuleType):\n\"\"\"\n    A virtual module imitates a Python module representing a DataJoint schema from table definitions in the database.\n    It declares the schema objects and a class for each table.\n    \"\"\"\n\n    def __init__(\n        self,\n        module_name,\n        schema_name,\n        *,\n        create_schema=False,\n        create_tables=False,\n        connection=None,\n        add_objects=None,\n    ):\n\"\"\"\n        Creates a python module with the given name from the name of a schema on the server and\n        automatically adds classes to it corresponding to the tables in the schema.\n\n        :param module_name: displayed module name\n        :param schema_name: name of the database in mysql\n        :param create_schema: if True, create the schema on the database server\n        :param create_tables: if True, module.schema can be used as the decorator for declaring new\n        :param connection: a dj.Connection object to pass into the schema\n        :param add_objects: additional objects to add to the module\n        :return: the python module containing classes from the schema object and the table classes\n        \"\"\"\n        super(VirtualModule, self).__init__(name=module_name)\n        _schema = Schema(\n            schema_name,\n            create_schema=create_schema,\n            create_tables=create_tables,\n            connection=connection,\n        )\n        if add_objects:\n            self.__dict__.update(add_objects)\n        self.__dict__[\"schema\"] = _schema\n        _schema.spawn_missing_classes(context=self.__dict__)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.list_schemas", "title": "<code>list_schemas(connection=None)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>connection</code> <p>a dj.Connection object</p> <code>None</code> <p>Returns:</p> Type Description <p>list of all accessible schemas on the server</p> Source code in <code>datajoint/schemas.py</code> <pre><code>def list_schemas(connection=None):\n\"\"\"\n\n    :param connection: a dj.Connection object\n    :return: list of all accessible schemas on the server\n    \"\"\"\n    return [\n        r[0]\n        for r in (connection or conn()).query(\n            \"SELECT schema_name \"\n            \"FROM information_schema.schemata \"\n            'WHERE schema_name &lt;&gt; \"information_schema\"'\n        )\n    ]\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.lookup_class_name", "title": "<code>lookup_class_name(name, context, depth=3)</code>", "text": "<p>given a table name in the form <code>schema_name</code>.<code>table_name</code>, find its class in the context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p><code>schema_name</code>.<code>table_name</code></p> required <code>context</code> <p>dictionary representing the namespace</p> required <code>depth</code> <p>search depth into imported modules, helps avoid infinite recursion.</p> <code>3</code> <p>Returns:</p> Type Description <p>class name found in the context or None if not found</p> Source code in <code>datajoint/table.py</code> <pre><code>def lookup_class_name(name, context, depth=3):\n\"\"\"\n    given a table name in the form `schema_name`.`table_name`, find its class in the context.\n\n    :param name: `schema_name`.`table_name`\n    :param context: dictionary representing the namespace\n    :param depth: search depth into imported modules, helps avoid infinite recursion.\n    :return: class name found in the context or None if not found\n    \"\"\"\n    # breadth-first search\n    nodes = [dict(context=context, context_name=\"\", depth=depth)]\n    while nodes:\n        node = nodes.pop(0)\n        for member_name, member in node[\"context\"].items():\n            if not member_name.startswith(\"_\"):  # skip IPython's implicit variables\n                if inspect.isclass(member) and issubclass(member, Table):\n                    if member.full_table_name == name:  # found it!\n                        return \".\".join([node[\"context_name\"], member_name]).lstrip(\".\")\n                    try:  # look for part tables\n                        parts = member.__dict__\n                    except AttributeError:\n                        pass  # not a UserTable -- cannot have part tables.\n                    else:\n                        for part in (\n                            getattr(member, p)\n                            for p in parts\n                            if p[0].isupper() and hasattr(member, p)\n                        ):\n                            if (\n                                inspect.isclass(part)\n                                and issubclass(part, Table)\n                                and part.full_table_name == name\n                            ):\n                                return \".\".join(\n                                    [node[\"context_name\"], member_name, part.__name__]\n                                ).lstrip(\".\")\n                elif (\n                    node[\"depth\"] &gt; 0\n                    and inspect.ismodule(member)\n                    and member.__name__ != \"datajoint\"\n                ):\n                    try:\n                        nodes.append(\n                            dict(\n                                context=dict(inspect.getmembers(member)),\n                                context_name=node[\"context_name\"] + \".\" + member_name,\n                                depth=node[\"depth\"] - 1,\n                            )\n                        )\n                    except ImportError:\n                        pass  # could not import, so do not attempt\n    return None\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n\"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.schemas.Log", "title": "<code>Log</code>", "text": "<p>         Bases: <code>Table</code></p> <p>The log table for each schema. Instances are callable.  Calls log the time and identifying information along with the event.</p> <p>Parameters:</p> Name Type Description Default <code>skip_logging</code> <p>if True, then log entry is skipped by default. See call</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>class Log(Table):\n\"\"\"\n    The log table for each schema.\n    Instances are callable.  Calls log the time and identifying information along with the event.\n\n    :param skip_logging: if True, then log entry is skipped by default. See __call__\n    \"\"\"\n\n    _table_name = \"~log\"\n\n    def __init__(self, conn, database, skip_logging=False):\n        self.database = database\n        self.skip_logging = skip_logging\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # event logging table for `{database}`\n        id       :int unsigned auto_increment     # event order id\n        ---\n        timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp\n        version  :varchar(12)                     # datajoint version\n        user     :varchar(255)                    # user@host\n        host=\"\"  :varchar(255)                    # system hostname\n        event=\"\" :varchar(255)                    # event message\n        \"\"\".format(\n            database=database\n        )\n\n        super().__init__()\n\n        if not self.is_declared:\n            self.declare()\n            self.connection.dependencies.clear()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    def __call__(self, event, skip_logging=None):\n\"\"\"\n\n        :param event: string to write into the log table\n        :param skip_logging: If True then do not log. If None, then use self.skip_logging\n        \"\"\"\n        skip_logging = self.skip_logging if skip_logging is None else skip_logging\n        if not skip_logging:\n            try:\n                self.insert1(\n                    dict(\n                        user=self._user,\n                        version=version + \"py\",\n                        host=platform.uname().node,\n                        event=event,\n                    ),\n                    skip_duplicates=True,\n                    ignore_extra_fields=True,\n                )\n            except DataJointError:\n                logger.info(\"could not log event in table ~log\")\n\n    def delete(self):\n\"\"\"\n        bypass interactive prompts and cascading dependencies\n\n        :return: number of deleted items\n        \"\"\"\n        return self.delete_quick(get_count=True)\n\n    def drop(self):\n\"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n        self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.table.Log.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> <p>Returns:</p> Type Description <p>number of deleted items</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(self):\n\"\"\"\n    bypass interactive prompts and cascading dependencies\n\n    :return: number of deleted items\n    \"\"\"\n    return self.delete_quick(get_count=True)\n</code></pre>"}, {"location": "api/datajoint/schemas/#datajoint.table.Log.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/settings/", "title": "settings.py", "text": "<p>Settings for DataJoint.</p>"}, {"location": "api/datajoint/settings/#datajoint.settings.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config", "title": "<code>Config</code>", "text": "<p>         Bases: <code>collections.abc.MutableMapping</code></p> Source code in <code>datajoint/settings.py</code> <pre><code>class Config(collections.abc.MutableMapping):\n    instance = None\n\n    def __init__(self, *args, **kwargs):\n        if not Config.instance:\n            Config.instance = Config.__Config(*args, **kwargs)\n        else:\n            Config.instance._conf.update(dict(*args, **kwargs))\n\n    def __getattr__(self, name):\n        return getattr(self.instance, name)\n\n    def __getitem__(self, item):\n        return self.instance.__getitem__(item)\n\n    def __setitem__(self, item, value):\n        self.instance.__setitem__(item, value)\n\n    def __str__(self):\n        return pprint.pformat(self.instance._conf, indent=4)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __delitem__(self, key):\n        del self.instance._conf[key]\n\n    def __iter__(self):\n        return iter(self.instance._conf)\n\n    def __len__(self):\n        return len(self.instance._conf)\n\n    def save(self, filename, verbose=False):\n\"\"\"\n        Saves the settings in JSON format to the given file path.\n\n        :param filename: filename of the local JSON settings file.\n        :param verbose: report having saved the settings file\n        \"\"\"\n        with open(filename, \"w\") as fid:\n            json.dump(self._conf, fid, indent=4)\n        if verbose:\n            logger.info(\"Saved settings in \" + filename)\n\n    def load(self, filename):\n\"\"\"\n        Updates the setting from config file in JSON format.\n\n        :param filename: filename of the local JSON settings file. If None, the local config file is used.\n        \"\"\"\n        if filename is None:\n            filename = LOCALCONFIG\n        with open(filename, \"r\") as fid:\n            self._conf.update(json.load(fid))\n\n    def save_local(self, verbose=False):\n\"\"\"\n        saves the settings in the local config file\n        \"\"\"\n        self.save(LOCALCONFIG, verbose)\n\n    def save_global(self, verbose=False):\n\"\"\"\n        saves the settings in the global config file\n        \"\"\"\n        self.save(os.path.expanduser(os.path.join(\"~\", GLOBALCONFIG)), verbose)\n\n    def get_store_spec(self, store):\n\"\"\"\n        find configuration of external stores for blobs and attachments\n        \"\"\"\n        try:\n            spec = self[\"stores\"][store]\n        except KeyError:\n            raise DataJointError(\n                \"Storage {store} is requested but not configured\".format(store=store)\n            )\n\n        spec[\"subfolding\"] = spec.get(\"subfolding\", DEFAULT_SUBFOLDING)\n        spec_keys = {  # REQUIRED in uppercase and allowed in lowercase\n            \"file\": (\"PROTOCOL\", \"LOCATION\", \"subfolding\", \"stage\"),\n            \"s3\": (\n                \"PROTOCOL\",\n                \"ENDPOINT\",\n                \"BUCKET\",\n                \"ACCESS_KEY\",\n                \"SECRET_KEY\",\n                \"LOCATION\",\n                \"secure\",\n                \"subfolding\",\n                \"stage\",\n                \"proxy_server\",\n            ),\n        }\n\n        try:\n            spec_keys = spec_keys[spec.get(\"protocol\", \"\").lower()]\n        except KeyError:\n            raise DataJointError(\n                'Missing or invalid protocol in dj.config[\"stores\"][\"{store}\"]'.format(\n                    store=store\n                )\n            )\n\n        # check that all required keys are present in spec\n        try:\n            raise DataJointError(\n                'dj.config[\"stores\"][\"{store}\"] is missing \"{k}\"'.format(\n                    store=store,\n                    k=next(\n                        k.lower()\n                        for k in spec_keys\n                        if k.isupper() and k.lower() not in spec\n                    ),\n                )\n            )\n        except StopIteration:\n            pass\n\n        # check that only allowed keys are present in spec\n        try:\n            raise DataJointError(\n                'Invalid key \"{k}\" in dj.config[\"stores\"][\"{store}\"]'.format(\n                    store=store,\n                    k=next(\n                        k\n                        for k in spec\n                        if k.upper() not in spec_keys and k.lower() not in spec_keys\n                    ),\n                )\n            )\n        except StopIteration:\n            pass  # no invalid keys\n\n        return spec\n\n    @contextmanager\n    def __call__(self, **kwargs):\n\"\"\"\n        The config object can also be used in a with statement to change the state of the configuration\n        temporarily. kwargs to the context manager are the keys into config, where '.' is replaced by a\n        double underscore '__'. The context manager yields the changed config object.\n\n        Example:\n        &gt;&gt;&gt; import datajoint as dj\n        &gt;&gt;&gt; with dj.config(safemode=False, database__host=\"localhost\") as cfg:\n        &gt;&gt;&gt;     # do dangerous stuff here\n        \"\"\"\n\n        try:\n            backup = self.instance\n            self.instance = Config.__Config(self.instance._conf)\n            new = {k.replace(\"__\", \".\"): v for k, v in kwargs.items()}\n            self.instance._conf.update(new)\n            yield self\n        except:\n            self.instance = backup\n            raise\n        else:\n            self.instance = backup\n\n    class __Config:\n\"\"\"\n        Stores datajoint settings. Behaves like a dictionary, but applies validator functions\n        when certain keys are set.\n\n        The default parameters are stored in datajoint.settings.default . If a local config file\n        exists, the settings specified in this file override the default settings.\n        \"\"\"\n\n        def __init__(self, *args, **kwargs):\n            self._conf = dict(default)\n            self._conf.update(dict(*args, **kwargs))  # use the free update to set keys\n\n        def __getitem__(self, key):\n            return self._conf[key]\n\n        def __setitem__(self, key, value):\n            logger.debug(\"Setting {0:s} to {1:s}\".format(str(key), str(value)))\n            if validators[key](value):\n                self._conf[key] = value\n            else:\n                raise DataJointError(\"Validator for {0:s} did not pass\".format(key))\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save", "title": "<code>save(filename, verbose=False)</code>", "text": "<p>Saves the settings in JSON format to the given file path.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>filename of the local JSON settings file.</p> required <code>verbose</code> <p>report having saved the settings file</p> <code>False</code> Source code in <code>datajoint/settings.py</code> <pre><code>def save(self, filename, verbose=False):\n\"\"\"\n    Saves the settings in JSON format to the given file path.\n\n    :param filename: filename of the local JSON settings file.\n    :param verbose: report having saved the settings file\n    \"\"\"\n    with open(filename, \"w\") as fid:\n        json.dump(self._conf, fid, indent=4)\n    if verbose:\n        logger.info(\"Saved settings in \" + filename)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.load", "title": "<code>load(filename)</code>", "text": "<p>Updates the setting from config file in JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>filename of the local JSON settings file. If None, the local config file is used.</p> required Source code in <code>datajoint/settings.py</code> <pre><code>def load(self, filename):\n\"\"\"\n    Updates the setting from config file in JSON format.\n\n    :param filename: filename of the local JSON settings file. If None, the local config file is used.\n    \"\"\"\n    if filename is None:\n        filename = LOCALCONFIG\n    with open(filename, \"r\") as fid:\n        self._conf.update(json.load(fid))\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_local", "title": "<code>save_local(verbose=False)</code>", "text": "<p>saves the settings in the local config file</p> Source code in <code>datajoint/settings.py</code> <pre><code>def save_local(self, verbose=False):\n\"\"\"\n    saves the settings in the local config file\n    \"\"\"\n    self.save(LOCALCONFIG, verbose)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.save_global", "title": "<code>save_global(verbose=False)</code>", "text": "<p>saves the settings in the global config file</p> Source code in <code>datajoint/settings.py</code> <pre><code>def save_global(self, verbose=False):\n\"\"\"\n    saves the settings in the global config file\n    \"\"\"\n    self.save(os.path.expanduser(os.path.join(\"~\", GLOBALCONFIG)), verbose)\n</code></pre>"}, {"location": "api/datajoint/settings/#datajoint.settings.Config.get_store_spec", "title": "<code>get_store_spec(store)</code>", "text": "<p>find configuration of external stores for blobs and attachments</p> Source code in <code>datajoint/settings.py</code> <pre><code>def get_store_spec(self, store):\n\"\"\"\n    find configuration of external stores for blobs and attachments\n    \"\"\"\n    try:\n        spec = self[\"stores\"][store]\n    except KeyError:\n        raise DataJointError(\n            \"Storage {store} is requested but not configured\".format(store=store)\n        )\n\n    spec[\"subfolding\"] = spec.get(\"subfolding\", DEFAULT_SUBFOLDING)\n    spec_keys = {  # REQUIRED in uppercase and allowed in lowercase\n        \"file\": (\"PROTOCOL\", \"LOCATION\", \"subfolding\", \"stage\"),\n        \"s3\": (\n            \"PROTOCOL\",\n            \"ENDPOINT\",\n            \"BUCKET\",\n            \"ACCESS_KEY\",\n            \"SECRET_KEY\",\n            \"LOCATION\",\n            \"secure\",\n            \"subfolding\",\n            \"stage\",\n            \"proxy_server\",\n        ),\n    }\n\n    try:\n        spec_keys = spec_keys[spec.get(\"protocol\", \"\").lower()]\n    except KeyError:\n        raise DataJointError(\n            'Missing or invalid protocol in dj.config[\"stores\"][\"{store}\"]'.format(\n                store=store\n            )\n        )\n\n    # check that all required keys are present in spec\n    try:\n        raise DataJointError(\n            'dj.config[\"stores\"][\"{store}\"] is missing \"{k}\"'.format(\n                store=store,\n                k=next(\n                    k.lower()\n                    for k in spec_keys\n                    if k.isupper() and k.lower() not in spec\n                ),\n            )\n        )\n    except StopIteration:\n        pass\n\n    # check that only allowed keys are present in spec\n    try:\n        raise DataJointError(\n            'Invalid key \"{k}\" in dj.config[\"stores\"][\"{store}\"]'.format(\n                store=store,\n                k=next(\n                    k\n                    for k in spec\n                    if k.upper() not in spec_keys and k.lower() not in spec_keys\n                ),\n            )\n        )\n    except StopIteration:\n        pass  # no invalid keys\n\n    return spec\n</code></pre>"}, {"location": "api/datajoint/table/", "title": "table.py", "text": ""}, {"location": "api/datajoint/table/#datajoint.table.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.user_choice", "title": "<code>user_choice(prompt, choices=('yes', 'no'), default=None)</code>", "text": "<p>Prompts the user for confirmation.  The default value, if any, is capitalized.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>Information to display to the user.</p> required <code>choices</code> <p>an iterable of possible choices.</p> <code>('yes', 'no')</code> <code>default</code> <p>default choice</p> <code>None</code> <p>Returns:</p> Type Description <p>the user's choice</p> Source code in <code>datajoint/utils.py</code> <pre><code>def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n\"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = \", \".join(\n        (choice.title() if choice == default else choice for choice in choices)\n    )\n    response = None\n    while response not in choices:\n        response = input(prompt + \" [\" + choice_list + \"]: \")\n        response = response.lower() if response else default\n    return response\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.QueryExpression", "title": "<code>QueryExpression</code>", "text": "<p>QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union.</p> <p>A QueryExpression object has a support, a restriction (an AndList), and heading. Property <code>heading</code> (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj.</p> <p>Property <code>support</code> is the list of table names or other QueryExpressions to be joined.</p> <p>The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute.</p> <p>Application of operators does not always lead to the creation of a subquery. A subquery is generated when:     1. A restriction is applied on any computed or renamed attributes     2. A projection is applied remapping remapped attributes     3. Subclasses: Join, Aggregation, and Union have additional specific rules.</p> Source code in <code>datajoint/expression.py</code> <pre><code>class QueryExpression:\n\"\"\"\n    QueryExpression implements query operators to derive new entity set from its input.\n    A QueryExpression object generates a SELECT statement in SQL.\n    QueryExpression operators are restrict, join, proj, aggr, and union.\n\n    A QueryExpression object has a support, a restriction (an AndList), and heading.\n    Property `heading` (type dj.Heading) contains information about the attributes.\n    It is loaded from the database and updated by proj.\n\n    Property `support` is the list of table names or other QueryExpressions to be joined.\n\n    The restriction is applied first without having access to the attributes generated by the projection.\n    Then projection is applied by selecting modifying the heading attribute.\n\n    Application of operators does not always lead to the creation of a subquery.\n    A subquery is generated when:\n        1. A restriction is applied on any computed or renamed attributes\n        2. A projection is applied remapping remapped attributes\n        3. Subclasses: Join, Aggregation, and Union have additional specific rules.\n    \"\"\"\n\n    _restriction = None\n    _restriction_attributes = None\n    _left = []  # list of booleans True for left joins, False for inner joins\n    _original_heading = None  # heading before projections\n\n    # subclasses or instantiators must provide values\n    _connection = None\n    _heading = None\n    _support = None\n\n    # If the query will be using distinct\n    _distinct = False\n\n    @property\n    def connection(self):\n\"\"\"a dj.Connection object\"\"\"\n        assert self._connection is not None\n        return self._connection\n\n    @property\n    def support(self):\n\"\"\"A list of table names or subqueries to from the FROM clause\"\"\"\n        assert self._support is not None\n        return self._support\n\n    @property\n    def heading(self):\n\"\"\"a dj.Heading object, reflects the effects of the projection operator .proj\"\"\"\n        return self._heading\n\n    @property\n    def original_heading(self):\n\"\"\"a dj.Heading object reflecting the attributes before projection\"\"\"\n        return self._original_heading or self.heading\n\n    @property\n    def restriction(self):\n\"\"\"a AndList object of restrictions applied to input to produce the result\"\"\"\n        if self._restriction is None:\n            self._restriction = AndList()\n        return self._restriction\n\n    @property\n    def restriction_attributes(self):\n\"\"\"the set of attribute names invoked in the WHERE clause\"\"\"\n        if self._restriction_attributes is None:\n            self._restriction_attributes = set()\n        return self._restriction_attributes\n\n    @property\n    def primary_key(self):\n        return self.heading.primary_key\n\n    _subquery_alias_count = count()  # count for alias names used in the FROM clause\n\n    def from_clause(self):\n        support = (\n            \"(\" + src.make_sql() + \") as `$%x`\" % next(self._subquery_alias_count)\n            if isinstance(src, QueryExpression)\n            else src\n            for src in self.support\n        )\n        clause = next(support)\n        for s, left in zip(support, self._left):\n            clause += \" NATURAL{left} JOIN {clause}\".format(\n                left=\" LEFT\" if left else \"\", clause=s\n            )\n        return clause\n\n    def where_clause(self):\n        return (\n            \"\"\n            if not self.restriction\n            else \" WHERE (%s)\" % \")AND(\".join(str(s) for s in self.restriction)\n        )\n\n    def make_sql(self, fields=None):\n\"\"\"\n        Make the SQL SELECT statement.\n\n        :param fields: used to explicitly set the select attributes\n        \"\"\"\n        return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n            distinct=\"DISTINCT \" if self._distinct else \"\",\n            fields=self.heading.as_sql(fields or self.heading.names),\n            from_=self.from_clause(),\n            where=self.where_clause(),\n        )\n\n    # --------- query operators -----------\n    def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = [self]\n        result._heading = self.heading.make_subquery_heading()\n        return result\n\n    def restrict(self, restriction):\n\"\"\"\n        Produces a new expression with the new restriction applied.\n        rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n        rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n        The primary key of the result is unaffected.\n        Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n        Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n        (logical disjunction of conditions)\n        Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n        The expressions in each row equivalent:\n\n        rel &amp; True                          rel\n        rel &amp; False                         the empty entity set\n        rel &amp; 'TRUE'                        rel\n        rel &amp; 'FALSE'                       the empty entity set\n        rel - cond                          rel &amp; Not(cond)\n        rel - 'TRUE'                        rel &amp; False\n        rel - 'FALSE'                       rel\n        rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n        rel &amp; AndList()                     rel\n        rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n        rel &amp; []                            rel &amp; False\n        rel &amp; None                          rel &amp; False\n        rel &amp; any_empty_entity_set          rel &amp; False\n        rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n        rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n        rel - AndList()                     rel &amp; False\n        rel - []                            rel\n        rel - None                          rel\n        rel - any_empty_entity_set          rel\n\n        When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n        one element in arg (hence arg is treated as an OrList).\n        Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n        Two elements match when their common attributes have equal values or when they have no common attributes.\n        All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n        QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n        ultimately call restrict()\n\n        :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n        string, or an AndList.\n        \"\"\"\n        attributes = set()\n        new_condition = make_condition(self, restriction, attributes)\n        if new_condition is True:\n            return self  # restriction has no effect, return the same object\n        # check that all attributes in condition are present in the query\n        try:\n            raise DataJointError(\n                \"Attribute `%s` is not found in query.\"\n                % next(attr for attr in attributes if attr not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n        # If the new condition uses any new attributes, a subquery is required.\n        # However, Aggregation's HAVING statement works fine with aliased attributes.\n        need_subquery = isinstance(self, Union) or (\n            not isinstance(self, Aggregation) and self.heading.new_attributes\n        )\n        if need_subquery:\n            result = self.make_subquery()\n        else:\n            result = copy.copy(self)\n            result._restriction = AndList(\n                self.restriction\n            )  # copy to preserve the original\n        result.restriction.append(new_condition)\n        result.restriction_attributes.update(attributes)\n        return result\n\n    def restrict_in_place(self, restriction):\n        self.__dict__.update(self.restrict(restriction).__dict__)\n\n    def __and__(self, restriction):\n\"\"\"\n        Restriction operator e.g. ``q1 &amp; q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(restriction)\n\n    def __xor__(self, restriction):\n\"\"\"\n        Permissive restriction operator ignoring compatibility check  e.g. ``q1 ^ q2``.\n        \"\"\"\n        if inspect.isclass(restriction) and issubclass(restriction, QueryExpression):\n            restriction = restriction()\n        if isinstance(restriction, Not):\n            return self.restrict(Not(PromiscuousOperand(restriction.restriction)))\n        return self.restrict(PromiscuousOperand(restriction))\n\n    def __sub__(self, restriction):\n\"\"\"\n        Inverted restriction e.g. ``q1 - q2``.\n        :return: a restricted copy of the input argument\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        return self.restrict(Not(restriction))\n\n    def __neg__(self):\n\"\"\"\n        Convert between restriction and inverted restriction e.g. ``-q1``.\n        :return: target restriction\n        See QueryExpression.restrict for more detail.\n        \"\"\"\n        if isinstance(self, Not):\n            return self.restriction\n        return Not(self)\n\n    def __mul__(self, other):\n\"\"\"\n        join of query expressions `self` and `other` e.g. ``q1 * q2``.\n        \"\"\"\n        return self.join(other)\n\n    def __matmul__(self, other):\n\"\"\"\n        Permissive join of query expressions `self` and `other` ignoring compatibility check\n            e.g. ``q1 @ q2``.\n        \"\"\"\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        return self.join(other, semantic_check=False)\n\n    def join(self, other, semantic_check=True, left=False):\n\"\"\"\n        create the joined QueryExpression.\n        a * b  is short for A.join(B)\n        a @ b  is short for A.join(B, semantic_check=False)\n        Additionally, left=True will retain the rows of self, effectively performing a left join.\n        \"\"\"\n        # trigger subqueries if joining on renamed attributes\n        if isinstance(other, U):\n            return other * self\n        if inspect.isclass(other) and issubclass(other, QueryExpression):\n            other = other()  # instantiate\n        if not isinstance(other, QueryExpression):\n            raise DataJointError(\"The argument of join must be a QueryExpression\")\n        if semantic_check:\n            assert_join_compatibility(self, other)\n        join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n        # needs subquery if self's FROM clause has common attributes with other's FROM clause\n        need_subquery1 = need_subquery2 = bool(\n            (set(self.original_heading.names) &amp; set(other.original_heading.names))\n            - join_attributes\n        )\n        # need subquery if any of the join attributes are derived\n        need_subquery1 = (\n            need_subquery1\n            or isinstance(self, Aggregation)\n            or any(n in self.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        need_subquery2 = (\n            need_subquery2\n            or isinstance(other, Aggregation)\n            or any(n in other.heading.new_attributes for n in join_attributes)\n            or isinstance(self, Union)\n        )\n        if need_subquery1:\n            self = self.make_subquery()\n        if need_subquery2:\n            other = other.make_subquery()\n        result = QueryExpression()\n        result._connection = self.connection\n        result._support = self.support + other.support\n        result._left = self._left + [left] + other._left\n        result._heading = self.heading.join(other.heading)\n        result._restriction = AndList(self.restriction)\n        result._restriction.append(other.restriction)\n        result._original_heading = self.original_heading.join(other.original_heading)\n        assert len(result.support) == len(result._left) + 1\n        return result\n\n    def __add__(self, other):\n\"\"\"union e.g. ``q1 + q2``.\"\"\"\n        return Union.create(self, other)\n\n    def proj(self, *attributes, **named_attributes):\n\"\"\"\n        Projection operator.\n\n        :param attributes:  attributes to be included in the result. (The primary key is already included).\n        :param named_attributes: new attributes computed or renamed from existing attributes.\n        :return: the projected expression.\n        Primary key attributes cannot be excluded but may be renamed.\n        If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n        Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n        Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n        self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n        self.proj() -- include only primary key\n        self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n        self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n        self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n        self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n        self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n        from other attributes available before the projection.\n        Each attribute name can only be used once.\n        \"\"\"\n        named_attributes = {\n            k: translate_attribute(v)[1] for k, v in named_attributes.items()\n        }\n        # new attributes in parentheses are included again with the new name without removing original\n        duplication_pattern = re.compile(\n            rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n        )\n        # attributes without parentheses renamed\n        rename_pattern = re.compile(\n            rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n        )\n        replicate_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        rename_map = {\n            k: m.group(\"name\")\n            for k, m in (\n                (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n            )\n            if m\n        }\n        compute_map = {\n            k: v\n            for k, v in named_attributes.items()\n            if not duplication_pattern.match(v) and not rename_pattern.match(v)\n        }\n        attributes = set(attributes)\n        # include primary key\n        attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n        # include all secondary attributes with Ellipsis\n        if Ellipsis in attributes:\n            attributes.discard(Ellipsis)\n            attributes.update(\n                (\n                    a\n                    for a in self.heading.secondary_attributes\n                    if a not in attributes and a not in rename_map.values()\n                )\n            )\n        try:\n            raise DataJointError(\n                \"%s is not a valid data type for an attribute in .proj\"\n                % next(a for a in attributes if not isinstance(a, str))\n            )\n        except StopIteration:\n            pass  # normal case\n        # remove excluded attributes, specified as `-attr'\n        excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n        attributes.difference_update(excluded)\n        excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n        attributes.difference_update(excluded)\n        try:\n            raise DataJointError(\n                \"Cannot exclude primary key attribute %s\",\n                next(a for a in excluded if a in self.primary_key),\n            )\n        except StopIteration:\n            pass  # all ok\n        # check that all attributes exist in heading\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(a for a in attributes if a not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that all mentioned names are present in heading\n        mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n        try:\n            raise DataJointError(\n                \"Attribute '%s' not found.\"\n                % next(a for a in mentions if not self.heading.names)\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # check that newly created attributes do not clash with any other selected attributes\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in rename_map\n                    if a in attributes.union(compute_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in compute_map\n                    if a in attributes.union(rename_map).union(replicate_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n        try:\n            raise DataJointError(\n                \"Attribute `%s` already exists\"\n                % next(\n                    a\n                    for a in replicate_map\n                    if a in attributes.union(rename_map).union(compute_map)\n                )\n            )\n        except StopIteration:\n            pass  # all ok\n\n        # need a subquery if the projection remaps any remapped attributes\n        used = set(q for v in compute_map.values() for q in extract_column_names(v))\n        used.update(rename_map.values())\n        used.update(replicate_map.values())\n        used.intersection_update(self.heading.names)\n        need_subquery = isinstance(self, Union) or any(\n            self.heading[name].attribute_expression is not None for name in used\n        )\n        if not need_subquery and self.restriction:\n            # need a subquery if the restriction applies to attributes that have been renamed\n            need_subquery = any(\n                name in self.restriction_attributes\n                for name in self.heading.new_attributes\n            )\n\n        result = self.make_subquery() if need_subquery else copy.copy(self)\n        result._original_heading = result.original_heading\n        result._heading = result.heading.select(\n            attributes,\n            rename_map=dict(**rename_map, **replicate_map),\n            compute_map=compute_map,\n        )\n        return result\n\n    def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n        Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n        has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n        :param group:  The query expression to be aggregated.\n        :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n        :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n        :return: The derived query expression\n        \"\"\"\n        if Ellipsis in attributes:\n            # expand ellipsis to include only attributes from the left table\n            attributes = set(attributes)\n            attributes.discard(Ellipsis)\n            attributes.update(self.heading.secondary_attributes)\n        return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n            *attributes, **named_attributes\n        )\n\n    aggregate = aggr  # alias for aggr\n\n    # ---------- Fetch operators --------------------\n    @property\n    def fetch1(self):\n        return Fetch1(self)\n\n    @property\n    def fetch(self):\n        return Fetch(self)\n\n    def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the first few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n\n    def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n        shortcut to fetch the last few entries from query expression.\n        Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n        :param limit:  number of entries\n        :param fetch_kwargs: kwargs for fetch\n        :return: query result\n        \"\"\"\n        return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n\n    def __len__(self):\n\"\"\":return: number of elements in the result set e.g. ``len(q1)``.\"\"\"\n        return self.connection.query(\n            \"SELECT {select_} FROM {from_}{where}\".format(\n                select_=(\n                    \"count(*)\"\n                    if any(self._left)\n                    else \"count(DISTINCT {fields})\".format(\n                        fields=self.heading.as_sql(\n                            self.primary_key, include_aliases=False\n                        )\n                    )\n                ),\n                from_=self.from_clause(),\n                where=self.where_clause(),\n            )\n        ).fetchone()[0]\n\n    def __bool__(self):\n\"\"\"\n        :return: True if the result is not empty. Equivalent to len(self) &gt; 0 but often\n            faster e.g. ``bool(q1)``.\n        \"\"\"\n        return bool(\n            self.connection.query(\n                \"SELECT EXISTS(SELECT 1 FROM {from_}{where})\".format(\n                    from_=self.from_clause(), where=self.where_clause()\n                )\n            ).fetchone()[0]\n        )\n\n    def __contains__(self, item):\n\"\"\"\n        returns True if the restriction in item matches any entries in self\n            e.g. ``restriction in q1``.\n\n        :param item: any restriction\n        (item in query_expression) is equivalent to bool(query_expression &amp; item) but may be\n        executed more efficiently.\n        \"\"\"\n        return bool(self &amp; item)  # May be optimized e.g. using an EXISTS query\n\n    def __iter__(self):\n\"\"\"\n        returns an iterator-compatible QueryExpression object e.g. ``iter(q1)``.\n\n        :param self: iterator-compatible QueryExpression object\n        \"\"\"\n        self._iter_only_key = all(v.in_key for v in self.heading.attributes.values())\n        self._iter_keys = self.fetch(\"KEY\")\n        return self\n\n    def __next__(self):\n\"\"\"\n        returns the next record on an iterator-compatible QueryExpression object\n            e.g. ``next(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: dict\n        \"\"\"\n        try:\n            key = self._iter_keys.pop(0)\n        except AttributeError:\n            # self._iter_keys is missing because __iter__ has not been called.\n            raise TypeError(\n                \"A QueryExpression object is not an iterator. \"\n                \"Use iter(obj) to create an iterator.\"\n            )\n        except IndexError:\n            raise StopIteration\n        else:\n            if self._iter_only_key:\n                return key\n            else:\n                try:\n                    return (self &amp; key).fetch1()\n                except DataJointError:\n                    # The data may have been deleted since the moment the keys were fetched\n                    # -- move on to next entry.\n                    return next(self)\n\n    def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n        See expression.fetch() for input description.\n        :return: query cursor\n        \"\"\"\n        if offset and limit is None:\n            raise DataJointError(\"limit is required when offset is set\")\n        sql = self.make_sql()\n        if order_by is not None:\n            sql += \" ORDER BY \" + \", \".join(order_by)\n        if limit is not None:\n            sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n        logger.debug(sql)\n        return self.connection.query(sql, as_dict=as_dict)\n\n    def __repr__(self):\n\"\"\"\n        returns the string representation of a QueryExpression object e.g. ``str(q1)``.\n\n        :param self: A query expression\n        :type self: :class:`QueryExpression`\n        :rtype: str\n        \"\"\"\n        return (\n            super().__repr__()\n            if config[\"loglevel\"].lower() == \"debug\"\n            else self.preview()\n        )\n\n    def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n        return preview(self, limit, width)\n\n    def _repr_html_(self):\n\"\"\":return: HTML to display table in Jupyter notebook.\"\"\"\n        return repr_html(self)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.connection", "title": "<code>connection</code>  <code>property</code>", "text": "<p>a dj.Connection object</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.support", "title": "<code>support</code>  <code>property</code>", "text": "<p>A list of table names or subqueries to from the FROM clause</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.heading", "title": "<code>heading</code>  <code>property</code>", "text": "<p>a dj.Heading object, reflects the effects of the projection operator .proj</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.original_heading", "title": "<code>original_heading</code>  <code>property</code>", "text": "<p>a dj.Heading object reflecting the attributes before projection</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.restriction", "title": "<code>restriction</code>  <code>property</code>", "text": "<p>a AndList object of restrictions applied to input to produce the result</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.restriction_attributes", "title": "<code>restriction_attributes</code>  <code>property</code>", "text": "<p>the set of attribute names invoked in the WHERE clause</p>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.make_sql", "title": "<code>make_sql(fields=None)</code>", "text": "<p>Make the SQL SELECT statement.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>used to explicitly set the select attributes</p> <code>None</code> Source code in <code>datajoint/expression.py</code> <pre><code>def make_sql(self, fields=None):\n\"\"\"\n    Make the SQL SELECT statement.\n\n    :param fields: used to explicitly set the select attributes\n    \"\"\"\n    return \"SELECT {distinct}{fields} FROM {from_}{where}\".format(\n        distinct=\"DISTINCT \" if self._distinct else \"\",\n        fields=self.heading.as_sql(fields or self.heading.names),\n        from_=self.from_clause(),\n        where=self.where_clause(),\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.make_subquery", "title": "<code>make_subquery()</code>", "text": "<p>create a new SELECT statement where self is the FROM clause</p> Source code in <code>datajoint/expression.py</code> <pre><code>def make_subquery(self):\n\"\"\"create a new SELECT statement where self is the FROM clause\"\"\"\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = [self]\n    result._heading = self.heading.make_subquery_heading()\n    return result\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.restrict", "title": "<code>restrict(restriction)</code>", "text": "<p>Produces a new expression with the new restriction applied. rel.restrict(restriction)  is equivalent to  rel &amp; restriction. rel.restrict(Not(restriction))  is equivalent to  rel - restriction The primary key of the result is unaffected. Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class.</p> <p>The expressions in each row equivalent:</p> <p>rel &amp; True                          rel rel &amp; False                         the empty entity set rel &amp; 'TRUE'                        rel rel &amp; 'FALSE'                       the empty entity set rel - cond                          rel &amp; Not(cond) rel - 'TRUE'                        rel &amp; False rel - 'FALSE'                       rel rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2 rel &amp; AndList()                     rel rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2)) rel &amp; []                            rel &amp; False rel &amp; None                          rel &amp; False rel &amp; any_empty_entity_set          rel &amp; False rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)] rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2) rel - AndList()                     rel &amp; False rel - []                            rel rel - None                          rel rel - any_empty_entity_set          rel</p> <p>When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.</p> <p>QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict()</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <p>a sequence or an array (treated as OR list), another QueryExpression, an SQL condition string, or an AndList.</p> required Source code in <code>datajoint/expression.py</code> <pre><code>def restrict(self, restriction):\n\"\"\"\n    Produces a new expression with the new restriction applied.\n    rel.restrict(restriction)  is equivalent to  rel &amp; restriction.\n    rel.restrict(Not(restriction))  is equivalent to  rel - restriction\n    The primary key of the result is unaffected.\n    Successive restrictions are combined as logical AND:   r &amp; a &amp; b  is equivalent to r &amp; AndList((a, b))\n    Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists\n    (logical disjunction of conditions)\n    Inverse restriction is accomplished by either using the subtraction operator or the Not class.\n\n    The expressions in each row equivalent:\n\n    rel &amp; True                          rel\n    rel &amp; False                         the empty entity set\n    rel &amp; 'TRUE'                        rel\n    rel &amp; 'FALSE'                       the empty entity set\n    rel - cond                          rel &amp; Not(cond)\n    rel - 'TRUE'                        rel &amp; False\n    rel - 'FALSE'                       rel\n    rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2\n    rel &amp; AndList()                     rel\n    rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2))\n    rel &amp; []                            rel &amp; False\n    rel &amp; None                          rel &amp; False\n    rel &amp; any_empty_entity_set          rel &amp; False\n    rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)]\n    rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2)\n    rel - AndList()                     rel &amp; False\n    rel - []                            rel\n    rel - None                          rel\n    rel - any_empty_entity_set          rel\n\n    When arg is another QueryExpression, the restriction  rel &amp; arg  restricts rel to elements that match at least\n    one element in arg (hence arg is treated as an OrList).\n    Conversely,  rel - arg  restricts rel to elements that do not match any elements in arg.\n    Two elements match when their common attributes have equal values or when they have no common attributes.\n    All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.\n\n    QueryExpression.restrict is the only access point that modifies restrictions. All other operators must\n    ultimately call restrict()\n\n    :param restriction: a sequence or an array (treated as OR list), another QueryExpression, an SQL condition\n    string, or an AndList.\n    \"\"\"\n    attributes = set()\n    new_condition = make_condition(self, restriction, attributes)\n    if new_condition is True:\n        return self  # restriction has no effect, return the same object\n    # check that all attributes in condition are present in the query\n    try:\n        raise DataJointError(\n            \"Attribute `%s` is not found in query.\"\n            % next(attr for attr in attributes if attr not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n    # If the new condition uses any new attributes, a subquery is required.\n    # However, Aggregation's HAVING statement works fine with aliased attributes.\n    need_subquery = isinstance(self, Union) or (\n        not isinstance(self, Aggregation) and self.heading.new_attributes\n    )\n    if need_subquery:\n        result = self.make_subquery()\n    else:\n        result = copy.copy(self)\n        result._restriction = AndList(\n            self.restriction\n        )  # copy to preserve the original\n    result.restriction.append(new_condition)\n    result.restriction_attributes.update(attributes)\n    return result\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.join", "title": "<code>join(other, semantic_check=True, left=False)</code>", "text": "<p>create the joined QueryExpression. a * b  is short for A.join(B) a @ b  is short for A.join(B, semantic_check=False) Additionally, left=True will retain the rows of self, effectively performing a left join.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def join(self, other, semantic_check=True, left=False):\n\"\"\"\n    create the joined QueryExpression.\n    a * b  is short for A.join(B)\n    a @ b  is short for A.join(B, semantic_check=False)\n    Additionally, left=True will retain the rows of self, effectively performing a left join.\n    \"\"\"\n    # trigger subqueries if joining on renamed attributes\n    if isinstance(other, U):\n        return other * self\n    if inspect.isclass(other) and issubclass(other, QueryExpression):\n        other = other()  # instantiate\n    if not isinstance(other, QueryExpression):\n        raise DataJointError(\"The argument of join must be a QueryExpression\")\n    if semantic_check:\n        assert_join_compatibility(self, other)\n    join_attributes = set(n for n in self.heading.names if n in other.heading.names)\n    # needs subquery if self's FROM clause has common attributes with other's FROM clause\n    need_subquery1 = need_subquery2 = bool(\n        (set(self.original_heading.names) &amp; set(other.original_heading.names))\n        - join_attributes\n    )\n    # need subquery if any of the join attributes are derived\n    need_subquery1 = (\n        need_subquery1\n        or isinstance(self, Aggregation)\n        or any(n in self.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    need_subquery2 = (\n        need_subquery2\n        or isinstance(other, Aggregation)\n        or any(n in other.heading.new_attributes for n in join_attributes)\n        or isinstance(self, Union)\n    )\n    if need_subquery1:\n        self = self.make_subquery()\n    if need_subquery2:\n        other = other.make_subquery()\n    result = QueryExpression()\n    result._connection = self.connection\n    result._support = self.support + other.support\n    result._left = self._left + [left] + other._left\n    result._heading = self.heading.join(other.heading)\n    result._restriction = AndList(self.restriction)\n    result._restriction.append(other.restriction)\n    result._original_heading = self.original_heading.join(other.original_heading)\n    assert len(result.support) == len(result._left) + 1\n    return result\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.proj", "title": "<code>proj(*attributes, **named_attributes)</code>", "text": "<p>Projection operator.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <p>attributes to be included in the result. (The primary key is already included).</p> <code>()</code> <code>named_attributes</code> <p>new attributes computed or renamed from existing attributes.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def proj(self, *attributes, **named_attributes):\n\"\"\"\n    Projection operator.\n\n    :param attributes:  attributes to be included in the result. (The primary key is already included).\n    :param named_attributes: new attributes computed or renamed from existing attributes.\n    :return: the projected expression.\n    Primary key attributes cannot be excluded but may be renamed.\n    If the attribute list contains an Ellipsis ..., then all secondary attributes are included too\n    Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present.\n    Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or\n    self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self)\n    self.proj() -- include only primary key\n    self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2\n    self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2\n    self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1\n    self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup'\n    self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax)\n    from other attributes available before the projection.\n    Each attribute name can only be used once.\n    \"\"\"\n    named_attributes = {\n        k: translate_attribute(v)[1] for k, v in named_attributes.items()\n    }\n    # new attributes in parentheses are included again with the new name without removing original\n    duplication_pattern = re.compile(\n        rf'^\\s*\\(\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*\\)\\s*$'\n    )\n    # attributes without parentheses renamed\n    rename_pattern = re.compile(\n        rf'^\\s*(?!{\"|\".join(CONSTANT_LITERALS)})(?P&lt;name&gt;[a-zA-Z_]\\w*)\\s*$'\n    )\n    replicate_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, duplication_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    rename_map = {\n        k: m.group(\"name\")\n        for k, m in (\n            (k, rename_pattern.match(v)) for k, v in named_attributes.items()\n        )\n        if m\n    }\n    compute_map = {\n        k: v\n        for k, v in named_attributes.items()\n        if not duplication_pattern.match(v) and not rename_pattern.match(v)\n    }\n    attributes = set(attributes)\n    # include primary key\n    attributes.update((k for k in self.primary_key if k not in rename_map.values()))\n    # include all secondary attributes with Ellipsis\n    if Ellipsis in attributes:\n        attributes.discard(Ellipsis)\n        attributes.update(\n            (\n                a\n                for a in self.heading.secondary_attributes\n                if a not in attributes and a not in rename_map.values()\n            )\n        )\n    try:\n        raise DataJointError(\n            \"%s is not a valid data type for an attribute in .proj\"\n            % next(a for a in attributes if not isinstance(a, str))\n        )\n    except StopIteration:\n        pass  # normal case\n    # remove excluded attributes, specified as `-attr'\n    excluded = set(a for a in attributes if a.strip().startswith(\"-\"))\n    attributes.difference_update(excluded)\n    excluded = set(a.lstrip(\"-\").strip() for a in excluded)\n    attributes.difference_update(excluded)\n    try:\n        raise DataJointError(\n            \"Cannot exclude primary key attribute %s\",\n            next(a for a in excluded if a in self.primary_key),\n        )\n    except StopIteration:\n        pass  # all ok\n    # check that all attributes exist in heading\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(a for a in attributes if a not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that all mentioned names are present in heading\n    mentions = attributes.union(replicate_map.values()).union(rename_map.values())\n    try:\n        raise DataJointError(\n            \"Attribute '%s' not found.\"\n            % next(a for a in mentions if not self.heading.names)\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # check that newly created attributes do not clash with any other selected attributes\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in rename_map\n                if a in attributes.union(compute_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in compute_map\n                if a in attributes.union(rename_map).union(replicate_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n    try:\n        raise DataJointError(\n            \"Attribute `%s` already exists\"\n            % next(\n                a\n                for a in replicate_map\n                if a in attributes.union(rename_map).union(compute_map)\n            )\n        )\n    except StopIteration:\n        pass  # all ok\n\n    # need a subquery if the projection remaps any remapped attributes\n    used = set(q for v in compute_map.values() for q in extract_column_names(v))\n    used.update(rename_map.values())\n    used.update(replicate_map.values())\n    used.intersection_update(self.heading.names)\n    need_subquery = isinstance(self, Union) or any(\n        self.heading[name].attribute_expression is not None for name in used\n    )\n    if not need_subquery and self.restriction:\n        # need a subquery if the restriction applies to attributes that have been renamed\n        need_subquery = any(\n            name in self.restriction_attributes\n            for name in self.heading.new_attributes\n        )\n\n    result = self.make_subquery() if need_subquery else copy.copy(self)\n    result._original_heading = result.original_heading\n    result._heading = result.heading.select(\n        attributes,\n        rename_map=dict(**rename_map, **replicate_map),\n        compute_map=compute_map,\n    )\n    return result\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.aggr", "title": "<code>aggr(group, *attributes, keep_all_rows=False, **named_attributes)</code>", "text": "<p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <p>The query expression to be aggregated.</p> required <code>keep_all_rows</code> <p>True=keep all the rows from self. False=keep only rows that match entries in group.</p> <code>False</code> <code>named_attributes</code> <p>computations of the form new_attribute=\"sql expression on attributes of group\"</p> <code>{}</code> <p>Returns:</p> Type Description <p>The derived query expression</p> Source code in <code>datajoint/expression.py</code> <pre><code>def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n\"\"\"\n    Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\")\n    has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of `group`.\n\n    :param group:  The query expression to be aggregated.\n    :param keep_all_rows: True=keep all the rows from self. False=keep only rows that match entries in group.\n    :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\"\n    :return: The derived query expression\n    \"\"\"\n    if Ellipsis in attributes:\n        # expand ellipsis to include only attributes from the left table\n        attributes = set(attributes)\n        attributes.discard(Ellipsis)\n        attributes.update(self.heading.secondary_attributes)\n    return Aggregation.create(self, group=group, keep_all_rows=keep_all_rows).proj(\n        *attributes, **named_attributes\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.head", "title": "<code>head(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the first few entries from query expression. Equivalent to fetch(order_by=\"KEY\", limit=25)</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def head(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the first few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY\", limit=25)\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY\", limit=limit, **fetch_kwargs)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.tail", "title": "<code>tail(limit=25, **fetch_kwargs)</code>", "text": "<p>shortcut to fetch the last few entries from query expression. Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <p>number of entries</p> <code>25</code> <code>fetch_kwargs</code> <p>kwargs for fetch</p> <code>{}</code> <p>Returns:</p> Type Description <p>query result</p> Source code in <code>datajoint/expression.py</code> <pre><code>def tail(self, limit=25, **fetch_kwargs):\n\"\"\"\n    shortcut to fetch the last few entries from query expression.\n    Equivalent to fetch(order_by=\"KEY DESC\", limit=25)[::-1]\n\n    :param limit:  number of entries\n    :param fetch_kwargs: kwargs for fetch\n    :return: query result\n    \"\"\"\n    return self.fetch(order_by=\"KEY DESC\", limit=limit, **fetch_kwargs)[::-1]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.cursor", "title": "<code>cursor(offset=0, limit=None, order_by=None, as_dict=False)</code>", "text": "<p>See expression.fetch() for input description.</p> <p>Returns:</p> Type Description <p>query cursor</p> Source code in <code>datajoint/expression.py</code> <pre><code>def cursor(self, offset=0, limit=None, order_by=None, as_dict=False):\n\"\"\"\n    See expression.fetch() for input description.\n    :return: query cursor\n    \"\"\"\n    if offset and limit is None:\n        raise DataJointError(\"limit is required when offset is set\")\n    sql = self.make_sql()\n    if order_by is not None:\n        sql += \" ORDER BY \" + \", \".join(order_by)\n    if limit is not None:\n        sql += \" LIMIT %d\" % limit + (\" OFFSET %d\" % offset if offset else \"\")\n    logger.debug(sql)\n    return self.connection.query(sql, as_dict=as_dict)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.expression.QueryExpression.preview", "title": "<code>preview(limit=None, width=None)</code>", "text": "<p>Returns:</p> Type Description <p>a string of preview of the contents of the query.</p> Source code in <code>datajoint/expression.py</code> <pre><code>def preview(self, limit=None, width=None):\n\"\"\":return: a string of preview of the contents of the query.\"\"\"\n    return preview(self, limit, width)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.get_master", "title": "<code>get_master(full_table_name)</code>", "text": "<p>If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + <code>__</code>.</p> <p>Example:    <code>ephys</code>.<code>session</code>    -- master    <code>ephys</code>.<code>session__recording</code>  -- part</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <code>str</code> <p>Full table name including part.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Supposed master full table name or empty string if not a part table name.</p> Source code in <code>datajoint/utils.py</code> <pre><code>def get_master(full_table_name: str) -&gt; str:\n\"\"\"\n    If the table name is that of a part table, then return what the master table name would be.\n    This follows DataJoint's table naming convention where a master and a part must be in the\n    same schema and the part table is prefixed with the master table name + ``__``.\n\n    Example:\n       `ephys`.`session`    -- master\n       `ephys`.`session__recording`  -- part\n\n    :param full_table_name: Full table name including part.\n    :type full_table_name: str\n    :return: Supposed master full table name or empty string if not a part table name.\n    :rtype: str\n    \"\"\"\n    match = re.match(r\"(?P&lt;master&gt;`\\w+`.`\\w+)__(?P&lt;part&gt;\\w+)`\", full_table_name)\n    return match[\"master\"] + \"`\" if match else \"\"\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/table/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.AccessError", "title": "<code>AccessError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>User access error: insufficient privileges.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class AccessError(QueryError):\n\"\"\"\n    User access error: insufficient privileges.\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.DuplicateError", "title": "<code>DuplicateError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An integrity error caused by a duplicate entry into a unique key</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DuplicateError(QueryError):\n\"\"\"\n    An integrity error caused by a duplicate entry into a unique key\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.IntegrityError", "title": "<code>IntegrityError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>An integrity error triggered by foreign key constraints</p> Source code in <code>datajoint/errors.py</code> <pre><code>class IntegrityError(QueryError):\n\"\"\"\n    An integrity error triggered by foreign key constraints\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Heading", "title": "<code>Heading</code>", "text": "<p>Local class for table headings. Heading contains the property attributes, which is an dict in which the keys are the attribute names and the values are Attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>class Heading:\n\"\"\"\n    Local class for table headings.\n    Heading contains the property attributes, which is an dict in which the keys are\n    the attribute names and the values are Attributes.\n    \"\"\"\n\n    def __init__(self, attribute_specs=None, table_info=None):\n\"\"\"\n\n        :param attribute_specs: a list of dicts with the same keys as Attribute\n        :param table_info: a dict with information to load the heading from the database\n        \"\"\"\n        self.indexes = None\n        self.table_info = table_info\n        self._table_status = None\n        self._attributes = (\n            None\n            if attribute_specs is None\n            else dict((q[\"name\"], Attribute(**q)) for q in attribute_specs)\n        )\n\n    def __len__(self):\n        return 0 if self.attributes is None else len(self.attributes)\n\n    @property\n    def table_status(self):\n        if self.table_info is None:\n            return None\n        if self._table_status is None:\n            self._init_from_database()\n        return self._table_status\n\n    @property\n    def attributes(self):\n        if self._attributes is None:\n            self._init_from_database()  # lazy loading from database\n        return self._attributes\n\n    @property\n    def names(self):\n        return [k for k in self.attributes]\n\n    @property\n    def primary_key(self):\n        return [k for k, v in self.attributes.items() if v.in_key]\n\n    @property\n    def secondary_attributes(self):\n        return [k for k, v in self.attributes.items() if not v.in_key]\n\n    @property\n    def blobs(self):\n        return [k for k, v in self.attributes.items() if v.is_blob]\n\n    @property\n    def non_blobs(self):\n        return [\n            k\n            for k, v in self.attributes.items()\n            if not (v.is_blob or v.is_attachment or v.is_filepath or v.json)\n        ]\n\n    @property\n    def new_attributes(self):\n        return [\n            k for k, v in self.attributes.items() if v.attribute_expression is not None\n        ]\n\n    def __getitem__(self, name):\n\"\"\"shortcut to the attribute\"\"\"\n        return self.attributes[name]\n\n    def __repr__(self):\n\"\"\"\n        :return:  heading representation in DataJoint declaration format but without foreign key expansion\n        \"\"\"\n        in_key = True\n        ret = \"\"\n        if self._table_status is not None:\n            ret += \"# \" + self.table_status[\"comment\"] + \"\\n\"\n        for v in self.attributes.values():\n            if in_key and not v.in_key:\n                ret += \"---\\n\"\n                in_key = False\n            ret += \"%-20s : %-28s # %s\\n\" % (\n                v.name if v.default is None else \"%s=%s\" % (v.name, v.default),\n                \"%s%s\" % (v.type, \"auto_increment\" if v.autoincrement else \"\"),\n                v.comment,\n            )\n        return ret\n\n    @property\n    def has_autoincrement(self):\n        return any(e.autoincrement for e in self.attributes.values())\n\n    @property\n    def as_dtype(self):\n\"\"\"\n        represent the heading as a numpy dtype\n        \"\"\"\n        return np.dtype(\n            dict(names=self.names, formats=[v.dtype for v in self.attributes.values()])\n        )\n\n    def as_sql(self, fields, include_aliases=True):\n\"\"\"\n        represent heading as the SQL SELECT clause.\n        \"\"\"\n        return \",\".join(\n            \"`%s`\" % name\n            if self.attributes[name].attribute_expression is None\n            else self.attributes[name].attribute_expression\n            + (\" as `%s`\" % name if include_aliases else \"\")\n            for name in fields\n        )\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def _init_from_database(self):\n\"\"\"initialize heading from an existing database table.\"\"\"\n        conn, database, table_name, context = (\n            self.table_info[k] for k in (\"conn\", \"database\", \"table_name\", \"context\")\n        )\n        info = conn.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE name=\"{table_name}\"'.format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        ).fetchone()\n        if info is None:\n            if table_name == \"~log\":\n                logger.warning(\"Could not create the ~log table\")\n                return\n            raise DataJointError(\n                \"The table `{database}`.`{table_name}` is not defined.\".format(\n                    table_name=table_name, database=database\n                )\n            )\n        self._table_status = {k.lower(): v for k, v in info.items()}\n        cur = conn.query(\n            \"SHOW FULL COLUMNS FROM `{table_name}` IN `{database}`\".format(\n                table_name=table_name, database=database\n            ),\n            as_dict=True,\n        )\n\n        attributes = cur.fetchall()\n\n        rename_map = {\n            \"Field\": \"name\",\n            \"Type\": \"type\",\n            \"Null\": \"nullable\",\n            \"Default\": \"default\",\n            \"Key\": \"in_key\",\n            \"Comment\": \"comment\",\n        }\n\n        fields_to_drop = (\"Privileges\", \"Collation\")\n\n        # rename and drop attributes\n        attributes = [\n            {\n                rename_map[k] if k in rename_map else k: v\n                for k, v in x.items()\n                if k not in fields_to_drop\n            }\n            for x in attributes\n        ]\n        numeric_types = {\n            (\"float\", False): np.float64,\n            (\"float\", True): np.float64,\n            (\"double\", False): np.float64,\n            (\"double\", True): np.float64,\n            (\"tinyint\", False): np.int64,\n            (\"tinyint\", True): np.int64,\n            (\"smallint\", False): np.int64,\n            (\"smallint\", True): np.int64,\n            (\"mediumint\", False): np.int64,\n            (\"mediumint\", True): np.int64,\n            (\"int\", False): np.int64,\n            (\"int\", True): np.int64,\n            (\"bigint\", False): np.int64,\n            (\"bigint\", True): np.uint64,\n        }\n\n        sql_literals = [\"CURRENT_TIMESTAMP\"]\n\n        # additional attribute properties\n        for attr in attributes:\n            attr.update(\n                in_key=(attr[\"in_key\"] == \"PRI\"),\n                database=database,\n                nullable=attr[\"nullable\"] == \"YES\",\n                autoincrement=bool(\n                    re.search(r\"auto_increment\", attr[\"Extra\"], flags=re.I)\n                ),\n                numeric=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"DECIMAL\", \"INTEGER\", \"FLOAT\")\n                ),\n                string=any(\n                    TYPE_PATTERN[t].match(attr[\"type\"])\n                    for t in (\"ENUM\", \"TEMPORAL\", \"STRING\")\n                ),\n                is_blob=bool(TYPE_PATTERN[\"INTERNAL_BLOB\"].match(attr[\"type\"])),\n                uuid=False,\n                json=bool(TYPE_PATTERN[\"JSON\"].match(attr[\"type\"])),\n                is_attachment=False,\n                is_filepath=False,\n                adapter=None,\n                store=None,\n                is_external=False,\n                attribute_expression=None,\n            )\n\n            if any(TYPE_PATTERN[t].match(attr[\"type\"]) for t in (\"INTEGER\", \"FLOAT\")):\n                attr[\"type\"] = re.sub(\n                    r\"\\(\\d+\\)\", \"\", attr[\"type\"], count=1\n                )  # strip size off integers and floats\n            attr[\"unsupported\"] = not any(\n                (attr[\"is_blob\"], attr[\"numeric\"], attr[\"numeric\"])\n            )\n            attr.pop(\"Extra\")\n\n            # process custom DataJoint types\n            special = re.match(r\":(?P&lt;type&gt;[^:]+):(?P&lt;comment&gt;.*)\", attr[\"comment\"])\n            if special:\n                special = special.groupdict()\n                attr.update(special)\n            # process adapted attribute types\n            if special and TYPE_PATTERN[\"ADAPTED\"].match(attr[\"type\"]):\n                assert context is not None, \"Declaration context is not set\"\n                adapter_name = special[\"type\"]\n                try:\n                    attr.update(adapter=get_adapter(context, adapter_name))\n                except DataJointError:\n                    # if no adapter, then delay the error until the first invocation\n                    attr.update(adapter=AttributeAdapter())\n                else:\n                    attr.update(type=attr[\"adapter\"].attribute_type)\n                    if not any(r.match(attr[\"type\"]) for r in TYPE_PATTERN.values()):\n                        raise DataJointError(\n                            \"Invalid attribute type '{type}' in adapter object &lt;{adapter_name}&gt;.\".format(\n                                adapter_name=adapter_name, **attr\n                            )\n                        )\n                    special = not any(\n                        TYPE_PATTERN[c].match(attr[\"type\"]) for c in NATIVE_TYPES\n                    )\n\n            if special:\n                try:\n                    category = next(\n                        c for c in SPECIAL_TYPES if TYPE_PATTERN[c].match(attr[\"type\"])\n                    )\n                except StopIteration:\n                    if attr[\"type\"].startswith(\"external\"):\n                        url = (\n                            \"https://docs.datajoint.io/python/admin/5-blob-config.html\"\n                            \"#migration-between-datajoint-v0-11-and-v0-12\"\n                        )\n                        raise DataJointError(\n                            \"Legacy datatype `{type}`. Migrate your external stores to \"\n                            \"datajoint 0.12: {url}\".format(url=url, **attr)\n                        )\n                    raise DataJointError(\n                        \"Unknown attribute type `{type}`\".format(**attr)\n                    )\n                if category == \"FILEPATH\" and not _support_filepath_types():\n                    raise DataJointError(\n\"\"\"\n                        The filepath data type is disabled until complete validation.\n                        To turn it on as experimental feature, set the environment variable\n                        {env} = TRUE or upgrade datajoint.\n                        \"\"\".format(\n                            env=FILEPATH_FEATURE_SWITCH\n                        )\n                    )\n                attr.update(\n                    unsupported=False,\n                    is_attachment=category in (\"INTERNAL_ATTACH\", \"EXTERNAL_ATTACH\"),\n                    is_filepath=category == \"FILEPATH\",\n                    # INTERNAL_BLOB is not a custom type but is included for completeness\n                    is_blob=category in (\"INTERNAL_BLOB\", \"EXTERNAL_BLOB\"),\n                    uuid=category == \"UUID\",\n                    is_external=category in EXTERNAL_TYPES,\n                    store=attr[\"type\"].split(\"@\")[1]\n                    if category in EXTERNAL_TYPES\n                    else None,\n                )\n\n            if attr[\"in_key\"] and any(\n                (\n                    attr[\"is_blob\"],\n                    attr[\"is_attachment\"],\n                    attr[\"is_filepath\"],\n                    attr[\"json\"],\n                )\n            ):\n                raise DataJointError(\n                    \"Json, Blob, attachment, or filepath attributes are not allowed in the primary key\"\n                )\n\n            if (\n                attr[\"string\"]\n                and attr[\"default\"] is not None\n                and attr[\"default\"] not in sql_literals\n            ):\n                attr[\"default\"] = '\"%s\"' % attr[\"default\"]\n\n            if attr[\"nullable\"]:  # nullable fields always default to null\n                attr[\"default\"] = \"null\"\n\n            # fill out dtype. All floats and non-nullable integers are turned into specific dtypes\n            attr[\"dtype\"] = object\n            if attr[\"numeric\"] and not attr[\"adapter\"]:\n                is_integer = TYPE_PATTERN[\"INTEGER\"].match(attr[\"type\"])\n                is_float = TYPE_PATTERN[\"FLOAT\"].match(attr[\"type\"])\n                if is_integer and not attr[\"nullable\"] or is_float:\n                    is_unsigned = bool(re.match(\"sunsigned\", attr[\"type\"], flags=re.I))\n                    t = re.sub(r\"\\(.*\\)\", \"\", attr[\"type\"])  # remove parentheses\n                    t = re.sub(r\" unsigned$\", \"\", t)  # remove unsigned\n                    assert (t, is_unsigned) in numeric_types, (\n                        \"dtype not found for type %s\" % t\n                    )\n                    attr[\"dtype\"] = numeric_types[(t, is_unsigned)]\n\n            if attr[\"adapter\"]:\n                # restore adapted type name\n                attr[\"type\"] = adapter_name\n\n        self._attributes = dict(((q[\"name\"], Attribute(**q)) for q in attributes))\n\n        # Read and tabulate secondary indexes\n        keys = defaultdict(dict)\n        for item in conn.query(\n            \"SHOW KEYS FROM `{db}`.`{tab}`\".format(db=database, tab=table_name),\n            as_dict=True,\n        ):\n            if item[\"Key_name\"] != \"PRIMARY\":\n                keys[item[\"Key_name\"]][item[\"Seq_in_index\"]] = dict(\n                    column=item[\"Column_name\"]\n                    or f\"({item['Expression']})\".replace(r\"\\'\", \"'\"),\n                    unique=(item[\"Non_unique\"] == 0),\n                    nullable=item[\"Null\"].lower() == \"yes\",\n                )\n        self.indexes = {\n            tuple(item[k][\"column\"] for k in sorted(item.keys())): dict(\n                unique=item[1][\"unique\"],\n                nullable=any(v[\"nullable\"] for v in item.values()),\n            )\n            for item in keys.values()\n        }\n\n    def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n        derive a new heading by selecting, renaming, or computing attributes.\n        In relational algebra these operators are known as project, rename, and extend.\n\n        :param select_list:  the full list of existing attributes to include\n        :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n        :param compute_map: a direction of computed attributes\n        This low-level method performs no error checking.\n        \"\"\"\n        rename_map = rename_map or {}\n        compute_map = compute_map or {}\n        copy_attrs = list()\n        for name in self.attributes:\n            if name in select_list:\n                copy_attrs.append(self.attributes[name].todict())\n            copy_attrs.extend(\n                (\n                    dict(\n                        self.attributes[old_name].todict(),\n                        name=new_name,\n                        attribute_expression=\"`%s`\" % old_name,\n                    )\n                    for new_name, old_name in rename_map.items()\n                    if old_name == name\n                )\n            )\n        compute_attrs = (\n            dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n            for new_name, expr in compute_map.items()\n        )\n        return Heading(chain(copy_attrs, compute_attrs))\n\n    def join(self, other):\n\"\"\"\n        Join two headings into a new one.\n        It assumes that self and other are headings that share no common dependent attributes.\n        \"\"\"\n        return Heading(\n            [self.attributes[name].todict() for name in self.primary_key]\n            + [\n                other.attributes[name].todict()\n                for name in other.primary_key\n                if name not in self.primary_key\n            ]\n            + [\n                self.attributes[name].todict()\n                for name in self.secondary_attributes\n                if name not in other.primary_key\n            ]\n            + [\n                other.attributes[name].todict()\n                for name in other.secondary_attributes\n                if name not in self.primary_key\n            ]\n        )\n\n    def set_primary_key(self, primary_key):\n\"\"\"\n        Create a new heading with the specified primary key.\n        This low-level method performs no error checking.\n        \"\"\"\n        return Heading(\n            chain(\n                (\n                    dict(self.attributes[name].todict(), in_key=True)\n                    for name in primary_key\n                ),\n                (\n                    dict(self.attributes[name].todict(), in_key=False)\n                    for name in self.names\n                    if name not in primary_key\n                ),\n            )\n        )\n\n    def make_subquery_heading(self):\n\"\"\"\n        Create a new heading with removed attribute sql_expressions.\n        Used by subqueries, which resolve the sql_expressions.\n        \"\"\"\n        return Heading(\n            dict(v.todict(), attribute_expression=None)\n            for v in self.attributes.values()\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.as_dtype", "title": "<code>as_dtype</code>  <code>property</code>", "text": "<p>represent the heading as a numpy dtype</p>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.as_sql", "title": "<code>as_sql(fields, include_aliases=True)</code>", "text": "<p>represent heading as the SQL SELECT clause.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def as_sql(self, fields, include_aliases=True):\n\"\"\"\n    represent heading as the SQL SELECT clause.\n    \"\"\"\n    return \",\".join(\n        \"`%s`\" % name\n        if self.attributes[name].attribute_expression is None\n        else self.attributes[name].attribute_expression\n        + (\" as `%s`\" % name if include_aliases else \"\")\n        for name in fields\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.select", "title": "<code>select(select_list, rename_map=None, compute_map=None)</code>", "text": "<p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>Parameters:</p> Name Type Description Default <code>select_list</code> <p>the full list of existing attributes to include</p> required <code>rename_map</code> <p>dictionary of renamed attributes: keys=new names, values=old names</p> <code>None</code> <code>compute_map</code> <p>a direction of computed attributes This low-level method performs no error checking.</p> <code>None</code> Source code in <code>datajoint/heading.py</code> <pre><code>def select(self, select_list, rename_map=None, compute_map=None):\n\"\"\"\n    derive a new heading by selecting, renaming, or computing attributes.\n    In relational algebra these operators are known as project, rename, and extend.\n\n    :param select_list:  the full list of existing attributes to include\n    :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names\n    :param compute_map: a direction of computed attributes\n    This low-level method performs no error checking.\n    \"\"\"\n    rename_map = rename_map or {}\n    compute_map = compute_map or {}\n    copy_attrs = list()\n    for name in self.attributes:\n        if name in select_list:\n            copy_attrs.append(self.attributes[name].todict())\n        copy_attrs.extend(\n            (\n                dict(\n                    self.attributes[old_name].todict(),\n                    name=new_name,\n                    attribute_expression=\"`%s`\" % old_name,\n                )\n                for new_name, old_name in rename_map.items()\n                if old_name == name\n            )\n        )\n    compute_attrs = (\n        dict(default_attribute_properties, name=new_name, attribute_expression=expr)\n        for new_name, expr in compute_map.items()\n    )\n    return Heading(chain(copy_attrs, compute_attrs))\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.join", "title": "<code>join(other)</code>", "text": "<p>Join two headings into a new one. It assumes that self and other are headings that share no common dependent attributes.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def join(self, other):\n\"\"\"\n    Join two headings into a new one.\n    It assumes that self and other are headings that share no common dependent attributes.\n    \"\"\"\n    return Heading(\n        [self.attributes[name].todict() for name in self.primary_key]\n        + [\n            other.attributes[name].todict()\n            for name in other.primary_key\n            if name not in self.primary_key\n        ]\n        + [\n            self.attributes[name].todict()\n            for name in self.secondary_attributes\n            if name not in other.primary_key\n        ]\n        + [\n            other.attributes[name].todict()\n            for name in other.secondary_attributes\n            if name not in self.primary_key\n        ]\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.set_primary_key", "title": "<code>set_primary_key(primary_key)</code>", "text": "<p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def set_primary_key(self, primary_key):\n\"\"\"\n    Create a new heading with the specified primary key.\n    This low-level method performs no error checking.\n    \"\"\"\n    return Heading(\n        chain(\n            (\n                dict(self.attributes[name].todict(), in_key=True)\n                for name in primary_key\n            ),\n            (\n                dict(self.attributes[name].todict(), in_key=False)\n                for name in self.names\n                if name not in primary_key\n            ),\n        )\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.heading.Heading.make_subquery_heading", "title": "<code>make_subquery_heading()</code>", "text": "<p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p> Source code in <code>datajoint/heading.py</code> <pre><code>def make_subquery_heading(self):\n\"\"\"\n    Create a new heading with removed attribute sql_expressions.\n    Used by subqueries, which resolve the sql_expressions.\n    \"\"\"\n    return Heading(\n        dict(v.todict(), attribute_expression=None)\n        for v in self.attributes.values()\n    )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.UnknownAttributeError", "title": "<code>UnknownAttributeError</code>", "text": "<p>         Bases: <code>QueryError</code></p> <p>User requests an attribute name not found in query heading</p> Source code in <code>datajoint/errors.py</code> <pre><code>class UnknownAttributeError(QueryError):\n\"\"\"\n    User requests an attribute name not found in query heading\n    \"\"\"\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.make_condition", "title": "<code>make_condition(query_expression, condition, columns)</code>", "text": "<p>Translate the input condition into the equivalent SQL condition (a string)</p> <p>Parameters:</p> Name Type Description Default <code>query_expression</code> <p>a dj.QueryExpression object to apply condition</p> required <code>condition</code> <p>any valid restriction object.</p> required <code>columns</code> <p>a set passed by reference to collect all column names used in the condition.</p> required <p>Returns:</p> Type Description <p>an SQL condition string or a boolean value.</p> Source code in <code>datajoint/condition.py</code> <pre><code>def make_condition(query_expression, condition, columns):\n\"\"\"\n    Translate the input condition into the equivalent SQL condition (a string)\n\n    :param query_expression: a dj.QueryExpression object to apply condition\n    :param condition: any valid restriction object.\n    :param columns: a set passed by reference to collect all column names used in the\n        condition.\n    :return: an SQL condition string or a boolean value.\n    \"\"\"\n    from .expression import QueryExpression, Aggregation, U\n\n    def prep_value(k, v):\n\"\"\"prepare SQL condition\"\"\"\n        key_match, k = translate_attribute(k)\n        if key_match[\"path\"] is None:\n            k = f\"`{k}`\"\n        if (\n            query_expression.heading[key_match[\"attr\"]].json\n            and key_match[\"path\"] is not None\n            and isinstance(v, dict)\n        ):\n            return f\"{k}='{json.dumps(v)}'\"\n        if v is None:\n            return f\"{k} IS NULL\"\n        if query_expression.heading[key_match[\"attr\"]].uuid:\n            if not isinstance(v, uuid.UUID):\n                try:\n                    v = uuid.UUID(v)\n                except (AttributeError, ValueError):\n                    raise DataJointError(\n                        \"Badly formed UUID {v} in restriction by `{k}`\".format(k=k, v=v)\n                    )\n            return f\"{k}=X'{v.bytes.hex()}'\"\n        if isinstance(\n            v,\n            (\n                datetime.date,\n                datetime.datetime,\n                datetime.time,\n                decimal.Decimal,\n                list,\n            ),\n        ):\n            return f'{k}=\"{v}\"'\n        if isinstance(v, str):\n            v = v.replace(\"%\", \"%%\").replace(\"\\\\\", \"\\\\\\\\\")\n            return f'{k}=\"{v}\"'\n        return f\"{k}={v}\"\n\n    def combine_conditions(negate, conditions):\n        return f\"{'NOT ' if negate else ''} ({')AND('.join(conditions)})\"\n\n    negate = False\n    while isinstance(condition, Not):\n        negate = not negate\n        condition = condition.restriction\n\n    # restrict by string\n    if isinstance(condition, str):\n        columns.update(extract_column_names(condition))\n        return combine_conditions(\n            negate, conditions=[condition.strip().replace(\"%\", \"%%\")]\n        )  # escape %, see issue #376\n\n    # restrict by AndList\n    if isinstance(condition, AndList):\n        # omit all conditions that evaluate to True\n        items = [\n            item\n            for item in (\n                make_condition(query_expression, cond, columns) for cond in condition\n            )\n            if item is not True\n        ]\n        if any(item is False for item in items):\n            return negate  # if any item is False, the whole thing is False\n        if not items:\n            return not negate  # and empty AndList is True\n        return combine_conditions(negate, conditions=items)\n\n    # restriction by dj.U evaluates to True\n    if isinstance(condition, U):\n        return not negate\n\n    # restrict by boolean\n    if isinstance(condition, bool):\n        return negate != condition\n\n    # restrict by a mapping/dict -- convert to an AndList of string equality conditions\n    if isinstance(condition, collections.abc.Mapping):\n        common_attributes = set(c.split(\".\", 1)[0] for c in condition).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluates to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[\n                prep_value(k, v)\n                for k, v in condition.items()\n                if k.split(\".\", 1)[0] in common_attributes  # handle json indexing\n            ],\n        )\n\n    # restrict by a numpy record -- convert to an AndList of string equality conditions\n    if isinstance(condition, numpy.void):\n        common_attributes = set(condition.dtype.fields).intersection(\n            query_expression.heading.names\n        )\n        if not common_attributes:\n            return not negate  # no matching attributes -&gt; evaluate to True\n        columns.update(common_attributes)\n        return combine_conditions(\n            negate,\n            conditions=[prep_value(k, condition[k]) for k in common_attributes],\n        )\n\n    # restrict by a QueryExpression subclass -- trigger instantiation and move on\n    if inspect.isclass(condition) and issubclass(condition, QueryExpression):\n        condition = condition()\n\n    # restrict by another expression (aka semijoin and antijoin)\n    check_compatibility = True\n    if isinstance(condition, PromiscuousOperand):\n        condition = condition.operand\n        check_compatibility = False\n\n    if isinstance(condition, QueryExpression):\n        if check_compatibility:\n            assert_join_compatibility(query_expression, condition)\n        common_attributes = [\n            q for q in condition.heading.names if q in query_expression.heading.names\n        ]\n        columns.update(common_attributes)\n        if isinstance(condition, Aggregation):\n            condition = condition.make_subquery()\n        return (\n            # without common attributes, any non-empty set matches everything\n            (not negate if condition else negate)\n            if not common_attributes\n            else \"({fields}) {not_}in ({subquery})\".format(\n                fields=\"`\" + \"`,`\".join(common_attributes) + \"`\",\n                not_=\"not \" if negate else \"\",\n                subquery=condition.make_sql(common_attributes),\n            )\n        )\n\n    # restrict by pandas.DataFrames\n    if isinstance(condition, pandas.DataFrame):\n        condition = condition.to_records()  # convert to numpy.recarray and move on\n\n    # if iterable (but not a string, a QueryExpression, or an AndList), treat as an OrList\n    try:\n        or_list = [make_condition(query_expression, q, columns) for q in condition]\n    except TypeError:\n        raise DataJointError(\"Invalid restriction type %r\" % condition)\n    else:\n        or_list = [\n            item for item in or_list if item is not False\n        ]  # ignore False conditions\n        if any(item is True for item in or_list):  # if any item is True, entirely True\n            return not negate\n        return (\n            f\"{'NOT ' if negate else ''} ({' OR '.join(or_list)})\"\n            if or_list\n            else negate\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.declare", "title": "<code>declare(full_table_name, definition, context)</code>", "text": "<p>Parse declaration and generate the SQL CREATE TABLE code</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <p>full name of the table</p> required <code>definition</code> <p>DataJoint table definition</p> required <code>context</code> <p>dictionary of objects that might be referred to in the table</p> required <p>Returns:</p> Type Description <p>SQL CREATE TABLE statement, list of external stores used</p> Source code in <code>datajoint/declare.py</code> <pre><code>def declare(full_table_name, definition, context):\n\"\"\"\n    Parse declaration and generate the SQL CREATE TABLE code\n\n    :param full_table_name: full name of the table\n    :param definition: DataJoint table definition\n    :param context: dictionary of objects that might be referred to in the table\n    :return: SQL CREATE TABLE statement, list of external stores used\n    \"\"\"\n    table_name = full_table_name.strip(\"`\").split(\".\")[1]\n    if len(table_name) &gt; MAX_TABLE_NAME_LENGTH:\n        raise DataJointError(\n            \"Table name `{name}` exceeds the max length of {max_length}\".format(\n                name=table_name, max_length=MAX_TABLE_NAME_LENGTH\n            )\n        )\n\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n\n    if not primary_key:\n        raise DataJointError(\"Table must have a primary key\")\n\n    return (\n        \"CREATE TABLE IF NOT EXISTS %s (\\n\" % full_table_name\n        + \",\\n\".join(\n            attribute_sql\n            + [\"PRIMARY KEY (`\" + \"`,`\".join(primary_key) + \"`)\"]\n            + foreign_key_sql\n            + index_sql\n        )\n        + '\\n) ENGINE=InnoDB, COMMENT \"%s\"' % table_comment\n    ), external_stores\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.alter", "title": "<code>alter(definition, old_definition, context)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>definition</code> <p>new table definition</p> required <code>old_definition</code> <p>current table definition</p> required <code>context</code> <p>the context in which to evaluate foreign key definitions</p> required <p>Returns:</p> Type Description <p>string SQL ALTER command, list of new stores used for external storage</p> Source code in <code>datajoint/declare.py</code> <pre><code>def alter(definition, old_definition, context):\n\"\"\"\n    :param definition: new table definition\n    :param old_definition: current table definition\n    :param context: the context in which to evaluate foreign key definitions\n    :return: string SQL ALTER command, list of new stores used for external storage\n    \"\"\"\n    (\n        table_comment,\n        primary_key,\n        attribute_sql,\n        foreign_key_sql,\n        index_sql,\n        external_stores,\n    ) = prepare_declare(definition, context)\n    (\n        table_comment_,\n        primary_key_,\n        attribute_sql_,\n        foreign_key_sql_,\n        index_sql_,\n        external_stores_,\n    ) = prepare_declare(old_definition, context)\n\n    # analyze differences between declarations\n    sql = list()\n    if primary_key != primary_key_:\n        raise NotImplementedError(\"table.alter cannot alter the primary key (yet).\")\n    if foreign_key_sql != foreign_key_sql_:\n        raise NotImplementedError(\"table.alter cannot alter foreign keys (yet).\")\n    if index_sql != index_sql_:\n        raise NotImplementedError(\"table.alter cannot alter indexes (yet)\")\n    if attribute_sql != attribute_sql_:\n        sql.extend(_make_attribute_alter(attribute_sql, attribute_sql_, primary_key))\n    if table_comment != table_comment_:\n        sql.append('COMMENT=\"%s\"' % table_comment)\n    return sql, [e for e in external_stores if e not in external_stores_]\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.lookup_class_name", "title": "<code>lookup_class_name(name, context, depth=3)</code>", "text": "<p>given a table name in the form <code>schema_name</code>.<code>table_name</code>, find its class in the context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p><code>schema_name</code>.<code>table_name</code></p> required <code>context</code> <p>dictionary representing the namespace</p> required <code>depth</code> <p>search depth into imported modules, helps avoid infinite recursion.</p> <code>3</code> <p>Returns:</p> Type Description <p>class name found in the context or None if not found</p> Source code in <code>datajoint/table.py</code> <pre><code>def lookup_class_name(name, context, depth=3):\n\"\"\"\n    given a table name in the form `schema_name`.`table_name`, find its class in the context.\n\n    :param name: `schema_name`.`table_name`\n    :param context: dictionary representing the namespace\n    :param depth: search depth into imported modules, helps avoid infinite recursion.\n    :return: class name found in the context or None if not found\n    \"\"\"\n    # breadth-first search\n    nodes = [dict(context=context, context_name=\"\", depth=depth)]\n    while nodes:\n        node = nodes.pop(0)\n        for member_name, member in node[\"context\"].items():\n            if not member_name.startswith(\"_\"):  # skip IPython's implicit variables\n                if inspect.isclass(member) and issubclass(member, Table):\n                    if member.full_table_name == name:  # found it!\n                        return \".\".join([node[\"context_name\"], member_name]).lstrip(\".\")\n                    try:  # look for part tables\n                        parts = member.__dict__\n                    except AttributeError:\n                        pass  # not a UserTable -- cannot have part tables.\n                    else:\n                        for part in (\n                            getattr(member, p)\n                            for p in parts\n                            if p[0].isupper() and hasattr(member, p)\n                        ):\n                            if (\n                                inspect.isclass(part)\n                                and issubclass(part, Table)\n                                and part.full_table_name == name\n                            ):\n                                return \".\".join(\n                                    [node[\"context_name\"], member_name, part.__name__]\n                                ).lstrip(\".\")\n                elif (\n                    node[\"depth\"] &gt; 0\n                    and inspect.ismodule(member)\n                    and member.__name__ != \"datajoint\"\n                ):\n                    try:\n                        nodes.append(\n                            dict(\n                                context=dict(inspect.getmembers(member)),\n                                context_name=node[\"context_name\"] + \".\" + member_name,\n                                depth=node[\"depth\"] - 1,\n                            )\n                        )\n                    except ImportError:\n                        pass  # could not import, so do not attempt\n    return None\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.FreeTable", "title": "<code>FreeTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>a dj.Connection object</p> required <code>full_table_name</code> <p>in format <code>database</code>.<code>table_name</code></p> required Source code in <code>datajoint/table.py</code> <pre><code>class FreeTable(Table):\n\"\"\"\n    A base table without a dedicated class. Each instance is associated with a table\n    specified by full_table_name.\n\n    :param conn:  a dj.Connection object\n    :param full_table_name: in format `database`.`table_name`\n    \"\"\"\n\n    def __init__(self, conn, full_table_name):\n        self.database, self._table_name = (\n            s.strip(\"`\") for s in full_table_name.split(\".\")\n        )\n        self._connection = conn\n        self._support = [full_table_name]\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn,\n                database=self.database,\n                table_name=self.table_name,\n                context=None,\n            )\n        )\n\n    def __repr__(self):\n        return (\n            \"FreeTable(`%s`.`%s`)\\n\" % (self.database, self._table_name)\n            + super().__repr__()\n        )\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log", "title": "<code>Log</code>", "text": "<p>         Bases: <code>Table</code></p> <p>The log table for each schema. Instances are callable.  Calls log the time and identifying information along with the event.</p> <p>Parameters:</p> Name Type Description Default <code>skip_logging</code> <p>if True, then log entry is skipped by default. See call</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>class Log(Table):\n\"\"\"\n    The log table for each schema.\n    Instances are callable.  Calls log the time and identifying information along with the event.\n\n    :param skip_logging: if True, then log entry is skipped by default. See __call__\n    \"\"\"\n\n    _table_name = \"~log\"\n\n    def __init__(self, conn, database, skip_logging=False):\n        self.database = database\n        self.skip_logging = skip_logging\n        self._connection = conn\n        self._heading = Heading(\n            table_info=dict(\n                conn=conn, database=database, table_name=self.table_name, context=None\n            )\n        )\n        self._support = [self.full_table_name]\n\n        self._definition = \"\"\"    # event logging table for `{database}`\n        id       :int unsigned auto_increment     # event order id\n        ---\n        timestamp = CURRENT_TIMESTAMP : timestamp # event timestamp\n        version  :varchar(12)                     # datajoint version\n        user     :varchar(255)                    # user@host\n        host=\"\"  :varchar(255)                    # system hostname\n        event=\"\" :varchar(255)                    # event message\n        \"\"\".format(\n            database=database\n        )\n\n        super().__init__()\n\n        if not self.is_declared:\n            self.declare()\n            self.connection.dependencies.clear()\n        self._user = self.connection.get_user()\n\n    @property\n    def definition(self):\n        return self._definition\n\n    def __call__(self, event, skip_logging=None):\n\"\"\"\n\n        :param event: string to write into the log table\n        :param skip_logging: If True then do not log. If None, then use self.skip_logging\n        \"\"\"\n        skip_logging = self.skip_logging if skip_logging is None else skip_logging\n        if not skip_logging:\n            try:\n                self.insert1(\n                    dict(\n                        user=self._user,\n                        version=version + \"py\",\n                        host=platform.uname().node,\n                        event=event,\n                    ),\n                    skip_duplicates=True,\n                    ignore_extra_fields=True,\n                )\n            except DataJointError:\n                logger.info(\"could not log event in table ~log\")\n\n    def delete(self):\n\"\"\"\n        bypass interactive prompts and cascading dependencies\n\n        :return: number of deleted items\n        \"\"\"\n        return self.delete_quick(get_count=True)\n\n    def drop(self):\n\"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n        self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log.delete", "title": "<code>delete()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> <p>Returns:</p> Type Description <p>number of deleted items</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(self):\n\"\"\"\n    bypass interactive prompts and cascading dependencies\n\n    :return: number of deleted items\n    \"\"\"\n    return self.delete_quick(get_count=True)\n</code></pre>"}, {"location": "api/datajoint/table/#datajoint.table.Log.drop", "title": "<code>drop()</code>", "text": "<p>bypass interactive prompts and cascading dependencies</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"bypass interactive prompts and cascading dependencies\"\"\"\n    self.drop_quick()\n</code></pre>"}, {"location": "api/datajoint/user_tables/", "title": "user_tables.py", "text": "<p>Hosts the table tiers, user tables should be derived from.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.AutoPopulate", "title": "<code>AutoPopulate</code>", "text": "<p>AutoPopulate is a mixin class that adds the method populate() to a Table class. Auto-populated tables must inherit from both Table and AutoPopulate, must define the property <code>key_source</code>, and must define the callback method <code>make</code>.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>class AutoPopulate:\n\"\"\"\n    AutoPopulate is a mixin class that adds the method populate() to a Table class.\n    Auto-populated tables must inherit from both Table and AutoPopulate,\n    must define the property `key_source`, and must define the callback method `make`.\n    \"\"\"\n\n    _key_source = None\n    _allow_insert = False\n\n    @property\n    def key_source(self):\n\"\"\"\n        :return: the query expression that yields primary key values to be passed,\n        sequentially, to the ``make`` method when populate() is called.\n        The default value is the join of the parent tables references from the primary key.\n        Subclasses may override they key_source to change the scope or the granularity\n        of the make calls.\n        \"\"\"\n\n        def _rename_attributes(table, props):\n            return (\n                table.proj(\n                    **{\n                        attr: ref\n                        for attr, ref in props[\"attr_map\"].items()\n                        if attr != ref\n                    }\n                )\n                if props[\"aliased\"]\n                else table.proj()\n            )\n\n        if self._key_source is None:\n            parents = self.target.parents(\n                primary=True, as_objects=True, foreign_key_info=True\n            )\n            if not parents:\n                raise DataJointError(\n                    \"A table must have dependencies \"\n                    \"from its primary key for auto-populate to work\"\n                )\n            self._key_source = _rename_attributes(*parents[0])\n            for q in parents[1:]:\n                self._key_source *= _rename_attributes(*q)\n        return self._key_source\n\n    def make(self, key):\n\"\"\"\n        Derived classes must implement method `make` that fetches data from tables\n        above them in the dependency hierarchy, restricting by the given key,\n        computes secondary attributes, and inserts the new tuples into self.\n        \"\"\"\n        raise NotImplementedError(\n            \"Subclasses of AutoPopulate must implement the method `make`\"\n        )\n\n    @property\n    def target(self):\n\"\"\"\n        :return: table to be populated.\n        In the typical case, dj.AutoPopulate is mixed into a dj.Table class by\n        inheritance and the target is self.\n        \"\"\"\n        return self\n\n    def _job_key(self, key):\n\"\"\"\n        :param key:  they key returned for the job from the key source\n        :return: the dict to use to generate the job reservation hash\n        This method allows subclasses to control the job reservation granularity.\n        \"\"\"\n        return key\n\n    def _jobs_to_do(self, restrictions):\n\"\"\"\n        :return: the query yeilding the keys to be computed (derived from self.key_source)\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"Cannot call populate on a restricted table. \"\n                \"Instead, pass conditions to populate() as arguments.\"\n            )\n        todo = self.key_source\n\n        # key_source is a QueryExpression subclass -- trigger instantiation\n        if inspect.isclass(todo) and issubclass(todo, QueryExpression):\n            todo = todo()\n\n        if not isinstance(todo, QueryExpression):\n            raise DataJointError(\"Invalid key_source value\")\n\n        try:\n            # check if target lacks any attributes from the primary key of key_source\n            raise DataJointError(\n                \"The populate target lacks attribute %s \"\n                \"from the primary key of key_source\"\n                % next(\n                    name\n                    for name in todo.heading.primary_key\n                    if name not in self.target.heading\n                )\n            )\n        except StopIteration:\n            pass\n        return (todo &amp; AndList(restrictions)).proj()\n\n    def populate(\n        self,\n        *restrictions,\n        suppress_errors=False,\n        return_exception_objects=False,\n        reserve_jobs=False,\n        order=\"original\",\n        limit=None,\n        max_calls=None,\n        display_progress=False,\n        processes=1,\n        make_kwargs=None,\n    ):\n\"\"\"\n        ``table.populate()`` calls ``table.make(key)`` for every primary key in\n        ``self.key_source`` for which there is not already a tuple in table.\n\n        :param restrictions: a list of restrictions each restrict\n            (table.key_source - target.proj())\n        :param suppress_errors: if True, do not terminate execution.\n        :param return_exception_objects: return error objects instead of just error messages\n        :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n        :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n        :param limit: if not None, check at most this many keys\n        :param max_calls: if not None, populate at most this many keys\n        :param display_progress: if True, report progress_bar\n        :param processes: number of processes to use. Set to None to use all cores\n        :param make_kwargs: Keyword arguments which do not affect the result of computation\n            to be passed down to each ``make()`` call. Computation arguments should be\n            specified within the pipeline e.g. using a `dj.Lookup` table.\n        :type make_kwargs: dict, optional\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n        valid_order = [\"original\", \"reverse\", \"random\"]\n        if order not in valid_order:\n            raise DataJointError(\n                \"The order argument must be one of %s\" % str(valid_order)\n            )\n        jobs = (\n            self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n        )\n\n        # define and set up signal handler for SIGTERM:\n        if reserve_jobs:\n\n            def handler(signum, frame):\n                logger.info(\"Populate terminated by SIGTERM\")\n                raise SystemExit(\"SIGTERM received\")\n\n            old_handler = signal.signal(signal.SIGTERM, handler)\n\n        keys = (self._jobs_to_do(restrictions) - self.target).fetch(\"KEY\", limit=limit)\n\n        # exclude \"error\" or \"ignore\" jobs\n        if reserve_jobs:\n            exclude_key_hashes = (\n                jobs\n                &amp; {\"table_name\": self.target.table_name}\n                &amp; 'status in (\"error\", \"ignore\")'\n            ).fetch(\"key_hash\")\n            keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n        if order == \"reverse\":\n            keys.reverse()\n        elif order == \"random\":\n            random.shuffle(keys)\n\n        logger.debug(\"Found %d keys to populate\" % len(keys))\n\n        keys = keys[:max_calls]\n        nkeys = len(keys)\n        if not nkeys:\n            return\n\n        processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n        error_list = []\n        populate_kwargs = dict(\n            suppress_errors=suppress_errors,\n            return_exception_objects=return_exception_objects,\n            make_kwargs=make_kwargs,\n        )\n\n        if processes == 1:\n            for key in (\n                tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n            ):\n                error = self._populate1(key, jobs, **populate_kwargs)\n                if error is not None:\n                    error_list.append(error)\n        else:\n            # spawn multiple processes\n            self.connection.close()  # disconnect parent process from MySQL server\n            del self.connection._conn.ctx  # SSLContext is not pickleable\n            with mp.Pool(\n                processes, _initialize_populate, (self, jobs, populate_kwargs)\n            ) as pool, (\n                tqdm(desc=\"Processes: \", total=nkeys)\n                if display_progress\n                else contextlib.nullcontext()\n            ) as progress_bar:\n                for error in pool.imap(_call_populate1, keys, chunksize=1):\n                    if error is not None:\n                        error_list.append(error)\n                    if display_progress:\n                        progress_bar.update()\n            self.connection.connect()  # reconnect parent process to MySQL server\n\n        # restore original signal handler:\n        if reserve_jobs:\n            signal.signal(signal.SIGTERM, old_handler)\n\n        if suppress_errors:\n            return error_list\n\n    def _populate1(\n        self, key, jobs, suppress_errors, return_exception_objects, make_kwargs=None\n    ):\n\"\"\"\n        populates table for one source key, calling self.make inside a transaction.\n        :param jobs: the jobs table or None if not reserve_jobs\n        :param key: dict specifying job to populate\n        :param suppress_errors: bool if errors should be suppressed and returned\n        :param return_exception_objects: if True, errors must be returned as objects\n        :return: (key, error) when suppress_errors=True, otherwise None\n        \"\"\"\n        make = self._make_tuples if hasattr(self, \"_make_tuples\") else self.make\n\n        if jobs is None or jobs.reserve(self.target.table_name, self._job_key(key)):\n            self.connection.start_transaction()\n            if key in self.target:  # already populated\n                self.connection.cancel_transaction()\n                if jobs is not None:\n                    jobs.complete(self.target.table_name, self._job_key(key))\n            else:\n                logger.debug(f\"Making {key} -&gt; {self.target.full_table_name}\")\n                self.__class__._allow_insert = True\n                try:\n                    make(dict(key), **(make_kwargs or {}))\n                except (KeyboardInterrupt, SystemExit, Exception) as error:\n                    try:\n                        self.connection.cancel_transaction()\n                    except LostConnectionError:\n                        pass\n                    error_message = \"{exception}{msg}\".format(\n                        exception=error.__class__.__name__,\n                        msg=\": \" + str(error) if str(error) else \"\",\n                    )\n                    logger.debug(\n                        f\"Error making {key} -&gt; {self.target.full_table_name} - {error_message}\"\n                    )\n                    if jobs is not None:\n                        # show error name and error message (if any)\n                        jobs.error(\n                            self.target.table_name,\n                            self._job_key(key),\n                            error_message=error_message,\n                            error_stack=traceback.format_exc(),\n                        )\n                    if not suppress_errors or isinstance(error, SystemExit):\n                        raise\n                    else:\n                        logger.error(error)\n                        return key, error if return_exception_objects else error_message\n                else:\n                    self.connection.commit_transaction()\n                    logger.debug(\n                        f\"Success making {key} -&gt; {self.target.full_table_name}\"\n                    )\n                    if jobs is not None:\n                        jobs.complete(self.target.table_name, self._job_key(key))\n                finally:\n                    self.__class__._allow_insert = False\n\n    def progress(self, *restrictions, display=False):\n\"\"\"\n        Report the progress of populating the table.\n        :return: (remaining, total) -- numbers of tuples to be populated\n        \"\"\"\n        todo = self._jobs_to_do(restrictions)\n        total = len(todo)\n        remaining = len(todo - self.target)\n        if display:\n            logger.info(\n                \"%-20s\" % self.__class__.__name__\n                + \" Completed %d of %d (%2.1f%%)   %s\"\n                % (\n                    total - remaining,\n                    total,\n                    100 - 100 * remaining / (total + 1e-12),\n                    datetime.datetime.strftime(\n                        datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                    ),\n                ),\n            )\n        return remaining, total\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.autopopulate.AutoPopulate.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>the query expression that yields primary key values to be passed, sequentially, to the <code>make</code> method when populate() is called. The default value is the join of the parent tables references from the primary key. Subclasses may override they key_source to change the scope or the granularity of the make calls.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.autopopulate.AutoPopulate.make", "title": "<code>make(key)</code>", "text": "<p>Derived classes must implement method <code>make</code> that fetches data from tables above them in the dependency hierarchy, restricting by the given key, computes secondary attributes, and inserts the new tuples into self.</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def make(self, key):\n\"\"\"\n    Derived classes must implement method `make` that fetches data from tables\n    above them in the dependency hierarchy, restricting by the given key,\n    computes secondary attributes, and inserts the new tuples into self.\n    \"\"\"\n    raise NotImplementedError(\n        \"Subclasses of AutoPopulate must implement the method `make`\"\n    )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.autopopulate.AutoPopulate.target", "title": "<code>target</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>table to be populated. In the typical case, dj.AutoPopulate is mixed into a dj.Table class by inheritance and the target is self.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.autopopulate.AutoPopulate.populate", "title": "<code>populate(*restrictions, suppress_errors=False, return_exception_objects=False, reserve_jobs=False, order='original', limit=None, max_calls=None, display_progress=False, processes=1, make_kwargs=None)</code>", "text": "<p><code>table.populate()</code> calls <code>table.make(key)</code> for every primary key in <code>self.key_source</code> for which there is not already a tuple in table.</p> <p>Parameters:</p> Name Type Description Default <code>restrictions</code> <p>a list of restrictions each restrict (table.key_source - target.proj())</p> <code>()</code> <code>suppress_errors</code> <p>if True, do not terminate execution.</p> <code>False</code> <code>return_exception_objects</code> <p>return error objects instead of just error messages</p> <code>False</code> <code>reserve_jobs</code> <p>if True, reserve jobs to populate in asynchronous fashion</p> <code>False</code> <code>order</code> <p>\"original\"|\"reverse\"|\"random\"  - the order of execution</p> <code>'original'</code> <code>limit</code> <p>if not None, check at most this many keys</p> <code>None</code> <code>max_calls</code> <p>if not None, populate at most this many keys</p> <code>None</code> <code>display_progress</code> <p>if True, report progress_bar</p> <code>False</code> <code>processes</code> <p>number of processes to use. Set to None to use all cores</p> <code>1</code> <code>make_kwargs</code> <code>dict, optional</code> <p>Keyword arguments which do not affect the result of computation to be passed down to each <code>make()</code> call. Computation arguments should be specified within the pipeline e.g. using a <code>dj.Lookup</code> table.</p> <code>None</code> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def populate(\n    self,\n    *restrictions,\n    suppress_errors=False,\n    return_exception_objects=False,\n    reserve_jobs=False,\n    order=\"original\",\n    limit=None,\n    max_calls=None,\n    display_progress=False,\n    processes=1,\n    make_kwargs=None,\n):\n\"\"\"\n    ``table.populate()`` calls ``table.make(key)`` for every primary key in\n    ``self.key_source`` for which there is not already a tuple in table.\n\n    :param restrictions: a list of restrictions each restrict\n        (table.key_source - target.proj())\n    :param suppress_errors: if True, do not terminate execution.\n    :param return_exception_objects: return error objects instead of just error messages\n    :param reserve_jobs: if True, reserve jobs to populate in asynchronous fashion\n    :param order: \"original\"|\"reverse\"|\"random\"  - the order of execution\n    :param limit: if not None, check at most this many keys\n    :param max_calls: if not None, populate at most this many keys\n    :param display_progress: if True, report progress_bar\n    :param processes: number of processes to use. Set to None to use all cores\n    :param make_kwargs: Keyword arguments which do not affect the result of computation\n        to be passed down to each ``make()`` call. Computation arguments should be\n        specified within the pipeline e.g. using a `dj.Lookup` table.\n    :type make_kwargs: dict, optional\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\"Populate cannot be called during a transaction.\")\n\n    valid_order = [\"original\", \"reverse\", \"random\"]\n    if order not in valid_order:\n        raise DataJointError(\n            \"The order argument must be one of %s\" % str(valid_order)\n        )\n    jobs = (\n        self.connection.schemas[self.target.database].jobs if reserve_jobs else None\n    )\n\n    # define and set up signal handler for SIGTERM:\n    if reserve_jobs:\n\n        def handler(signum, frame):\n            logger.info(\"Populate terminated by SIGTERM\")\n            raise SystemExit(\"SIGTERM received\")\n\n        old_handler = signal.signal(signal.SIGTERM, handler)\n\n    keys = (self._jobs_to_do(restrictions) - self.target).fetch(\"KEY\", limit=limit)\n\n    # exclude \"error\" or \"ignore\" jobs\n    if reserve_jobs:\n        exclude_key_hashes = (\n            jobs\n            &amp; {\"table_name\": self.target.table_name}\n            &amp; 'status in (\"error\", \"ignore\")'\n        ).fetch(\"key_hash\")\n        keys = [key for key in keys if key_hash(key) not in exclude_key_hashes]\n\n    if order == \"reverse\":\n        keys.reverse()\n    elif order == \"random\":\n        random.shuffle(keys)\n\n    logger.debug(\"Found %d keys to populate\" % len(keys))\n\n    keys = keys[:max_calls]\n    nkeys = len(keys)\n    if not nkeys:\n        return\n\n    processes = min(_ for _ in (processes, nkeys, mp.cpu_count()) if _)\n\n    error_list = []\n    populate_kwargs = dict(\n        suppress_errors=suppress_errors,\n        return_exception_objects=return_exception_objects,\n        make_kwargs=make_kwargs,\n    )\n\n    if processes == 1:\n        for key in (\n            tqdm(keys, desc=self.__class__.__name__) if display_progress else keys\n        ):\n            error = self._populate1(key, jobs, **populate_kwargs)\n            if error is not None:\n                error_list.append(error)\n    else:\n        # spawn multiple processes\n        self.connection.close()  # disconnect parent process from MySQL server\n        del self.connection._conn.ctx  # SSLContext is not pickleable\n        with mp.Pool(\n            processes, _initialize_populate, (self, jobs, populate_kwargs)\n        ) as pool, (\n            tqdm(desc=\"Processes: \", total=nkeys)\n            if display_progress\n            else contextlib.nullcontext()\n        ) as progress_bar:\n            for error in pool.imap(_call_populate1, keys, chunksize=1):\n                if error is not None:\n                    error_list.append(error)\n                if display_progress:\n                    progress_bar.update()\n        self.connection.connect()  # reconnect parent process to MySQL server\n\n    # restore original signal handler:\n    if reserve_jobs:\n        signal.signal(signal.SIGTERM, old_handler)\n\n    if suppress_errors:\n        return error_list\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.autopopulate.AutoPopulate.progress", "title": "<code>progress(*restrictions, display=False)</code>", "text": "<p>Report the progress of populating the table.</p> <p>Returns:</p> Type Description <p>(remaining, total) -- numbers of tuples to be populated</p> Source code in <code>datajoint/autopopulate.py</code> <pre><code>def progress(self, *restrictions, display=False):\n\"\"\"\n    Report the progress of populating the table.\n    :return: (remaining, total) -- numbers of tuples to be populated\n    \"\"\"\n    todo = self._jobs_to_do(restrictions)\n    total = len(todo)\n    remaining = len(todo - self.target)\n    if display:\n        logger.info(\n            \"%-20s\" % self.__class__.__name__\n            + \" Completed %d of %d (%2.1f%%)   %s\"\n            % (\n                total - remaining,\n                total,\n                100 - 100 * remaining / (total + 1e-12),\n                datetime.datetime.strftime(\n                    datetime.datetime.now(), \"%Y-%m-%d %H:%M:%S\"\n                ),\n            ),\n        )\n    return remaining, total\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.TableMeta", "title": "<code>TableMeta</code>", "text": "<p>         Bases: <code>type</code></p> <p>TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.fetch() instead of Table().fetch().</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class TableMeta(type):\n\"\"\"\n    TableMeta subclasses allow applying some instance methods and properties directly\n    at class level. For example, this allows Table.fetch() instead of Table().fetch().\n    \"\"\"\n\n    def __getattribute__(cls, name):\n        # trigger instantiation for supported class attrs\n        return (\n            cls().__getattribute__(name)\n            if name in supported_class_attrs\n            else super().__getattribute__(name)\n        )\n\n    def __and__(cls, arg):\n        return cls() &amp; arg\n\n    def __xor__(cls, arg):\n        return cls() ^ arg\n\n    def __sub__(cls, arg):\n        return cls() - arg\n\n    def __neg__(cls):\n        return -cls()\n\n    def __mul__(cls, arg):\n        return cls() * arg\n\n    def __matmul__(cls, arg):\n        return cls() @ arg\n\n    def __add__(cls, arg):\n        return cls() + arg\n\n    def __iter__(cls):\n        return iter(cls())\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Table", "title": "<code>Table</code>", "text": "<p>         Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p> Source code in <code>datajoint/table.py</code> <pre><code>class Table(QueryExpression):\n\"\"\"\n    Table is an abstract class that represents a table in the schema.\n    It implements insert and delete methods and inherits query functionality.\n    To make it a concrete class, override the abstract properties specifying the connection,\n    table name, database, and definition.\n    \"\"\"\n\n    _table_name = None  # must be defined in subclass\n    _log_ = None  # placeholder for the Log table object\n\n    # These properties must be set by the schema decorator (schemas.py) at class level\n    # or by FreeTable at instance level\n    database = None\n    declaration_context = None\n\n    @property\n    def table_name(self):\n        return self._table_name\n\n    @property\n    def definition(self):\n        raise NotImplementedError(\n            \"Subclasses of Table must implement the `definition` property\"\n        )\n\n    def declare(self, context=None):\n\"\"\"\n        Declare the table in the schema based on self.definition.\n\n        :param context: the context for foreign key resolution. If None, foreign keys are\n            not allowed.\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot declare new tables inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        sql, external_stores = declare(self.full_table_name, self.definition, context)\n        sql = sql.format(database=self.database)\n        try:\n            # declare all external tables before declaring main table\n            for store in external_stores:\n                self.connection.schemas[self.database].external[store]\n            self.connection.query(sql)\n        except AccessError:\n            # skip if no create privilege\n            pass\n        else:\n            self._log(\"Declared \" + self.full_table_name)\n\n    def alter(self, prompt=True, context=None):\n\"\"\"\n        Alter the table definition from self.definition\n        \"\"\"\n        if self.connection.in_transaction:\n            raise DataJointError(\n                \"Cannot update table declaration inside a transaction, \"\n                \"e.g. from inside a populate/make call\"\n            )\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        old_definition = self.describe(context=context)\n        sql, external_stores = alter(self.definition, old_definition, context)\n        if not sql:\n            if prompt:\n                logger.warn(\"Nothing to alter.\")\n        else:\n            sql = \"ALTER TABLE {tab}\\n\\t\".format(\n                tab=self.full_table_name\n            ) + \",\\n\\t\".join(sql)\n            if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n                try:\n                    # declare all external tables before declaring main table\n                    for store in external_stores:\n                        self.connection.schemas[self.database].external[store]\n                    self.connection.query(sql)\n                except AccessError:\n                    # skip if no create privilege\n                    pass\n                else:\n                    # reset heading\n                    self.__class__._heading = Heading(\n                        table_info=self.heading.table_info\n                    )\n                    if prompt:\n                        logger.info(\"Table altered\")\n                    self._log(\"Altered \" + self.full_table_name)\n\n    def from_clause(self):\n\"\"\"\n        :return: the FROM clause of SQL SELECT statements.\n        \"\"\"\n        return self.full_table_name\n\n    def get_select_fields(self, select_fields=None):\n\"\"\"\n        :return: the selected attributes from the SQL SELECT statement.\n        \"\"\"\n        return (\n            \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n        )\n\n    def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of parents as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.parents\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n        :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n            primary key attributes are considered.  If False, return foreign keys including at least one\n            secondary attribute.\n        :param as_objects: if False, return table names. If True, return table objects.\n        :param foreign_key_info: if True, each element in result also includes foreign key info.\n        :return: list of children as table names or table objects\n            with (optional) foreign key information.\n        \"\"\"\n        get_edge = self.connection.dependencies.children\n        nodes = [\n            next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n            for name, props in get_edge(self.full_table_name, primary).items()\n        ]\n        if as_objects:\n            nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n        if not foreign_key_info:\n            nodes = [name for name, props in nodes]\n        return nodes\n\n    def descendants(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables descendants in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.descendants(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def ancestors(self, as_objects=False):\n\"\"\"\n\n        :param as_objects: False - a list of table names; True - a list of table objects.\n        :return: list of tables ancestors in topological order.\n        \"\"\"\n        return [\n            FreeTable(self.connection, node) if as_objects else node\n            for node in self.connection.dependencies.ancestors(self.full_table_name)\n            if not node.isdigit()\n        ]\n\n    def parts(self, as_objects=False):\n\"\"\"\n        return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n        :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n        \"\"\"\n        nodes = [\n            node\n            for node in self.connection.dependencies.nodes\n            if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n        ]\n        return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n\n    @property\n    def is_declared(self):\n\"\"\"\n        :return: True is the table is declared in the schema.\n        \"\"\"\n        return (\n            self.connection.query(\n                'SHOW TABLES in `{database}` LIKE \"{table_name}\"'.format(\n                    database=self.database, table_name=self.table_name\n                )\n            ).rowcount\n            &gt; 0\n        )\n\n    @property\n    def full_table_name(self):\n\"\"\"\n        :return: full table name in the schema\n        \"\"\"\n        return r\"`{0:s}`.`{1:s}`\".format(self.database, self.table_name)\n\n    @property\n    def _log(self):\n        if self._log_ is None:\n            self._log_ = Log(\n                self.connection,\n                database=self.database,\n                skip_logging=self.table_name.startswith(\"~\"),\n            )\n        return self._log_\n\n    @property\n    def external(self):\n        return self.connection.schemas[self.database].external\n\n    def update1(self, row):\n\"\"\"\n        ``update1`` updates one existing entry in the table.\n        Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n        ``delete`` entire records since referential integrity works on the level of records,\n        not fields. Therefore, updates are reserved for corrective operations outside of main\n        workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n        assumptions.\n\n        :param row: a ``dict`` containing the primary key values and the attributes to update.\n            Setting an attribute value to None will reset it to the default value (if any).\n\n        The primary key attributes must always be provided.\n\n        Examples:\n\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n        &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n        \"\"\"\n        # argument validations\n        if not isinstance(row, collections.abc.Mapping):\n            raise DataJointError(\"The argument of update1 must be dict-like.\")\n        if not set(row).issuperset(self.primary_key):\n            raise DataJointError(\n                \"The argument of update1 must supply all primary key values.\"\n            )\n        try:\n            raise DataJointError(\n                \"Attribute `%s` not found.\"\n                % next(k for k in row if k not in self.heading.names)\n            )\n        except StopIteration:\n            pass  # ok\n        if len(self.restriction):\n            raise DataJointError(\"Update cannot be applied to a restricted table.\")\n        key = {k: row[k] for k in self.primary_key}\n        if len(self &amp; key) != 1:\n            raise DataJointError(\"Update can only be applied to one existing entry.\")\n        # UPDATE query\n        row = [\n            self.__make_placeholder(k, v)\n            for k, v in row.items()\n            if k not in self.primary_key\n        ]\n        query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n            table=self.full_table_name,\n            assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n            where=make_condition(self, key, set()),\n        )\n        self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n\n    def insert1(self, row, **kwargs):\n\"\"\"\n        Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n        :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n            as one row.\n        \"\"\"\n        self.insert((row,), **kwargs)\n\n    def insert(\n        self,\n        rows,\n        replace=False,\n        skip_duplicates=False,\n        ignore_extra_fields=False,\n        allow_direct_insert=None,\n    ):\n\"\"\"\n        Insert a collection of rows.\n\n        :param rows: Either (a) an iterable where an element is a numpy record, a\n            dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n            the same heading as self, or (b) a pathlib.Path object specifying a path\n            relative to the current directory with a CSV file, the contents of which\n            will be inserted.\n        :param replace: If True, replaces the existing tuple.\n        :param skip_duplicates: If True, silently skip duplicate inserts.\n        :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n        :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n            insert may only be called from inside the make callback.\n\n        Example:\n\n            &gt;&gt;&gt; Table.insert([\n            &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n            &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n        \"\"\"\n        if isinstance(rows, pandas.DataFrame):\n            # drop 'extra' synthetic index for 1-field index case -\n            # frames with more advanced indices should be prepared by user.\n            rows = rows.reset_index(\n                drop=len(rows.index.names) == 1 and not rows.index.names[0]\n            ).to_records(index=False)\n\n        if isinstance(rows, Path):\n            with open(rows, newline=\"\") as data_file:\n                rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n        # prohibit direct inserts into auto-populated tables\n        if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n            raise DataJointError(\n                \"Inserts into an auto-populated table can only be done inside \"\n                \"its make method during a populate call.\"\n                \" To override, set keyword argument allow_direct_insert=True.\"\n            )\n\n        if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n            rows = rows()  # instantiate if a class\n        if isinstance(rows, QueryExpression):\n            # insert from select\n            if not ignore_extra_fields:\n                try:\n                    raise DataJointError(\n                        \"Attribute %s not found. To ignore extra attributes in insert, \"\n                        \"set ignore_extra_fields=True.\"\n                        % next(\n                            name for name in rows.heading if name not in self.heading\n                        )\n                    )\n                except StopIteration:\n                    pass\n            fields = list(name for name in rows.heading if name in self.heading)\n            query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                fields=\"`\" + \"`,`\".join(fields) + \"`\",\n                table=self.full_table_name,\n                select=rows.make_sql(fields),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                        table=self.full_table_name, pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(query)\n            return\n\n        field_list = []  # collects the field list from first row (passed by reference)\n        rows = list(\n            self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n            for row in rows\n        )\n        if rows:\n            try:\n                query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                    command=\"REPLACE\" if replace else \"INSERT\",\n                    destination=self.from_clause(),\n                    fields=\"`,`\".join(field_list),\n                    placeholders=\",\".join(\n                        \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                    ),\n                    duplicate=(\n                        \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                            pk=self.primary_key[0]\n                        )\n                        if skip_duplicates\n                        else \"\"\n                    ),\n                )\n                self.connection.query(\n                    query,\n                    args=list(\n                        itertools.chain.from_iterable(\n                            (v for v in r[\"values\"] if v is not None) for r in rows\n                        )\n                    ),\n                )\n            except UnknownAttributeError as err:\n                raise err.suggest(\n                    \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n                )\n            except DuplicateError as err:\n                raise err.suggest(\n                    \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n                )\n\n    def delete_quick(self, get_count=False):\n\"\"\"\n        Deletes the table without cascading and without user prompt.\n        If this table has populated dependent tables, this will fail.\n        \"\"\"\n        query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n        self.connection.query(query)\n        count = (\n            self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n            if get_count\n            else None\n        )\n        self._log(query[:255])\n        return count\n\n    def delete(\n        self,\n        transaction: bool = True,\n        safemode: Union[bool, None] = None,\n        force_parts: bool = False,\n    ) -&gt; int:\n\"\"\"\n        Deletes the contents of the table and its dependent tables, recursively.\n\n        Args:\n            transaction: If `True`, use of the entire delete becomes an atomic transaction.\n                This is the default and recommended behavior. Set to `False` if this delete is\n                nested within another transaction.\n            safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n                is `dj.config['safemode']`.\n            force_parts: Delete from parts even when not deleting from their masters.\n\n        Returns:\n            Number of deleted rows (excluding those from dependent tables).\n\n        Raises:\n            DataJointError: Delete exceeds maximum number of delete attempts.\n            DataJointError: When deleting within an existing transaction.\n            DataJointError: Deleting a part table before its master.\n        \"\"\"\n        deleted = set()\n\n        def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n            max_attempts = 50\n            for _ in range(max_attempts):\n                try:\n                    delete_count = table.delete_quick(get_count=True)\n                except IntegrityError as error:\n                    match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                    if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                        match[\"child\"] = \"{}.{}\".format(\n                            table.full_table_name.split(\".\")[0], match[\"child\"]\n                        )\n                    if (\n                        match[\"pk_attrs\"] is not None\n                    ):  # fully matched, adjusting the keys\n                        match[\"fk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                        ]\n                        match[\"pk_attrs\"] = [\n                            k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                        ]\n                    else:  # only partially matched, querying with constraint to determine keys\n                        match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                            map(\n                                list,\n                                zip(\n                                    *table.connection.query(\n                                        constraint_info_query,\n                                        args=(\n                                            match[\"name\"].strip(\"`\"),\n                                            *[\n                                                _.strip(\"`\")\n                                                for _ in match[\"child\"].split(\"`.`\")\n                                            ],\n                                        ),\n                                    ).fetchall()\n                                ),\n                            )\n                        )\n                        match[\"parent\"] = match[\"parent\"][0]\n\n                    # Restrict child by table if\n                    #   1. if table's restriction attributes are not in child's primary key\n                    #   2. if child renames any attributes\n                    # Otherwise restrict child by table's restriction.\n                    child = FreeTable(table.connection, match[\"child\"])\n                    if (\n                        set(table.restriction_attributes) &lt;= set(child.primary_key)\n                        and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                    ):\n                        child._restriction = table._restriction\n                    elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                        child &amp;= table.proj(\n                            **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                        )\n                    else:\n                        child &amp;= table.proj()\n                    cascade(child)\n                else:\n                    deleted.add(table.full_table_name)\n                    logger.info(\n                        \"Deleting {count} rows from {table}\".format(\n                            count=delete_count, table=table.full_table_name\n                        )\n                    )\n                    break\n            else:\n                raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n            return delete_count\n\n        safemode = config[\"safemode\"] if safemode is None else safemode\n\n        # Start transaction\n        if transaction:\n            if not self.connection.in_transaction:\n                self.connection.start_transaction()\n            else:\n                if not safemode:\n                    transaction = False\n                else:\n                    raise DataJointError(\n                        \"Delete cannot use a transaction within an ongoing transaction. \"\n                        \"Set transaction=False or safemode=False).\"\n                    )\n\n        # Cascading delete\n        try:\n            delete_count = cascade(self)\n        except:\n            if transaction:\n                self.connection.cancel_transaction()\n            raise\n\n        if not force_parts:\n            # Avoid deleting from child before master (See issue #151)\n            for part in deleted:\n                master = get_master(part)\n                if master and master not in deleted:\n                    if transaction:\n                        self.connection.cancel_transaction()\n                    raise DataJointError(\n                        \"Attempt to delete part table {part} before deleting from \"\n                        \"its master {master} first.\".format(part=part, master=master)\n                    )\n\n        # Confirm and commit\n        if delete_count == 0:\n            if safemode:\n                logger.warn(\"Nothing to delete.\")\n            if transaction:\n                self.connection.cancel_transaction()\n        else:\n            if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n                if transaction:\n                    self.connection.commit_transaction()\n                if safemode:\n                    logger.info(\"Deletes committed.\")\n            else:\n                if transaction:\n                    self.connection.cancel_transaction()\n                if safemode:\n                    logger.warn(\"Deletes cancelled\")\n        return delete_count\n\n    def drop_quick(self):\n\"\"\"\n        Drops the table without cascading to dependent tables and without user prompt.\n        \"\"\"\n        if self.is_declared:\n            query = \"DROP TABLE %s\" % self.full_table_name\n            self.connection.query(query)\n            logger.info(\"Dropped table %s\" % self.full_table_name)\n            self._log(query[:255])\n        else:\n            logger.info(\n                \"Nothing to drop: table %s is not declared\" % self.full_table_name\n            )\n\n    def drop(self):\n\"\"\"\n        Drop the table and all tables that reference it, recursively.\n        User is prompted for confirmation if config['safemode'] is set to True.\n        \"\"\"\n        if self.restriction:\n            raise DataJointError(\n                \"A table with an applied restriction cannot be dropped.\"\n                \" Call drop() on the unrestricted Table.\"\n            )\n        self.connection.dependencies.load()\n        do_drop = True\n        tables = [\n            table\n            for table in self.connection.dependencies.descendants(self.full_table_name)\n            if not table.isdigit()\n        ]\n\n        # avoid dropping part tables without their masters: See issue #374\n        for part in tables:\n            master = get_master(part)\n            if master and master not in tables:\n                raise DataJointError(\n                    \"Attempt to drop part table {part} before dropping \"\n                    \"its master. Drop {master} first.\".format(part=part, master=master)\n                )\n\n        if config[\"safemode\"]:\n            for table in tables:\n                logger.info(\n                    table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n                )\n            do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n        if do_drop:\n            for table in reversed(tables):\n                FreeTable(self.connection, table).drop_quick()\n            logger.info(\"Tables dropped. Restart kernel.\")\n\n    @property\n    def size_on_disk(self):\n\"\"\"\n        :return: size of data and indices in bytes on the storage device\n        \"\"\"\n        ret = self.connection.query(\n            'SHOW TABLE STATUS FROM `{database}` WHERE NAME=\"{table}\"'.format(\n                database=self.database, table=self.table_name\n            ),\n            as_dict=True,\n        ).fetchone()\n        return ret[\"Data_length\"] + ret[\"Index_length\"]\n\n    def show_definition(self):\n        raise AttributeError(\n            \"show_definition is deprecated. Use the describe method instead.\"\n        )\n\n    def describe(self, context=None, printout=False):\n\"\"\"\n        :return:  the definition string for the query using DataJoint DDL.\n        \"\"\"\n        if context is None:\n            frame = inspect.currentframe().f_back\n            context = dict(frame.f_globals, **frame.f_locals)\n            del frame\n        if self.full_table_name not in self.connection.dependencies:\n            self.connection.dependencies.load()\n        parents = self.parents(foreign_key_info=True)\n        in_key = True\n        definition = (\n            \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n            if self.heading.table_status[\"comment\"]\n            else \"\"\n        )\n        attributes_thus_far = set()\n        attributes_declared = set()\n        indexes = self.heading.indexes.copy()\n        for attr in self.heading.attributes.values():\n            if in_key and not attr.in_key:\n                definition += \"---\\n\"\n                in_key = False\n            attributes_thus_far.add(attr.name)\n            do_include = True\n            for parent_name, fk_props in parents:\n                if attr.name in fk_props[\"attr_map\"]:\n                    do_include = False\n                    if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                        # foreign key properties\n                        try:\n                            index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                        except KeyError:\n                            index_props = \"\"\n                        else:\n                            index_props = [k for k, v in index_props.items() if v]\n                            index_props = (\n                                \" [{}]\".format(\", \".join(index_props))\n                                if index_props\n                                else \"\"\n                            )\n\n                        if not fk_props[\"aliased\"]:\n                            # simple foreign key\n                            definition += \"-&gt;{props} {class_name}\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                            )\n                        else:\n                            # projected foreign key\n                            definition += (\n                                \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                    props=index_props,\n                                    class_name=lookup_class_name(parent_name, context)\n                                    or parent_name,\n                                    proj_list=\",\".join(\n                                        '{}=\"{}\"'.format(attr, ref)\n                                        for attr, ref in fk_props[\"attr_map\"].items()\n                                        if ref != attr\n                                    ),\n                                )\n                            )\n                            attributes_declared.update(fk_props[\"attr_map\"])\n            if do_include:\n                attributes_declared.add(attr.name)\n                definition += \"%-20s : %-28s %s\\n\" % (\n                    attr.name\n                    if attr.default is None\n                    else \"%s=%s\" % (attr.name, attr.default),\n                    \"%s%s\"\n                    % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                    \"# \" + attr.comment if attr.comment else \"\",\n                )\n        # add remaining indexes\n        for k, v in indexes.items():\n            definition += \"{unique}INDEX ({attrs})\\n\".format(\n                unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n            )\n        if printout:\n            logger.info(\"\\n\" + definition)\n        return definition\n\n    # --- private helper functions ----\n    def __make_placeholder(self, name, value, ignore_extra_fields=False):\n\"\"\"\n        For a given attribute `name` with `value`, return its processed value or value placeholder\n        as a string to be included in the query and the value, if any, to be submitted for\n        processing by mysql API.\n\n        :param name:  name of attribute to be inserted\n        :param value: value of attribute to be inserted\n        \"\"\"\n        if ignore_extra_fields and name not in self.heading:\n            return None\n        attr = self.heading[name]\n        if attr.adapter:\n            value = attr.adapter.put(value)\n        if value is None or (attr.numeric and (value == \"\" or np.isnan(float(value)))):\n            # set default value\n            placeholder, value = \"DEFAULT\", None\n        else:  # not NULL\n            placeholder = \"%s\"\n            if attr.uuid:\n                if not isinstance(value, uuid.UUID):\n                    try:\n                        value = uuid.UUID(value)\n                    except (AttributeError, ValueError):\n                        raise DataJointError(\n                            \"badly formed UUID value {v} for attribute `{n}`\".format(\n                                v=value, n=name\n                            )\n                        )\n                value = value.bytes\n            elif attr.is_blob:\n                value = blob.pack(value)\n                value = (\n                    self.external[attr.store].put(value).bytes\n                    if attr.is_external\n                    else value\n                )\n            elif attr.is_attachment:\n                attachment_path = Path(value)\n                if attr.is_external:\n                    # value is hash of contents\n                    value = (\n                        self.external[attr.store]\n                        .upload_attachment(attachment_path)\n                        .bytes\n                    )\n                else:\n                    # value is filename + contents\n                    value = (\n                        str.encode(attachment_path.name)\n                        + b\"\\0\"\n                        + attachment_path.read_bytes()\n                    )\n            elif attr.is_filepath:\n                value = self.external[attr.store].upload_filepath(value).bytes\n            elif attr.numeric:\n                value = str(int(value) if isinstance(value, bool) else value)\n            elif attr.json:\n                value = json.dumps(value)\n        return name, placeholder, value\n\n    def __make_row_to_insert(self, row, field_list, ignore_extra_fields):\n\"\"\"\n        Helper function for insert and update\n\n        :param row:  A tuple to insert\n        :return: a dict with fields 'names', 'placeholders', 'values'\n        \"\"\"\n\n        def check_fields(fields):\n\"\"\"\n            Validates that all items in `fields` are valid attributes in the heading\n\n            :param fields: field names of a tuple\n            \"\"\"\n            if not field_list:\n                if not ignore_extra_fields:\n                    for field in fields:\n                        if field not in self.heading:\n                            raise KeyError(\n                                \"`{0:s}` is not in the table heading\".format(field)\n                            )\n            elif set(field_list) != set(fields).intersection(self.heading.names):\n                raise DataJointError(\"Attempt to insert rows with different fields.\")\n\n        if isinstance(row, np.void):  # np.array\n            check_fields(row.dtype.fields)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row.dtype.fields\n            ]\n        elif isinstance(row, collections.abc.Mapping):  # dict-based\n            check_fields(row)\n            attributes = [\n                self.__make_placeholder(name, row[name], ignore_extra_fields)\n                for name in self.heading\n                if name in row\n            ]\n        else:  # positional\n            try:\n                if len(row) != len(self.heading):\n                    raise DataJointError(\n                        \"Invalid insert argument. Incorrect number of attributes: \"\n                        \"{given} given; {expected} expected\".format(\n                            given=len(row), expected=len(self.heading)\n                        )\n                    )\n            except TypeError:\n                raise DataJointError(\"Datatype %s cannot be inserted\" % type(row))\n            else:\n                attributes = [\n                    self.__make_placeholder(name, value, ignore_extra_fields)\n                    for name, value in zip(self.heading, row)\n                ]\n        if ignore_extra_fields:\n            attributes = [a for a in attributes if a is not None]\n\n        assert len(attributes), \"Empty tuple\"\n        row_to_insert = dict(zip((\"names\", \"placeholders\", \"values\"), zip(*attributes)))\n        if not field_list:\n            # first row sets the composition of the field list\n            field_list.extend(row_to_insert[\"names\"])\n        else:\n            #  reorder attributes in row_to_insert to match field_list\n            order = list(row_to_insert[\"names\"].index(field) for field in field_list)\n            row_to_insert[\"names\"] = list(row_to_insert[\"names\"][i] for i in order)\n            row_to_insert[\"placeholders\"] = list(\n                row_to_insert[\"placeholders\"][i] for i in order\n            )\n            row_to_insert[\"values\"] = list(row_to_insert[\"values\"][i] for i in order)\n        return row_to_insert\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.declare", "title": "<code>declare(context=None)</code>", "text": "<p>Declare the table in the schema based on self.definition.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the context for foreign key resolution. If None, foreign keys are not allowed.</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def declare(self, context=None):\n\"\"\"\n    Declare the table in the schema based on self.definition.\n\n    :param context: the context for foreign key resolution. If None, foreign keys are\n        not allowed.\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot declare new tables inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    sql, external_stores = declare(self.full_table_name, self.definition, context)\n    sql = sql.format(database=self.database)\n    try:\n        # declare all external tables before declaring main table\n        for store in external_stores:\n            self.connection.schemas[self.database].external[store]\n        self.connection.query(sql)\n    except AccessError:\n        # skip if no create privilege\n        pass\n    else:\n        self._log(\"Declared \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.alter", "title": "<code>alter(prompt=True, context=None)</code>", "text": "<p>Alter the table definition from self.definition</p> Source code in <code>datajoint/table.py</code> <pre><code>def alter(self, prompt=True, context=None):\n\"\"\"\n    Alter the table definition from self.definition\n    \"\"\"\n    if self.connection.in_transaction:\n        raise DataJointError(\n            \"Cannot update table declaration inside a transaction, \"\n            \"e.g. from inside a populate/make call\"\n        )\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    old_definition = self.describe(context=context)\n    sql, external_stores = alter(self.definition, old_definition, context)\n    if not sql:\n        if prompt:\n            logger.warn(\"Nothing to alter.\")\n    else:\n        sql = \"ALTER TABLE {tab}\\n\\t\".format(\n            tab=self.full_table_name\n        ) + \",\\n\\t\".join(sql)\n        if not prompt or user_choice(sql + \"\\n\\nExecute?\") == \"yes\":\n            try:\n                # declare all external tables before declaring main table\n                for store in external_stores:\n                    self.connection.schemas[self.database].external[store]\n                self.connection.query(sql)\n            except AccessError:\n                # skip if no create privilege\n                pass\n            else:\n                # reset heading\n                self.__class__._heading = Heading(\n                    table_info=self.heading.table_info\n                )\n                if prompt:\n                    logger.info(\"Table altered\")\n                self._log(\"Altered \" + self.full_table_name)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.from_clause", "title": "<code>from_clause()</code>", "text": "<p>Returns:</p> Type Description <p>the FROM clause of SQL SELECT statements.</p> Source code in <code>datajoint/table.py</code> <pre><code>def from_clause(self):\n\"\"\"\n    :return: the FROM clause of SQL SELECT statements.\n    \"\"\"\n    return self.full_table_name\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.get_select_fields", "title": "<code>get_select_fields(select_fields=None)</code>", "text": "<p>Returns:</p> Type Description <p>the selected attributes from the SQL SELECT statement.</p> Source code in <code>datajoint/table.py</code> <pre><code>def get_select_fields(self, select_fields=None):\n\"\"\"\n    :return: the selected attributes from the SQL SELECT statement.\n    \"\"\"\n    return (\n        \"*\" if select_fields is None else self.heading.project(select_fields).as_sql\n    )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.parents", "title": "<code>parents(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all parents are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of parents as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def parents(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all parents are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of parents as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.parents\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.children", "title": "<code>children(primary=None, as_objects=False, foreign_key_info=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>primary</code> <p>if None, then all children are returned. If True, then only foreign keys composed of primary key attributes are considered.  If False, return foreign keys including at least one secondary attribute.</p> <code>None</code> <code>as_objects</code> <p>if False, return table names. If True, return table objects.</p> <code>False</code> <code>foreign_key_info</code> <p>if True, each element in result also includes foreign key info.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of children as table names or table objects with (optional) foreign key information.</p> Source code in <code>datajoint/table.py</code> <pre><code>def children(self, primary=None, as_objects=False, foreign_key_info=False):\n\"\"\"\n\n    :param primary: if None, then all children are returned. If True, then only foreign keys composed of\n        primary key attributes are considered.  If False, return foreign keys including at least one\n        secondary attribute.\n    :param as_objects: if False, return table names. If True, return table objects.\n    :param foreign_key_info: if True, each element in result also includes foreign key info.\n    :return: list of children as table names or table objects\n        with (optional) foreign key information.\n    \"\"\"\n    get_edge = self.connection.dependencies.children\n    nodes = [\n        next(iter(get_edge(name).items())) if name.isdigit() else (name, props)\n        for name, props in get_edge(self.full_table_name, primary).items()\n    ]\n    if as_objects:\n        nodes = [(FreeTable(self.connection, name), props) for name, props in nodes]\n    if not foreign_key_info:\n        nodes = [name for name, props in nodes]\n    return nodes\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.descendants", "title": "<code>descendants(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables descendants in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def descendants(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables descendants in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.descendants(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.ancestors", "title": "<code>ancestors(as_objects=False)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>False - a list of table names; True - a list of table objects.</p> <code>False</code> <p>Returns:</p> Type Description <p>list of tables ancestors in topological order.</p> Source code in <code>datajoint/table.py</code> <pre><code>def ancestors(self, as_objects=False):\n\"\"\"\n\n    :param as_objects: False - a list of table names; True - a list of table objects.\n    :return: list of tables ancestors in topological order.\n    \"\"\"\n    return [\n        FreeTable(self.connection, node) if as_objects else node\n        for node in self.connection.dependencies.ancestors(self.full_table_name)\n        if not node.isdigit()\n    ]\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.parts", "title": "<code>parts(as_objects=False)</code>", "text": "<p>return part tables either as entries in a dict with foreign key informaiton or a list of objects</p> <p>Parameters:</p> Name Type Description Default <code>as_objects</code> <p>if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p> <code>False</code> Source code in <code>datajoint/table.py</code> <pre><code>def parts(self, as_objects=False):\n\"\"\"\n    return part tables either as entries in a dict with foreign key informaiton or a list of objects\n\n    :param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.\n    \"\"\"\n    nodes = [\n        node\n        for node in self.connection.dependencies.nodes\n        if not node.isdigit() and node.startswith(self.full_table_name[:-1] + \"__\")\n    ]\n    return [FreeTable(self.connection, c) for c in nodes] if as_objects else nodes\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.is_declared", "title": "<code>is_declared</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>True is the table is declared in the schema.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.full_table_name", "title": "<code>full_table_name</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>full table name in the schema</p>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.update1", "title": "<code>update1(row)</code>", "text": "<p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a <code>dict</code> containing the primary key values and the attributes to update. Setting an attribute value to None will reset it to the default value (if any).  The primary key attributes must always be provided.  Examples:  &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1 &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default</p> required Source code in <code>datajoint/table.py</code> <pre><code>def update1(self, row):\n\"\"\"\n    ``update1`` updates one existing entry in the table.\n    Caution: In DataJoint the primary modes for data manipulation is to ``insert`` and\n    ``delete`` entire records since referential integrity works on the level of records,\n    not fields. Therefore, updates are reserved for corrective operations outside of main\n    workflow. Use UPDATE methods sparingly with full awareness of potential violations of\n    assumptions.\n\n    :param row: a ``dict`` containing the primary key values and the attributes to update.\n        Setting an attribute value to None will reset it to the default value (if any).\n\n    The primary key attributes must always be provided.\n\n    Examples:\n\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': 3})  # update value in record with id=1\n    &gt;&gt;&gt; table.update1({'id': 1, 'value': None})  # reset value to default\n    \"\"\"\n    # argument validations\n    if not isinstance(row, collections.abc.Mapping):\n        raise DataJointError(\"The argument of update1 must be dict-like.\")\n    if not set(row).issuperset(self.primary_key):\n        raise DataJointError(\n            \"The argument of update1 must supply all primary key values.\"\n        )\n    try:\n        raise DataJointError(\n            \"Attribute `%s` not found.\"\n            % next(k for k in row if k not in self.heading.names)\n        )\n    except StopIteration:\n        pass  # ok\n    if len(self.restriction):\n        raise DataJointError(\"Update cannot be applied to a restricted table.\")\n    key = {k: row[k] for k in self.primary_key}\n    if len(self &amp; key) != 1:\n        raise DataJointError(\"Update can only be applied to one existing entry.\")\n    # UPDATE query\n    row = [\n        self.__make_placeholder(k, v)\n        for k, v in row.items()\n        if k not in self.primary_key\n    ]\n    query = \"UPDATE {table} SET {assignments} WHERE {where}\".format(\n        table=self.full_table_name,\n        assignments=\",\".join(\"`%s`=%s\" % r[:2] for r in row),\n        where=make_condition(self, key, set()),\n    )\n    self.connection.query(query, args=list(r[2] for r in row if r[2] is not None))\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.insert1", "title": "<code>insert1(row, **kwargs)</code>", "text": "<p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <p>a numpy record, a dict-like object, or an ordered sequence to be inserted as one row.</p> required Source code in <code>datajoint/table.py</code> <pre><code>def insert1(self, row, **kwargs):\n\"\"\"\n    Insert one data record into the table. For ``kwargs``, see ``insert()``.\n\n    :param row: a numpy record, a dict-like object, or an ordered sequence to be inserted\n        as one row.\n    \"\"\"\n    self.insert((row,), **kwargs)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.insert", "title": "<code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None)</code>", "text": "<p>Insert a collection of rows.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <p>Either (a) an iterable where an element is a numpy record, a dict-like object, a pandas.DataFrame, a sequence, or a query expression with the same heading as self, or (b) a pathlib.Path object specifying a path relative to the current directory with a CSV file, the contents of which will be inserted.</p> required <code>replace</code> <p>If True, replaces the existing tuple.</p> <code>False</code> <code>skip_duplicates</code> <p>If True, silently skip duplicate inserts.</p> <code>False</code> <code>ignore_extra_fields</code> <p>If False, fields that are not in the heading raise error.</p> <code>False</code> <code>allow_direct_insert</code> <p>Only applies in auto-populated tables. If False (default), insert may only be called from inside the make callback.  Example:  &gt;&gt;&gt; Table.insert([ &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"), &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])</p> <code>None</code> Source code in <code>datajoint/table.py</code> <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n):\n\"\"\"\n    Insert a collection of rows.\n\n    :param rows: Either (a) an iterable where an element is a numpy record, a\n        dict-like object, a pandas.DataFrame, a sequence, or a query expression with\n        the same heading as self, or (b) a pathlib.Path object specifying a path\n        relative to the current directory with a CSV file, the contents of which\n        will be inserted.\n    :param replace: If True, replaces the existing tuple.\n    :param skip_duplicates: If True, silently skip duplicate inserts.\n    :param ignore_extra_fields: If False, fields that are not in the heading raise error.\n    :param allow_direct_insert: Only applies in auto-populated tables. If False (default),\n        insert may only be called from inside the make callback.\n\n    Example:\n\n        &gt;&gt;&gt; Table.insert([\n        &gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n        &gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n    \"\"\"\n    if isinstance(rows, pandas.DataFrame):\n        # drop 'extra' synthetic index for 1-field index case -\n        # frames with more advanced indices should be prepared by user.\n        rows = rows.reset_index(\n            drop=len(rows.index.names) == 1 and not rows.index.names[0]\n        ).to_records(index=False)\n\n    if isinstance(rows, Path):\n        with open(rows, newline=\"\") as data_file:\n            rows = list(csv.DictReader(data_file, delimiter=\",\"))\n\n    # prohibit direct inserts into auto-populated tables\n    if not allow_direct_insert and not getattr(self, \"_allow_insert\", True):\n        raise DataJointError(\n            \"Inserts into an auto-populated table can only be done inside \"\n            \"its make method during a populate call.\"\n            \" To override, set keyword argument allow_direct_insert=True.\"\n        )\n\n    if inspect.isclass(rows) and issubclass(rows, QueryExpression):\n        rows = rows()  # instantiate if a class\n    if isinstance(rows, QueryExpression):\n        # insert from select\n        if not ignore_extra_fields:\n            try:\n                raise DataJointError(\n                    \"Attribute %s not found. To ignore extra attributes in insert, \"\n                    \"set ignore_extra_fields=True.\"\n                    % next(\n                        name for name in rows.heading if name not in self.heading\n                    )\n                )\n            except StopIteration:\n                pass\n        fields = list(name for name in rows.heading if name in self.heading)\n        query = \"{command} INTO {table} ({fields}) {select}{duplicate}\".format(\n            command=\"REPLACE\" if replace else \"INSERT\",\n            fields=\"`\" + \"`,`\".join(fields) + \"`\",\n            table=self.full_table_name,\n            select=rows.make_sql(fields),\n            duplicate=(\n                \" ON DUPLICATE KEY UPDATE `{pk}`={table}.`{pk}`\".format(\n                    table=self.full_table_name, pk=self.primary_key[0]\n                )\n                if skip_duplicates\n                else \"\"\n            ),\n        )\n        self.connection.query(query)\n        return\n\n    field_list = []  # collects the field list from first row (passed by reference)\n    rows = list(\n        self.__make_row_to_insert(row, field_list, ignore_extra_fields)\n        for row in rows\n    )\n    if rows:\n        try:\n            query = \"{command} INTO {destination}(`{fields}`) VALUES {placeholders}{duplicate}\".format(\n                command=\"REPLACE\" if replace else \"INSERT\",\n                destination=self.from_clause(),\n                fields=\"`,`\".join(field_list),\n                placeholders=\",\".join(\n                    \"(\" + \",\".join(row[\"placeholders\"]) + \")\" for row in rows\n                ),\n                duplicate=(\n                    \" ON DUPLICATE KEY UPDATE `{pk}`=`{pk}`\".format(\n                        pk=self.primary_key[0]\n                    )\n                    if skip_duplicates\n                    else \"\"\n                ),\n            )\n            self.connection.query(\n                query,\n                args=list(\n                    itertools.chain.from_iterable(\n                        (v for v in r[\"values\"] if v is not None) for r in rows\n                    )\n                ),\n            )\n        except UnknownAttributeError as err:\n            raise err.suggest(\n                \"To ignore extra fields in insert, set ignore_extra_fields=True\"\n            )\n        except DuplicateError as err:\n            raise err.suggest(\n                \"To ignore duplicate entries in insert, set skip_duplicates=True\"\n            )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.delete_quick", "title": "<code>delete_quick(get_count=False)</code>", "text": "<p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete_quick(self, get_count=False):\n\"\"\"\n    Deletes the table without cascading and without user prompt.\n    If this table has populated dependent tables, this will fail.\n    \"\"\"\n    query = \"DELETE FROM \" + self.full_table_name + self.where_clause()\n    self.connection.query(query)\n    count = (\n        self.connection.query(\"SELECT ROW_COUNT()\").fetchone()[0]\n        if get_count\n        else None\n    )\n    self._log(query[:255])\n    return count\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.delete", "title": "<code>delete(transaction=True, safemode=None, force_parts=False)</code>", "text": "<p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     safemode: If <code>True</code>, prohibit nested transactions and prompt to confirm. Default         is <code>dj.config['safemode']</code>.     force_parts: Delete from parts even when not deleting from their masters.</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master.</p> Source code in <code>datajoint/table.py</code> <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    safemode: Union[bool, None] = None,\n    force_parts: bool = False,\n) -&gt; int:\n\"\"\"\n    Deletes the contents of the table and its dependent tables, recursively.\n\n    Args:\n        transaction: If `True`, use of the entire delete becomes an atomic transaction.\n            This is the default and recommended behavior. Set to `False` if this delete is\n            nested within another transaction.\n        safemode: If `True`, prohibit nested transactions and prompt to confirm. Default\n            is `dj.config['safemode']`.\n        force_parts: Delete from parts even when not deleting from their masters.\n\n    Returns:\n        Number of deleted rows (excluding those from dependent tables).\n\n    Raises:\n        DataJointError: Delete exceeds maximum number of delete attempts.\n        DataJointError: When deleting within an existing transaction.\n        DataJointError: Deleting a part table before its master.\n    \"\"\"\n    deleted = set()\n\n    def cascade(table):\n\"\"\"service function to perform cascading deletes recursively.\"\"\"\n        max_attempts = 50\n        for _ in range(max_attempts):\n            try:\n                delete_count = table.delete_quick(get_count=True)\n            except IntegrityError as error:\n                match = foreign_key_error_regexp.match(error.args[0]).groupdict()\n                if \"`.`\" not in match[\"child\"]:  # if schema name missing, use table\n                    match[\"child\"] = \"{}.{}\".format(\n                        table.full_table_name.split(\".\")[0], match[\"child\"]\n                    )\n                if (\n                    match[\"pk_attrs\"] is not None\n                ):  # fully matched, adjusting the keys\n                    match[\"fk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"fk_attrs\"].split(\",\")\n                    ]\n                    match[\"pk_attrs\"] = [\n                        k.strip(\"`\") for k in match[\"pk_attrs\"].split(\",\")\n                    ]\n                else:  # only partially matched, querying with constraint to determine keys\n                    match[\"fk_attrs\"], match[\"parent\"], match[\"pk_attrs\"] = list(\n                        map(\n                            list,\n                            zip(\n                                *table.connection.query(\n                                    constraint_info_query,\n                                    args=(\n                                        match[\"name\"].strip(\"`\"),\n                                        *[\n                                            _.strip(\"`\")\n                                            for _ in match[\"child\"].split(\"`.`\")\n                                        ],\n                                    ),\n                                ).fetchall()\n                            ),\n                        )\n                    )\n                    match[\"parent\"] = match[\"parent\"][0]\n\n                # Restrict child by table if\n                #   1. if table's restriction attributes are not in child's primary key\n                #   2. if child renames any attributes\n                # Otherwise restrict child by table's restriction.\n                child = FreeTable(table.connection, match[\"child\"])\n                if (\n                    set(table.restriction_attributes) &lt;= set(child.primary_key)\n                    and match[\"fk_attrs\"] == match[\"pk_attrs\"]\n                ):\n                    child._restriction = table._restriction\n                elif match[\"fk_attrs\"] != match[\"pk_attrs\"]:\n                    child &amp;= table.proj(\n                        **dict(zip(match[\"fk_attrs\"], match[\"pk_attrs\"]))\n                    )\n                else:\n                    child &amp;= table.proj()\n                cascade(child)\n            else:\n                deleted.add(table.full_table_name)\n                logger.info(\n                    \"Deleting {count} rows from {table}\".format(\n                        count=delete_count, table=table.full_table_name\n                    )\n                )\n                break\n        else:\n            raise DataJointError(\"Exceeded maximum number of delete attempts.\")\n        return delete_count\n\n    safemode = config[\"safemode\"] if safemode is None else safemode\n\n    # Start transaction\n    if transaction:\n        if not self.connection.in_transaction:\n            self.connection.start_transaction()\n        else:\n            if not safemode:\n                transaction = False\n            else:\n                raise DataJointError(\n                    \"Delete cannot use a transaction within an ongoing transaction. \"\n                    \"Set transaction=False or safemode=False).\"\n                )\n\n    # Cascading delete\n    try:\n        delete_count = cascade(self)\n    except:\n        if transaction:\n            self.connection.cancel_transaction()\n        raise\n\n    if not force_parts:\n        # Avoid deleting from child before master (See issue #151)\n        for part in deleted:\n            master = get_master(part)\n            if master and master not in deleted:\n                if transaction:\n                    self.connection.cancel_transaction()\n                raise DataJointError(\n                    \"Attempt to delete part table {part} before deleting from \"\n                    \"its master {master} first.\".format(part=part, master=master)\n                )\n\n    # Confirm and commit\n    if delete_count == 0:\n        if safemode:\n            logger.warn(\"Nothing to delete.\")\n        if transaction:\n            self.connection.cancel_transaction()\n    else:\n        if not safemode or user_choice(\"Commit deletes?\", default=\"no\") == \"yes\":\n            if transaction:\n                self.connection.commit_transaction()\n            if safemode:\n                logger.info(\"Deletes committed.\")\n        else:\n            if transaction:\n                self.connection.cancel_transaction()\n            if safemode:\n                logger.warn(\"Deletes cancelled\")\n    return delete_count\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.drop_quick", "title": "<code>drop_quick()</code>", "text": "<p>Drops the table without cascading to dependent tables and without user prompt.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop_quick(self):\n\"\"\"\n    Drops the table without cascading to dependent tables and without user prompt.\n    \"\"\"\n    if self.is_declared:\n        query = \"DROP TABLE %s\" % self.full_table_name\n        self.connection.query(query)\n        logger.info(\"Dropped table %s\" % self.full_table_name)\n        self._log(query[:255])\n    else:\n        logger.info(\n            \"Nothing to drop: table %s is not declared\" % self.full_table_name\n        )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.drop", "title": "<code>drop()</code>", "text": "<p>Drop the table and all tables that reference it, recursively. User is prompted for confirmation if config['safemode'] is set to True.</p> Source code in <code>datajoint/table.py</code> <pre><code>def drop(self):\n\"\"\"\n    Drop the table and all tables that reference it, recursively.\n    User is prompted for confirmation if config['safemode'] is set to True.\n    \"\"\"\n    if self.restriction:\n        raise DataJointError(\n            \"A table with an applied restriction cannot be dropped.\"\n            \" Call drop() on the unrestricted Table.\"\n        )\n    self.connection.dependencies.load()\n    do_drop = True\n    tables = [\n        table\n        for table in self.connection.dependencies.descendants(self.full_table_name)\n        if not table.isdigit()\n    ]\n\n    # avoid dropping part tables without their masters: See issue #374\n    for part in tables:\n        master = get_master(part)\n        if master and master not in tables:\n            raise DataJointError(\n                \"Attempt to drop part table {part} before dropping \"\n                \"its master. Drop {master} first.\".format(part=part, master=master)\n            )\n\n    if config[\"safemode\"]:\n        for table in tables:\n            logger.info(\n                table + \" (%d tuples)\" % len(FreeTable(self.connection, table))\n            )\n        do_drop = user_choice(\"Proceed?\", default=\"no\") == \"yes\"\n    if do_drop:\n        for table in reversed(tables):\n            FreeTable(self.connection, table).drop_quick()\n        logger.info(\"Tables dropped. Restart kernel.\")\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.size_on_disk", "title": "<code>size_on_disk</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>size of data and indices in bytes on the storage device</p>"}, {"location": "api/datajoint/user_tables/#datajoint.table.Table.describe", "title": "<code>describe(context=None, printout=False)</code>", "text": "<p>Returns:</p> Type Description <p>the definition string for the query using DataJoint DDL.</p> Source code in <code>datajoint/table.py</code> <pre><code>def describe(self, context=None, printout=False):\n\"\"\"\n    :return:  the definition string for the query using DataJoint DDL.\n    \"\"\"\n    if context is None:\n        frame = inspect.currentframe().f_back\n        context = dict(frame.f_globals, **frame.f_locals)\n        del frame\n    if self.full_table_name not in self.connection.dependencies:\n        self.connection.dependencies.load()\n    parents = self.parents(foreign_key_info=True)\n    in_key = True\n    definition = (\n        \"# \" + self.heading.table_status[\"comment\"] + \"\\n\"\n        if self.heading.table_status[\"comment\"]\n        else \"\"\n    )\n    attributes_thus_far = set()\n    attributes_declared = set()\n    indexes = self.heading.indexes.copy()\n    for attr in self.heading.attributes.values():\n        if in_key and not attr.in_key:\n            definition += \"---\\n\"\n            in_key = False\n        attributes_thus_far.add(attr.name)\n        do_include = True\n        for parent_name, fk_props in parents:\n            if attr.name in fk_props[\"attr_map\"]:\n                do_include = False\n                if attributes_thus_far.issuperset(fk_props[\"attr_map\"]):\n                    # foreign key properties\n                    try:\n                        index_props = indexes.pop(tuple(fk_props[\"attr_map\"]))\n                    except KeyError:\n                        index_props = \"\"\n                    else:\n                        index_props = [k for k, v in index_props.items() if v]\n                        index_props = (\n                            \" [{}]\".format(\", \".join(index_props))\n                            if index_props\n                            else \"\"\n                        )\n\n                    if not fk_props[\"aliased\"]:\n                        # simple foreign key\n                        definition += \"-&gt;{props} {class_name}\\n\".format(\n                            props=index_props,\n                            class_name=lookup_class_name(parent_name, context)\n                            or parent_name,\n                        )\n                    else:\n                        # projected foreign key\n                        definition += (\n                            \"-&gt;{props} {class_name}.proj({proj_list})\\n\".format(\n                                props=index_props,\n                                class_name=lookup_class_name(parent_name, context)\n                                or parent_name,\n                                proj_list=\",\".join(\n                                    '{}=\"{}\"'.format(attr, ref)\n                                    for attr, ref in fk_props[\"attr_map\"].items()\n                                    if ref != attr\n                                ),\n                            )\n                        )\n                        attributes_declared.update(fk_props[\"attr_map\"])\n        if do_include:\n            attributes_declared.add(attr.name)\n            definition += \"%-20s : %-28s %s\\n\" % (\n                attr.name\n                if attr.default is None\n                else \"%s=%s\" % (attr.name, attr.default),\n                \"%s%s\"\n                % (attr.type, \" auto_increment\" if attr.autoincrement else \"\"),\n                \"# \" + attr.comment if attr.comment else \"\",\n            )\n    # add remaining indexes\n    for k, v in indexes.items():\n        definition += \"{unique}INDEX ({attrs})\\n\".format(\n            unique=\"UNIQUE \" if v[\"unique\"] else \"\", attrs=\", \".join(k)\n        )\n    if printout:\n        logger.info(\"\\n\" + definition)\n    return definition\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.from_camel_case", "title": "<code>from_camel_case(s)</code>", "text": "<p>Convert names in camel case into underscore (_) separated names</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in CamelCase notation</p> required <p>Returns:</p> Type Description <p>string in under_score notation Example: &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def from_camel_case(s):\n\"\"\"\n    Convert names in camel case into underscore (_) separated names\n\n    :param s: string in CamelCase notation\n    :returns: string in under_score notation\n    Example:\n    &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"\n    \"\"\"\n\n    def convert(match):\n        return (\"_\" if match.groups()[0] else \"\") + match.group(0).lower()\n\n    if not re.match(r\"[A-Z][a-zA-Z0-9]*\", s):\n        raise DataJointError(\n            \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\"\n        )\n    return re.sub(r\"(\\B[A-Z])|(\\b[A-Z])\", convert, s)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable", "title": "<code>UserTable</code>", "text": "<p>         Bases: <code>Table</code></p> <p>A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema().</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class UserTable(Table, metaclass=TableMeta):\n\"\"\"\n    A subclass of UserTable is a dedicated class interfacing a base table.\n    UserTable is initialized by the decorator generated by schema().\n    \"\"\"\n\n    # set by @schema\n    _connection = None\n    _heading = None\n    _support = None\n\n    # set by subclass\n    tier_regexp = None\n    _prefix = None\n\n    @property\n    def definition(self):\n\"\"\"\n        :return: a string containing the table definition using the DataJoint DDL.\n        \"\"\"\n        raise NotImplementedError(\n            'Subclasses of Table must implement the property \"definition\"'\n        )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def table_name(cls):\n\"\"\"\n        :return: the table name of the table formatted for mysql.\n        \"\"\"\n        if cls._prefix is None:\n            raise AttributeError(\"Class prefix is not defined!\")\n        return cls._prefix + from_camel_case(cls.__name__)\n\n    @ClassProperty\n    def full_table_name(cls):\n        if cls not in {Manual, Imported, Lookup, Computed, Part, UserTable}:\n            # for derived classes only\n            if cls.database is None:\n                raise DataJointError(\n                    \"Class %s is not properly declared (schema decorator not applied?)\"\n                    % cls.__name__\n                )\n            return r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.definition", "title": "<code>definition</code>  <code>property</code>", "text": "<p>Returns:</p> Type Description <p>a string containing the table definition using the DataJoint DDL.</p>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.UserTable.table_name", "title": "<code>table_name()</code>", "text": "<p>Returns:</p> Type Description <p>the table name of the table formatted for mysql.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>@ClassProperty\ndef table_name(cls):\n\"\"\"\n    :return: the table name of the table formatted for mysql.\n    \"\"\"\n    if cls._prefix is None:\n        raise AttributeError(\"Class prefix is not defined!\")\n    return cls._prefix + from_camel_case(cls.__name__)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Manual", "title": "<code>Manual</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Manual(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are entered manually.\n    \"\"\"\n\n    _prefix = r\"\"\n    tier_regexp = r\"(?P&lt;manual&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Lookup", "title": "<code>Lookup</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Lookup(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are for lookup. This is\n    currently equivalent to defining the table as Manual and serves semantic\n    purposes only.\n    \"\"\"\n\n    _prefix = \"#\"\n    tier_regexp = (\n        r\"(?P&lt;lookup&gt;\" + _prefix + _base_regexp.replace(\"TIER\", \"lookup\") + \")\"\n    )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Imported", "title": "<code>Imported</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Imported(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are imported from external data sources.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"_\"\n    tier_regexp = r\"(?P&lt;imported&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Computed", "title": "<code>Computed</code>", "text": "<p>         Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Computed(UserTable, AutoPopulate):\n\"\"\"\n    Inherit from this class if the table's values are computed from other tables in the schema.\n    The inherited class must at least provide the function `_make_tuples`.\n    \"\"\"\n\n    _prefix = \"__\"\n    tier_regexp = r\"(?P&lt;computed&gt;\" + _prefix + _base_regexp + \")\"\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part", "title": "<code>Part</code>", "text": "<p>         Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>class Part(UserTable):\n\"\"\"\n    Inherit from this class if the table's values are details of an entry in another table\n    and if this table is populated by the other table. For example, the entries inheriting from\n    dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix.\n    Part tables are implemented as classes inside classes.\n    \"\"\"\n\n    _connection = None\n    _master = None\n\n    tier_regexp = (\n        r\"(?P&lt;master&gt;\"\n        + \"|\".join([c.tier_regexp for c in (Manual, Lookup, Imported, Computed)])\n        + r\"){1,1}\"\n        + \"__\"\n        + r\"(?P&lt;part&gt;\"\n        + _base_regexp\n        + \")\"\n    )\n\n    @ClassProperty\n    def connection(cls):\n        return cls._connection\n\n    @ClassProperty\n    def full_table_name(cls):\n        return (\n            None\n            if cls.database is None or cls.table_name is None\n            else r\"`{0:s}`.`{1:s}`\".format(cls.database, cls.table_name)\n        )\n\n    @ClassProperty\n    def master(cls):\n        return cls._master\n\n    @ClassProperty\n    def table_name(cls):\n        return (\n            None\n            if cls.master is None\n            else cls.master.table_name + \"__\" + from_camel_case(cls.__name__)\n        )\n\n    def delete(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().delete(force_parts=True)\n        else:\n            raise DataJointError(\n                \"Cannot delete from a Part directly. Delete from master instead\"\n            )\n\n    def drop(self, force=False):\n\"\"\"\n        unless force is True, prohibits direct deletes from parts.\n        \"\"\"\n        if force:\n            super().drop()\n        else:\n            raise DataJointError(\n                \"Cannot drop a Part directly.  Delete from master instead\"\n            )\n\n    def alter(self, prompt=True, context=None):\n        # without context, use declaration context which maps master keyword to master table\n        super().alter(prompt=prompt, context=context or self.declaration_context)\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.delete", "title": "<code>delete(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def delete(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().delete(force_parts=True)\n    else:\n        raise DataJointError(\n            \"Cannot delete from a Part directly. Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/user_tables/#datajoint.user_tables.Part.drop", "title": "<code>drop(force=False)</code>", "text": "<p>unless force is True, prohibits direct deletes from parts.</p> Source code in <code>datajoint/user_tables.py</code> <pre><code>def drop(self, force=False):\n\"\"\"\n    unless force is True, prohibits direct deletes from parts.\n    \"\"\"\n    if force:\n        super().drop()\n    else:\n        raise DataJointError(\n            \"Cannot drop a Part directly.  Delete from master instead\"\n        )\n</code></pre>"}, {"location": "api/datajoint/utils/", "title": "utils.py", "text": "<p>General-purpose utilities</p>"}, {"location": "api/datajoint/utils/#datajoint.utils.DataJointError", "title": "<code>DataJointError</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p> Source code in <code>datajoint/errors.py</code> <pre><code>class DataJointError(Exception):\n\"\"\"\n    Base class for errors specific to DataJoint internal operation.\n    \"\"\"\n\n    def __init__(self, *args):\n        from .plugin import connection_plugins, type_plugins\n\n        self.__cause__ = (\n            PluginWarning(\"Unverified DataJoint plugin detected.\")\n            if any(\n                [\n                    any([not plugins[k][\"verified\"] for k in plugins])\n                    for plugins in [connection_plugins, type_plugins]\n                    if plugins\n                ]\n            )\n            else None\n        )\n\n    def suggest(self, *args):\n\"\"\"\n        regenerate the exception with additional arguments\n\n        :param args: addition arguments\n        :return: a new exception of the same type with the additional arguments\n        \"\"\"\n        return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.errors.DataJointError.suggest", "title": "<code>suggest(*args)</code>", "text": "<p>regenerate the exception with additional arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <p>addition arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>a new exception of the same type with the additional arguments</p> Source code in <code>datajoint/errors.py</code> <pre><code>def suggest(self, *args):\n\"\"\"\n    regenerate the exception with additional arguments\n\n    :param args: addition arguments\n    :return: a new exception of the same type with the additional arguments\n    \"\"\"\n    return self.__class__(*(self.args + args))\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.user_choice", "title": "<code>user_choice(prompt, choices=('yes', 'no'), default=None)</code>", "text": "<p>Prompts the user for confirmation.  The default value, if any, is capitalized.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>Information to display to the user.</p> required <code>choices</code> <p>an iterable of possible choices.</p> <code>('yes', 'no')</code> <code>default</code> <p>default choice</p> <code>None</code> <p>Returns:</p> Type Description <p>the user's choice</p> Source code in <code>datajoint/utils.py</code> <pre><code>def user_choice(prompt, choices=(\"yes\", \"no\"), default=None):\n\"\"\"\n    Prompts the user for confirmation.  The default value, if any, is capitalized.\n\n    :param prompt: Information to display to the user.\n    :param choices: an iterable of possible choices.\n    :param default: default choice\n    :return: the user's choice\n    \"\"\"\n    assert default is None or default in choices\n    choice_list = \", \".join(\n        (choice.title() if choice == default else choice for choice in choices)\n    )\n    response = None\n    while response not in choices:\n        response = input(prompt + \" [\" + choice_list + \"]: \")\n        response = response.lower() if response else default\n    return response\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.get_master", "title": "<code>get_master(full_table_name)</code>", "text": "<p>If the table name is that of a part table, then return what the master table name would be. This follows DataJoint's table naming convention where a master and a part must be in the same schema and the part table is prefixed with the master table name + <code>__</code>.</p> <p>Example:    <code>ephys</code>.<code>session</code>    -- master    <code>ephys</code>.<code>session__recording</code>  -- part</p> <p>Parameters:</p> Name Type Description Default <code>full_table_name</code> <code>str</code> <p>Full table name including part.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Supposed master full table name or empty string if not a part table name.</p> Source code in <code>datajoint/utils.py</code> <pre><code>def get_master(full_table_name: str) -&gt; str:\n\"\"\"\n    If the table name is that of a part table, then return what the master table name would be.\n    This follows DataJoint's table naming convention where a master and a part must be in the\n    same schema and the part table is prefixed with the master table name + ``__``.\n\n    Example:\n       `ephys`.`session`    -- master\n       `ephys`.`session__recording`  -- part\n\n    :param full_table_name: Full table name including part.\n    :type full_table_name: str\n    :return: Supposed master full table name or empty string if not a part table name.\n    :rtype: str\n    \"\"\"\n    match = re.match(r\"(?P&lt;master&gt;`\\w+`.`\\w+)__(?P&lt;part&gt;\\w+)`\", full_table_name)\n    return match[\"master\"] + \"`\" if match else \"\"\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.to_camel_case", "title": "<code>to_camel_case(s)</code>", "text": "<p>Convert names with under score (_) separation into camel case names.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in under_score notation</p> required <p>Returns:</p> Type Description <p>string in CamelCase notation Example: &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def to_camel_case(s):\n\"\"\"\n    Convert names with under score (_) separation into camel case names.\n\n    :param s: string in under_score notation\n    :returns: string in CamelCase notation\n    Example:\n    &gt;&gt;&gt; to_camel_case(\"table_name\")  # returns \"TableName\"\n    \"\"\"\n\n    def to_upper(match):\n        return match.group(0)[-1].upper()\n\n    return re.sub(r\"(^|[_\\W])+[a-zA-Z]\", to_upper, s)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.from_camel_case", "title": "<code>from_camel_case(s)</code>", "text": "<p>Convert names in camel case into underscore (_) separated names</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <p>string in CamelCase notation</p> required <p>Returns:</p> Type Description <p>string in under_score notation Example: &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"</p> Source code in <code>datajoint/utils.py</code> <pre><code>def from_camel_case(s):\n\"\"\"\n    Convert names in camel case into underscore (_) separated names\n\n    :param s: string in CamelCase notation\n    :returns: string in under_score notation\n    Example:\n    &gt;&gt;&gt; from_camel_case(\"TableName\") # yields \"table_name\"\n    \"\"\"\n\n    def convert(match):\n        return (\"_\" if match.groups()[0] else \"\") + match.group(0).lower()\n\n    if not re.match(r\"[A-Z][a-zA-Z0-9]*\", s):\n        raise DataJointError(\n            \"ClassName must be alphanumeric in CamelCase, begin with a capital letter\"\n        )\n    return re.sub(r\"(\\B[A-Z])|(\\b[A-Z])\", convert, s)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_write", "title": "<code>safe_write(filepath, blob)</code>", "text": "<p>A two-step write.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>full path</p> required <code>blob</code> <p>binary data</p> required Source code in <code>datajoint/utils.py</code> <pre><code>def safe_write(filepath, blob):\n\"\"\"\n    A two-step write.\n\n    :param filename: full path\n    :param blob: binary data\n    \"\"\"\n    filepath = Path(filepath)\n    if not filepath.is_file():\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = filepath.with_suffix(filepath.suffix + \".saving\")\n        temp_file.write_bytes(blob)\n        temp_file.rename(filepath)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.safe_copy", "title": "<code>safe_copy(src, dest, overwrite=False)</code>", "text": "<p>Copy the contents of src file into dest file as a two-step process. Skip if dest exists already</p> Source code in <code>datajoint/utils.py</code> <pre><code>def safe_copy(src, dest, overwrite=False):\n\"\"\"\n    Copy the contents of src file into dest file as a two-step process. Skip if dest exists already\n    \"\"\"\n    src, dest = Path(src), Path(dest)\n    if not (dest.exists() and src.samefile(dest)) and (overwrite or not dest.is_file()):\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        temp_file = dest.with_suffix(dest.suffix + \".copying\")\n        shutil.copyfile(str(src), str(temp_file))\n        temp_file.rename(dest)\n</code></pre>"}, {"location": "api/datajoint/utils/#datajoint.utils.parse_sql", "title": "<code>parse_sql(filepath)</code>", "text": "<p>yield SQL statements from an SQL file</p> Source code in <code>datajoint/utils.py</code> <pre><code>def parse_sql(filepath):\n\"\"\"\n    yield SQL statements from an SQL file\n    \"\"\"\n    delimiter = \";\"\n    statement = []\n    with Path(filepath).open(\"rt\") as f:\n        for line in f:\n            line = line.strip()\n            if not line.startswith(\"--\") and len(line) &gt; 1:\n                if line.startswith(\"delimiter\"):\n                    delimiter = line.split()[1]\n                else:\n                    statement.append(line)\n                    if line.endswith(delimiter):\n                        yield \" \".join(statement)\n                        statement = []\n</code></pre>"}, {"location": "api/datajoint/version/", "title": "version.py", "text": ""}, {"location": "client/creds/", "title": "Credentials", "text": ""}, {"location": "client/creds/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "client/install/", "title": "Install", "text": ""}, {"location": "client/install/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "client/settings/", "title": "Settings", "text": ""}, {"location": "client/settings/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "client/stores/", "title": "File Stores", "text": ""}, {"location": "client/stores/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "compute/distributed/", "title": "Distributed Computing", "text": ""}, {"location": "compute/distributed/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "compute/key-source/", "title": "Key Source", "text": ""}, {"location": "compute/key-source/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "compute/make/", "title": "Make Method", "text": "<p>For auto-populated Imported and Computed tables[^1], a <code>make</code> method gives exact instructions for generating the content. By making these steps explicit, we keep a careful record of data provenance and ensure reproducibility. Data should never be entered using the <code>insert</code> method directly.</p> <p>[^1]: For information on differentiating these data tiers, see the Table Tier section on Automation.</p> <p>The <code>make</code> method receives one argument: the key, which represents the upstream table entries that need populating. The <code>key</code> is a <code>dict</code> in Python. </p> <p>A <code>make</code> function should do three things:</p> <ol> <li> <p>Fetch data from tables upstream in the pipeline using the key for restriction.</p> </li> <li> <p>Compute and add any missing attributes to the fields already in the key.</p> </li> <li> <p>Inserts the entire entity into the triggering table.</p> </li> </ol>"}, {"location": "compute/make/#populate", "title": "Populate", "text": "<p>The <code>make</code> method is sometimes referred to as the <code>populate</code> function because this is the class method called to run the <code>make</code> method on all relevant keys[^2].</p> <p>[^2]: For information on reprocessing keys that resulted in an error, see information on the Jobs table. </p> <pre><code>Segmentation.populate()\n</code></pre>"}, {"location": "compute/populate/", "title": "Populate", "text": ""}, {"location": "compute/populate/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "concepts/glossary/", "title": "Glossary", "text": ""}, {"location": "concepts/glossary/#glossary", "title": "Glossary", "text": "<p>We've taken careful consideration to use consistent terminology. </p> Term Definition DAG directed acyclic graph (DAG) is a set of nodes and connected with a set of directed edges that form no cycles. This means that there is never a path back to a node after passing through it by following the directed edges. Formal workflow management systems represent workflows in the form of DAGs. data pipeline A sequence of data transformation steps from data sources through multiple intermediate structures. More generally, a data pipeline is a directed acyclic graph.  In DataJoint, each step is represented by a table in a relational database. DataJoint a software framework for database programming directly from matlab and python. Thanks to its support of automated computational dependencies, DataJoint serves as a workflow management system. DataJoint Elements software modules implementing portions of experiment workflows designed for ease of integration into diverse custom workflows. DataJoint pipeline the data schemas and transformations underlying a DataJoint workflow. DataJoint allows defining code that specifies both the workflow and the data pipeline, and we have used the words \"pipeline\" and \"workflow\" almost interchangeably. DataJoint schema a software module implementing a portion of an experiment workflow. Includes database table definitions, dependencies, and associated computations. foreign key a field that is linked to another table's primary key. primary key the subset of table attributes that uniquely identify each entity in the table. secondray attribute any field in a table not in the primary key. workflow a formal representation of the steps for executing an experiment from data collection to analysis. Also the software configured for performing these steps. A typical workflow is composed of tables with inter-dependencies and processes to compute and insert data into the tables."}, {"location": "concepts/principles/", "title": "Principles", "text": ""}, {"location": "concepts/principles/#theoretical-foundations", "title": "Theoretical Foundations", "text": "<p>DataJoint Core implements a systematic framework for the joint management of structured scientific data and its associated computations.  The framework builds on the theoretical foundations of the Relational Model and the Entity-Relationship Model, introducing a number of critical clarifications for the effective use of databases as scientific data pipelines.  Notably, DataJoint introduces the concept of computational dependencies as a native first-class citizen of the data model. This integration of data structure and computation into a single model, defines a new class of computational scientific databases.</p> <p>This page defines the key principles of this model without attachment to a specific implementation while  a more complete description of the model can be found in Yatsenko et al, 2018.</p> <p>DataJoint developers are developing these principles into an  open standard to allow multiple alternative implementations.</p>"}, {"location": "concepts/principles/#data-representation", "title": "Data Representation", "text": ""}, {"location": "concepts/principles/#tables-entity-sets", "title": "Tables = Entity Sets", "text": "<p>DataJoint uses only one data structure in all its operations\u2014the entity set.</p> <ol> <li>All data are represented in the form of entity sets, i.e. an ordered collection of entities. </li> <li>All entities of an entity set belong to the same well-defined entity class and have the same set of named attributes. </li> <li>Attributes in an entity set has a data type (or domain), representing the set of its valid values.</li> <li>Each entity in an entity set provides the attribute values for all of the attributes of its entity class.</li> <li>Each entity set has a primary key, i.e. a subset of attributes that, jointly, uniquely identify any entity in the set.</li> </ol> <p>These formal terms have more common (even if less precise) variants: </p> formal common entity set table attribute column attribute value field <p>A collection of stored tables make up a database. Derived tables are formed through query expressions.</p>"}, {"location": "concepts/principles/#table-definition", "title": "Table Definition", "text": "<p>DataJoint introduces a streamlined syntax for defining a stored table.</p> <p>Each line in the definition defines an attribute with its name, data type, an optional default value, and an optional comment in the format: <pre><code>name [=default] : type [# comment]\n</code></pre></p> <p>Primary attributes come first and are separated from the rest of the attributes with the divider <code>---</code>.</p> <p>For example, the following code defines the entity set for entities of class <code>Employee</code>:</p> <pre><code>employee_id : int\n---\nssn = null : int     # optional social security number\ndate_of_birth : date\ngender : enum('male', 'female', 'other')\nhome_address=\"\" : varchar(1000) \nprimary_phone=\"\" : varchar(12)\n</code></pre>"}, {"location": "concepts/principles/#data-tiers", "title": "Data Tiers", "text": "<p>Stored tables are designated into one of four tiers indicating how their data originates.</p> table tier data origin lookup contents are part of the table definition, defined a priori rather than entered externally. Typical stores general facts, parameters, options, etc. manual contents are populated by external mechanisms such as manual entry through web apps or by data ingest scripts imported contents are populated automatically by pipeline computations accessing data from upstream in the pipeline and from external data sources such as raw data stores. computed contents are populated automatically by pipeline computations accessing data from upstream in the pipeline."}, {"location": "concepts/principles/#object-serialization", "title": "Object Serialization", "text": ""}, {"location": "concepts/principles/#data-normalization", "title": "Data Normalization", "text": "<p>A collection of data is considered normalized when organized into a collection of entity sets,  where each entity set represents a well-defined entity class with all its attributes applicable  to each entity in the set and the same primary key identifying </p> <p>The normalization procedure often includes splitting data from one table into several tables,  one for each proper entity set. </p>"}, {"location": "concepts/principles/#databases-and-schemas", "title": "Databases and Schemas", "text": "<p>Stored tables are named and grouped into namespaces called schemas.  A collection of schemas make up a database.  A database has a globally unique address or name.  A schema has a unique name within its database.  Within a connection to a particular database, a stored table is identified as <code>schema.Table</code>. A schema typically groups tables that are logically related.</p>"}, {"location": "concepts/principles/#dependencies", "title": "Dependencies", "text": "<p>Entity sets can form referential dependencies that express and </p>"}, {"location": "concepts/principles/#diagramming", "title": "Diagramming", "text": ""}, {"location": "concepts/principles/#data-integrity", "title": "Data integrity", "text": ""}, {"location": "concepts/principles/#entity-integrity", "title": "Entity integrity", "text": "<p>Entity integrity is the guarantee made by the data management process of the 1:1 mapping between  real-world entities and their digital representations.  In practice, entity integrity is ensured when it is made clear </p>"}, {"location": "concepts/principles/#referential-integrity", "title": "Referential integrity", "text": ""}, {"location": "concepts/principles/#group-integrity", "title": "Group integrity", "text": ""}, {"location": "concepts/principles/#data-manipulations", "title": "Data manipulations", "text": ""}, {"location": "concepts/principles/#data-queries", "title": "Data queries", "text": ""}, {"location": "concepts/principles/#query-operators", "title": "Query Operators", "text": ""}, {"location": "concepts/principles/#pipeline-computations", "title": "Pipeline computations", "text": ""}, {"location": "design/alter/", "title": "Schema Modification", "text": ""}, {"location": "design/alter/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/attribute-types/", "title": "Datatypes", "text": "<p>Throughout the DataJoint ecosystem, there are several datatypes that are used to define tables with cross-platform support (i.e. Python, MATLAB). It is important to understand these types as they can have implications in the queries you form and the capacity of their storage.</p>"}, {"location": "design/attribute-types/#standard-types", "title": "Standard Types", "text": "<p>These types are largely wrappers around existing types in the current  query backend for data pipelines.</p>"}, {"location": "design/attribute-types/#common-types", "title": "Common Types", "text": "Datatype Description Size Example Range int integer 4 bytes <code>8</code> -231 to 231-1 enum[^1] category 1-2 bytes <code>M</code>, <code>F</code> -231 to 231-1 datetime[^2] date and time in <code>YYYY-MM-DD HH:MM:SS</code> format 5 bytes <code>'2020-01-02 03:04:05'</code> varchar(N) string of length M, up to N M + 1-2 bytes <code>text</code> float[^3] floating point number 4 bytes <code>2.04</code> 3.40E+38 to -1.17E-38, 0, and 1.17E-38 to 3.40E+38 longblob[^4] arbitrary numeric data \u2264 4 GiB"}, {"location": "design/attribute-types/#less-common-types", "title": "Less Common Types", "text": "<p>The following types add more specificity to the options above. Note that any integer type can be unsigned, shifting their range from the listed \u00b12n to from 0 - 2n+1. Float and decimal types can be similarly unsigned</p> Datatype Description Size Example Range tinyint tiny integer 1 byte <code>2</code> -27 to 27-1 smallint small integer 2 bytes <code>21,000</code> -215 to 215-1 mediumint medium integer 3 bytes <code>401,000</code> -223 to 223-1 date date 5 bytes <code>'2020-01-02'</code> time time 5 bytes <code>'03:04:05'</code> datetime[^5] date and time 5 bytes <code>'2020-01-02 03:04:05'</code> char(N) string of exactly length N N bytes <code>text</code> double double-precision floating point number 8 bytes decimal(N,F) a fixed-point number with N total and F fractional digits 4 bytes per 9 digits tinyblob[^4] arbitrary numeric data \u2272 256 bytes blob[^4] arbitrary numeric data \u2264 64 KiB mediumblob[^4] arbitrary numeric data \u2264 16 MiB"}, {"location": "design/attribute-types/#unique-types", "title": "Unique Types", "text": "Datatype Description Size Example uuid a unique GUID value 16 bytes <code>6ed5ed09-e69c-466f-8d06-a5afbf273e61</code> attach file attachment filepath path to external file"}, {"location": "design/attribute-types/#unsupported-datatypes-for-now", "title": "Unsupported Datatypes (for now)", "text": "<ul> <li>binary</li> <li>text</li> <li>longtext</li> <li>bit</li> </ul> <p>For more information about datatypes, see  additional documentation</p> <p>[^1]: enum datatypes can be useful to standardize spelling with limited categories, but use with caution. enum should not be included in primary keys, as specified values cannot be changed later.</p> <p>[^2]: The default datetime value may be set to <code>CURRENT_TIMESTAMP</code>. </p> <p>[^3]: Because equality comparisons are error-prone, neither float nor double should be used in primary keys. For these cases, consider decimal.</p> <p>[^4]: Numeric arrays (e.g. matrix, image, structure) are compatible between MATLAB and Python(NumPy). The longblob and other blob datatypes can be configured to store data externally by using the <code>blob@store</code> syntax. For more information on storage limits see this article</p> <p>[^5]: Unlike datetime, a timestamp value will be adjusted to the local time zone.</p>"}, {"location": "design/diagrams/", "title": "Diagrams", "text": "<p>Diagrams are a great way to visualize all or part of a pipeline and understand the flow of data. DataJoint diagrams are based on entity relationship diagram (ERD), with some minor departures fom this standard. </p> <p>Here, tables are depicted as nodes and dependencies as directed edges between them. The <code>draw</code> method plots the graph, with many other methods (Python) to save or adjust the output.</p> <p>Because DataJoint pipelines are directional (see DAG), the tables at the top will need to be populated first, followed by those tables one step below and so forth until the last table is populated at the bottom of the pipeline. The top of the pipeline tends to be dominated by Lookup and manual tables. The middle has many imported tables, and the bottom has computed tables.</p>"}, {"location": "design/diagrams/#notation", "title": "Notation", "text": "<p>DataJoint uses the following conventions:</p> <ul> <li>Tables are indicated as nodes in the graph. The     corresponding class name is indicated by each node.</li> </ul> <ul> <li>Table type is indicated by colors and symbols: <p>- Lookup: gray, rectangle or asterisk</p> <p>- Manual: green, rectangle or square</p> <p>- Imported: blue, circle or oval</p> <p>- Computed: red, rectangle or star</p> <p>- Part: black dot with smaller font or black text</p> </li> </ul> <ul> <li>Dependencies indicated as edges in the graph and always     directed downward (see DAG)</li> </ul> <ul> <li>Dependency type is indicated by the line.<p>- Solid lines: The foreign key in the     primary key.</p> <p>- Dashed lines: The foreign key outside the     primary key. </p> <p>- Thick line: The foreign key the only item in     the primary key. This is a 1-to-1 relationship.</p> <p>- Dot on the line: The foreign key was renamed     via the projection</p> </li> </ul>"}, {"location": "design/diagrams/#example", "title": "Example", "text": "<p>The following diagram example is an approximation of a DataJoint diagram using Mermaid.</p> <p>--8&lt;-- \"src/images/concepts-table-tiers-diagram.md\"</p> <p>Here, we see ...</p> <ol> <li> <p>A 1-to-1 relationship between Session and Scan, as designated by the thick edge.</p> </li> <li> <p>A non-primary foreign key linking SegmentationMethod and Segmentation</p> </li> <li> <p>Manual tables for Mouse, Session, Scan, and Stimulus.</p> </li> <li> <p>A Lookup table: SegmentationMethod</p> </li> <li> <p>An Imported table: Alignment</p> </li> <li> <p>Several Computed tables: Segmentation, Trace, and RF</p> </li> <li> <p>A part table: Field</p> </li> </ol>"}, {"location": "design/drop/", "title": "Schema Drop", "text": ""}, {"location": "design/drop/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/integrity/", "title": "Data Integrity", "text": ""}, {"location": "design/integrity/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/normalization/", "title": "Entity Normalization", "text": ""}, {"location": "design/normalization/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/recall/", "title": "Schema Recall", "text": ""}, {"location": "design/recall/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/schema/", "title": "Schema Creation", "text": ""}, {"location": "design/schema/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/attach/", "title": "Attachments", "text": ""}, {"location": "design/tables/attach/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/attributes/", "title": "Attributes", "text": ""}, {"location": "design/tables/attributes/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/blobs/", "title": "Blobs", "text": ""}, {"location": "design/tables/blobs/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/customtype/", "title": "Custom Datatypes", "text": ""}, {"location": "design/tables/customtype/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/declare/", "title": "Declaration Syntax", "text": ""}, {"location": "design/tables/declare/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/dependencies/", "title": "Dependencies", "text": ""}, {"location": "design/tables/dependencies/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/filepath/", "title": "Filepaths", "text": ""}, {"location": "design/tables/filepath/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/indexes/", "title": "Indexes", "text": ""}, {"location": "design/tables/indexes/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/lookup/", "title": "Lookup Tables", "text": ""}, {"location": "design/tables/lookup/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/master-part/", "title": "Master-Part Relationships", "text": ""}, {"location": "design/tables/master-part/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/primary/", "title": "Primary Key", "text": ""}, {"location": "design/tables/primary/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "design/tables/tiers/", "title": "Table Tiers", "text": "<p>The key to reproducibility in DataJoint is clear data provenance. In any experiment, there are stages for data entry, ingestion, and processing or analysis. DataJoint helps make these stages explicit with data tiers, indicating data origin.</p> Table Type Description Example Lookup Small reference tables containing general information or settings. Analysis parameter set. Manual Data entered entered with by hand or with external helper scripts. Manual subject metadata entry. Imported Data ingested automatically from outside files. Loading a raw data file. Computed Data computed automatically entirely inside the pipeline. Running analyses and storing results. Part* Data in a many-to-one relationship with the corresponding master table. Independent unit results from a given analysis. *Part tables <p>While all other types correspond to their data tier, Part tables inherit the tier of their master table.</p> <p>Lookup and Manual tables generally handle manually added data. Imported and Computed tables both allow for automation, but differ in the source of information. And Part tables have a unique relationship to their corresponding Master table.</p>"}, {"location": "design/tables/tiers/#data-entry-lookup-and-manual", "title": "Data Entry: Lookup and Manual", "text": "<p>Manual tables are populated during experiments through a variety of interfaces. Not all manual information is entered by typing. Automated software can enter it directly into the database. What makes a manual table manual is that it does not perform any computations within the DataJoint pipeline. </p> <p>Lookup tables contain basic facts that are not specific to an experiment and are fairly persistent. In GUIs, lookup tables are often used for drop-down menus or radio buttons. In Computed tables, the contents of Lookup tables are often used to specify alternative methods for computations. Unlike Manual tables, Lookup tables can specify contents in the schema definition.</p> <p>Lookup tables are especially useful for entities with many unique features. Rather than adding many primary keys, this information can be retrieved through an index. For an example, see ClusteringParamSet in Element Array Ephys.</p> <p>While this distinction is useful for structuring a pipeline, it is not enforced, and left to the best judgement of the researcher.</p>"}, {"location": "design/tables/tiers/#automation-imported-and-computed", "title": "Automation: Imported and Computed", "text": "<p>Auto-populated tables are used to define, execute, and coordinate computations in a DataJoint pipeline. These tables belong to one of the two auto-populated data tiers: Imported and Computed. The difference is not strictly enforced, but the convention helps researchers understand data provenance at a glance.</p> <p>Imported tables require access to external files, such as raw storage, outside the  database. If a entry were deleted, it could be retrieved from the raw files on disk.  An EphysRecording table, for example, would load metadata and raw data from  experimental recordings. </p> <p>Computed tables only require to other data within the pipeline. If an entry were  deleted, it could could be recovered by simply running the relevant command. For   analysis, many pipelines feature a task table that pairs sets of primary keys ready  for computation. The  PoseEstimationTask  in Element DeepLabCut pairs videos and models. The   PoseEstimation  table executes these computations and stores the results.</p> <p>Data should never be directly inserted into auto-populated tables. Instead, these tables specify a <code>make</code> method. </p>"}, {"location": "design/tables/tiers/#master-part-relationship", "title": "Master-Part Relationship", "text": "<p>An entity in one table might be inseparably associated with a group of entities in another, forming a master-part relationship, with two important features.</p> <ol> <li> <p>Part tables permit a many-to-one relationship with the master. </p> </li> <li> <p>Data entry and deletion should impact all part tables as well as the master. </p> </li> </ol> <p>If you're considering adding a Part table, consider whether or not there could be a reason to modify the part but not the master. If so, Manual and/or Lookup tables are likely more appropriate. Populate and delete commands should always target the master, and never individual parts. This facilitates data integrity by treating the entire process as one transaction. Either (a) all data are inserted/committed or deleted, or (b) the entire transaction is rolled back. This ensures that partial results never appear in the database.</p> <p>As an example, Element Calcium Imaging features a MotionCorrection computed table segmenting an image into masks. The resulting correction is inseparable from the rigid and nonrigid correction parameters that it produces, with MotionCorrection.RigidMotionCorrection and MotionCorrection.NonRigidMotionCorrection  part tables. </p> <p>The master-part relationship cannot be chained or nested. DataJoint does not allow part tables of other part tables. However, it is common to have a master table with multiple part tables that depend on each other. See link above.</p>"}, {"location": "design/tables/tiers/#example", "title": "Example", "text": "<p>--8&lt;-- \"src/images/concepts-table-tiers-diagram.md\"</p> <p>In this example, the experimenter first enters information into the Manual tables, shown in green. They enter information about a mouse, then a session, and then each scan performed, with the stimuli. Next the automated portion of the pipeline takes over, Importing the raw data and performing image alignment, shown in blue. Computed tables are shown in red. Image segmentation identifies cells in the images, and extraction of calcium traces. In grey, the segmentation method is a Lookup table. Finally, the receptive field (RF) computation is performed by relating the imaging signals to the visual stimulus information.</p> <p>For more information on table dependencies and diagrams, see their respective articles:</p> <ul> <li>Dependencies</li> <li>Diagrams</li> </ul>"}, {"location": "getting-started/", "title": "Getting Started", "text": ""}, {"location": "getting-started/#installation", "title": "Installation", "text": "<p>First, please install Python version 3.7 or later. We recommend 3.8.</p> <p>Next, please install DataJoint via one of the following:</p> condapip + pip + pip +  <p>Pre-Requisites</p> <ul> <li>Ensure you have conda installed.</li> </ul> <p>To add the <code>conda-forge</code> channel:</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>To install:</p> <pre><code>conda install -c conda-forge datajoint\n</code></pre> <p>Pre-Requisites</p> <ul> <li>Ensure you have pip installed.</li> <li>Install graphviz pre-requisite for   diagram visualization.</li> </ul> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre> <p>Pre-Requisites</p> <ul> <li>Ensure you have pip installed.</li> <li>Install graphviz pre-requisite for   diagram visualization.</li> </ul> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre> <p>Pre-Requisites</p> <ul> <li>Ensure you have pip installed.</li> <li>Install graphviz pre-requisite for   diagram visualization.</li> </ul> <p>To install:</p> <pre><code>pip install datajoint\n</code></pre>"}, {"location": "getting-started/#connection", "title": "Connection", "text": "<p>Note</p> <p>Although you may connect to any MySQL server of your choice, the DataJoint company offers an online tutorial environment. Simply sign up for a free DataJoint account.  You will be granted privileges to create schemas that are prefixed as <code>{user}_</code>.</p> environment variablesmemoryfile <p>Before using <code>datajoint</code>, set the following environment variables like so:</p> <pre><code>DJ_HOST=tutorial-db.datajoint.io\nDJ_USER={user}\nDJ_PASS={password}\n</code></pre> <p>To set connection settings within Python, perform:</p> <pre><code>import datajoint as dj\n\ndj.config[\"database.host\"] = \"tutorial-db.datajoint.io\"\ndj.config[\"database.user\"] = \"{user}\"\ndj.config[\"database.password\"] = \"{password}\"\n</code></pre> <p>These configuration settings can be saved either locally or system-wide using one  of the following commands: <pre><code>dj.config.save_local()\ndj.config.save_global()\n</code></pre></p> <p>Before using <code>datajoint</code>, create a file named <code>dj_local_conf.json</code> in the current directory like so:</p> <pre><code>{\n\"database.host\": \"tutorial-db.datajoint.io\",\n\"database.user\": \"{user}\",\n\"database.password\": \"{password}\"\n}\n</code></pre> <p>These settings will be loaded whenever a Python instance is launched from this  directory. To configure settings globally, save a similar file as  <code>.datajoint_config.json</code> in your home directory. A local config, if present, will  take precedent over global settings.</p>"}, {"location": "getting-started/#data-pipeline-definition", "title": "Data Pipeline Definition", "text": "<p>Let's definite a simple data pipeline.</p> <pre><code>import datajoint as dj\nschema = dj.Schema(f\"{dj.config['database.user']}_shapes\") # (1)\n\n@schema # (2)\nclass Rectangle(dj.Manual):\n    definition = \"\"\" # (3)\n    shape_id: int\n    ---\n    shape_height: float\n    shape_width: float\n    \"\"\"\n\n\n@schema\nclass Area(dj.Computed):\n    definition = \"\"\"\n    -&gt; Rectangle\n    ---\n    shape_area: float\n    \"\"\"\n    def make(self, key):\n        rectangle = (Rectangle &amp; key).fetch1()\n        Area.insert1(\n            dict(\n                shape_id=rectangle[\"shape_id\"],\n                shape_area=rectangle[\"shape_height\"] * rectangle[\"shape_width\"],\n            )\n        )\n</code></pre> <ol> <li> <p>This statement creates the database schema <code>{username}_shapes</code> on the server.</p> </li> <li> <p>The <code>@schema</code> decorator for DataJoint classes creates the table on the server.</p> </li> <li> <p>The table is defined by the the <code>definition</code> property.</p> </li> </ol> <p>It is a common practice to have a separate Python module for each schema. Therefore, each such module has only one <code>dj.Schema</code> object defined and is usually named <code>schema</code>.</p> <p>The <code>dj.Schema</code> constructor can take a number of optional parameters after the schema name.</p> <ul> <li><code>context</code> - Dictionary for looking up foreign key references.     Defaults to <code>None</code> to use local context.</li> <li><code>connection</code> - Specifies the DataJoint connection object. Defaults     to <code>dj.conn()</code>.</li> <li><code>create_schema</code> - When <code>False</code>, the schema object will not create a     schema on the database and will raise an error if one does not     already exist. Defaults to <code>True</code>.</li> <li><code>create_tables</code> - When <code>False</code>, the schema object will not create     tables on the database and will raise errors when accessing missing     tables. Defaults to <code>True</code>.</li> </ul> <p>The <code>@schema</code> decorator uses the class name and the data tier to check whether an appropriate table exists on the database. If a table does not already exist, the decorator creates one on the database using the definition property. The decorator attaches the information about the table to the class, and then returns the class.</p>"}, {"location": "getting-started/#diagram", "title": "Diagram", "text": ""}, {"location": "getting-started/#display", "title": "Display", "text": "<p>The diagram displays the relationship of the data model in the data pipeline.</p> <p>This can be done for an entire schema:</p> <pre><code>dj.Diagram(schema)\n</code></pre> <p></p> <p>Or for individual or sets of tables: <pre><code>dj.Diagram(schema.Rectangle)\ndj.Diagram(schema.Rectangle) + dj.Diagram(schema.Area)\n</code></pre></p> What if I don't see the diagram? <p>Some Python interfaces may require additional <code>draw</code> method.</p> <pre><code>dj.Diagram(schema).draw()\n</code></pre> <p>Calling the <code>.draw()</code> method is not necessary when working in a Jupyter notebook by entering <code>dj.Diagram(schema)</code> in a notebook cell. The Diagram will automatically render in the notebook by calling its <code>_repr_html_</code> method. A Diagram displayed without <code>.draw()</code> will be rendered as an SVG, and hovering the mouse over a table will reveal a compact version of the output of the <code>.describe()</code> method.</p>"}, {"location": "getting-started/#customize", "title": "Customize", "text": "<p>Adding or substracting a number to a diagram object adds nodes downstream or upstream, respectively, in the pipeline.</p> <pre><code>(dj.Diagram(schema.Rectangle)+1).draw() # (1)\n</code></pre> <ol> <li>Plot all the tables directly downstream from <code>schema.Rectangle</code></li> </ol> <pre><code>(dj.Diagram('my_schema')-1+1).draw() # (1)\n</code></pre> <ol> <li>Plot all tables directly downstream of those directly upstream of this schema.</li> </ol>"}, {"location": "getting-started/#save", "title": "Save", "text": "<p>The diagram can be saved as either <code>png</code> or <code>svg</code>.</p> <pre><code>dj.Diagram(schema).save(filename='my-diagram', format='png')\n</code></pre>"}, {"location": "getting-started/#add-data", "title": "Add data", "text": "<p>Let's add data for a rectangle:</p> <pre><code>Rectangle.insert1(dict(shape_id=1, shape_height=2, shape_width=4))\n</code></pre>"}, {"location": "getting-started/#run-computation", "title": "Run computation", "text": "<p>Let's start the computations on our entity: <code>Area</code>.</p> <pre><code>Area.populate(display_progress=True)\n</code></pre>"}, {"location": "getting-started/#query", "title": "Query", "text": "<p>Let's inspect the results.</p> <pre><code>Area &amp; \"shape_area &gt;= 8\"\n</code></pre> shaped_id shape_area 1 8.0"}, {"location": "internal/transpilation/", "title": "SQL Transpilation", "text": ""}, {"location": "internal/transpilation/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "manipulation/delete/", "title": "Delete", "text": ""}, {"location": "manipulation/delete/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "manipulation/insert/", "title": "Common Commands", "text": ""}, {"location": "manipulation/insert/#insert", "title": "Insert", "text": "<p>Data entry is as easy as providing the appropriate data structure to a permitted table.</p> <p>Given the following table definition, we can insert data as follows.</p> <pre><code>    mouse_id: int            # unique mouse id\n    ---\n    dob: date                # mouse date of birth\n    sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown\n</code></pre> <pre><code>mouse.insert1( (0, '2017-03-01', 'M') ) # Single entry\ndata = [\n    (1, '2016-11-19', 'M'),\n    (2, '2016-11-20', 'U'),\n    (5, '2016-12-25', 'F')\n]\nmouse.insert(data) # Multi-entry\n</code></pre>"}, {"location": "manipulation/insert/#make", "title": "Make", "text": "<p>The <code>make</code> method populates automated tables from inserted data. Read more in the full article here</p>"}, {"location": "manipulation/insert/#fetch", "title": "Fetch", "text": "<p>Data queries in DataJoint comprise two distinct steps:</p> <ol> <li>Construct the <code>query</code> object to represent the required data using     tables and operators.</li> <li>Fetch the data from <code>query</code> into the workspace of the host language.</li> </ol> <p>Note that entities returned by <code>fetch</code> methods are not guaranteed to be sorted in any particular order unless specifically requested. Furthermore, the order is not guaranteed to be the same in any two queries, and the contents of two identical queries may change between two sequential invocations unless they are wrapped in a transaction. Therefore, if you wish to fetch matching pairs of attributes, do so in one <code>fetch</code> call.</p> <pre><code>data = query.fetch()\n</code></pre>"}, {"location": "manipulation/insert/#drop", "title": "Drop", "text": "<p>The <code>drop</code> method completely removes a table from the database, including its definition. It also removes all dependent tables, recursively. DataJoint will first display the tables being dropped and the number of entities in each before prompting the user for confirmation to proceed.</p> <p>The <code>drop</code> method is often used during initial design to allow altered table definitions to take effect.</p> <pre><code># drop the Person table from its schema\nPerson.drop()\n</code></pre>"}, {"location": "manipulation/insert/#diagrams", "title": "Diagrams", "text": "<p>The <code>Diagram</code> command can help you visualize your pipeline, or understand an existing pipeline. </p> <pre><code>import datajoint as dj\nschema = dj.Schema('my_database')\ndj.Diagram(schema).draw()\n</code></pre> <p>For more information about diagrams, see this article.</p>"}, {"location": "manipulation/transactions/", "title": "Transactions", "text": ""}, {"location": "manipulation/transactions/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "manipulation/update/", "title": "Update", "text": ""}, {"location": "manipulation/update/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/aggregation/", "title": "Aggregation", "text": ""}, {"location": "query/aggregation/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/common-commands/", "title": "Common Commands", "text": ""}, {"location": "query/common-commands/#insert", "title": "Insert", "text": "<p>Data entry is as easy as providing the appropriate data structure to a permitted table. Given the following table definition, we can insert data as tuples, dicts, pandas dataframes, or pathlib <code>Path</code> relative paths to local CSV files.</p> <pre><code>    mouse_id: int            # unique mouse id\n    ---\n    dob: date                # mouse date of birth\n    sex: enum('M', 'F', 'U') # sex of mouse - Male, Female, or Unknown\n</code></pre> TupleDictPandasCSV <pre><code>mouse.insert1( (0, '2017-03-01', 'M') ) # Single entry\ndata = [\n    (1, '2016-11-19', 'M'),\n    (2, '2016-11-20', 'U'),\n    (5, '2016-12-25', 'F')\n]\nmouse.insert(data) # Multi-entry\n</code></pre> <pre><code>mouse.insert1( dict(mouse_id=0, dob='2017-03-01', sex='M') ) # Single entry\ndata = [\n    {'mouse_id':1, 'dob':'2016-11-19', 'sex':'M'},\n    {'mouse_id':2, 'dob':'2016-11-20', 'sex':'U'},\n    {'mouse_id':5, 'dob':'2016-12-25', 'sex':'F'}\n]\nmouse.insert(data) # Multi-entry\n</code></pre> <pre><code>import pandas as pd\ndata = pd.DataFrame(\n    [[1, \"2016-11-19\", \"M\"], [2, \"2016-11-20\", \"U\"], [5, \"2016-12-25\", \"F\"]],\n    columns=[\"mouse_id\", \"dob\", \"sex\"],\n)\nmouse.insert(data)\n</code></pre> <p>Given the following CSV in the current working directory as <code>mice.csv</code></p> <pre><code>mouse_id,dob,sex\n1,2016-11-19,M\n2,2016-11-20,U\n5,2016-12-25,F\n</code></pre> <p>We can import as follows:</p> <pre><code>from pathlib import Path\nmouse.insert(Path('./mice.csv'))\n</code></pre>"}, {"location": "query/common-commands/#make", "title": "Make", "text": "<p>See the article on <code>make</code> methods</p>"}, {"location": "query/common-commands/#fetch", "title": "Fetch", "text": ""}, {"location": "query/common-commands/#entire-table", "title": "Entire table", "text": "<p>A <code>fetch</code> command can either retrieve table data as a NumPy recarray or a as a list of <code>dict</code></p> <pre><code>data = query.fetch() # (1)\ndata = query.fetch(as_dict=True) # (2)\n</code></pre> <ol> <li>NumPy recarray</li> <li>List of <code>dict</code>:</li> </ol> For very large tables... <p>In some cases, the amount of data returned by fetch can be quite large; it can be useful to use the <code>size_on_disk</code> attribute to determine if running a bare fetch would be wise. Please note that it is only currently possible to query the size of entire tables stored directly in the database at this time.</p>"}, {"location": "query/common-commands/#separate-variables", "title": "Separate variables", "text": "<pre><code>name, img = query.fetch1('mouse_id', 'dob')  # when query has exactly one entity\nname, img = query.fetch('mouse_id', 'dob')   # [mouse_id, ...] [dob, ...]\n</code></pre>"}, {"location": "query/common-commands/#primary-key-values", "title": "Primary key values", "text": "<pre><code>keydict = tab.fetch1(\"KEY\")  # single key dict when tab has exactly one entity\nkeylist = tab.fetch(\"KEY\")   # list of key dictionaries [{}, ...]\n</code></pre> <p><code>KEY</code> can also used when returning attribute values as separate variables, such that one of the returned variables contains the entire primary keys.</p>"}, {"location": "query/common-commands/#sorting-results", "title": "Sorting results", "text": "<p>To sort the result, use the <code>order_by</code> keyword argument.</p> <pre><code>data = query.fetch(order_by='mouse_id')                # ascending order\ndata = query.fetch(order_by='mouse_id desc')           # descending order\ndata = query.fetch(order_by=('mouse_id', 'dob'))       # by ID first, dob second\ndata = query.fetch(order_by='KEY')                     # sort by the primary key\n</code></pre> <p>The <code>order_by</code> argument can be a string specifying the attribute to sort by. By default the sort is in ascending order. Use <code>'attr desc'</code> to sort in descending order by attribute <code>attr</code>. The value can also be a sequence of strings, in which case, the sort performed on all the attributes jointly in the order specified.</p> <p>The special attribute named <code>'KEY'</code> represents the primary key attributes in order that they appear in the index. Otherwise, this name can be used as any other argument.</p> <p>If an attribute happens to be a SQL reserved word, it needs to be enclosed in backquotes. For example:</p> <pre><code>data = query.fetch(order_by='`select` desc')\n</code></pre> <p>The <code>order_by</code> value is eventually passed to the <code>ORDER BY</code> clause.</p>"}, {"location": "query/common-commands/#limiting-results", "title": "Limiting results", "text": "<p>Similar to sorting, the <code>limit</code> and <code>offset</code> arguments can be used to limit the result to a subset of entities.</p> <pre><code>data = query.fetch(order_by='mouse_id', limit=10, offset=5)\n</code></pre> <p>Note that an <code>offset</code> cannot be used without specifying a <code>limit</code> as well.</p>"}, {"location": "query/common-commands/#usage-with-pandas", "title": "Usage with Pandas", "text": "<p>The <code>pandas</code> library is a popular library for data analysis in Python which can easily be used with DataJoint query results. Since the records returned by <code>fetch()</code> are contained within a <code>numpy.recarray</code>, they can be easily converted to <code>pandas.DataFrame</code> objects by passing them into the <code>pandas.DataFrame</code> constructor. For example:</p> <pre><code>import pandas as pd\nframe = pd.DataFrame(tab.fetch())\n</code></pre> <p>Calling <code>fetch()</code> with the argument <code>format=\"frame\"</code> returns results as <code>pandas.DataFrame</code> objects indexed by the table's primary key attributes.</p> <pre><code>frame = tab.fetch(format=\"frame\")\n</code></pre> <p>Returning results as a <code>DataFrame</code> is not possible when fetching a particular subset of attributes or when <code>as_dict</code> is set to <code>True</code>.</p>"}, {"location": "query/fetch/", "title": "Query Objects", "text": "<p>Data queries retrieve data from the database. A data query is performed with the   help of a query object, which is a symbolic representation of the query that does   not in itself contain any actual data. The simplest query object is an instance of   a table class, representing the contents of an entire table.</p>"}, {"location": "query/fetch/#querying-a-database", "title": "Querying a database", "text": "<p>For example, if given a <code>Session</code> table, you can create a query object to retrieve its entire contents as follows:</p> <pre><code>query  = Session()\n</code></pre> <p>More generally, a query object may be formed as a query expression constructed by applying operators to other query objects.</p> <p>For example, the following query retrieves information about all experiments and scans for mouse 001:</p> <pre><code>query = Session * Scan &amp; 'animal_id = 001'\n</code></pre> <p>Note that for brevity, query operators can be applied directly to class, as <code>Session</code> instead of <code>Session()</code>.</p> <p>Alternatively, we could query all scans with a sample rate over 1000, and preview the contents of the query simply displaying the object. </p> <pre><code>Scan &amp; 'sample_rate &gt; 1000'\n</code></pre> <p>The above command shows the following table:</p> <pre><code>```text\n| id* |    start_time*      | sample_rate | signal |  times | duration |\n|-----|---------------------|-------------|--------|--------|----------| \n|  1  | 2020-01-02 22:15:00 |   1893.00   | =BLOB= | =BLOB= |  1981.29 |\n|  2  | 2020-01-03 00:15:00 |   4800.00   | =BLOB= | =BLOB= |   548.0  |\n|  3  | 2020-01-19 14:03:03 |   4800.00   | =BLOB= | =BLOB= |   336.0  |\n|  4  | 2020-01-19 14:13:03 |   4800.00   | =BLOB= | =BLOB= |  2501.0  |\n|  5  | 2020-01-23 11:05:23 |   4800.00   | =BLOB= | =BLOB= |  1800.0  |\n|  6  | 2020-01-27 14:03:03 |   4800.00   | =BLOB= | =BLOB= |   600.0  |\n|  7  | 2020-01-31 20:15:00 |   4800.00   | =BLOB= | =BLOB= |   600.0  |\n...\n11 tuples\n```\n</code></pre> <p>Note that this preview (a) only lists a few of the entities that will be returned and  (b) does not contain any data for attributes of datatype <code>blob</code>.</p> <p>Once the desired query object is formed, the query can be executed using its [fetch] (./fetch) methods. To fetch means to transfer the data represented by the query object from the database server into the workspace of the host language.</p> <pre><code>query = Scan &amp; 'sample_rate &gt; 1000'\ns = query.fetch()\n</code></pre> <p>Here fetching from the <code>query</code> object produces the NumPy record array <code>s</code> of the queried data.</p>"}, {"location": "query/fetch/#checking-for-entities", "title": "Checking for entities", "text": "<p>The preview of the query object shown above displayed only a few of the entities returned by the query but also displayed the total number of entities that would be returned. It can be useful to know the number of entities returned by a query, or even whether a query will return any entities at all, without having to fetch all the data themselves.</p> <p>The <code>bool</code> function applied to a query object evaluates to <code>True</code> if the query returns any entities and to <code>False</code> if the query result is empty.</p> <p>The <code>len</code> function applied to a query object determines the number of entities returned by the query.</p> <pre><code># number of sessions since the start of 2018.\nn = len(Session &amp; 'session_date &gt;= \"2018-01-01\"')\n</code></pre>"}, {"location": "query/fetch/#normalization-in-queries", "title": "Normalization in queries", "text": "<p>Query objects adhere to entity entity normalization. The result of a query will include the uniquely defining attributes jointly distinguish any two entities from each other. The query operators are designed to keep the result normalized even in complex query expressions.</p>"}, {"location": "query/iteration/", "title": "Iteration", "text": "<p>The DataJoint model primarily handles data as sets, in the form of tables. However, it can sometimes be useful to access or to perform actions such as visualization upon individual entities sequentially. In DataJoint this is accomplished through iteration.</p> <p>In the simple example below, iteration is used to display the names and values of the attributes of each entity in the simple table or table expression.</p> <pre><code>for entity in table:\n    print(entity)\n</code></pre> <p>This example illustrates the function of the iterator: DataJoint iterates through the whole table expression, returning the entire entity during each step. In this case, each entity will be returned as a <code>dict</code> containing all attributes.</p> <p>At the start of the above loop, DataJoint internally fetches only the primary keys of the entities. Since only the primary keys are needed to distinguish between entities, DataJoint can then iterate over the list of primary keys to execute the loop. At each step of the loop, DataJoint uses a single primary key to fetch an entire entity for use in the iteration, such that <code>print(entity)</code> will print all attributes of each entity. By first fetching only the primary keys and then fetching each entity individually, DataJoint saves memory at the cost of network overhead. This can be particularly useful for tables containing large amounts of data in secondary attributes.</p> <p>The memory savings of the above syntax may not be worth the additional network overhead in all cases, such as for tables with little data stored as secondary attributes. In the example below, DataJoint fetches all of the attributes of each entity in a single call and then iterates over the list of entities stored in memory.</p> <pre><code>for entity in table.fetch(as_dict=True):\n    print(entity)\n</code></pre>"}, {"location": "query/join/", "title": "Join", "text": ""}, {"location": "query/join/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/operators/", "title": "Operators", "text": "<p>Data queries make use of operators to derive the desired table. They represent the desired data symbolically, but do not contain any data. Once a query is formed, we can fetch the data into the local workspace. Since the expressions are only symbolic, repeated <code>fetch</code> calls may yield different results as the state of the database is modified.</p> <p>DataJoint implements a complete algebra of operators on tables:</p> operator notation meaning join A * B All matching information from A and B restriction A &amp; cond The subset of entities from A that meet the condition restriction A - cond The subset of entities from A that do not meet the condition proj A.proj(...) Selects and renames attributes from A or computes new attributes aggr A.aggr(B, ...) Same as projection with computations based on matching information in B union A + B All unique entities from both A and B universal set* dj.U() All unique entities from both A and B <p>*While not technically a query operator, it is useful to discuss Universal Set in the  same context.</p> Notes on relational algebra <p>DataJoint's algebra improves upon the classical relational algebra and upon other query languages to simplify and enhance the construction and interpretation of precise and efficient data queries.</p> <ol> <li> <p>Entity integrity: Data are represented and manipulated in the form of tables representing well-formed entity sets. This applies to the inputs and outputs of query operators. The output of a query operator is an entity set with a well-defined entity type, a primary key, unique attribute names, etc.</p> </li> <li> <p>Algebraic closure: All operators operate on entity sets and yield entity sets. Thus query expressions may be used as operands in other expressions or may be assigned to variables to be used in other expressions.</p> </li> <li> <p>Attributes are identified by names: All attributes have explicit names. This includes results of queries. Operators use attribute names to determine how to perform the operation. The order of the attributes is not significant.</p> </li> </ol> <p>These operators are based on the concept of matching entities. Two entities match when they have no shared fields, or when their shared fields contain the same values. Any shared fields should have compatible datatypes to allow equality comparisons. Matching entities can be merged into a single entity without any conflicts of attribute names and values.</p> <p>In order for these operators to be applied to tables, they must also be  join-compatible, which means that:</p> <ol> <li> <p>All fields in both tables must be part of either the  primary key or a foreign key.</p> </li> <li> <p>All common fields must be of a compatible datatype for equality comparisons.</p> </li> </ol> Why join compatibility restrictions? <p>These restrictions are introduced both for performance reasons and for conceptual reasons. For performance, they encourage queries that rely on indexes. For conceptual reasons, they encourage database design in which entities in different tables are related to each other by the use of primary keys and foreign keys.</p>"}, {"location": "query/operators/#join", "title": "Join", "text": "<p>The Join operator <code>A * B</code> combines the matching information in <code>A</code> and <code>B</code>. The result contains all matching combinations of entities from both arguments, including all unique primary keys from both arguments. </p> <p>In the example below, we look at the union of (A) a table pairing sessions with users and (B) a table pairing sessions with scan. </p>  ![Join example](../images/concepts-operators-join1.png){: style=\"height:200px\"}  <p>This has all the primary keys of both tables (a union thereof, shown in bold) as well as all secondary attributes (i.e., user and duration). This also excludes the session for which we don't have a scan.</p> <p>We can also join based on secondary attributes, as shown in the example below.</p>  ![Join example](../images/concepts-operators-join2.png){: style=\"height:200px\"}  Additional join properties <p>When the operands have no common attributes, the result is the cross product -- all combinations of entities. In all cases, however ...</p> <ol> <li>When <code>A</code> and <code>B</code> have the same attributes, the join <code>A * B</code> becomes equivalent to the set intersection <code>A</code> \u2229 <code>B</code>. Hence, DataJoint does not need a separate intersection operator.</li> <li>Commutativity: <code>A * B</code> is equivalent to <code>B * A</code>.</li> <li>Associativity: <code>(A * B) * C</code> is equivalent to <code>A * (B * C)</code>.</li> </ol>"}, {"location": "query/operators/#restriction", "title": "Restriction", "text": "<p>The restriction operator <code>A &amp; cond</code> selects the subset of entities from <code>A</code> that meet the condition <code>cond</code>. The exclusion operator <code>A - cond</code> selects the complement of restriction, i.e. the subset of entities from <code>A</code> that do not meet the condition <code>cond</code>. This means that the restriction and exclusion operators are complementary. The same query could be constructed using either <code>A &amp; cond</code> or <code>A - Not(cond)</code>.</p>  ![Restriction and exclusion.](../../../images/concepts-operators-restriction.png){: style=\"height:200px\"}  <p>The condition <code>cond</code> may be one of the following:</p> Python <ul> <li>another table</li> <li>a mapping, e.g. <code>dict</code></li> <li>an expression in a character string</li> <li>a collection of conditions as a <code>list</code>, <code>tuple</code>, or Pandas <code>DataFrame</code></li> <li>a Boolean expression (<code>True</code> or <code>False</code>)</li> <li>an <code>AndList</code></li> <li>a <code>Not</code> object</li> <li>a query expression</li> </ul> Permissive Operators <p>To circumvent compatibility checks, DataJoint offers permissive operators for  Restriction (<code>^</code>) and Join (<code>@</code>). Use with Caution.</p>"}, {"location": "query/operators/#proj", "title": "Proj", "text": "<p>The <code>proj</code> operator represents projection and is used to select attributes (columns) from a table, to rename them, or to create new calculated attributes.</p> <ol> <li> <p>A simple projection selects a subset of attributes of the original table, which may not include the primary key.</p> </li> <li> <p>A more complex projection renames an attribute in another table. This could be useful when one table should be referenced multiple times in another. A user table, could contain all personnel. A project table references one person for the lead and another the coordinator, both referencing the common personnel pool.</p> </li> <li> <p>Projection can also perform calculations (as available in  MySQL) on a single attribute.</p> </li> </ol>"}, {"location": "query/operators/#aggr", "title": "Aggr", "text": "<p>Aggregation is a special form of <code>proj</code> with the added feature of allowing   aggregation calculations on another table. It has the form <code>table.aggr   (other, ...)</code> where <code>other</code> is another table. Aggregation allows adding calculated   attributes to each entity in <code>table</code> based on aggregation functions over attributes   in the matching entities of <code>other</code>.</p> <p>Aggregation functions include <code>count</code>, <code>sum</code>, <code>min</code>, <code>max</code>, <code>avg</code>, <code>std</code>, <code>variance</code>, and others.</p>"}, {"location": "query/operators/#union", "title": "Union", "text": "<p>The result of the union operator <code>A + B</code> contains all the entities from both operands. </p> <p>Entity normalization requires that <code>A</code> and <code>B</code> are of the same type, with with the same primary key, using homologous attributes. Without secondary attributes, the result is the simple set union. With secondary attributes, they must have the same names and datatypes. The two operands must also be disjoint, without any duplicate primary key values across both inputs. These requirements prevent ambiguity of attribute values and preserve entity identity.</p> Principles of union <ol> <li> <p>As in all operators, the order of the attributes in the operands is not significant.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> must have the same primary key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>Operands <code>A</code> and <code>B</code> may not have any common non-key attributes. Otherwise, an error will be raised.</p> </li> <li> <p>The result <code>A + B</code> will have the same primary key as <code>A</code> and <code>B</code>.</p> </li> <li> <p>The result <code>A + B</code> will have all the non-key attributes from both <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are found in both <code>A</code> and <code>B</code> (based on the primary key), the secondary attributes will be filled from the corresponding entities in <code>A</code> and <code>B</code>.</p> </li> <li> <p>For entities that are only found in either <code>A</code> or <code>B</code>, the other operand's secondary attributes will filled with null values.</p> </li> </ol> <p>For union, order does not matter.</p>  ![Union Example 1](../../../images/concepts-operators-union1.png){: style=\"height:200px\"}   ![Union Example 2](../../../images/concepts-operators-union2.png){: style=\"height:200px\"}  Properties of union <ol> <li>Commutative: <code>A + B</code> is equivalent to <code>B + A</code>.</li> <li>Associative: <code>(A + B) + C</code> is equivalent to <code>A + (B + C)</code>.</li> </ol>"}, {"location": "query/operators/#universal-set", "title": "Universal Set", "text": "<p>All of the above operators are designed to preserve their input type. Some queries may require creating a new entity type not already represented by existing tables. This means that the new type must be defined as part of the query. </p> <p>Universal sets fulfill this role using <code>dj.U</code> notation. They denote the set of all possible entities with given attributes of any possible datatype. Attributes of universal sets are allowed to be matched to any namesake attributes, even those that do not come from the same initial source.</p> <p>Universal sets should be used sparingly when no suitable base tables already exist. In some cases, defining a new base table can make queries clearer and more semantically constrained.</p> <p>The examples below will use the table definitions in table tiers.</p>"}, {"location": "query/operators/#restriction_1", "title": "Restriction", "text": "<p><code>&amp;</code> and <code>-</code> operators permit restriction.</p>"}, {"location": "query/operators/#by-a-mapping", "title": "By a mapping", "text": "<p>For a Session table, that has the attribute <code>session_date</code>, we can restrict to sessions from January 1st, 2022:</p> <pre><code>Session &amp; {'session_date': \"2022-01-01\"}\n</code></pre> <p>If there were any typos (e.g., using <code>sess_date</code> instead of <code>session_date</code>), our query will return all of the entities of <code>Session</code>.</p>"}, {"location": "query/operators/#by-a-string", "title": "By a string", "text": "<p>Conditions may include arithmetic operations, functions, range tests, etc. Restriction of table <code>A</code> by a string containing an attribute not found in table <code>A</code> produces an error.</p> <pre><code>Session &amp; 'user = \"Alice\"' # (1)\nSession &amp; 'session_date &gt;= \"2022-01-01\"' # (2)\n</code></pre> <ol> <li>All the sessions performed by Alice</li> <li>All of the sessions on or after January 1st, 2022</li> </ol>"}, {"location": "query/operators/#by-a-collection", "title": "By a collection", "text": "<p>When <code>cond</code> is a collection of conditions, the conditions are applied by logical disjunction (logical OR). Restricting a table by a collection will return all entities that meet any of the conditions in the collection. </p> <p>For example, if we restrict the <code>Session</code> table by a collection containing two conditions, one for user and one for date, the query will return any sessions with a matching user or date.</p> <p>A collection can be a list, a tuple, or a Pandas <code>DataFrame</code>.</p> <pre><code>cond_list = ['user = \"Alice\"', 'session_date = \"2022-01-01\"'] # (1)\ncond_tuple = ('user = \"Alice\"', 'session_date = \"2022-01-01\"') # (2)\nimport pandas as pd\ncond_frame = pd.DataFrame(data={'user': ['Alice'], 'session_date': ['2022-01-01']}) # (3)\n\nSession() &amp; ['user = \"Alice\"', 'session_date = \"2022-01-01\"']\n</code></pre> <ol> <li>A list</li> <li>A tuple</li> <li>A data frame</li> </ol> <p><code>dj.AndList</code> represents logical conjunction(logical AND). Restricting a table by an <code>AndList</code> will return all entities that meet all of the conditions in the list. <code>A &amp; dj.AndList([c1, c2, c3])</code> is equivalent to <code>A &amp; c1 &amp; c2 &amp; c3</code>.</p> <pre><code>Student() &amp; dj.AndList(['user = \"Alice\"', 'session_date = \"2022-01-01\"'])\n</code></pre> <p>The above will show all the sessions that Alice conducted on the given day.</p>"}, {"location": "query/operators/#by-a-not-object", "title": "By a <code>Not</code> object", "text": "<p>The special function <code>dj.Not</code> represents logical negation, such that <code>A &amp; dj.Not (cond)</code> is equivalent to <code>A - cond</code>.</p>"}, {"location": "query/operators/#by-a-query", "title": "By a query", "text": "<p>Restriction by a query object is a generalization of restriction by a table. The example below creates a query object corresponding to all the users named Alice. The <code>Session</code> table is then restricted by the query object, returning all the sessions performed by Alice.</p> <pre><code>query = User &amp; 'user = \"Alice\"'\nSession &amp; query\n</code></pre>"}, {"location": "query/operators/#proj_1", "title": "Proj", "text": "<p>Renaming an attribute in python can be done via keyword arguments: </p> <pre><code>table.proj(new_attr='old_attr')\n</code></pre> <p>This can be done in the context of a table definition:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    # Experiment Session\n    -&gt; Animal\n    session             : smallint  # session number for the animal\n    ---\n    session_datetime    : datetime  # YYYY-MM-DD HH:MM:SS\n    session_start_time  : float     # seconds relative to session_datetime\n    session_end_time    : float     # seconds relative to session_datetime\n    -&gt; User.proj(experimenter='username')\n    -&gt; User.proj(supervisor='username')\n    \"\"\"\n</code></pre> <p>Or to rename multiple values in a table with the following syntax:  <code>Table.proj(*existing_attributes,*renamed_attributes)</code></p> <pre><code>Session.proj('session','session_date',start='session_start_time',end='session_end_time')\n</code></pre> <p>Projection can also be used to to compute new attributes from existing ones.</p> <pre><code>Session.proj(duration='session_end_time-session_start_time') &amp; 'duration &gt; 10'\n</code></pre>"}, {"location": "query/operators/#aggr_1", "title": "Aggr", "text": "<p>For more complicated calculations, we can use aggregation.</p> <pre><code>Subject.aggr(Session,n=\"count(*)\") # (1)\nSubject.aggr(Session,average_start=\"avg(session_start_time)\") # (2)\n</code></pre> <ol> <li>Number of sessions per subject.</li> <li>Average <code>session_start_time</code> for each subject</li> </ol>"}, {"location": "query/operators/#universal-set_1", "title": "Universal set", "text": "<p>Universal sets offer the complete list of combinations of attributes.</p> <pre><code># All home cities of students\ndj.U('laser_wavelength', 'laser_power') &amp; Scan # (1)\ndj.U('laser_wavelength', 'laser_power').aggr(Scan, n=\"count(*)\") # (2)\ndj.U().aggr(Session, n=\"max(session)\") # (3)\n</code></pre> <ol> <li>All combinations of wavelength and power.</li> <li>Total number of scans for each combination.</li> <li>Largest session number.</li> </ol> <p><code>dj.U()</code>, as shown in the last example above, is often useful for integer IDs. For an example of this process, see the source code for  Element Array Electrophysiology's <code>insert_new_params</code>.</p>"}, {"location": "query/principles/", "title": "Principles", "text": ""}, {"location": "query/principles/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/project/", "title": "Projection", "text": ""}, {"location": "query/project/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/query-caching/", "title": "Query Caching", "text": "<p>Query caching allows avoiding repeated queries to the database by caching the results locally for faster retrieval.</p> <p>To enable queries, set the query cache local path in <code>dj.config</code>, create the directory, and activate the query caching.</p> <pre><code>dj.config['query_cache'] = os.path.expanduser('~/dj_query_cache') # (1)\n# (2)\nconn = dj.conn()                # if queries co-located with tables\nconn = module.schema.connection # if schema co-located with tables\nconn = module.table.connection  # most flexible\n\nconn.set_query_cache(query_cache='main') # (3)\n</code></pre> <ol> <li>Set the query cache path</li> <li>Access the active connection object for the tables</li> <li>Activate query caching for a namespace called 'main'</li> </ol> <p>The <code>query_cache</code> argument is an arbitrary string serving to differentiate cache states; setting a new value will effectively start a new cache, triggering retrieval of new values once.</p> <p>To turn off query caching, use the following:</p> <pre><code>conn.set_query_cache(query_cache=None)\n## OR\nconn.set_query_cache()\n</code></pre> <p>While query caching is enabled, any insert or delete calls and any transactions are disabled and will raise an error. This ensures that stale data are not used for updating the database in violation of data integrity.</p> <p>To clear and remove the query cache, use the following:</p> <pre><code>conn.purge_query_cache() # Purge the cached queries\n</code></pre>"}, {"location": "query/restrict/", "title": "Restrict", "text": ""}, {"location": "query/restrict/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/union/", "title": "Union", "text": ""}, {"location": "query/union/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "query/universals/", "title": "Universal Sets", "text": ""}, {"location": "query/universals/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "reproduce/make-method/", "title": "Make Method", "text": "<p>Consider the following table definition from the article on  table tiers:</p> <pre><code>@schema\nclass FilteredImage(dj.Computed):\n    definition = \"\"\" # Filtered image\n    -&gt; Image\n    ---\n    filtered_image : longblob\n    \"\"\"\n\n    def make(self, key):\n        img = (test.Image &amp; key).fetch1('image')\n        key['filtered_image'] = my_filter(img)\n        self.insert1(key)\n</code></pre> <p>The <code>FilteredImage</code> table can be populated as</p> <pre><code>FilteredImage.populate()\n</code></pre> <p>The <code>make</code> method receives one argument: the dict <code>key</code> containing the primary key value of an element of <code>key source</code> to be worked on. </p>"}, {"location": "reproduce/make-method/#optional-arguments", "title": "Optional Arguments", "text": "<p>The <code>make</code> method also accepts a number of optional arguments that provide more features and allow greater control over the method's behavior.</p> Argument Default Description <code>restrictions</code> A list of restrictions, restricting as <code>(tab.key_source &amp; AndList (restrictions)) - tab.proj()</code>. Here <code>target</code> is the table to be populated, usually <code>tab</code> itself. <code>suppress_errors</code> <code>False</code> If <code>True</code>, encountering an error will cancel the current <code>make</code> call, log the error, and continue to the next <code>make</code> call. Error messages will be logged in the job reservation table (if <code>reserve_jobs</code> is <code>True</code>) and returned as a list. See also <code>return_exception_objects</code> and <code>reserve_jobs</code>. <code>return_exception_objects</code> <code>False</code> If <code>True</code>, error objects are returned instead of error messages. This applies only when <code>suppress_errors</code> is <code>True</code>. <code>reserve_jobs</code> <code>False</code> If <code>True</code>, reserves job to indicate to other distributed processes. The job reservation table may be access as <code>schema.jobs</code>. Errors are logged in the jobs table. <code>order</code> <code>original</code> The order of execution, either <code>\"original\"</code>, <code>\"reverse\"</code>, or <code>\"random\"</code>. <code>limit</code> <code>None</code> If not <code>None</code>, checks at most this number of keys. <code>max_calls</code> <code>None</code> If not <code>None</code>, populates at most this many keys. Defaults to no limit. <code>display_progress</code> <code>False</code> If <code>True</code>, displays a progress bar. <code>processes</code> <code>1</code> Number of processes to use. Set to <code>None</code> to use all cores <code>make_kwargs</code> <code>None</code> Keyword arguments which do not affect the result of computation to be passed down to each <code>make()</code> call. Computation arguments should be specified within the pipeline e.g. using a <code>dj.Lookup</code> table."}, {"location": "reproduce/make-method/#progress", "title": "Progress", "text": "<p>The method <code>table.progress</code> reports how many <code>key_source</code> entries have been populated and how many remain. Two optional parameters allow more advanced use of the method. A parameter of restriction conditions can be provided, specifying which entities to consider. A Boolean parameter <code>display</code> (default is <code>True</code>) allows disabling the output, such that the numbers of remaining and total entities are returned but not printed.</p>"}, {"location": "reproduce/table-tiers/", "title": "Table Tiers", "text": "<p>To define a DataJoint table in Python:</p> <ol> <li>Define a class inheriting from the appropriate DataJoint class:    <code>dj.Lookup</code>, <code>dj.Manual</code>, <code>dj.Imported</code> or <code>dj.Computed</code>.</li> <li>Decorate the class with the schema object (see schema)</li> <li>Define the class property <code>definition</code> to define the table heading.</li> </ol> <p>DataJoint for Python is implemented through the use of classes providing access to the actual tables stored on the database. Since only a single table exists on the database for any class, interactions with all instances of the class are equivalent. As such, most methods can be called on the classes themselves rather than on an object, for convenience. Whether calling a DataJoint method on a class or on an instance, the result will only depend on or apply to the corresponding table. All of the basic functionality of DataJoint is built to operate on the classes themselves, even when called on an instance. For example, calling <code>Person.insert(...)</code> (on the class) and <code>Person.insert(...)</code> (on an instance) both have the identical effect of inserting data into the table on the database server. DataJoint does not prevent a user from working with instances, but the workflow is complete without the need for instantiation. It is up to the user whether to implement additional functionality as class methods or methods called on instances.</p>"}, {"location": "reproduce/table-tiers/#manual-tables", "title": "Manual Tables", "text": "<p>The following code defines two manual tables, <code>Animal</code> and <code>Session</code>:</p> <pre><code>@schema\nclass Animal(dj.Manual):\n    definition = \"\"\"\n    # information about animal\n    animal_id          : int                # animal id assigned by the lab\n    ---\n    -&gt; Species\n    date_of_birth=null : date               # YYYY-MM-DD optional\n    sex=''             : enum('M', 'F', '') # leave empty if unspecified \n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    # Experiment Session\n    -&gt; Animal\n    session             : smallint  # session number for the animal\n    ---\n    session_datetime    : datetime  # YYYY-MM-DD HH:MM:SS\n    session_start_time  : float     # seconds relative to session_datetime\n    session_end_time    : float     # seconds relative to session_datetime\n    -&gt; [nullable] User\n    \"\"\"\n</code></pre> <p>Note that the notation to permit null entries differs for attributes versus foreign key references.</p>"}, {"location": "reproduce/table-tiers/#lookup-tables", "title": "Lookup Tables", "text": "<p>Lookup tables are commonly populated from their <code>contents</code> property. </p> <p>The table below is declared as a lookup table with its contents property provided to generate entities.</p> <pre><code>@schema\nclass User(dj.Lookup):\n    definition = \"\"\"\n    # users in the lab\n    username : varchar(20)   # user in the lab\n    ---\n    first_name  : varchar(20)   # user first name\n    last_name   : varchar(20)   # user last name\n    \"\"\"\n    contents = [\n        ['cajal', 'Santiago', 'Cajal'],\n        ['hubel', 'David', 'Hubel'],\n        ['wiesel', 'Torsten', 'Wiesel']\n]\n\n@schema\nclass ProcessingParamSet(dj.Lookup):\n    definition = \"\"\"  #  Parameter set used for processing of calcium imaging data\n    paramset_idx:  smallint\n    ---\n    -&gt; ProcessingMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash) (1)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n</code></pre> <ol> <li>This syntax enforces uniqueness of a secondary attribute.</li> </ol>"}, {"location": "reproduce/table-tiers/#imported-and-computed-tables", "title": "Imported and Computed Tables", "text": "<p>Imported and Computed tables provide <code>make</code> methods to determine how they are populated, either from files or other tables.</p> <p>Imagine that there is a table <code>test.Image</code> that contains 2D grayscale images in its <code>image</code> attribute. We can define the Computed table, <code>test.FilteredImage</code> that filters the image in some way and saves the result in its <code>filtered_image</code> attribute.</p> <pre><code>@schema\nclass FilteredImage(dj.Computed):\n    definition = \"\"\" # Filtered image\n    -&gt; Image\n    ---\n    filtered_image : longblob\n    \"\"\"\n\n    def make(self, key):\n        img = (test.Image &amp; key).fetch1('image')\n        key['filtered_image'] = my_filter(img)\n        self.insert1(key)\n</code></pre>"}, {"location": "reproduce/table-tiers/#part-tables", "title": "Part Tables", "text": "<p>The following code defines a Imported table with an associated part table. In Python, the master-part relationship is expressed by making the part a nested class of the master. The part is subclassed from <code>dj.Part</code> and does not need the <code>@schema</code> decorator.</p> <pre><code>@schema\nclass Scan(dj.Imported):\n    definition = \"\"\"\n    # Two-photon imaging scan\n    -&gt; Session\n    scan : smallint  # scan number within the session\n    ---\n    -&gt; Lens\n    laser_wavelength : decimal(5,1)  # um\n    laser_power      : decimal(4,1)  # mW\n    \"\"\"\n\n    class ScanField(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        ROI: longblob  # Region of interest\n        \"\"\"\n\n    def make(self, key):\n        ... # (1)\n        self.insert1(key)\n        self.ScanField.insert1(ROI_information)\n</code></pre> <ol> <li>This make method is truncated for the sake of brevity. For more detailed examples, please visit Element Calcium Imaging table definitions</li> </ol>"}, {"location": "sysadmin/dba/", "title": "Database Administration", "text": ""}, {"location": "sysadmin/dba/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "sysadmin/filestore/", "title": "File Storage", "text": ""}, {"location": "sysadmin/filestore/#work-in-progress", "title": "Work in progress", "text": "<p>You may ask questions in the chat window below or refer to legacy documentation</p>"}, {"location": "tutorials/json/", "title": "Using the json type", "text": "<p>\u26a0\ufe0f Note the following before using the <code>json</code> type</p> <ul> <li>Supported only for MySQL &gt;= 8.0 when JSON_VALUE introduced.</li> <li>Equivalent Percona is fully-compatible.</li> <li>MariaDB is not supported since JSON_VALUE does not allow type specification like MySQL's.</li> <li>Not yet supported in DataJoint MATLAB</li> </ul> <p>First you will need to install and connect to a DataJoint data pipeline.</p> <p>Now let's start by importing the <code>datajoint</code> client.</p> In\u00a0[1]: Copied! <pre>import datajoint as dj\n</pre> import datajoint as dj  <p>For this exercise, let's imagine we work for an awesome company that is organizing a fun RC car race across various teams in the company. Let's see which team has the fastest car! \ud83c\udfce\ufe0f</p> <p>This establishes 2 important entities: a <code>Team</code> and a <code>Car</code>. Normally we'd map this to their own dedicated table, however, let's assume that <code>Team</code> is well-structured but <code>Car</code> is less structured then we'd prefer. In other words, the structure for what makes up a car is varing too much between entries (perhaps because users of the pipeline haven't agreed yet on the definition? \ud83e\udd37).</p> <p>This would make it a good use-case to keep <code>Team</code> as a table but make <code>Car</code> actually a <code>json</code> type defined within the <code>Team</code> table.</p> <p>Let's begin.</p> In\u00a0[2]: Copied! <pre>schema = dj.Schema(f\"{dj.config['database.user']}_json\")\n</pre> schema = dj.Schema(f\"{dj.config['database.user']}_json\")  <pre>[2023-02-12 00:14:33,027][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-02-12 00:14:33,039][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[3]: Copied! <pre>@schema\nclass Team(dj.Lookup):\n    definition = \"\"\"\n    # A team within a company\n    name: varchar(40)  # team name\n    ---\n    car=null: json  # A car belonging to a team (null to allow registering first but specifying car later)\n    unique index(car.length:decimal(4, 1))  # Add an index if this key is frequently accessed\n    \"\"\"\n</pre> @schema class Team(dj.Lookup):     definition = \"\"\"     # A team within a company     name: varchar(40)  # team name     ---     car=null: json  # A car belonging to a team (null to allow registering first but specifying car later)          unique index(car.length:decimal(4, 1))  # Add an index if this key is frequently accessed     \"\"\"  <p>Let's suppose that engineering is first up to register their car.</p> In\u00a0[4]: Copied! <pre>Team.insert1(\n    {\n        \"name\": \"engineering\",\n        \"car\": {\n            \"name\": \"Rever\",\n            \"length\": 20.5,\n            \"inspected\": True,\n            \"tire_pressure\": [32, 31, 33, 34],\n            \"headlights\": [\n                {\n                    \"side\": \"left\",\n                    \"hyper_white\": None,\n                },\n                {\n                    \"side\": \"right\",\n                    \"hyper_white\": None,\n                },\n            ],\n        },\n    }\n)\n</pre> Team.insert1(     {         \"name\": \"engineering\",         \"car\": {             \"name\": \"Rever\",             \"length\": 20.5,             \"inspected\": True,             \"tire_pressure\": [32, 31, 33, 34],             \"headlights\": [                 {                     \"side\": \"left\",                     \"hyper_white\": None,                 },                 {                     \"side\": \"right\",                     \"hyper_white\": None,                 },             ],         },     } )  <p>Next, business and marketing teams are up and register their cars.</p> <p>A few points to notice below:</p> <ul> <li>The person signing up on behalf of marketing does not know the specifics of the car during registration but another team member will be updating this soon before the race.</li> <li>Notice how the <code>business</code> and <code>engineering</code> teams appear to specify the same property but refer to it as <code>safety_inspected</code> and <code>inspected</code> respectfully.</li> </ul> In\u00a0[5]: Copied! <pre>Team.insert(\n    [\n        {\n            \"name\": \"marketing\",\n            \"car\": None,\n        },\n        {\n            \"name\": \"business\",\n            \"car\": {\n                \"name\": \"Chaching\",\n                \"length\": 100,\n                \"safety_inspected\": False,\n                \"tire_pressure\": [34, 30, 27, 32],\n                \"headlights\": [\n                    {\n                        \"side\": \"left\",\n                        \"hyper_white\": True,\n                    },\n                    {\n                        \"side\": \"right\",\n                        \"hyper_white\": True,\n                    },\n                ],\n            },\n        },\n    ]\n)\n</pre> Team.insert(     [         {             \"name\": \"marketing\",             \"car\": None,         },         {             \"name\": \"business\",             \"car\": {                 \"name\": \"Chaching\",                 \"length\": 100,                 \"safety_inspected\": False,                 \"tire_pressure\": [34, 30, 27, 32],                 \"headlights\": [                     {                         \"side\": \"left\",                         \"hyper_white\": True,                     },                     {                         \"side\": \"right\",                         \"hyper_white\": True,                     },                 ],             },         },     ] )  <p>We can preview the table data much like normal but notice how the value of <code>car</code> behaves like other BLOB-like attributes.</p> In\u00a0[6]: Copied! <pre>Team()\n</pre> Team() Out[6]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) marketing =BLOB=engineering =BLOB=business =BLOB= <p>Total: 3</p> <p>Now let's see what kinds of queries we can form to demostrate how we can query this pipeline.</p> In\u00a0[7]: Copied! <pre># Which team has a `car` equal to 100 inches long?\nTeam &amp; {'car.length': 100}\n</pre> # Which team has a `car` equal to 100 inches long? Team &amp; {'car.length': 100} Out[7]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) business =BLOB= <p>Total: 1</p> In\u00a0[8]: Copied! <pre># Which team has a `car` less than 50 inches long?\nTeam &amp; \"car-&gt;&gt;'$.length' &lt; 50\"\n</pre> # Which team has a `car` less than 50 inches long? Team &amp; \"car-&gt;&gt;'$.length' &lt; 50\" Out[8]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= <p>Total: 1</p> In\u00a0[9]: Copied! <pre># Any team that has had their car inspected?\nTeam &amp; [{'car.inspected:unsigned': True}, {'car.safety_inspected:unsigned': True}]\n</pre> # Any team that has had their car inspected? Team &amp; [{'car.inspected:unsigned': True}, {'car.safety_inspected:unsigned': True}] Out[9]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB= <p>Total: 1</p> In\u00a0[10]: Copied! <pre># Which teams do not have hyper white lights for their first head light?\nTeam &amp; {\"car.headlights[0].hyper_white\": None}\n</pre> # Which teams do not have hyper white lights for their first head light? Team &amp; {\"car.headlights[0].hyper_white\": None} Out[10]: A team within a company <p>name</p> team name <p>car</p> A car belonging to a team (null to allow registering first but specifying car later) engineering =BLOB=marketing =BLOB= <p>Total: 2</p> <p>Notice that the previous query will satisfy the <code>None</code> check if it experiences any of the following scenarious:</p> <ul> <li>if entire record missing (<code>marketing</code> satisfies this)</li> <li>JSON key is missing</li> <li>JSON value is set to JSON <code>null</code> (<code>engineering</code> satisfies this)</li> </ul> <p>Projections can be quite useful with the <code>json</code> type since we can extract out just what we need. This allows greater query flexibility but more importantly, for us to be able to fetch only what is pertinent.</p> In\u00a0[11]: Copied! <pre># Only interested in the car names and the length but let the type be inferred\nq_untyped = Team.proj(\n    car_name='car.name',\n    car_length=\"car.length\",\n)\nq_untyped\n</pre> # Only interested in the car names and the length but let the type be inferred q_untyped = Team.proj(     car_name='car.name',     car_length=\"car.length\", ) q_untyped Out[11]: <p>name</p> team name <p>car_name</p> calculated attribute <p>car_length</p> calculated attribute business Chaching 100engineering Rever 20.5marketing None None <p>Total: 3</p> In\u00a0[12]: Copied! <pre>q_untyped.fetch(as_dict=True)\n</pre> q_untyped.fetch(as_dict=True) Out[12]: <pre>[{'name': 'business', 'car_name': 'Chaching', 'car_length': '100'},\n {'name': 'engineering', 'car_name': 'Rever', 'car_length': '20.5'},\n {'name': 'marketing', 'car_name': None, 'car_length': None}]</pre> In\u00a0[13]: Copied! <pre># Nevermind, I'll specify the type explicitly\nq_typed = Team.proj(\n    car_name='car.name',\n    car_length=\"car.length:float\",\n)\nq_typed\n</pre> # Nevermind, I'll specify the type explicitly q_typed = Team.proj(     car_name='car.name',     car_length=\"car.length:float\", ) q_typed Out[13]: <p>name</p> team name <p>car_name</p> calculated attribute <p>car_length</p> calculated attribute business Chaching 100.0engineering Rever 20.5marketing None None <p>Total: 3</p> In\u00a0[14]: Copied! <pre>q_typed.fetch(as_dict=True)\n</pre> q_typed.fetch(as_dict=True) Out[14]: <pre>[{'name': 'business', 'car_name': 'Chaching', 'car_length': 100.0},\n {'name': 'engineering', 'car_name': 'Rever', 'car_length': 20.5},\n {'name': 'marketing', 'car_name': None, 'car_length': None}]</pre> <p>Lastly, the <code>.describe()</code> function on the <code>Team</code> table can help us generate the table's definition. This is useful if we are connected directly to the pipeline without the original source.</p> In\u00a0[16]: Copied! <pre>rebuilt_definition = Team.describe()\nprint(rebuilt_definition)\n</pre> rebuilt_definition = Team.describe() print(rebuilt_definition) <pre># A team within a company\nname                 : varchar(40)                  # team name\n---\ncar=null             : json                         # A car belonging to a team (null to allow registering first but specifying car later)\nUNIQUE INDEX ((json_value(`car`, _utf8mb4'$.length' returning decimal(4, 1))))\n\n</pre> <p>Finally, let's clean up what we created in this tutorial.</p> In\u00a0[17]: Copied! <pre>schema.drop()\n</pre> schema.drop()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "tutorials/json/#using-the-json-type", "title": "Using the <code>json</code> type\u00b6", "text": ""}, {"location": "tutorials/json/#table-definition", "title": "Table Definition\u00b6", "text": ""}, {"location": "tutorials/json/#insert", "title": "Insert\u00b6", "text": ""}, {"location": "tutorials/json/#restriction", "title": "Restriction\u00b6", "text": ""}, {"location": "tutorials/json/#projection", "title": "Projection\u00b6", "text": ""}, {"location": "tutorials/json/#describe", "title": "Describe\u00b6", "text": ""}, {"location": "tutorials/json/#cleanup", "title": "Cleanup\u00b6", "text": ""}]}